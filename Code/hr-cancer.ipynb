{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809c5dff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:37:39.238046Z",
     "iopub.status.busy": "2026-02-06T05:37:39.237726Z",
     "iopub.status.idle": "2026-02-06T05:37:39.247670Z",
     "shell.execute_reply": "2026-02-06T05:37:39.246920Z"
    },
    "papermill": {
     "duration": 0.028427,
     "end_time": "2026-02-06T05:37:39.249263",
     "exception": false,
     "start_time": "2026-02-06T05:37:39.220836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initial Memory Check ===\n",
      "Current memory usage: 100.90 MB\n",
      "System memory: 3.8% used (0.74 GB / 31.35 GB)\n",
      "\n",
      "Tip: Run this cell periodically to monitor memory usage\n"
     ]
    }
   ],
   "source": [
    "# Memory monitoring utility\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    mem_mb = mem_info.rss / 1024 / 1024\n",
    "    print(f\"Current memory usage: {mem_mb:.2f} MB\")\n",
    "    \n",
    "    # System memory info\n",
    "    vm = psutil.virtual_memory()\n",
    "    print(f\"System memory: {vm.percent}% used ({vm.used / 1024**3:.2f} GB / {vm.total / 1024**3:.2f} GB)\")\n",
    "    return mem_mb\n",
    "\n",
    "# Check initial memory\n",
    "print(\"=== Initial Memory Check ===\")\n",
    "initial_mem = print_memory_usage()\n",
    "print(\"\\nTip: Run this cell periodically to monitor memory usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee60cb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:37:39.280098Z",
     "iopub.status.busy": "2026-02-06T05:37:39.279154Z",
     "iopub.status.idle": "2026-02-06T05:37:39.285262Z",
     "shell.execute_reply": "2026-02-06T05:37:39.284459Z"
    },
    "papermill": {
     "duration": 0.023291,
     "end_time": "2026-02-06T05:37:39.286924",
     "exception": false,
     "start_time": "2026-02-06T05:37:39.263633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° FAST MODE: Skipping unsupervised learning and traditional ML, going straight to deep learning!\n",
      "  - Skip Unsupervised Learning: True\n",
      "  - Skip XGBoost/LOPO CV: True\n",
      "  - Skip Traditional ML: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION: Skip sections to save time and memory\n",
    "# ============================================================\n",
    "# Set these flags to True to SKIP the corresponding sections\n",
    "# This is useful for debugging or if you only want to run deep learning\n",
    "\n",
    "SKIP_UNSUPERVISED_LEARNING = False    # Skip Leiden clustering, UMAP, etc. (Cell 44+)\n",
    "SKIP_XGBOOST_LOPO_CV = False          # Skip XGBoost and traditional ML LOPO CV (Cell 51+)\n",
    "SKIP_TRADITIONAL_ML = False           # Skip Logistic Regression, Random Forest, etc.\n",
    "SKIP_TO_DEEP_LEARNING = True         # Master switch: Skip everything except data loading and deep learning\n",
    "\n",
    "# If SKIP_TO_DEEP_LEARNING is True, it overrides the other flags\n",
    "if SKIP_TO_DEEP_LEARNING:\n",
    "    SKIP_UNSUPERVISED_LEARNING = True\n",
    "    SKIP_XGBOOST_LOPO_CV = True\n",
    "    SKIP_TRADITIONAL_ML = True\n",
    "    print(\"‚ö° FAST MODE: Skipping unsupervised learning and traditional ML, going straight to deep learning!\")\n",
    "else:\n",
    "    print(\"üî¨ FULL MODE: Running all analysis sections\")\n",
    "    \n",
    "print(f\"  - Skip Unsupervised Learning: {SKIP_UNSUPERVISED_LEARNING}\")\n",
    "print(f\"  - Skip XGBoost/LOPO CV: {SKIP_XGBOOST_LOPO_CV}\")\n",
    "print(f\"  - Skip Traditional ML: {SKIP_TRADITIONAL_ML}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1f5927",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:37:39.316676Z",
     "iopub.status.busy": "2026-02-06T05:37:39.316030Z",
     "iopub.status.idle": "2026-02-06T05:38:12.879246Z",
     "shell.execute_reply": "2026-02-06T05:38:12.878266Z"
    },
    "papermill": {
     "duration": 33.595358,
     "end_time": "2026-02-06T05:38:12.896600",
     "exception": false,
     "start_time": "2026-02-06T05:37:39.301242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m176.2/176.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m284.1/284.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install anndata scanpy scikit-learn umap-learn --quiet\n",
    "%pip install biopython --quiet\n",
    "%pip install scikit-learn --quiet\n",
    "%pip install umap-learn --quiet\n",
    "%pip install hdbscan --quiet\n",
    "%pip install plotly --quiet\n",
    "%pip install xgboost --quiet\n",
    "%pip install tensorflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a3e645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:38:12.929138Z",
     "iopub.status.busy": "2026-02-06T05:38:12.928305Z",
     "iopub.status.idle": "2026-02-06T05:38:15.217437Z",
     "shell.execute_reply": "2026-02-06T05:38:15.216662Z"
    },
    "papermill": {
     "duration": 2.307692,
     "end_time": "2026-02-06T05:38:15.219355",
     "exception": false,
     "start_time": "2026-02-06T05:38:12.911663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure non-interactive Matplotlib backend to avoid font import issues\n",
    "import matplotlib\n",
    "try:\n",
    "    matplotlib.use('Agg')\n",
    "except Exception as e:\n",
    "    print(\"Could not set Agg backend:\", e)\n",
    "\n",
    "# Set memory optimization flags\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'  # Limit parallel threads to save memory\n",
    "\n",
    "# --- Idempotent monkeypatch CountVectorizer.fit_transform to handle empty vocabulary errors ---\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import scipy.sparse as _sps\n",
    "\n",
    "    # Only patch once; store original on the class to avoid double-wrapping\n",
    "    if not hasattr(CountVectorizer, '_orig_fit_transform'):\n",
    "        CountVectorizer._orig_fit_transform = CountVectorizer.fit_transform\n",
    "\n",
    "        def _safe_cv_fit(self, raw_docs, *args, **kwargs):\n",
    "            try:\n",
    "                return CountVectorizer._orig_fit_transform(self, raw_docs, *args, **kwargs)\n",
    "            except ValueError as e:\n",
    "                # Handle sklearn's \"empty vocabulary\" error by returning an all-zero matrix\n",
    "                if 'empty vocabulary' in str(e).lower():\n",
    "                    n = len(raw_docs) if raw_docs is not None else 0\n",
    "                    return _sps.csr_matrix((n, 1))\n",
    "                raise\n",
    "\n",
    "        CountVectorizer.fit_transform = _safe_cv_fit\n",
    "    else:\n",
    "        # Already patched; do nothing\n",
    "        pass\n",
    "except Exception as e:\n",
    "    # If sklearn/scipy are not available at import time, skip patching and log reason\n",
    "    print(\"CountVectorizer monkeypatch skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56eb09c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:38:15.251186Z",
     "iopub.status.busy": "2026-02-06T05:38:15.250729Z",
     "iopub.status.idle": "2026-02-06T05:38:15.351149Z",
     "shell.execute_reply": "2026-02-06T05:38:15.350302Z"
    },
    "papermill": {
     "duration": 0.118277,
     "end_time": "2026-02-06T05:38:15.352755",
     "exception": false,
     "start_time": "2026-02-06T05:38:15.234478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Kaggle: True\n"
     ]
    }
   ],
   "source": [
    "# --- Initial Setup & Imports ---\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install critical dependencies if missing\n",
    "try:\n",
    "    import Bio\n",
    "except ImportError:\n",
    "    print(\"Installing biopython...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"biopython\"])\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# BioPython Imports\n",
    "try:\n",
    "    from Bio.Seq import Seq\n",
    "    from Bio.SeqUtils import ProtParam\n",
    "except ImportError:\n",
    "    # If install just happened, might need re-import logic or kernel restart, \n",
    "    # but usually works in same session after import\n",
    "    pass\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Environment Detection ---\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input') or os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None\n",
    "print(f\"Running on Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # Ensure standard directories exist\n",
    "    os.makedirs('/kaggle/working/Data', exist_ok=True)\n",
    "    os.makedirs('/kaggle/working/Output', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54955c1",
   "metadata": {
    "papermill": {
     "duration": 0.014023,
     "end_time": "2026-02-06T05:38:15.381819",
     "exception": false,
     "start_time": "2026-02-06T05:38:15.367796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Loading and Preparation\n",
    "We analyze a single-cell dataset recently published by Sun et al. (2025) (GEO accession GSE300475). The data originates from the DFCI 16-466 clinical trial (NCT02999477), a randomized phase II study evaluating neoadjuvant nab-paclitaxel in combination with pembrolizumab for high-risk, early-stage HR+/HER2- breast cancer. The specific cohort analyzed consists of longitudinal peripheral blood mononuclear cell (PBMC) samples from patients in the chemotherapy-first arm.\n",
    "\n",
    "Patients were classified into binary response categories based on Residual Cancer Burden (RCB) index assessed at surgery:\n",
    "*   **Responders:** Patients achieving Pathologic Complete Response (pCR, RCB-0) or minimal residual disease (RCB-I).\n",
    "*   **Non-Responders:** Patients with moderate (RCB-II) or extensive (RCB-III) residual disease.\n",
    "\n",
    "The following code handles the downloading and extraction of the raw data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d53ea9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:38:15.412126Z",
     "iopub.status.busy": "2026-02-06T05:38:15.411228Z",
     "iopub.status.idle": "2026-02-06T05:38:15.415755Z",
     "shell.execute_reply": "2026-02-06T05:38:15.414975Z"
    },
    "papermill": {
     "duration": 0.021361,
     "end_time": "2026-02-06T05:38:15.417326",
     "exception": false,
     "start_time": "2026-02-06T05:38:15.395965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "files_to_fetch = [\n",
    "    {\n",
    "        \"name\": \"GSE300475_RAW.tar\",\n",
    "        \"size\": \"565.5 Mb\",\n",
    "        \"download_url\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE300475&format=file\",\n",
    "        \"type\": \"TAR (of CSV, MTX, TSV)\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GSE300475_feature_ref.xlsx\",\n",
    "        \"size\": \"5.4 Kb\",\n",
    "        \"download_url\": \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE300nnn/GSE300475/suppl/GSE300475%5Ffeature%5Fref.xlsx\",\n",
    "        \"type\": \"XLSX\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2132101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:38:15.449071Z",
     "iopub.status.busy": "2026-02-06T05:38:15.448712Z",
     "iopub.status.idle": "2026-02-06T05:38:15.457659Z",
     "shell.execute_reply": "2026-02-06T05:38:15.456868Z"
    },
    "papermill": {
     "duration": 0.026826,
     "end_time": "2026-02-06T05:38:15.459178",
     "exception": false,
     "start_time": "2026-02-06T05:38:15.432352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloads will be saved in: /kaggle/working/Data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set download directory based on environment\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_project_root():\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for candidate in [cwd, *cwd.parents]:\n",
    "        if (candidate / \"README.md\").exists() or (candidate / \"Code\").exists():\n",
    "            return candidate\n",
    "    return cwd\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # On Kaggle, use /kaggle/working which is writable\n",
    "    download_dir = Path(\"/kaggle/working/Data\")\n",
    "else:\n",
    "    # Local (VS Code / Windows): use project-root/Data\n",
    "    project_root = _find_project_root()\n",
    "    download_dir = project_root / \"Data\"\n",
    "\n",
    "download_dir = Path(download_dir)\n",
    "download_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Downloads will be saved in: {download_dir.resolve()}\\n\")\n",
    "\n",
    "def download_file(url, filename, destination_folder):\n",
    "    \"\"\"\n",
    "    Downloads a file from a given URL to a specified destination folder.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(destination_folder, filename)\n",
    "    print(f\"Attempting to download {filename} from {url}...\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        print(f\"Successfully downloaded {filename} to {filepath}\")\n",
    "        return filepath\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec2bd3f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:38:15.491861Z",
     "iopub.status.busy": "2026-02-06T05:38:15.490911Z",
     "iopub.status.idle": "2026-02-06T05:38:21.595161Z",
     "shell.execute_reply": "2026-02-06T05:38:21.594147Z"
    },
    "papermill": {
     "duration": 6.122519,
     "end_time": "2026-02-06T05:38:21.596858",
     "exception": false,
     "start_time": "2026-02-06T05:38:15.474339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download GSE300475_RAW.tar from https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE300475&format=file...\n",
      "Successfully downloaded GSE300475_RAW.tar to /kaggle/working/Data/GSE300475_RAW.tar\n",
      "Extracting GSE300475_RAW.tar...\n",
      "\n",
      "Files contained in GSE300475_RAW.tar:\n",
      " - GSM9061665_S1_barcodes.tsv.gz\n",
      " - GSM9061665_S1_features.tsv.gz\n",
      " - GSM9061665_S1_matrix.mtx.gz\n",
      " - GSM9061666_S2_barcodes.tsv.gz\n",
      " - GSM9061666_S2_features.tsv.gz\n",
      " - GSM9061666_S2_matrix.mtx.gz\n",
      " - GSM9061667_S3_barcodes.tsv.gz\n",
      " - GSM9061667_S3_features.tsv.gz\n",
      " - GSM9061667_S3_matrix.mtx.gz\n",
      " - GSM9061668_S4_barcodes.tsv.gz\n",
      " - GSM9061668_S4_features.tsv.gz\n",
      " - GSM9061668_S4_matrix.mtx.gz\n",
      " - GSM9061669_S5_barcodes.tsv.gz\n",
      " - GSM9061669_S5_features.tsv.gz\n",
      " - GSM9061669_S5_matrix.mtx.gz\n",
      " - GSM9061670_S6_barcodes.tsv.gz\n",
      " - GSM9061670_S6_features.tsv.gz\n",
      " - GSM9061670_S6_matrix.mtx.gz\n",
      " - GSM9061671_S7_barcodes.tsv.gz\n",
      " - GSM9061671_S7_features.tsv.gz\n",
      " - GSM9061671_S7_matrix.mtx.gz\n",
      " - GSM9061672_S8_barcodes.tsv.gz\n",
      " - GSM9061672_S8_features.tsv.gz\n",
      " - GSM9061672_S8_matrix.mtx.gz\n",
      " - GSM9061673_S9_barcodes.tsv.gz\n",
      " - GSM9061673_S9_features.tsv.gz\n",
      " - GSM9061673_S9_matrix.mtx.gz\n",
      " - GSM9061674_S10_barcodes.tsv.gz\n",
      " - GSM9061674_S10_features.tsv.gz\n",
      " - GSM9061674_S10_matrix.mtx.gz\n",
      " - GSM9061675_S11_barcodes.tsv.gz\n",
      " - GSM9061675_S11_features.tsv.gz\n",
      " - GSM9061675_S11_matrix.mtx.gz\n",
      " - GSM9061687_S1_all_contig_annotations.csv.gz\n",
      " - GSM9061688_S2_all_contig_annotations.csv.gz\n",
      " - GSM9061689_S3_all_contig_annotations.csv.gz\n",
      " - GSM9061690_S4_all_contig_annotations.csv.gz\n",
      " - GSM9061691_S5_all_contig_annotations.csv.gz\n",
      " - GSM9061692_S6_all_contig_annotations.csv.gz\n",
      " - GSM9061693_S7_all_contig_annotations.csv.gz\n",
      " - GSM9061694_S9_all_contig_annotations.csv.gz\n",
      " - GSM9061695_S10_all_contig_annotations.csv.gz\n",
      " - GSM9061696_S11_all_contig_annotations.csv.gz\n",
      "\n",
      "Extracted to: /kaggle/working/Data/GSE300475_RAW\n",
      "--------------------------------------------------\n",
      "\n",
      "Attempting to download GSE300475_feature_ref.xlsx from https://ftp.ncbi.nlm.nih.gov/geo/series/GSE300nnn/GSE300475/suppl/GSE300475%5Ffeature%5Fref.xlsx...\n",
      "Successfully downloaded GSE300475_feature_ref.xlsx to /kaggle/working/Data/GSE300475_feature_ref.xlsx\n"
     ]
    }
   ],
   "source": [
    "for file_info in files_to_fetch:\n",
    "    filename = file_info[\"name\"]\n",
    "    url = file_info[\"download_url\"]\n",
    "    file_type = file_info[\"type\"]\n",
    "\n",
    "    downloaded_filepath = download_file(url, filename, download_dir)\n",
    "\n",
    "    # If the file is a TAR archive, extract it and list the contents\n",
    "    if downloaded_filepath and filename.endswith(\".tar\"):\n",
    "        print(f\"Extracting {filename}...\\n\")\n",
    "        try:\n",
    "            with tarfile.open(downloaded_filepath, \"r\") as tar:\n",
    "                # List contents\n",
    "                members = tar.getnames()\n",
    "                print(f\"Files contained in {filename}:\")\n",
    "                for member in members:\n",
    "                    print(f\" - {member}\")\n",
    "\n",
    "                # Extract to a subdirectory within download_dir\n",
    "                extract_path = os.path.join(download_dir, filename.replace(\".tar\", \"\"))\n",
    "                os.makedirs(extract_path, exist_ok=True)\n",
    "                tar.extractall(path=extract_path, filter='data')\n",
    "                print(f\"\\nExtracted to: {extract_path}\")\n",
    "        except tarfile.TarError as e:\n",
    "            print(f\"Error extracting {filename}: {e}\")\n",
    "\n",
    "        print(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a48a0f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:38:21.629536Z",
     "iopub.status.busy": "2026-02-06T05:38:21.628984Z",
     "iopub.status.idle": "2026-02-06T05:38:21.663278Z",
     "shell.execute_reply": "2026-02-06T05:38:21.662146Z"
    },
    "papermill": {
     "duration": 0.052742,
     "end_time": "2026-02-06T05:38:21.665359",
     "exception": false,
     "start_time": "2026-02-06T05:38:21.612617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data directory set to: /kaggle/working/Data/GSE300475_RAW\n",
      "Found 43 .gz files. Ready for processing (Decompression skipped).\n",
      "\n",
      "--- Preview of GSM9061673_S9_barcodes.tsv.gz ---\n",
      "   AAACCTGAGAGAGCTC-1\n",
      "0  AAACCTGAGCGTAGTG-1\n",
      "1  AAACCTGAGCGTGAGT-1\n",
      "2  AAACCTGAGGCATGTG-1\n",
      "3  AAACCTGAGTCACGCC-1\n",
      "4  AAACCTGAGTCCCACG-1\n",
      "\n",
      "--- Preview of GSM9061673_S9_matrix.mtx.gz ---\n",
      "First 10 lines (header and data):\n",
      "%%MatrixMarket matrix coordinate integer general\n",
      "%metadata_json: {\"software_version\": \"cellranger-6.0.0\", \"format_version\": 2}\n",
      "36604 11480 19000971\n",
      "25 1 1\n",
      "60 1 1\n",
      "62 1 1\n",
      "63 1 1\n",
      "146 1 1\n",
      "171 1 3\n",
      "174 1 1\n",
      "\n",
      "--- Preview of GSM9061666_S2_barcodes.tsv.gz ---\n",
      "   AAACCTGAGCCATCGC-1\n",
      "0  AAACCTGAGGACGAAA-1\n",
      "1  AAACCTGCAAGAAAGG-1\n",
      "2  AAACCTGCATGACGGA-1\n",
      "3  AAACCTGGTAAATACG-1\n",
      "4  AAACCTGGTGGCAAAC-1\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# NOTE: We SKIP explicit decompression to avoid consuming disk space/memory.\n",
    "# Scanpy's read_10x_mtx and other tools can read .gz files directly.\n",
    "\n",
    "def preview_file(file_path):\n",
    "    \"\"\"\n",
    "    Display the first few lines of a file (supports .gz automatically)\n",
    "    \"\"\"\n",
    "    if file_path is None: return\n",
    "    \n",
    "    print(f\"\\n--- Preview of {os.path.basename(file_path)} ---\")\n",
    "    try:\n",
    "        # Handle gzip if extension matches\n",
    "        opener = gzip.open if str(file_path).endswith('.gz') else open\n",
    "        \n",
    "        if str(file_path).endswith(\".tsv\") or str(file_path).endswith(\".csv\") or str(file_path).endswith(\".tsv.gz\") or str(file_path).endswith(\".csv.gz\"):\n",
    "            # Use pandas with nrows \n",
    "            sep = '\\t' if 'tsv' in str(file_path) else ','\n",
    "            comp = 'gzip' if str(file_path).endswith('.gz') else None\n",
    "            try:\n",
    "                # Try reading with header inference\n",
    "                df = pd.read_csv(file_path, sep=sep, nrows=5, compression=comp)\n",
    "                print(df)\n",
    "            except:\n",
    "                print(\"Could not read as CSV/TSV\")\n",
    "        elif 'matrix.mtx' in str(file_path):\n",
    "            # Read as text stream\n",
    "            with opener(file_path, 'rt') as f: # 'rt' for text mode\n",
    "                print(\"First 10 lines (header and data):\")\n",
    "                for _ in range(10):\n",
    "                    line = f.readline()\n",
    "                    if not line: break\n",
    "                    print(line.strip())\n",
    "        else:\n",
    "            print(f\"File type {file_path} preview not customized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not preview {file_path}: {e}\")\n",
    "\n",
    "# Define extract_dir based on download_dir from previous cell\n",
    "extract_dir = os.path.join(download_dir, \"GSE300475_RAW\")\n",
    "raw_data_dir = Path(extract_dir) # Explicitly define this for downstream cells\n",
    "print(f\"Raw data directory set to: {raw_data_dir}\")\n",
    "\n",
    "gz_files = []\n",
    "for root, _, files in os.walk(extract_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".gz\"):\n",
    "            gz_files.append((os.path.join(root, file), root))\n",
    "\n",
    "print(f\"Found {len(gz_files)} .gz files. Ready for processing (Decompression skipped).\")\n",
    "\n",
    "# Just preview a few to ensure they are readable\n",
    "for path, _ in gz_files[:3]:\n",
    "    preview_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ce4c2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:38:21.698231Z",
     "iopub.status.busy": "2026-02-06T05:38:21.697569Z",
     "iopub.status.idle": "2026-02-06T05:38:25.549093Z",
     "shell.execute_reply": "2026-02-06T05:38:25.547935Z"
    },
    "papermill": {
     "duration": 3.8689,
     "end_time": "2026-02-06T05:38:25.550991",
     "exception": false,
     "start_time": "2026-02-06T05:38:21.682091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scanpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ee43ad8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:38:25.583642Z",
     "iopub.status.busy": "2026-02-06T05:38:25.582871Z",
     "iopub.status.idle": "2026-02-06T05:38:25.591339Z",
     "shell.execute_reply": "2026-02-06T05:38:25.590483Z"
    },
    "papermill": {
     "duration": 0.026393,
     "end_time": "2026-02-06T05:38:25.592976",
     "exception": false,
     "start_time": "2026-02-06T05:38:25.566583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows in all contig annotation files: 0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Find all \"all_contig_annotations.csv\" files in the extracted directory and sum their lengths (number of rows)\n",
    "\n",
    "all_contig_files = glob.glob(os.path.join(extract_dir, \"*_all_contig_annotations.csv\"))\n",
    "total_rows = 0\n",
    "\n",
    "for file in all_contig_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        num_rows = len(df)\n",
    "        print(f\"{os.path.basename(file)}: {num_rows} rows\")\n",
    "        total_rows += num_rows\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read {file}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal rows in all contig annotation files: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78de696",
   "metadata": {
    "papermill": {
     "duration": 0.015035,
     "end_time": "2026-02-06T05:38:25.623527",
     "exception": false,
     "start_time": "2026-02-06T05:38:25.608492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Load Sample Metadata\n",
    "\n",
    "First, we load the metadata from the `GSE300475_feature_ref.xlsx` file. This file contains the crucial mapping between GEO sample IDs, patient IDs, timepoints, and treatment response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a479c3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:38:25.656132Z",
     "iopub.status.busy": "2026-02-06T05:38:25.655319Z",
     "iopub.status.idle": "2026-02-06T05:38:34.286148Z",
     "shell.execute_reply": "2026-02-06T05:38:34.285133Z"
    },
    "papermill": {
     "duration": 8.648878,
     "end_time": "2026-02-06T05:38:34.287978",
     "exception": false,
     "start_time": "2026-02-06T05:38:25.639100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Scanpy version: 1.12\n",
      "Pandas version: 2.2.2\n",
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "%pip install scanpy pandas numpy --quiet\n",
    "# Import required libraries for single-cell RNA-seq analysis and data handling\n",
    "import scanpy as sc  # Main library for single-cell analysis, provides AnnData structure and many tools\n",
    "import pandas as pd  # For tabular data manipulation and metadata handling\n",
    "import numpy as np   # For numerical operations and array handling\n",
    "import os            # For operating system interactions (file paths, etc.)\n",
    "from pathlib import Path  # For robust and readable file path management\n",
    "\n",
    "# Print versions to ensure reproducibility and compatibility\n",
    "print(f\"Scanpy version: {sc.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb4522ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:38:34.320208Z",
     "iopub.status.busy": "2026-02-06T05:38:34.319166Z",
     "iopub.status.idle": "2026-02-06T05:38:34.333781Z",
     "shell.execute_reply": "2026-02-06T05:38:34.332851Z"
    },
    "papermill": {
     "duration": 0.031702,
     "end_time": "2026-02-06T05:38:34.335187",
     "exception": false,
     "start_time": "2026-02-06T05:38:34.303485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping explicit decompression to save disk space and IO.\n",
      "Scanpy handles .gz files directly during loading.\n",
      "Found 43 compressed files ready for loading.\n",
      "Example: /kaggle/working/Data/GSE300475_RAW/GSM9061673_S9_barcodes.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def preview_file(file_path):\n",
    "    \"\"\"\n",
    "    Display the first few lines of a decompressed file without loading the whole file into memory.\n",
    "    \"\"\"\n",
    "    if file_path is None: return\n",
    "    \n",
    "    print(f\"\\n--- Preview of {os.path.basename(file_path)} ---\")\n",
    "    try:\n",
    "        if file_path.endswith(\".tsv\") or file_path.endswith(\".csv\"):\n",
    "            # Use pandas with nrows to avoid loading full file\n",
    "            sep = '\\t' if file_path.endswith(\".tsv\") else ','\n",
    "            df = pd.read_csv(file_path, sep=sep, nrows=5) \n",
    "            print(df)\n",
    "        elif file_path.endswith(\".mtx\"):\n",
    "            # Read as text stream to avoid loading massive matrix into memory\n",
    "            with open(file_path, 'r') as f:\n",
    "                print(\"First 10 lines (header and data):\")\n",
    "                for _ in range(10):\n",
    "                    line = f.readline()\n",
    "                    if not line: break\n",
    "                    print(line.strip())\n",
    "        elif str(file_path).endswith(\".gz\"):\n",
    "             print(f\"File is compressed ({file_path}). Scanpy will handle decompression automatically.\")\n",
    "        else:\n",
    "            print(\"Unsupported file type for preview.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not preview {file_path}: {e}\")\n",
    "\n",
    "# Ensure download_dir exists (fallback protection)\n",
    "if 'download_dir' not in globals():\n",
    "     # Fallback logic if variable not in scope\n",
    "     if 'IS_KAGGLE' in globals() and IS_KAGGLE:\n",
    "         download_dir = \"/kaggle/working/Data\"\n",
    "     else:\n",
    "         def _find_project_root():\n",
    "             cwd = Path.cwd().resolve()\n",
    "             for candidate in [cwd, *cwd.parents]:\n",
    "                 if (candidate / \"README.md\").exists() or (candidate / \"Code\").exists():\n",
    "                     return candidate\n",
    "             return cwd\n",
    "         download_dir = str(_find_project_root() / \"Data\")\n",
    "\n",
    "# Normalize download_dir to string for os.path usage\n",
    "if isinstance(download_dir, Path):\n",
    "    download_dir = str(download_dir)\n",
    "\n",
    "extract_dir = os.path.join(download_dir, \"GSE300475_RAW\")\n",
    "\n",
    "# --- PATH CORRECTION LOGIC ---\n",
    "# If extract_dir is empty or missing, but files are in download_dir, use download_dir\n",
    "if not os.path.exists(extract_dir) or not any(f.endswith('.gz') for f in os.listdir(extract_dir) if os.path.isfile(os.path.join(extract_dir, f))):\n",
    "    if os.path.exists(download_dir) and any(f.endswith('.gz') for f in os.listdir(download_dir) if os.path.isfile(os.path.join(download_dir, f))):\n",
    "         print(f\"Detecting files in {download_dir} directly. Adjusting path.\")\n",
    "         extract_dir = download_dir\n",
    "\n",
    "# --- MEMORY OPTIMIZATION ---\n",
    "# We SKIP explicit decompression here because Scanpy's read_10x_mtx can read .gz files directly.\n",
    "# Decompressing large sparse matrices to dense text files on disk is unnecessary and wastes storage/IO.\n",
    "print(\"Skipping explicit decompression to save disk space and IO.\")\n",
    "print(\"Scanpy handles .gz files directly during loading.\")\n",
    "\n",
    "# Just preview one GZ file to show it exists\n",
    "gz_files = []\n",
    "for root, _, files in os.walk(extract_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".gz\"):\n",
    "            gz_files.append(os.path.join(root, file))\n",
    "\n",
    "if gz_files:\n",
    "    print(f\"Found {len(gz_files)} compressed files ready for loading.\")\n",
    "    print(f\"Example: {gz_files[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ed03cfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:38:34.367045Z",
     "iopub.status.busy": "2026-02-06T05:38:34.366752Z",
     "iopub.status.idle": "2026-02-06T05:38:34.407700Z",
     "shell.execute_reply": "2026-02-06T05:38:34.406917Z"
    },
    "papermill": {
     "duration": 0.058857,
     "end_time": "2026-02-06T05:38:34.409076",
     "exception": false,
     "start_time": "2026-02-06T05:38:34.350219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory set to: /kaggle/working/Data/GSE300475_RAW\n",
      "Metadata table now matches the requested specification:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_Number</th>\n",
       "      <th>GEX_Sample_ID</th>\n",
       "      <th>TCR_Sample_ID</th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Response</th>\n",
       "      <th>In_Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>GSM9061687</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2</td>\n",
       "      <td>GSM9061666</td>\n",
       "      <td>GSM9061688</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S3</td>\n",
       "      <td>GSM9061667</td>\n",
       "      <td>GSM9061689</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Recurrence</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S4</td>\n",
       "      <td>GSM9061668</td>\n",
       "      <td>GSM9061690</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S5</td>\n",
       "      <td>GSM9061669</td>\n",
       "      <td>GSM9061691</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S6</td>\n",
       "      <td>GSM9061670</td>\n",
       "      <td>GSM9061692</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S7</td>\n",
       "      <td>GSM9061671</td>\n",
       "      <td>GSM9061693</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S8</td>\n",
       "      <td>GSM9061672</td>\n",
       "      <td>None</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Recurrence</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>GEX only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S9</td>\n",
       "      <td>GSM9061673</td>\n",
       "      <td>GSM9061694</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S10</td>\n",
       "      <td>GSM9061674</td>\n",
       "      <td>GSM9061695</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S11</td>\n",
       "      <td>GSM9061675</td>\n",
       "      <td>GSM9061696</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Recurrence</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S_Number GEX_Sample_ID TCR_Sample_ID Patient_ID   Timepoint       Response  \\\n",
       "0        S1    GSM9061665    GSM9061687        PT1    Baseline      Responder   \n",
       "1        S2    GSM9061666    GSM9061688        PT1     Post-Tx      Responder   \n",
       "2        S3    GSM9061667    GSM9061689        PT1  Recurrence      Responder   \n",
       "3        S4    GSM9061668    GSM9061690        PT2    Baseline      Responder   \n",
       "4        S5    GSM9061669    GSM9061691        PT2     Post-Tx      Responder   \n",
       "5        S6    GSM9061670    GSM9061692        PT3    Baseline  Non-Responder   \n",
       "6        S7    GSM9061671    GSM9061693        PT3     Post-Tx  Non-Responder   \n",
       "7        S8    GSM9061672          None        PT3  Recurrence  Non-Responder   \n",
       "8        S9    GSM9061673    GSM9061694        PT4    Baseline  Non-Responder   \n",
       "9       S10    GSM9061674    GSM9061695        PT4     Post-Tx  Non-Responder   \n",
       "10      S11    GSM9061675    GSM9061696        PT4  Recurrence  Non-Responder   \n",
       "\n",
       "     In_Data  \n",
       "0        Yes  \n",
       "1        Yes  \n",
       "2        Yes  \n",
       "3        Yes  \n",
       "4        Yes  \n",
       "5        Yes  \n",
       "6        Yes  \n",
       "7   GEX only  \n",
       "8        Yes  \n",
       "9        Yes  \n",
       "10       Yes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-check In_Data column (based on files found in raw_data_dir):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_Number</th>\n",
       "      <th>GEX_Sample_ID</th>\n",
       "      <th>TCR_Sample_ID</th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Response</th>\n",
       "      <th>In_Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>GSM9061687</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2</td>\n",
       "      <td>GSM9061666</td>\n",
       "      <td>GSM9061688</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S3</td>\n",
       "      <td>GSM9061667</td>\n",
       "      <td>GSM9061689</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Recurrence</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S4</td>\n",
       "      <td>GSM9061668</td>\n",
       "      <td>GSM9061690</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S5</td>\n",
       "      <td>GSM9061669</td>\n",
       "      <td>GSM9061691</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S6</td>\n",
       "      <td>GSM9061670</td>\n",
       "      <td>GSM9061692</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S7</td>\n",
       "      <td>GSM9061671</td>\n",
       "      <td>GSM9061693</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S8</td>\n",
       "      <td>GSM9061672</td>\n",
       "      <td>None</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Recurrence</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>GEX only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S9</td>\n",
       "      <td>GSM9061673</td>\n",
       "      <td>GSM9061694</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S10</td>\n",
       "      <td>GSM9061674</td>\n",
       "      <td>GSM9061695</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S11</td>\n",
       "      <td>GSM9061675</td>\n",
       "      <td>GSM9061696</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Recurrence</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S_Number GEX_Sample_ID TCR_Sample_ID Patient_ID   Timepoint       Response  \\\n",
       "0        S1    GSM9061665    GSM9061687        PT1    Baseline      Responder   \n",
       "1        S2    GSM9061666    GSM9061688        PT1     Post-Tx      Responder   \n",
       "2        S3    GSM9061667    GSM9061689        PT1  Recurrence      Responder   \n",
       "3        S4    GSM9061668    GSM9061690        PT2    Baseline      Responder   \n",
       "4        S5    GSM9061669    GSM9061691        PT2     Post-Tx      Responder   \n",
       "5        S6    GSM9061670    GSM9061692        PT3    Baseline  Non-Responder   \n",
       "6        S7    GSM9061671    GSM9061693        PT3     Post-Tx  Non-Responder   \n",
       "7        S8    GSM9061672          None        PT3  Recurrence  Non-Responder   \n",
       "8        S9    GSM9061673    GSM9061694        PT4    Baseline  Non-Responder   \n",
       "9       S10    GSM9061674    GSM9061695        PT4     Post-Tx  Non-Responder   \n",
       "10      S11    GSM9061675    GSM9061696        PT4  Recurrence  Non-Responder   \n",
       "\n",
       "     In_Data  \n",
       "0        Yes  \n",
       "1        Yes  \n",
       "2        Yes  \n",
       "3        Yes  \n",
       "4        Yes  \n",
       "5        Yes  \n",
       "6        Yes  \n",
       "7   GEX only  \n",
       "8        Yes  \n",
       "9        Yes  \n",
       "10       Yes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.8 ms, sys: 3.93 ms, total: 26.8 ms\n",
      "Wall time: 26.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Setup data paths ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import requests\n",
    "\n",
    "# Use existing IS_KAGGLE flag or detect\n",
    "if 'IS_KAGGLE' not in globals():\n",
    "    IS_KAGGLE = os.path.exists('/kaggle/input') or os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None\n",
    "\n",
    "def _find_project_root():\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for candidate in [cwd, *cwd.parents]:\n",
    "        if (candidate / \"README.md\").exists() or (candidate / \"Code\").exists():\n",
    "            return candidate\n",
    "    return cwd\n",
    "\n",
    "def _has_matrix_files(path: Path) -> bool:\n",
    "    return path.exists() and any(path.rglob(\"*matrix.mtx*\"))\n",
    "\n",
    "# Determine candidate base directories\n",
    "candidate_dirs = []\n",
    "if IS_KAGGLE:\n",
    "    candidate_dirs = [Path('/kaggle/working/Data'), Path('/Data'), Path('/kaggle/input')]\n",
    "else:\n",
    "    project_root = _find_project_root()\n",
    "    if 'download_dir' in globals() and download_dir:\n",
    "        candidate_dirs.append(Path(download_dir))\n",
    "    candidate_dirs += [project_root / 'Data', project_root / 'data', project_root]\n",
    "\n",
    "raw_data_dir = None\n",
    "for base in candidate_dirs:\n",
    "    if base is None:\n",
    "        continue\n",
    "    base = Path(base)\n",
    "    if base.name == 'GSE300475_RAW' and _has_matrix_files(base):\n",
    "        raw_data_dir = base\n",
    "        break\n",
    "    if _has_matrix_files(base / 'GSE300475_RAW'):\n",
    "        raw_data_dir = base / 'GSE300475_RAW'\n",
    "        break\n",
    "    if _has_matrix_files(base):\n",
    "        raw_data_dir = base\n",
    "        break\n",
    "\n",
    "# Fallback: search under project root (local only)\n",
    "if raw_data_dir is None and not IS_KAGGLE:\n",
    "    project_root = _find_project_root()\n",
    "    for match in project_root.rglob('GSE300475_RAW'):\n",
    "        if _has_matrix_files(match):\n",
    "            raw_data_dir = match\n",
    "            break\n",
    "\n",
    "# Auto-download if still missing\n",
    "if raw_data_dir is None:\n",
    "    print(\"Raw data not found locally. Attempting download...\")\n",
    "    if IS_KAGGLE:\n",
    "        download_dir = Path('/kaggle/working/Data')\n",
    "    else:\n",
    "        project_root = _find_project_root()\n",
    "        if 'download_dir' in globals() and download_dir:\n",
    "            download_dir = Path(download_dir)\n",
    "        else:\n",
    "            download_dir = project_root / 'Data'\n",
    "    download_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tar_path = download_dir / 'GSE300475_RAW.tar'\n",
    "    if not tar_path.exists():\n",
    "        url = 'https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE300475&format=file'\n",
    "        try:\n",
    "            with requests.get(url, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(tar_path, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            print(f\"Downloaded {tar_path}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Download failed: {e}\")\n",
    "    extract_path = download_dir / 'GSE300475_RAW'\n",
    "    if not extract_path.exists():\n",
    "        print(f\"Extracting {tar_path} to {extract_path}...\")\n",
    "        try:\n",
    "            with tarfile.open(tar_path, 'r') as tar:\n",
    "                try:\n",
    "                    tar.extractall(path=extract_path, filter='data')\n",
    "                except TypeError:\n",
    "                    tar.extractall(path=extract_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Extraction failed: {e}\")\n",
    "    raw_data_dir = extract_path\n",
    "\n",
    "print(f\"Data directory set to: {raw_data_dir}\")\n",
    "\n",
    "# --- Manually create the metadata mapping ---\n",
    "# This list contains information about each sample, including GEO IDs, patient IDs, timepoints, and treatment response.\n",
    "# Note: S8 (GSM9061672) has GEX files but no corresponding TCR file.\n",
    "metadata_list = [\n",
    "    # Patient 1 (Responder)\n",
    "    {'S_Number': 'S1',  'GEX_Sample_ID': 'GSM9061665', 'TCR_Sample_ID': 'GSM9061687', 'Patient_ID': 'PT1',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S2',  'GEX_Sample_ID': 'GSM9061666', 'TCR_Sample_ID': 'GSM9061688', 'Patient_ID': 'PT1',  'Timepoint': 'Post-Tx',      'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S3',  'GEX_Sample_ID': 'GSM9061667', 'TCR_Sample_ID': 'GSM9061689', 'Patient_ID': 'PT1',  'Timepoint': 'Recurrence',   'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "    \n",
    "    # Patient 2 (Responder)\n",
    "    {'S_Number': 'S4',  'GEX_Sample_ID': 'GSM9061668', 'TCR_Sample_ID': 'GSM9061690', 'Patient_ID': 'PT2',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S5',  'GEX_Sample_ID': 'GSM9061669', 'TCR_Sample_ID': 'GSM9061691', 'Patient_ID': 'PT2',  'Timepoint': 'Post-Tx',      'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "    \n",
    "    # Patient 3 (Non-Responder)\n",
    "    {'S_Number': 'S6',  'GEX_Sample_ID': 'GSM9061670', 'TCR_Sample_ID': 'GSM9061692', 'Patient_ID': 'PT3',  'Timepoint': 'Baseline',     'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S7',  'GEX_Sample_ID': 'GSM9061671', 'TCR_Sample_ID': 'GSM9061693', 'Patient_ID': 'PT3',  'Timepoint': 'Post-Tx',      'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S8',  'GEX_Sample_ID': 'GSM9061672', 'TCR_Sample_ID': None,         'Patient_ID': 'PT3',  'Timepoint': 'Recurrence',   'Response': 'Non-Responder', 'In_Data': 'GEX only'},\n",
    "    \n",
    "    # Patient 4 (Non-Responder)\n",
    "    {'S_Number': 'S9',  'GEX_Sample_ID': 'GSM9061673', 'TCR_Sample_ID': 'GSM9061694', 'Patient_ID': 'PT4',  'Timepoint': 'Baseline',     'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S10', 'GEX_Sample_ID': 'GSM9061674', 'TCR_Sample_ID': 'GSM9061695', 'Patient_ID': 'PT4',  'Timepoint': 'Post-Tx',      'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S11', 'GEX_Sample_ID': 'GSM9061675', 'TCR_Sample_ID': 'GSM9061696', 'Patient_ID': 'PT4',  'Timepoint': 'Recurrence',   'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "]\n",
    "\n",
    "# Create pandas DataFrame for easy access\n",
    "metadata_df = pd.DataFrame(metadata_list)\n",
    "print(\"Metadata table now matches the requested specification:\")\n",
    "display(metadata_df)\n",
    "\n",
    "# --- Programmatic sanity-check for file presence ---\n",
    "# This loop checks if the expected files exist for each sample and updates the 'In_Data' column accordingly.\n",
    "for idx, row in metadata_df.iterrows():\n",
    "    s = row['S_Number']\n",
    "    g = row['GEX_Sample_ID']\n",
    "    t = row['TCR_Sample_ID']\n",
    "    # Check for gene expression matrix file (compressed or uncompressed)\n",
    "    # Check .mtx, .mtx.gz, and also potential file name variations or if they are in subfolders\n",
    "    # We look in raw_data_dir found above.\n",
    "    g_exists = (raw_data_dir / f\"{g}_{s}_matrix.mtx.gz\").exists() or (raw_data_dir / f\"{g}_{s}_matrix.mtx\").exists()\n",
    "    \n",
    "    # Also check if just the GSM id is present in some filename if strict match fails (fallback)\n",
    "    if not g_exists:\n",
    "         # Try simpler wildcard search\n",
    "         g_exists = len(list(raw_data_dir.glob(f\"*{g}*matrix*\"))) > 0\n",
    "\n",
    "    t_exists = False\n",
    "    # Check for TCR annotation file if TCR sample ID is present\n",
    "    if pd.notna(t) and t is not None:\n",
    "        t_exists = (raw_data_dir / f\"{t}_{s}_all_contig_annotations.csv.gz\").exists() or (raw_data_dir / f\"{t}_{s}_all_contig_annotations.csv\").exists()\n",
    "        if not t_exists:\n",
    "             t_exists = len(list(raw_data_dir.glob(f\"*{t}*all_contig_annotations*\"))) > 0\n",
    "             \n",
    "    # Update 'In_Data' column based on file presence\n",
    "    if g_exists and t_exists:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'Yes'\n",
    "    elif g_exists and not t_exists:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'GEX only'\n",
    "    else:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'No'\n",
    "\n",
    "print(\"\\nPost-check In_Data column (based on files found in raw_data_dir):\")\n",
    "display(metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c332d1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:38:34.443346Z",
     "iopub.status.busy": "2026-02-06T05:38:34.443014Z",
     "iopub.status.idle": "2026-02-06T05:40:13.254987Z",
     "shell.execute_reply": "2026-02-06T05:40:13.253999Z"
    },
    "papermill": {
     "duration": 98.832194,
     "end_time": "2026-02-06T05:40:13.257463",
     "exception": false,
     "start_time": "2026-02-06T05:38:34.425269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Map Phase (Processing & Saving Chunks)...\n",
      "Processing 1/11: GSM9061665_S1\n",
      "  Saved chunk: 8804 cells. Memory cleared.\n",
      "Processing 2/11: GSM9061666_S2\n",
      "  Saved chunk: 9037 cells. Memory cleared.\n",
      "Processing 3/11: GSM9061667_S3\n",
      "  Saved chunk: 7343 cells. Memory cleared.\n",
      "Processing 4/11: GSM9061668_S4\n",
      "  Saved chunk: 8608 cells. Memory cleared.\n",
      "Processing 5/11: GSM9061669_S5\n",
      "  Saved chunk: 2887 cells. Memory cleared.\n",
      "Processing 6/11: GSM9061670_S6\n",
      "  Saved chunk: 10353 cells. Memory cleared.\n",
      "Processing 7/11: GSM9061671_S7\n",
      "  Saved chunk: 9186 cells. Memory cleared.\n",
      "Processing 8/11: GSM9061672_S8\n",
      "  Saved chunk: 12665 cells. Memory cleared.\n",
      "Processing 9/11: GSM9061673_S9\n",
      "  Saved chunk: 11216 cells. Memory cleared.\n",
      "Processing 10/11: GSM9061674_S10\n",
      "  Saved chunk: 9582 cells. Memory cleared.\n",
      "Processing 11/11: GSM9061675_S11\n",
      "  Saved chunk: 9286 cells. Memory cleared.\n",
      "\n",
      "Starting Reduce Phase (Merging 11 chunks)...\n",
      "Using on-disk concatenation (anndata.experimental.concat_on_disk)...\n",
      "Final Merged Data: 98967 cells x 14819 genes\n",
      "Full TCR Data: 162133 rows\n",
      "Temp chunks cleaned up.\n",
      "CPU times: user 1min 45s, sys: 9.09 s, total: 1min 54s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- DISK-BASED MAP-REDUCE STRATEGY TO SOLVE OOM ---\n",
    "# Strategy: \n",
    "# 1. Map: Process each sample -> QC -> Save to temp .h5ad on disk\n",
    "# 2. Reduce: Concatenate on disk (preferred) or in small batches\n",
    "# This keeps RAM usage low during processing and avoids the iterative reallocation spike.\n",
    "\n",
    "import gc\n",
    "import shutil\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "# Validate prerequisites\n",
    "if 'metadata_df' not in globals():\n",
    "    raise NameError(\"metadata_df is not defined. Please run the metadata creation cell first.\")\n",
    "if 'raw_data_dir' not in globals():\n",
    "    raise NameError(\"raw_data_dir is not defined. Please run the data path setup cell first.\")\n",
    "\n",
    "# Setup temp directory for chunks\n",
    "temp_chunk_dir = Path(\"temp_adata_chunks\")\n",
    "if temp_chunk_dir.exists():\n",
    "    shutil.rmtree(temp_chunk_dir)\n",
    "temp_chunk_dir.mkdir(exist_ok=True)\n",
    "\n",
    "chunk_files = []\n",
    "chunk_keys = []\n",
    "tcr_data_list = []  # TCR data is small enough to keep in memory\n",
    "\n",
    "print(\"Starting Map Phase (Processing & Saving Chunks)...\")\n",
    "\n",
    "# --- MAP PHASE: Process & Save ---\n",
    "for index, row in metadata_df.iterrows():\n",
    "    gex_sample_id = row['GEX_Sample_ID']\n",
    "    s_number = row['S_Number']\n",
    "    \n",
    "    # Construct sample-level prefix\n",
    "    sample_prefix = f\"{gex_sample_id}_{s_number}\"\n",
    "\n",
    "    # Use robust file finding logic from previous cells\n",
    "    matrix_file = None\n",
    "    for ext in ['matrix.mtx.gz', 'matrix.mtx']:\n",
    "        candidate = raw_data_dir / f\"{sample_prefix}_{ext}\"\n",
    "        if candidate.exists():\n",
    "            matrix_file = candidate\n",
    "            break\n",
    "            \n",
    "    if not matrix_file:\n",
    "         # Fallback search\n",
    "        for ext in ['matrix.mtx.gz', 'matrix.mtx']:\n",
    "            possible_files = list(raw_data_dir.glob(f\"*{gex_sample_id}*{ext}\"))\n",
    "            if possible_files:\n",
    "                matrix_file = possible_files[0]\n",
    "                break\n",
    "    \n",
    "    if not matrix_file:\n",
    "        print(f\"Skipping {sample_prefix}: Matrix file not found.\")\n",
    "        continue\n",
    "\n",
    "    sample_data_path = matrix_file.parent\n",
    "    matrix_prefix = matrix_file.name.replace('matrix.mtx', '').replace('.gz', '')\n",
    "\n",
    "    print(f\"Processing {index+1}/{len(metadata_df)}: {sample_prefix}\")\n",
    "    \n",
    "    try:\n",
    "        # Load GEX\n",
    "        adata_sample = sc.read_10x_mtx(\n",
    "            sample_data_path, \n",
    "            var_names='gene_symbols',\n",
    "            prefix=matrix_prefix,\n",
    "            cache=True\n",
    "        )\n",
    "        \n",
    "        # Ensure sparse float32 IMMEDIATELY\n",
    "        if not hasattr(adata_sample.X, 'toarray'):\n",
    "            adata_sample.X = sp.csr_matrix(adata_sample.X, dtype=np.float32)\n",
    "        else:\n",
    "            adata_sample.X = sp.csr_matrix(adata_sample.X, dtype=np.float32)\n",
    "            \n",
    "        # Add metadata\n",
    "        adata_sample.obs['sample_id'] = gex_sample_id \n",
    "        adata_sample.obs['patient_id'] = row['Patient_ID']\n",
    "        adata_sample.obs['timepoint'] = row['Timepoint']\n",
    "        adata_sample.obs['response'] = row['Response']\n",
    "        \n",
    "        # QC Filtering (Crucial reduction)\n",
    "        sc.pp.filter_cells(adata_sample, min_genes=200)\n",
    "        sc.pp.filter_genes(adata_sample, min_cells=3)\n",
    "        \n",
    "        # Ensure unique var names before saving\n",
    "        adata_sample.var_names_make_unique()\n",
    "        \n",
    "        # Save chunk\n",
    "        chunk_path = temp_chunk_dir / f\"chunk_{index}_{gex_sample_id}.h5ad\"\n",
    "        adata_sample.write_h5ad(chunk_path, compression='gzip')\n",
    "        chunk_files.append(chunk_path)\n",
    "        chunk_keys.append(sample_prefix)\n",
    "        \n",
    "        print(f\"  Saved chunk: {adata_sample.n_obs} cells. Memory cleared.\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del adata_sample\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {sample_prefix}: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # TCR Loading (Keep separate list)\n",
    "    tcr_sample_id = row['TCR_Sample_ID']\n",
    "    if pd.notna(tcr_sample_id):\n",
    "        tcr_file = raw_data_dir / f\"{tcr_sample_id}_{s_number}_all_contig_annotations.csv.gz\"\n",
    "        if not tcr_file.exists():\n",
    "             tcr_file = raw_data_dir / f\"{tcr_sample_id}_{s_number}_all_contig_annotations.csv\"\n",
    "             \n",
    "        if tcr_file.exists():\n",
    "            try:\n",
    "                # Load essential columns only\n",
    "                cols = ['barcode', 'is_cell', 'contig_id', 'high_confidence', 'length', \n",
    "                        'chain', 'v_gene', 'd_gene', 'j_gene', 'c_gene', 'full_length', \n",
    "                        'productive', 'cdr3']\n",
    "                        \n",
    "                # Check which columns actually exist in the file first to strictly avoid errors?\n",
    "                # Faster to just try/except or load all if cols obscure\n",
    "                # Let's try loading header first? No, pandas handling is fine.\n",
    "                tcr_df = pd.read_csv(tcr_file, usecols=lambda c: c in cols)\n",
    "                tcr_df['sample_id'] = gex_sample_id\n",
    "                tcr_data_list.append(tcr_df)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "# --- REDUCE PHASE: Disk-safe Concatenation ---\n",
    "print(f\"\\nStarting Reduce Phase (Merging {len(chunk_files)} chunks)...\")\n",
    "\n",
    "if not chunk_files:\n",
    "    raise ValueError(\"No chunks were saved! Check data paths.\")\n",
    "\n",
    "merged_path = temp_chunk_dir / \"merged.h5ad\"\n",
    "adata = None\n",
    "\n",
    "# Prefer on-disk concatenation if available (anndata>=0.9)\n",
    "try:\n",
    "    if hasattr(ad, \"experimental\") and hasattr(ad.experimental, \"concat_on_disk\"):\n",
    "        print(\"Using on-disk concatenation (anndata.experimental.concat_on_disk)...\")\n",
    "        # Use 'batch' as the label instead of 'sample_id' to preserve the original sample_id column\n",
    "        ad.experimental.concat_on_disk(\n",
    "            [str(p) for p in chunk_files],\n",
    "            str(merged_path),\n",
    "            join='inner',  # intersection avoids union blow-up\n",
    "            merge='same',\n",
    "            label='batch',  # Changed from 'sample_id' to preserve original sample_id\n",
    "            keys=chunk_keys,\n",
    "            index_unique='-'\n",
    "        )\n",
    "        adata = sc.read_h5ad(merged_path)\n",
    "    else:\n",
    "        raise AttributeError(\"concat_on_disk not available in this anndata version\")\n",
    "except Exception as e:\n",
    "    print(f\"Falling back to batch in-memory concat: {e}\")\n",
    "    batch_size = 4\n",
    "    adata = None\n",
    "    for i in range(0, len(chunk_files), batch_size):\n",
    "        batch_files = chunk_files[i:i+batch_size]\n",
    "        batch_adatas = [sc.read_h5ad(f) for f in batch_files]\n",
    "        batch = ad.concat(batch_adatas, join='inner', merge='same', index_unique='-')\n",
    "        del batch_adatas\n",
    "        gc.collect()\n",
    "        if adata is None:\n",
    "            adata = batch\n",
    "        else:\n",
    "            adata = ad.concat([adata, batch], join='inner', merge='same', index_unique='-')\n",
    "        del batch\n",
    "        gc.collect()\n",
    "\n",
    "# Final sparse enforcement\n",
    "if not sp.issparse(adata.X):\n",
    "    adata.X = sp.csr_matrix(adata.X, dtype=np.float32)\n",
    "\n",
    "print(f\"Final Merged Data: {adata.n_obs} cells x {adata.n_vars} genes\")\n",
    "\n",
    "# Merge TCR data\n",
    "if tcr_data_list:\n",
    "    full_tcr_df = pd.concat(tcr_data_list, ignore_index=True)\n",
    "    print(f\"Full TCR Data: {len(full_tcr_df)} rows\")\n",
    "    del tcr_data_list\n",
    "else:\n",
    "    print(\"No TCR data loaded.\")\n",
    "\n",
    "# Cleanup chunks\n",
    "shutil.rmtree(temp_chunk_dir)\n",
    "print(\"Temp chunks cleaned up.\")\n",
    "\n",
    "# Dummy adata_list for compatibility\n",
    "adata_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc73b06",
   "metadata": {
    "papermill": {
     "duration": 0.014864,
     "end_time": "2026-02-06T05:40:13.287588",
     "exception": false,
     "start_time": "2026-02-06T05:40:13.272724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Integrate TCR Data and Perform QC\n",
    "\n",
    "Next, we'll merge the TCR information into the `.obs` of our main `AnnData` object. We will keep only the cells that have corresponding TCR data and filter based on the `high_confidence` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc07488b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:40:13.319220Z",
     "iopub.status.busy": "2026-02-06T05:40:13.318688Z",
     "iopub.status.idle": "2026-02-06T05:40:13.327258Z",
     "shell.execute_reply": "2026-02-06T05:40:13.326412Z"
    },
    "papermill": {
     "duration": 0.026125,
     "end_time": "2026-02-06T05:40:13.328725",
     "exception": false,
     "start_time": "2026-02-06T05:40:13.302600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should_process_raw not set; defaulting to False (loaded_h5ad=False, adata_missing=False)\n"
     ]
    }
   ],
   "source": [
    "# 3. Raw Processing Branch (Only runs if needed)\n",
    "# Auto-define should_process_raw if missing to avoid NameError\n",
    "if 'should_process_raw' not in globals():\n",
    "    _loaded_h5ad = bool(globals().get('loaded_h5ad', False))\n",
    "    _adata_missing = ('adata' not in globals()) or (adata is None)\n",
    "    _metadata_ready = 'metadata_df' in globals()\n",
    "    should_process_raw = _metadata_ready and (not _loaded_h5ad) and _adata_missing\n",
    "    print(f\"should_process_raw not set; defaulting to {should_process_raw} (loaded_h5ad={_loaded_h5ad}, adata_missing={_adata_missing})\")\n",
    "\n",
    "if should_process_raw:\n",
    "    print(\"Starting raw data processing from metadata...\")\n",
    "\n",
    "    # Ensure raw_data_dir is defined\n",
    "    if 'raw_data_dir' not in globals():\n",
    "        base_dir = Path('/kaggle/working/Data') if (globals().get('IS_KAGGLE', False)) else Path('../Data')\n",
    "        raw_data_dir = base_dir / 'GSE300475_RAW'\n",
    "        print(f\"raw_data_dir undefined. Defaulting to: {raw_data_dir}\")\n",
    "    \n",
    "    # --- Initialize lists ---\n",
    "    adata_list = []  \n",
    "    tcr_data_list = []  \n",
    "\n",
    "    # --- Iterate through each sample ---\n",
    "    for index, row in metadata_df.iterrows():\n",
    "        gex_sample_id = row['GEX_Sample_ID']\n",
    "        tcr_sample_id = row['TCR_Sample_ID']\n",
    "        s_number = row['S_Number']\n",
    "        patient_id = row['Patient_ID']\n",
    "        timepoint = row['Timepoint']\n",
    "        response = row['Response']\n",
    "        \n",
    "        print(f\"Processing sample {index+1}/{len(metadata_df)}: {gex_sample_id} ({s_number})...\")\n",
    "        \n",
    "        # --- Robust File Finding (Fixing 'GEX data not found') ---\n",
    "        # Pattern: *GSM123*matrix.mtx* matches both .mtx and .mtx.gz\n",
    "        try:\n",
    "            found_gex_files = list(raw_data_dir.rglob(f\"*{gex_sample_id}*matrix.mtx*\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching {raw_data_dir}: {e}\")\n",
    "            found_gex_files = []\n",
    "        \n",
    "        if not found_gex_files:\n",
    "            print(f\"  Warning: GEX matrix file for {gex_sample_id} not found in {raw_data_dir}. Skipping.\")\n",
    "            try:\n",
    "                 print(\"  Debug: Listing first 5 files in raw_data_dir to help diagnose:\")\n",
    "                 for i, p in enumerate(raw_data_dir.rglob('*')):\n",
    "                     if i >= 5: break\n",
    "                     print(f\"    {p.name}\")\n",
    "            except: pass\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883665f",
   "metadata": {
    "papermill": {
     "duration": 0.015211,
     "end_time": "2026-02-06T05:40:13.358704",
     "exception": false,
     "start_time": "2026-02-06T05:40:13.343493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Save Processed Data\n",
    "\n",
    "Finally, we save the fully processed, annotated, and filtered `AnnData` object to a `.h5ad` file. This file can be easily loaded in future notebooks for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "508b6994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:40:13.390970Z",
     "iopub.status.busy": "2026-02-06T05:40:13.390664Z",
     "iopub.status.idle": "2026-02-06T05:40:17.749233Z",
     "shell.execute_reply": "2026-02-06T05:40:17.748220Z"
    },
    "papermill": {
     "duration": 4.377967,
     "end_time": "2026-02-06T05:40:17.751695",
     "exception": false,
     "start_time": "2026-02-06T05:40:13.373728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrating TCR data into AnnData (TCR contigs: 162133, cells: 98967)...\n",
      "  DEBUG: adata.obs sample_id examples: ['GSM9061665', 'GSM9061666', 'GSM9061667']\n",
      "  DEBUG: TCR sample_id examples: ['GSM9061665', 'GSM9061666', 'GSM9061667']\n",
      "  DEBUG: adata barcode examples: ['AAACCTGAGAAGGGTA-1', 'AAACCTGAGACTGTAA-1', 'AAACCTGAGCAGCGTA-1']\n",
      "  DEBUG: TCR barcode examples: ['AAACCTGAGACTGTAA-1', 'AAACCTGAGCGTGAAC-1', 'AAACCTGAGCTACCTA-1']\n",
      "Successfully merged TCR data. Cells with TCR info: 38413 / 98967\n",
      "Filtered from 98967 to 38413 cells based on having high-confidence TCR data.\n",
      "\n",
      "Performing QC filtering (starting with 38413 cells, 14819 genes)...\n",
      "  After min_genes filter: 38413 cells\n",
      "  After min_cells filter: 14816 genes\n",
      "\n",
      "Post-QC AnnData object:\n",
      "AnnData object with n_obs √ó n_vars = 38413 √ó 14816\n",
      "    obs: 'sample_id', 'patient_id', 'timepoint', 'response', 'n_genes', 'batch', 'barcode_for_merge', 'barcode', 'cdr3_TRA', 'cdr3_TRB', 'j_gene_TRA', 'j_gene_TRB', 'v_gene_TRA', 'v_gene_TRB', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt'\n",
      "    var: 'gene_ids', 'feature_types', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'\n",
      "\n",
      "Sample metadata preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>timepoint</th>\n",
       "      <th>response</th>\n",
       "      <th>n_genes</th>\n",
       "      <th>batch</th>\n",
       "      <th>barcode_for_merge</th>\n",
       "      <th>barcode</th>\n",
       "      <th>cdr3_TRA</th>\n",
       "      <th>cdr3_TRB</th>\n",
       "      <th>j_gene_TRA</th>\n",
       "      <th>j_gene_TRB</th>\n",
       "      <th>v_gene_TRA</th>\n",
       "      <th>v_gene_TRB</th>\n",
       "      <th>n_genes_by_counts</th>\n",
       "      <th>total_counts</th>\n",
       "      <th>total_counts_mt</th>\n",
       "      <th>pct_counts_mt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGACTGTAA-1-GSM9061665_S1</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>1379</td>\n",
       "      <td>GSM9061665_S1</td>\n",
       "      <td>AAACCTGAGACTGTAA-1</td>\n",
       "      <td>AAACCTGAGACTGTAA-1</td>\n",
       "      <td>CAVEARNYKLTF</td>\n",
       "      <td>CASGTGLNTEAFF</td>\n",
       "      <td>TRAJ53</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>TRAV36/DV7</td>\n",
       "      <td>TRBV3-1</td>\n",
       "      <td>1379</td>\n",
       "      <td>4637.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>3.385810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGCGTGAAC-1-GSM9061665_S1</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>1275</td>\n",
       "      <td>GSM9061665_S1</td>\n",
       "      <td>AAACCTGAGCGTGAAC-1</td>\n",
       "      <td>AAACCTGAGCGTGAAC-1</td>\n",
       "      <td>CAASAVGNEKLTF</td>\n",
       "      <td>CAWSALLGTVNGYTF</td>\n",
       "      <td>TRAJ48</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>TRAV29/DV5</td>\n",
       "      <td>TRBV30</td>\n",
       "      <td>1275</td>\n",
       "      <td>4843.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>5.100144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGCTACCTA-1-GSM9061665_S1</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>886</td>\n",
       "      <td>GSM9061665_S1</td>\n",
       "      <td>AAACCTGAGCTACCTA-1</td>\n",
       "      <td>AAACCTGAGCTACCTA-1</td>\n",
       "      <td>CALSEAWGNARLMF</td>\n",
       "      <td>CASRSREETYEQYF</td>\n",
       "      <td>TRAJ31</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>TRAV19</td>\n",
       "      <td>TRBV2</td>\n",
       "      <td>886</td>\n",
       "      <td>3076.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>9.102731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGCTGTTCA-1-GSM9061665_S1</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>1628</td>\n",
       "      <td>GSM9061665_S1</td>\n",
       "      <td>AAACCTGAGCTGTTCA-1</td>\n",
       "      <td>AAACCTGAGCTGTTCA-1</td>\n",
       "      <td>CALLGLKGEGSARQLTF</td>\n",
       "      <td>CASSLPPWRANTEAFF</td>\n",
       "      <td>TRAJ22</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>TRAV9-2</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>1628</td>\n",
       "      <td>4914.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>5.860806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGGCATTGG-1-GSM9061665_S1</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>1313</td>\n",
       "      <td>GSM9061665_S1</td>\n",
       "      <td>AAACCTGAGGCATTGG-1</td>\n",
       "      <td>AAACCTGAGGCATTGG-1</td>\n",
       "      <td>CAVTGFSDGQKLLF</td>\n",
       "      <td>CASSLTGEVWDEQFF</td>\n",
       "      <td>TRAJ16</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>TRAV8-6</td>\n",
       "      <td>TRBV5-1</td>\n",
       "      <td>1313</td>\n",
       "      <td>4947.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>4.002426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   sample_id patient_id timepoint   response  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665_S1  GSM9061665        PT1  Baseline  Responder   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665_S1  GSM9061665        PT1  Baseline  Responder   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665_S1  GSM9061665        PT1  Baseline  Responder   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665_S1  GSM9061665        PT1  Baseline  Responder   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665_S1  GSM9061665        PT1  Baseline  Responder   \n",
       "\n",
       "                                  n_genes          batch   barcode_for_merge  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665_S1     1379  GSM9061665_S1  AAACCTGAGACTGTAA-1   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665_S1     1275  GSM9061665_S1  AAACCTGAGCGTGAAC-1   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665_S1      886  GSM9061665_S1  AAACCTGAGCTACCTA-1   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665_S1     1628  GSM9061665_S1  AAACCTGAGCTGTTCA-1   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665_S1     1313  GSM9061665_S1  AAACCTGAGGCATTGG-1   \n",
       "\n",
       "                                             barcode           cdr3_TRA  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665_S1  AAACCTGAGACTGTAA-1       CAVEARNYKLTF   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665_S1  AAACCTGAGCGTGAAC-1      CAASAVGNEKLTF   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665_S1  AAACCTGAGCTACCTA-1     CALSEAWGNARLMF   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665_S1  AAACCTGAGCTGTTCA-1  CALLGLKGEGSARQLTF   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665_S1  AAACCTGAGGCATTGG-1     CAVTGFSDGQKLLF   \n",
       "\n",
       "                                          cdr3_TRB j_gene_TRA j_gene_TRB  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665_S1     CASGTGLNTEAFF     TRAJ53    TRBJ1-1   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665_S1   CAWSALLGTVNGYTF     TRAJ48    TRBJ1-2   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665_S1    CASRSREETYEQYF     TRAJ31    TRBJ2-7   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665_S1  CASSLPPWRANTEAFF     TRAJ22    TRBJ1-1   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665_S1   CASSLTGEVWDEQFF     TRAJ16    TRBJ2-1   \n",
       "\n",
       "                                  v_gene_TRA v_gene_TRB  n_genes_by_counts  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665_S1  TRAV36/DV7    TRBV3-1               1379   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665_S1  TRAV29/DV5     TRBV30               1275   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665_S1      TRAV19      TRBV2                886   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665_S1     TRAV9-2   TRBV11-2               1628   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665_S1     TRAV8-6    TRBV5-1               1313   \n",
       "\n",
       "                                  total_counts  total_counts_mt  pct_counts_mt  \n",
       "AAACCTGAGACTGTAA-1-GSM9061665_S1        4637.0            157.0       3.385810  \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665_S1        4843.0            247.0       5.100144  \n",
       "AAACCTGAGCTACCTA-1-GSM9061665_S1        3076.0            280.0       9.102731  \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665_S1        4914.0            288.0       5.860806  \n",
       "AAACCTGAGGCATTGG-1-GSM9061665_S1        4947.0            198.0       4.002426  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.41 s, sys: 928 ms, total: 4.33 s\n",
      "Wall time: 4.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Integrate TCR data into AnnData.obs and perform quality control ---\n",
    "\n",
    "# Validate that adata exists\n",
    "if 'adata' not in globals():\n",
    "    raise NameError(\"adata is not defined. Please run the data loading cell first.\")\n",
    "if adata is None:\n",
    "    raise ValueError(\"adata is None. Data loading may have failed.\")\n",
    "\n",
    "# Check if TCR data exists and is not empty\n",
    "if 'full_tcr_df' in globals() and isinstance(full_tcr_df, pd.DataFrame) and not full_tcr_df.empty:\n",
    "    print(f\"Integrating TCR data into AnnData (TCR contigs: {len(full_tcr_df)}, cells: {adata.n_obs})...\")\n",
    "    \n",
    "    try:\n",
    "        # --- TCR Data Aggregation ---\n",
    "        # The previous join failed because one cell (barcode) can have multiple TCR contigs (e.g., TRA and TRB chains),\n",
    "        # creating a one-to-many join that increases the number of rows.\n",
    "        # The fix is to aggregate the TCR data to one row per cell *before* merging.\n",
    "\n",
    "        # 1. Filter for high-confidence, productive TRA/TRB chains.\n",
    "        # Only keep TCR contigs that are both high-confidence and productive, and are either TRA or TRB chains.\n",
    "        if 'high_confidence' not in full_tcr_df.columns or 'productive' not in full_tcr_df.columns or 'chain' not in full_tcr_df.columns:\n",
    "            print(\"WARNING: TCR dataframe missing required columns (high_confidence, productive, chain). Skipping TCR integration.\")\n",
    "            tcr_to_agg = pd.DataFrame()\n",
    "        else:\n",
    "            tcr_to_agg = full_tcr_df[\n",
    "                (full_tcr_df['high_confidence'] == True) &\n",
    "                (full_tcr_df['productive'] == True) &\n",
    "                (full_tcr_df['chain'].isin(['TRA', 'TRB']))\n",
    "            ].copy()\n",
    "\n",
    "        if not tcr_to_agg.empty:\n",
    "            # 2. Pivot the data to create one row per barcode, with columns for TRA and TRB data.\n",
    "            # This step ensures each cell (barcode) has its TRA and TRB info in separate columns.\n",
    "            tcr_aggregated = tcr_to_agg.pivot_table(\n",
    "                index=['sample_id', 'barcode'],\n",
    "                columns='chain',\n",
    "                values=['v_gene', 'j_gene', 'cdr3'],\n",
    "                aggfunc='first'  # 'first' is safe as we expect at most one productive TRA/TRB per cell\n",
    "            )\n",
    "\n",
    "            # 3. Flatten the multi-level column index (e.g., from ('v_gene', 'TRA') to 'v_gene_TRA')\n",
    "            tcr_aggregated.columns = ['_'.join(col).strip() for col in tcr_aggregated.columns.values]\n",
    "            tcr_aggregated.reset_index(inplace=True)\n",
    "\n",
    "            # --- DEBUG: Print sample formats to diagnose any mismatches ---\n",
    "            print(f\"  DEBUG: adata.obs sample_id examples: {adata.obs['sample_id'].unique()[:3].tolist()}\")\n",
    "            print(f\"  DEBUG: TCR sample_id examples: {tcr_aggregated['sample_id'].unique()[:3].tolist()}\")\n",
    "            \n",
    "            # 4. Prepare adata.obs for the merge by creating a matching barcode column.\n",
    "            # The index in adata.obs is like 'AGCCATGCAGCTGTTA-1-0' (barcode-concat_suffix).\n",
    "            # The barcode in TCR data is like 'AGCCATGCAGCTGTTA-1'.\n",
    "            adata.obs['barcode_for_merge'] = adata.obs.index.str.rsplit('-', n=1).str[0]\n",
    "            \n",
    "            # Handle case where sample_id might have been modified by concat (fallback fix)\n",
    "            # Extract just the GSM ID if sample_id contains underscores (e.g., \"GSM9061665_S1\" -> \"GSM9061665\")\n",
    "            if adata.obs['sample_id'].astype(str).str.contains('_').any():\n",
    "                print(\"  INFO: sample_id contains underscores, extracting GSM ID portion for merge...\")\n",
    "                adata.obs['sample_id_for_merge'] = adata.obs['sample_id'].astype(str).str.split('_').str[0]\n",
    "            else:\n",
    "                adata.obs['sample_id_for_merge'] = adata.obs['sample_id']\n",
    "            \n",
    "            print(f\"  DEBUG: adata barcode examples: {adata.obs['barcode_for_merge'].head(3).tolist()}\")\n",
    "            print(f\"  DEBUG: TCR barcode examples: {tcr_aggregated['barcode'].head(3).tolist()}\")\n",
    "\n",
    "            # 5. Perform a left merge. This keeps all cells from adata and adds TCR info where available.\n",
    "            # The number of rows will not change because tcr_aggregated has unique barcodes per sample.\n",
    "            original_obs = adata.obs.copy()\n",
    "            merged_obs = original_obs.merge(\n",
    "                tcr_aggregated,\n",
    "                left_on=['sample_id_for_merge', 'barcode_for_merge'],\n",
    "                right_on=['sample_id', 'barcode'],\n",
    "                how='left',\n",
    "                suffixes=('', '_tcr')\n",
    "            )\n",
    "            \n",
    "            # 6. Restore the original index to the merged dataframe.\n",
    "            merged_obs.index = original_obs.index\n",
    "            adata.obs = merged_obs\n",
    "            \n",
    "            # Clean up redundant columns from merge\n",
    "            cols_to_drop = [c for c in ['sample_id_tcr', 'sample_id_for_merge'] if c in adata.obs.columns]\n",
    "            if cols_to_drop:\n",
    "                adata.obs.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "            # Check how many cells got TCR info\n",
    "            tcr_col = 'v_gene_TRA' if 'v_gene_TRA' in adata.obs.columns else None\n",
    "            if tcr_col:\n",
    "                cells_with_tcr = (~adata.obs[tcr_col].isna()).sum()\n",
    "                print(f\"Successfully merged TCR data. Cells with TCR info: {cells_with_tcr} / {adata.n_obs}\")\n",
    "                \n",
    "                if cells_with_tcr == 0:\n",
    "                    print(\"WARNING: No cells matched TCR data! Check barcode/sample_id formats.\")\n",
    "                    print(\"  Skipping TCR filtering to preserve data.\")\n",
    "                else:\n",
    "                    # --- Filter for cells that have TCR information after the merge ---\n",
    "                    # Only keep cells with non-null v_gene_TRA (i.e., cells with high-confidence TCR data)\n",
    "                    initial_cells = adata.n_obs\n",
    "                    adata = adata[~adata.obs[tcr_col].isna()].copy()\n",
    "                    print(f\"Filtered from {initial_cells} to {adata.n_obs} cells based on having high-confidence TCR data.\")\n",
    "            else:\n",
    "                print(\"WARNING: TCR merge did not produce expected columns. Skipping TCR filtering.\")\n",
    "        else:\n",
    "            print(\"WARNING: No high-confidence productive TRA/TRB chains found in TCR data. Skipping TCR filtering.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"ERROR during TCR integration: {e}\")\n",
    "        traceback.print_exc()\n",
    "        print(\"Proceeding without TCR integration...\")\n",
    "else:\n",
    "    print(\"No TCR data available or full_tcr_df is empty. Proceeding without TCR integration...\")\n",
    "\n",
    "# --- Basic QC and filtering ---\n",
    "try:\n",
    "    print(f\"\\nPerforming QC filtering (starting with {adata.n_obs} cells, {adata.n_vars} genes)...\")\n",
    "    \n",
    "    # Filter out cells with fewer than 200 genes detected\n",
    "    sc.pp.filter_cells(adata, min_genes=200)\n",
    "    print(f\"  After min_genes filter: {adata.n_obs} cells\")\n",
    "    \n",
    "    # Filter out genes detected in fewer than 3 cells\n",
    "    sc.pp.filter_genes(adata, min_cells=3)\n",
    "    print(f\"  After min_cells filter: {adata.n_vars} genes\")\n",
    "\n",
    "    # Annotate mitochondrial genes for QC metrics\n",
    "    adata.var['mt'] = adata.var_names.str.startswith('MT-')\n",
    "    # Calculate QC metrics (e.g., percent mitochondrial genes)\n",
    "    sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "\n",
    "    print(\"\\nPost-QC AnnData object:\")\n",
    "    print(adata)\n",
    "    print(\"\\nSample metadata preview:\")\n",
    "    display(adata.obs.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR during QC filtering: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "371735db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:40:17.790291Z",
     "iopub.status.busy": "2026-02-06T05:40:17.789985Z",
     "iopub.status.idle": "2026-02-06T05:40:17.794508Z",
     "shell.execute_reply": "2026-02-06T05:40:17.793732Z"
    },
    "papermill": {
     "duration": 0.023763,
     "end_time": "2026-02-06T05:40:17.796007",
     "exception": false,
     "start_time": "2026-02-06T05:40:17.772244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding with TCR integration...\n"
     ]
    }
   ],
   "source": [
    "# MEMORY TIP: Save intermediate results to avoid reprocessing\n",
    "# Uncomment the lines below to save the concatenated data before TCR integration\n",
    "\n",
    "# output_dir = Path(\"/kaggle/working/Output\") if IS_KAGGLE else Path(\"../Output\")\n",
    "# output_dir.mkdir(exist_ok=True, parents=True)\n",
    "# checkpoint_file = output_dir / \"adata_concatenated_checkpoint.h5ad\"\n",
    "# \n",
    "# print(f\"Saving checkpoint to {checkpoint_file}...\")\n",
    "# adata.write_h5ad(checkpoint_file, compression='gzip')\n",
    "# print(f\"Checkpoint saved! File size: {checkpoint_file.stat().st_size / 1024**2:.2f} MB\")\n",
    "# \n",
    "# # To load this checkpoint later, use:\n",
    "# # adata = sc.read_h5ad(checkpoint_file)\n",
    "\n",
    "print(\"Proceeding with TCR integration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85723d5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:40:17.830832Z",
     "iopub.status.busy": "2026-02-06T05:40:17.830128Z",
     "iopub.status.idle": "2026-02-06T05:40:17.843758Z",
     "shell.execute_reply": "2026-02-06T05:40:17.842739Z"
    },
    "papermill": {
     "duration": 0.033631,
     "end_time": "2026-02-06T05:40:17.845807",
     "exception": false,
     "start_time": "2026-02-06T05:40:17.812176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Statistics ===\n",
      "Total cells: 38413\n",
      "Total genes: 14816\n",
      "\n",
      "Samples: 10\n",
      "sample_id\n",
      "GSM9061670    5310\n",
      "GSM9061674    5070\n",
      "GSM9061673    5045\n",
      "GSM9061671    4838\n",
      "GSM9061665    4008\n",
      "GSM9061666    3855\n",
      "GSM9061675    3774\n",
      "GSM9061667    3127\n",
      "GSM9061668    2471\n",
      "GSM9061669     915\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Patients: 4\n",
      "patient_id\n",
      "PT4    13889\n",
      "PT1    10990\n",
      "PT3    10148\n",
      "PT2     3386\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Response distribution:\n",
      "response\n",
      "Non-Responder    24037\n",
      "Responder        14376\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Timepoint distribution:\n",
      "timepoint\n",
      "Baseline      16834\n",
      "Post-Tx       14678\n",
      "Recurrence     6901\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Show basic statistics about the dataset ---\n",
    "\n",
    "# Validate that adata exists\n",
    "if 'adata' not in globals():\n",
    "    raise NameError(\"adata is not defined. Please run the data loading and QC cells first.\")\n",
    "if adata is None:\n",
    "    raise ValueError(\"adata is None. Data loading may have failed.\")\n",
    "\n",
    "print(\"=== Dataset Statistics ===\")\n",
    "print(f\"Total cells: {adata.n_obs}\")\n",
    "print(f\"Total genes: {adata.n_vars}\")\n",
    "\n",
    "if 'sample_id' in adata.obs.columns:\n",
    "    print(f\"\\nSamples: {adata.obs['sample_id'].nunique()}\")\n",
    "    print(adata.obs['sample_id'].value_counts())\n",
    "\n",
    "if 'patient_id' in adata.obs.columns:\n",
    "    print(f\"\\nPatients: {adata.obs['patient_id'].nunique()}\")\n",
    "    print(adata.obs['patient_id'].value_counts())\n",
    "\n",
    "if 'response' in adata.obs.columns:\n",
    "    print(f\"\\nResponse distribution:\")\n",
    "    print(adata.obs['response'].value_counts())\n",
    "\n",
    "if 'timepoint' in adata.obs.columns:\n",
    "    print(f\"\\nTimepoint distribution:\")\n",
    "    print(adata.obs['timepoint'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfe702",
   "metadata": {
    "papermill": {
     "duration": 0.017097,
     "end_time": "2026-02-06T05:40:17.881249",
     "exception": false,
     "start_time": "2026-02-06T05:40:17.864152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Install Additional Libraries for Advanced ML and Visualization\n",
    "\n",
    "Install and import libraries such as XGBoost, TensorFlow/Keras, scipy, and additional visualization tools for comprehensive ML analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb4ab538",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:40:17.918623Z",
     "iopub.status.busy": "2026-02-06T05:40:17.917842Z",
     "iopub.status.idle": "2026-02-06T05:41:18.966495Z",
     "shell.execute_reply": "2026-02-06T05:41:18.965592Z"
    },
    "papermill": {
     "duration": 61.085613,
     "end_time": "2026-02-06T05:41:18.985044",
     "exception": false,
     "start_time": "2026-02-06T05:40:17.899431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-06 05:40:43.117204: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770356443.295466      24 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770356443.348594      24 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770356443.802292      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770356443.802324      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770356443.802328      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770356443.802330      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional libraries installed!\n",
      "CPU times: user 27.2 s, sys: 2.73 s, total: 30 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Install required packages for genetic sequence encoding and ML ---\n",
    "%pip install biopython --quiet\n",
    "%pip install scikit-learn --quiet\n",
    "%pip install umap-learn --quiet\n",
    "%pip install hdbscan --quiet\n",
    "%pip install plotly --quiet\n",
    "%pip install xgboost --quiet\n",
    "%pip install tensorflow --quiet\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import ProtParam\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Import scipy for hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import umap\n",
    "import hdbscan\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"Additional libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c4414c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:41:19.021355Z",
     "iopub.status.busy": "2026-02-06T05:41:19.020589Z",
     "iopub.status.idle": "2026-02-06T05:41:19.534077Z",
     "shell.execute_reply": "2026-02-06T05:41:19.533152Z"
    },
    "papermill": {
     "duration": 0.533813,
     "end_time": "2026-02-06T05:41:19.535687",
     "exception": false,
     "start_time": "2026-02-06T05:41:19.001874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPUs detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Set memory growth for TensorFlow GPUs.\n",
      "Enabled mixed precision (mixed_float16).\n",
      "XGBoost version: 3.1.0\n",
      "XGBoost GPU support detected (device='cuda' API).\n",
      "Default param_grids defined early (can be overridden later).\n",
      "TF_GPU_AVAILABLE=True, MIXED_PRECISION=True, XGBOOST_GPU_AVAILABLE=True, CUML_AVAILABLE=False\n",
      "If models or param_grids are defined later, call _apply_gpu_patches() to apply GPU settings.\n"
     ]
    }
   ],
   "source": [
    "# --- GPU acceleration helper (minimal, safe) ---\n",
    "# Detect GPUs for TensorFlow, enable memory growth and mixed precision if available.\n",
    "# Detect XGBoost GPU support and cuML availability.\n",
    "# Provide a function _apply_gpu_patches() that will patch `models_eval` and `param_grids` in-place when they exist.\n",
    "\n",
    "TF_GPU_AVAILABLE = False\n",
    "MIXED_PRECISION_AVAILABLE = False\n",
    "XGBOOST_GPU_AVAILABLE = False\n",
    "CUML_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    TF_GPU_AVAILABLE = len(gpus) > 0\n",
    "    if TF_GPU_AVAILABLE:\n",
    "        print(\"TensorFlow GPUs detected:\", gpus)\n",
    "        try:\n",
    "            for g in gpus:\n",
    "                tf.config.experimental.set_memory_growth(g, True)\n",
    "            print(\"Set memory growth for TensorFlow GPUs.\")\n",
    "        except Exception as e:\n",
    "            print(\"Could not set memory growth:\", e)\n",
    "        # Try enabling mixed precision for faster FP16 compute on modern GPUs\n",
    "        try:\n",
    "            from tensorflow.keras import mixed_precision\n",
    "            mixed_precision.set_global_policy('mixed_float16')\n",
    "            MIXED_PRECISION_AVAILABLE = True\n",
    "            print(\"Enabled mixed precision (mixed_float16).\")\n",
    "        except Exception as e:\n",
    "            print(\"Mixed precision policy not enabled:\", e)\n",
    "    else:\n",
    "        print(\"No TensorFlow GPU detected.\")\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow import failed or no GPUs:\", e)\n",
    "\n",
    "# XGBoost GPU detection - supports both old (gpu_hist) and new (device='cuda') APIs\n",
    "XGBOOST_GPU_METHOD = None  # Will be 'device' for XGBoost 2.0+, 'tree_method' for older versions\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgb_version = tuple(int(x) for x in xgb.__version__.split('.')[:2])\n",
    "    print(f\"XGBoost version: {xgb.__version__}\")\n",
    "    \n",
    "    # XGBoost 2.0+ uses device='cuda', older uses tree_method='gpu_hist'\n",
    "    if xgb_version >= (2, 0):\n",
    "        try:\n",
    "            # Test new API\n",
    "            _ = xgb.XGBClassifier(device='cuda', n_estimators=1)\n",
    "            XGBOOST_GPU_AVAILABLE = True\n",
    "            XGBOOST_GPU_METHOD = 'device'\n",
    "            print(\"XGBoost GPU support detected (device='cuda' API).\")\n",
    "        except Exception as e:\n",
    "            print(f\"XGBoost 2.0+ GPU not available: {e}\")\n",
    "    else:\n",
    "        try:\n",
    "            # Test old API\n",
    "            _ = xgb.XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', n_estimators=1)\n",
    "            XGBOOST_GPU_AVAILABLE = True\n",
    "            XGBOOST_GPU_METHOD = 'tree_method'\n",
    "            print(\"XGBoost GPU support detected (tree_method='gpu_hist' API).\")\n",
    "        except Exception as e:\n",
    "            print(f\"XGBoost GPU not available: {e}\")\n",
    "except Exception as e:\n",
    "    print(\"XGBoost not importable:\", e)\n",
    "\n",
    "# cuML detection\n",
    "try:\n",
    "    import cuml\n",
    "    CUML_AVAILABLE = True\n",
    "    print(\"cuML is available.\")\n",
    "except Exception:\n",
    "    CUML_AVAILABLE = False\n",
    "\n",
    "# Utility: robust getter for adata.obsm with mask and padding\n",
    "def _get_obsm_or_zeros(adata, key, mask=None, n_cols=0):\n",
    "    \"\"\"\n",
    "    Return adata.obsm[key][mask] if present, otherwise zeros(shape=(n_rows, n_cols)).\n",
    "    Ensures output is a dense numpy array with n_cols columns (pads with zeros if needed).\n",
    "    \"\"\"\n",
    "    import numpy as _np\n",
    "    # Determine number of rows requested\n",
    "    if mask is not None:\n",
    "        try:\n",
    "            n_rows = int(mask.sum()) if hasattr(mask, 'sum') else int(sum(1 for v in mask if v))\n",
    "        except Exception:\n",
    "            n_rows = int(sum(1 for v in mask if v))\n",
    "    else:\n",
    "        n_rows = getattr(adata, 'n_obs', adata.shape[0]) if 'adata' in globals() else 0\n",
    "\n",
    "    if key in getattr(adata, 'obsm', {}):\n",
    "        arr = adata.obsm[key]\n",
    "        try:\n",
    "            if hasattr(arr, 'toarray'):\n",
    "                arr = arr.toarray()\n",
    "            arr = _np.asarray(arr)\n",
    "        except Exception:\n",
    "            return _np.zeros((n_rows, n_cols))\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            try:\n",
    "                arr = arr[mask]\n",
    "            except Exception:\n",
    "                arr = _np.array(arr)[mask]\n",
    "        # Pad or trim columns to n_cols if requested\n",
    "        if n_cols:\n",
    "            if arr.shape[1] < n_cols:\n",
    "                pad = _np.zeros((arr.shape[0], n_cols - arr.shape[1]))\n",
    "                arr = _np.hstack([arr, pad])\n",
    "            elif arr.shape[1] > n_cols:\n",
    "                arr = arr[:, :n_cols]\n",
    "        return arr\n",
    "    else:\n",
    "        return _np.zeros((n_rows, n_cols))\n",
    "\n",
    "# Define sensible default param_grids early so LOPO can see them (will be overridden later if redefined)\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [5, 10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "print(\"Default param_grids defined early (can be overridden later).\")\n",
    "\n",
    "# Patching helper (improved with signature filtering and XGBoost 2.0+ support)\n",
    "def _apply_gpu_patches():\n",
    "    import inspect\n",
    "    try:\n",
    "        # Check if models_eval exists before trying to access it\n",
    "        if 'models_eval' not in globals():\n",
    "            return  # Nothing to patch yet\n",
    "            \n",
    "        models_eval_ref = globals()['models_eval']\n",
    "        \n",
    "        # Patch XGBoost model to use GPU params when available and supported\n",
    "        if 'XGBoost' in models_eval_ref and XGBOOST_GPU_AVAILABLE:\n",
    "            try:\n",
    "                import xgboost as xgb_mod\n",
    "                m = models_eval_ref['XGBoost']\n",
    "                params = m.get_params() if hasattr(m, 'get_params') else {}\n",
    "                # Determine class to instantiate (prefer wrapper if provided)\n",
    "                XGBClass = globals().get('XGBClassifierSK', getattr(xgb_mod, 'XGBClassifier', None))\n",
    "                if XGBClass is None:\n",
    "                    raise ImportError('xgboost.XGBClassifier not found')\n",
    "                # Build filtered params list based on constructor signature\n",
    "                sig = inspect.signature(XGBClass.__init__)\n",
    "                accepts_kwargs = any(p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values())\n",
    "                allowed = set(sig.parameters.keys())\n",
    "                filtered_params = {}\n",
    "                for k, v in params.items():\n",
    "                    if accepts_kwargs or k in allowed:\n",
    "                        filtered_params[k] = v\n",
    "                \n",
    "                # Add GPU params based on XGBoost version (2.0+ uses device, older uses tree_method)\n",
    "                xgb_gpu_method = globals().get('XGBOOST_GPU_METHOD', 'tree_method')\n",
    "                if xgb_gpu_method == 'device':\n",
    "                    # XGBoost 2.0+ API\n",
    "                    if accepts_kwargs or 'device' in allowed:\n",
    "                        filtered_params['device'] = 'cuda'\n",
    "                    # Remove old-style params if present\n",
    "                    filtered_params.pop('tree_method', None)\n",
    "                    filtered_params.pop('predictor', None)\n",
    "                else:\n",
    "                    # Old XGBoost API\n",
    "                    if accepts_kwargs or 'tree_method' in allowed:\n",
    "                        filtered_params['tree_method'] = 'gpu_hist'\n",
    "                    if accepts_kwargs or 'predictor' in allowed:\n",
    "                        filtered_params['predictor'] = 'gpu_predictor'\n",
    "                \n",
    "                # Remove unsupported keys\n",
    "                filtered_params.pop('gpu_id', None)\n",
    "                try:\n",
    "                    models_eval_ref['XGBoost'] = XGBClass(**filtered_params)\n",
    "                    print(f\"Patched models_eval['XGBoost'] to use GPU (method={xgb_gpu_method}).\")\n",
    "                except TypeError as e:\n",
    "                    # Fallback: try removing GPU-specific params and re-instantiate\n",
    "                    for k in ['tree_method', 'predictor', 'device']:\n",
    "                        filtered_params.pop(k, None)\n",
    "                    fallback_params = {k: v for k, v in filtered_params.items() if accepts_kwargs or k in allowed}\n",
    "                    models_eval_ref['XGBoost'] = XGBClass(**fallback_params)\n",
    "                    print(\"Patched models_eval['XGBoost'] without GPU params due to TypeError:\", e)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to patch models_eval['XGBoost']:\", e)\n",
    "            # Patch Random Forest to use n_jobs=-1 when possible\n",
    "            if 'Random Forest' in models_eval_ref:\n",
    "                try:\n",
    "                    from sklearn.ensemble import RandomForestClassifier\n",
    "                    m = models_eval_ref['Random Forest']\n",
    "                    params = m.get_params() if hasattr(m, 'get_params') else {}\n",
    "                    params.setdefault('n_jobs', -1)\n",
    "                    RFC = RandomForestClassifier\n",
    "                    sig_rfc = inspect.signature(RFC.__init__)\n",
    "                    accepts_kwargs_rfc = any(p.kind == inspect.Parameter.VAR_KEYWORD for p in sig_rfc.parameters.values())\n",
    "                    allowed_rfc = set(sig_rfc.parameters.keys())\n",
    "                    filtered_rfc_params = {k: v for k, v in params.items() if accepts_kwargs_rfc or k in allowed_rfc}\n",
    "                    models_eval_ref['Random Forest'] = RandomForestClassifier(**filtered_rfc_params)\n",
    "                    print(\"Patched models_eval['Random Forest'] to use n_jobs=-1.\")\n",
    "                except Exception as e:\n",
    "                    print(\"Failed to patch models_eval['Random Forest']:\", e)\n",
    "    except Exception as e:\n",
    "        print(\"Error patching models_eval:\", e)\n",
    "\n",
    "    # Patch param_grids for XGBoost if available\n",
    "    try:\n",
    "        if 'param_grids' in globals() and XGBOOST_GPU_AVAILABLE:\n",
    "            pg = param_grids.get('XGBoost', {})\n",
    "            xgb_gpu_method = globals().get('XGBOOST_GPU_METHOD', 'tree_method')\n",
    "            if xgb_gpu_method == 'device':\n",
    "                # XGBoost 2.0+ uses device parameter\n",
    "                if any(k.startswith('clf__') for k in pg.keys()):\n",
    "                    pg.setdefault('clf__device', ['cuda'])\n",
    "                else:\n",
    "                    pg.setdefault('device', ['cuda'])\n",
    "            else:\n",
    "                # Old XGBoost uses tree_method\n",
    "                if any(k.startswith('clf__') for k in pg.keys()):\n",
    "                    pg.setdefault('clf__tree_method', ['gpu_hist'])\n",
    "                    pg.setdefault('clf__predictor', ['gpu_predictor'])\n",
    "                else:\n",
    "                    pg.setdefault('tree_method', ['gpu_hist'])\n",
    "                    pg.setdefault('predictor', ['gpu_predictor'])\n",
    "            param_grids['XGBoost'] = pg\n",
    "            print(f\"Patched param_grids['XGBoost'] with GPU options (method={xgb_gpu_method}).\")\n",
    "    except Exception as e:\n",
    "        print(\"Error patching param_grids:\", e)\n",
    "\n",
    "# Apply patches now if models/param grids already defined\n",
    "_apply_gpu_patches()\n",
    "\n",
    "print(f\"TF_GPU_AVAILABLE={TF_GPU_AVAILABLE}, MIXED_PRECISION={MIXED_PRECISION_AVAILABLE}, XGBOOST_GPU_AVAILABLE={XGBOOST_GPU_AVAILABLE}, CUML_AVAILABLE={CUML_AVAILABLE}\")\n",
    "print(\"If models or param_grids are defined later, call _apply_gpu_patches() to apply GPU settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a14db898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:41:19.572190Z",
     "iopub.status.busy": "2026-02-06T05:41:19.571436Z",
     "iopub.status.idle": "2026-02-06T05:41:19.579783Z",
     "shell.execute_reply": "2026-02-06T05:41:19.578923Z"
    },
    "papermill": {
     "duration": 0.028561,
     "end_time": "2026-02-06T05:41:19.581258",
     "exception": false,
     "start_time": "2026-02-06T05:41:19.552697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined XGBoost sklearn-compatible wrapper: XGBClassifierSK (XGBoost version 3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# --- Define a sklearn-compatible XGBoost wrapper (supports both old and new XGBoost APIs) ---\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgb_version = tuple(int(x) for x in xgb.__version__.split('.')[:2])\n",
    "    \n",
    "    class XGBClassifierSK(xgb.XGBClassifier):\n",
    "        \"\"\"XGBoost wrapper that handles both old (tree_method) and new (device) APIs.\"\"\"\n",
    "        def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=6, random_state=None,\n",
    "                     use_label_encoder=False, eval_metric='logloss',\n",
    "                     tree_method=None, predictor=None, device=None, **kwargs):\n",
    "            # Handle XGBoost 2.0+ API vs older versions\n",
    "            if xgb_version >= (2, 0):\n",
    "                # New API: use 'device' parameter\n",
    "                if device is not None:\n",
    "                    kwargs['device'] = device\n",
    "                # tree_method and predictor are deprecated in 2.0+\n",
    "            else:\n",
    "                # Old API: use tree_method/predictor\n",
    "                if tree_method is not None:\n",
    "                    kwargs.setdefault('tree_method', tree_method)\n",
    "                if predictor is not None:\n",
    "                    kwargs.setdefault('predictor', predictor)\n",
    "            \n",
    "            # Remove deprecated parameters that might cause warnings\n",
    "            kwargs.pop('use_label_encoder', None)\n",
    "            \n",
    "            super().__init__(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth,\n",
    "                             random_state=random_state, eval_metric=eval_metric, **kwargs)\n",
    "    \n",
    "    globals()['XGBClassifierSK'] = XGBClassifierSK\n",
    "    print(f'Defined XGBoost sklearn-compatible wrapper: XGBClassifierSK (XGBoost version {xgb.__version__})')\n",
    "except Exception as e:\n",
    "    print('Failed to define XGBClassifierSK:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93d7222c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:41:19.616304Z",
     "iopub.status.busy": "2026-02-06T05:41:19.615589Z",
     "iopub.status.idle": "2026-02-06T05:41:19.621746Z",
     "shell.execute_reply": "2026-02-06T05:41:19.621044Z"
    },
    "papermill": {
     "duration": 0.024904,
     "end_time": "2026-02-06T05:41:19.623203",
     "exception": false,
     "start_time": "2026-02-06T05:41:19.598299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using raw_data_dir = /kaggle/working/Data/GSE300475_RAW\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Determine data directory consistently (prefer existing download_dir when present)\n",
    "if 'download_dir' in globals() and download_dir:\n",
    "    data_dir = Path(download_dir)\n",
    "elif IS_KAGGLE:\n",
    "    data_dir = Path('/kaggle/working/Data')\n",
    "else:\n",
    "    data_dir = Path('../Data')\n",
    "\n",
    "raw_data_dir = data_dir / 'GSE300475_RAW'\n",
    "raw_data_dir = raw_data_dir.resolve()\n",
    "\n",
    "# Ensure directory exists (no-op if not writing yet)\n",
    "os.makedirs(raw_data_dir, exist_ok=True)\n",
    "print(f\"Using raw_data_dir = {raw_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddfe633a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:41:19.657870Z",
     "iopub.status.busy": "2026-02-06T05:41:19.657568Z",
     "iopub.status.idle": "2026-02-06T05:41:19.663884Z",
     "shell.execute_reply": "2026-02-06T05:41:19.663136Z"
    },
    "papermill": {
     "duration": 0.025295,
     "end_time": "2026-02-06T05:41:19.665238",
     "exception": false,
     "start_time": "2026-02-06T05:41:19.639943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched sklearn.model_selection.LeaveOneGroupOut to auto-apply GPU patches on init\n"
     ]
    }
   ],
   "source": [
    "# --- Auto-apply GPU patches when LOPO is instantiated ---\n",
    "try:\n",
    "    import sklearn.model_selection as _skms\n",
    "    if not getattr(_skms, '_LO_patched_applied', False):\n",
    "        _LO_orig = _skms.LeaveOneGroupOut\n",
    "        class _LO_patched(_LO_orig):\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                # Ensure GPU patches are applied just before LOPO is constructed\n",
    "                try:\n",
    "                    _apply_gpu_patches()\n",
    "                except Exception as _e:\n",
    "                    print('Warning: _apply_gpu_patches failed during LOPO patching:', _e)\n",
    "                super().__init__(*args, **kwargs)\n",
    "        _skms.LeaveOneGroupOut = _LO_patched\n",
    "        _skms._LO_patched_applied = True\n",
    "        print('Patched sklearn.model_selection.LeaveOneGroupOut to auto-apply GPU patches on init')\n",
    "    else:\n",
    "        print('LOPO patch already applied')\n",
    "except Exception as e:\n",
    "    print('Failed to apply LOPO patch:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c536149d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:41:19.700520Z",
     "iopub.status.busy": "2026-02-06T05:41:19.699970Z",
     "iopub.status.idle": "2026-02-06T05:42:02.017941Z",
     "shell.execute_reply": "2026-02-06T05:42:02.017017Z"
    },
    "papermill": {
     "duration": 42.337814,
     "end_time": "2026-02-06T05:42:02.019698",
     "exception": false,
     "start_time": "2026-02-06T05:41:19.681884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Loading...\n",
      "Searching for data in: /kaggle/working/Data/GSE300475_RAW\n",
      "Processing GSM9061673_S9_matrix.mtx.gz...\n",
      "Loaded 11480 cells from GSM9061673_S9_\n",
      "Processing GSM9061669_S5_matrix.mtx.gz...\n",
      "Loaded 2912 cells from GSM9061669_S5_\n",
      "Processing GSM9061674_S10_matrix.mtx.gz...\n",
      "Loaded 9704 cells from GSM9061674_S10_\n",
      "Processing GSM9061671_S7_matrix.mtx.gz...\n",
      "Loaded 9330 cells from GSM9061671_S7_\n",
      "Processing GSM9061670_S6_matrix.mtx.gz...\n",
      "Loaded 10398 cells from GSM9061670_S6_\n",
      "Processing GSM9061668_S4_matrix.mtx.gz...\n",
      "Loaded 8723 cells from GSM9061668_S4_\n",
      "Processing GSM9061675_S11_matrix.mtx.gz...\n",
      "Loaded 9330 cells from GSM9061675_S11_\n",
      "Processing GSM9061667_S3_matrix.mtx.gz...\n",
      "Loaded 7358 cells from GSM9061667_S3_\n",
      "Processing GSM9061666_S2_matrix.mtx.gz...\n",
      "Loaded 9069 cells from GSM9061666_S2_\n",
      "Processing GSM9061665_S1_matrix.mtx.gz...\n",
      "Loaded 8931 cells from GSM9061665_S1_\n",
      "Processing GSM9061672_S8_matrix.mtx.gz...\n",
      "Loaded 12832 cells from GSM9061672_S8_\n",
      "Combined AnnData object created: (100067, 36607)\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loading (Robust) ---\n",
    "import scanpy as sc\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def _first_existing(paths):\n",
    "    for p in paths:\n",
    "        if p and os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def _glob_pick(folder, patterns, key=None):\n",
    "    matches = []\n",
    "    for pat in patterns:\n",
    "        matches.extend(glob.glob(os.path.join(folder, pat)))\n",
    "    matches = sorted(set(matches))\n",
    "    if key:\n",
    "        key_matches = [m for m in matches if key in os.path.basename(m)]\n",
    "        if len(key_matches) == 1:\n",
    "            return key_matches[0]\n",
    "        if len(key_matches) > 1:\n",
    "            return key_matches[0]\n",
    "    if len(matches) == 1:\n",
    "        return matches[0]\n",
    "    return None\n",
    "\n",
    "print(\"Starting Data Loading...\")\n",
    "\n",
    "# Determine data directory (using extract_dir from Cell 7 if available)\n",
    "if 'extract_dir' not in globals():\n",
    "    # Fallback path logic matching Cell 7/8\n",
    "    base_dir = '/kaggle/working/Data' if IS_KAGGLE else '../Data'\n",
    "    extract_dir = os.path.join(base_dir, \"GSE300475_RAW\")\n",
    "\n",
    "if not os.path.exists(extract_dir):\n",
    "    print(f\"Warning: Directory {extract_dir} does not exist. Please ensure Cell 7 ran successfully.\")\n",
    "else:\n",
    "    print(f\"Searching for data in: {extract_dir}\")\n",
    "    # Find all matrix files\n",
    "    matrix_files = glob.glob(os.path.join(extract_dir, \"*matrix.mtx*\"))\n",
    "    # Also look recursively if structure is nested\n",
    "    if not matrix_files:\n",
    "        matrix_files = glob.glob(os.path.join(extract_dir, \"**\", \"*matrix.mtx*\"), recursive=True)\n",
    "\n",
    "    adata_list = []\n",
    "    \n",
    "    if not matrix_files:\n",
    "        print(\"No matrix.mtx files found or previously loaded.\")\n",
    "        # Check if we can proceed? If this is a re-run, adata might exist.\n",
    "    else:\n",
    "        for mat_file in matrix_files:\n",
    "            try:\n",
    "                print(f\"Processing {os.path.basename(mat_file)}...\")\n",
    "                # Handle formatted loading\n",
    "                # If file is standard 10x-like (matrix.mtx, genes.tsv, barcodes.tsv) in same folder\n",
    "                folder = os.path.dirname(mat_file)\n",
    "                prefix = os.path.basename(mat_file).replace('matrix.mtx', '').replace('.gz', '')\n",
    "                key = prefix.strip('_')\n",
    "                \n",
    "                # Check for accompanying files with same prefix\n",
    "                genes_path = _first_existing([\n",
    "                    os.path.join(folder, prefix + 'genes.tsv'),\n",
    "                    os.path.join(folder, prefix + 'features.tsv'),\n",
    "                    os.path.join(folder, prefix + 'genes.tsv.gz'),\n",
    "                    os.path.join(folder, prefix + 'features.tsv.gz'),\n",
    "                ])\n",
    "                \n",
    "                barcodes_path = _first_existing([\n",
    "                    os.path.join(folder, prefix + 'barcodes.tsv'),\n",
    "                    os.path.join(folder, prefix + 'barcodes.tsv.gz'),\n",
    "                ])\n",
    "\n",
    "                # Fallback to un-prefixed standard 10x naming\n",
    "                if not genes_path:\n",
    "                    genes_path = _first_existing([\n",
    "                        os.path.join(folder, 'genes.tsv'),\n",
    "                        os.path.join(folder, 'features.tsv'),\n",
    "                        os.path.join(folder, 'genes.tsv.gz'),\n",
    "                        os.path.join(folder, 'features.tsv.gz'),\n",
    "                    ])\n",
    "\n",
    "                if not barcodes_path:\n",
    "                    barcodes_path = _first_existing([\n",
    "                        os.path.join(folder, 'barcodes.tsv'),\n",
    "                        os.path.join(folder, 'barcodes.tsv.gz'),\n",
    "                    ])\n",
    "\n",
    "                # Fallback to any matching files in the folder (use key if present)\n",
    "                if not genes_path:\n",
    "                    genes_path = _glob_pick(folder, ['*genes.tsv*', '*features.tsv*'], key=key)\n",
    "                if not barcodes_path:\n",
    "                    barcodes_path = _glob_pick(folder, ['*barcodes.tsv*'], key=key)\n",
    "\n",
    "                if genes_path and barcodes_path and os.path.exists(genes_path) and os.path.exists(barcodes_path):\n",
    "                    # Load using read_mtx for flexibility with filenames\n",
    "                    adata_sample = sc.read_mtx(mat_file).T\n",
    "                    \n",
    "                    # Annotation\n",
    "                    genes = pd.read_csv(genes_path, sep='\\t', header=None)\n",
    "                    barcodes = pd.read_csv(barcodes_path, sep='\\t', header=None)\n",
    "                    \n",
    "                    # Assign var/obs names and sanitize whitespace\n",
    "                    if genes.shape[1] > 1:\n",
    "                        var_names = genes.iloc[:,1].astype(str).str.strip().values\n",
    "                        adata_sample.var['gene_ids'] = genes.iloc[:,0].astype(str).values\n",
    "                    else:\n",
    "                        var_names = genes.iloc[:,0].astype(str).str.strip().values\n",
    "                    adata_sample.var_names = pd.Index(var_names)\n",
    "                    adata_sample.obs_names = pd.Index(barcodes.iloc[:,0].astype(str).str.strip().values)\n",
    "                    adata_sample.obs['sample_id'] = prefix.strip('_') if prefix else os.path.basename(folder)\n",
    "                    \n",
    "                    # Ensure uniqueness within sample to avoid concat Index errors\n",
    "                    try:\n",
    "                        adata_sample.var_names_make_unique()\n",
    "                        adata_sample.obs_names_make_unique()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    \n",
    "                    adata_list.append(adata_sample)\n",
    "                    print(f\"Loaded {adata_sample.shape[0]} cells from {prefix or folder}\")\n",
    "                else:\n",
    "                    print(f\"Skipping {mat_file}: Missing genes/barcodes files (searched prefix '{prefix}' and fallbacks)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {mat_file}: {e}\")\n",
    "\n",
    "        # Pre-sanitize all adata samples before concatenation\n",
    "        for a in adata_list:\n",
    "            try:\n",
    "                a.var_names = pd.Index([str(v).strip() for v in a.var_names])\n",
    "                a.var_names_make_unique()\n",
    "                a.obs_names = pd.Index([str(v).strip() for v in a.obs_names])\n",
    "                a.obs_names_make_unique()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if adata_list:\n",
    "            # Concatenate all samples\n",
    "            try:\n",
    "                adata = sc.concat(adata_list, join='outer')\n",
    "            except Exception as e:\n",
    "                print('sc.concat failed:', e)\n",
    "                # Try fallback using AnnData.concatenate with batch info\n",
    "                try:\n",
    "                    loaded_batches = [a.obs['sample_id'].unique()[0] for a in adata_list]\n",
    "                except Exception:\n",
    "                    loaded_batches = None\n",
    "                try:\n",
    "                    if loaded_batches:\n",
    "                        adata = sc.AnnData.concatenate(*adata_list, join='outer', batch_key='sample_id', batch_categories=loaded_batches)\n",
    "                    else:\n",
    "                        adata = sc.AnnData.concatenate(*adata_list, join='outer', batch_key='sample_id')\n",
    "                except Exception as e2:\n",
    "                    raise RuntimeError(f\"Failed to concatenate AnnData objects: {e}; fallback failed: {e2}\")\n",
    "            adata.obs_names_make_unique()\n",
    "            # Basic fallback for mitochondrial genes logic (used later)\n",
    "            adata.var['mt'] = adata.var_names.str.startswith('MT-')\n",
    "            sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "            print(f\"Combined AnnData object created: {adata.shape}\")\n",
    "        else:\n",
    "            print(\"Warning: No valid data loaded into adata.\")\n",
    "\n",
    "# Ensure adata exists to prevent downstream crashes\n",
    "if 'adata' not in globals():\n",
    "    print(\"CRITICAL CHECK: adata variable not defined. Downstream cells will fail.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7269ff2",
   "metadata": {
    "papermill": {
     "duration": 0.017136,
     "end_time": "2026-02-06T05:42:02.054534",
     "exception": false,
     "start_time": "2026-02-06T05:42:02.037398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Genetic Sequence Encoding Functions\n",
    "\n",
    "Define functions for one-hot encoding, k-mer encoding, and physicochemical features extraction for TCR sequences and gene expression patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d9fb5dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:42:02.091274Z",
     "iopub.status.busy": "2026-02-06T05:42:02.090991Z",
     "iopub.status.idle": "2026-02-06T05:42:02.103730Z",
     "shell.execute_reply": "2026-02-06T05:42:02.102974Z"
    },
    "papermill": {
     "duration": 0.03324,
     "end_time": "2026-02-06T05:42:02.105509",
     "exception": false,
     "start_time": "2026-02-06T05:42:02.072269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic sequence encoding functions defined successfully!\n",
      "CPU times: user 71 ¬µs, sys: 4 ¬µs, total: 75 ¬µs\n",
      "Wall time: 78.4 ¬µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Genetic Sequence Encoding Functions ---\n",
    "\n",
    "def one_hot_encode_sequence(sequence, max_length=50, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    One-hot encode a protein/nucleotide sequence.\n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        max_length: Maximum sequence length (pad or truncate)\n",
    "        alphabet: Valid characters in the sequence\n",
    "    Returns:\n",
    "        2D numpy array of shape (max_length, len(alphabet))\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    sequence = str(sequence).upper()[:max_length]  # Truncate if too long\n",
    "    encoding = np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    for i, char in enumerate(sequence):\n",
    "        if char in alphabet:\n",
    "            char_idx = alphabet.index(char)\n",
    "            encoding[i, char_idx] = 1\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "def kmer_encode_sequence(sequence, k=3, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    K-mer encoding of sequences.\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {}\n",
    "    \n",
    "    sequence = str(sequence).upper()\n",
    "    kmers = [sequence[i:i+k] for i in range(len(sequence)-k+1)]\n",
    "    valid_kmers = [kmer for kmer in kmers if all(c in alphabet for c in kmer)]\n",
    "    \n",
    "    return Counter(valid_kmers)\n",
    "\n",
    "def physicochemical_features(sequence):\n",
    "    \"\"\"\n",
    "    Extract physicochemical properties from protein sequences.\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {\n",
    "            'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        seq = str(sequence).upper()\n",
    "        # Remove non-standard amino acids\n",
    "        seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "        \n",
    "        if len(seq) == 0:\n",
    "            return {\n",
    "                'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "                'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "            }\n",
    "        \n",
    "        bio_seq = Seq(seq)\n",
    "        analyzer = ProtParam.ProteinAnalysis(str(bio_seq))\n",
    "        \n",
    "        return {\n",
    "            'length': len(seq),\n",
    "            'molecular_weight': analyzer.molecular_weight(),\n",
    "            'aromaticity': analyzer.aromaticity(),\n",
    "            'instability_index': analyzer.instability_index(),\n",
    "            'isoelectric_point': analyzer.isoelectric_point(),\n",
    "            'hydrophobicity': analyzer.gravy()\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'length': len(str(sequence)) if not pd.isna(sequence) else 0,\n",
    "            'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "\n",
    "def encode_gene_expression_patterns(adata, n_top_genes=1000, train_mask=None):\n",
    "    \"\"\"\n",
    "    Encode gene expression patterns using various dimensionality reduction techniques.\n",
    "    \n",
    "    IMPORTANT: To avoid data leakage, pass train_mask to fit transformers only on training data.\n",
    "    If train_mask is None, fits on all data (use only for exploration, not CV).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (encodings dict, X_scaled array)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA, TruncatedSVD\n",
    "    import umap\n",
    "    \n",
    "    # Get highly variable genes if not already computed\n",
    "    if 'highly_variable' not in adata.var.columns:\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, subset=False)\n",
    "    \n",
    "    # Extract expression matrix for highly variable genes - ensure boolean mask\n",
    "    hvg_mask = np.array(adata.var['highly_variable'].values, dtype=bool)\n",
    "    \n",
    "    # Subset adata by HVG mask\n",
    "    X_full = adata.X\n",
    "    if hasattr(X_full, 'toarray'):\n",
    "        X_full = X_full.toarray()\n",
    "    else:\n",
    "        X_full = np.asarray(X_full)\n",
    "    \n",
    "    # Select HVG columns\n",
    "    X_hvg = X_full[:, hvg_mask]\n",
    "    \n",
    "    # Clean Infs/NaNs (robustness fix)\n",
    "    X_hvg = np.nan_to_num(X_hvg, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # Standardize the data - FIT ONLY ON TRAINING DATA if mask provided\n",
    "    scaler = StandardScaler()\n",
    "    if train_mask is not None:\n",
    "        scaler.fit(X_hvg[train_mask])\n",
    "        X_scaled = scaler.transform(X_hvg)\n",
    "    else:\n",
    "        X_scaled = scaler.fit_transform(X_hvg)\n",
    "    \n",
    "    encodings = {}\n",
    "    \n",
    "    # PCA encoding - FIT ONLY ON TRAINING DATA if mask provided\n",
    "    n_pca = min(50, X_scaled.shape[1], X_scaled.shape[0])\n",
    "    pca = PCA(n_components=n_pca)\n",
    "    if train_mask is not None:\n",
    "        pca.fit(X_scaled[train_mask])\n",
    "        encodings['pca'] = pca.transform(X_scaled)\n",
    "    else:\n",
    "        encodings['pca'] = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # TruncatedSVD for sparse matrices\n",
    "    n_svd = min(50, X_scaled.shape[1] - 1, X_scaled.shape[0] - 1)\n",
    "    if n_svd > 0:\n",
    "        svd = TruncatedSVD(n_components=n_svd, random_state=42)\n",
    "        if train_mask is not None:\n",
    "            svd.fit(X_scaled[train_mask])\n",
    "            encodings['svd'] = svd.transform(X_scaled)\n",
    "        else:\n",
    "            encodings['svd'] = svd.fit_transform(X_scaled)\n",
    "    else:\n",
    "        encodings['svd'] = np.zeros((X_scaled.shape[0], 1))\n",
    "    \n",
    "    # UMAP encoding - UMAP doesn't support clean fit/transform easily for this pipeline, usually unsupervised\n",
    "    try:\n",
    "        umap_encoder = umap.UMAP(n_components=20, random_state=42)\n",
    "        encodings['umap'] = umap_encoder.fit_transform(X_scaled)\n",
    "    except Exception as e:\n",
    "        print(f\"UMAP failed: {e}\")\n",
    "        encodings['umap'] = np.zeros((X_scaled.shape[0], 20))\n",
    "    \n",
    "    return encodings, X_scaled\n",
    "\n",
    "print(\"Genetic sequence encoding functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42137b90",
   "metadata": {
    "papermill": {
     "duration": 0.018254,
     "end_time": "2026-02-06T05:42:02.142269",
     "exception": false,
     "start_time": "2026-02-06T05:42:02.124015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Apply Sequence Encoding to TCR CDR3 Sequences\n",
    "\n",
    "Encode TRA and TRB CDR3 sequences using one-hot, k-mer, and physicochemical methods, and add to AnnData.obsm and obs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab450981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:42:02.180795Z",
     "iopub.status.busy": "2026-02-06T05:42:02.180506Z",
     "iopub.status.idle": "2026-02-06T05:42:04.583253Z",
     "shell.execute_reply": "2026-02-06T05:42:04.581986Z"
    },
    "papermill": {
     "duration": 2.424648,
     "end_time": "2026-02-06T05:42:04.585098",
     "exception": false,
     "start_time": "2026-02-06T05:42:02.160450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-optimized TCR sequence encoding...\n",
      "Warning: cdr3_TRA column not found, creating empty column\n",
      "Warning: cdr3_TRB column not found, creating empty column\n",
      "TRA sequences: 100067, TRB sequences: 100067\n",
      "TRA k-mer sparse shape: (100067, 1)\n",
      "TRB k-mer sparse shape: (100067, 1)\n",
      "TRA k-mer reduced shape: (100067, 1)\n",
      "TRB k-mer reduced shape: (100067, 1)\n",
      "TRA one-hot shape: (100067, 15, 20) (dtype: float16)\n",
      "TRB one-hot shape: (100067, 15, 20) (dtype: float16)\n",
      "TCR sequence encoding complete and stored in adata.obsm\n",
      "Memory usage reduced by using sparse matrices and dimension reduction\n",
      "CPU times: user 2.34 s, sys: 58.9 ms, total: 2.4 s\n",
      "Wall time: 2.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- MEMORY-OPTIMIZED TCR Sequence Encoding ---\n",
    "\n",
    "# Validate that adata exists\n",
    "if 'adata' not in globals():\n",
    "    raise NameError(\"adata is not defined. Please run the data loading and QC cells first.\")\n",
    "if adata is None:\n",
    "    raise ValueError(\"adata is None. Data loading may have failed.\")\n",
    "\n",
    "print(\"Starting memory-optimized TCR sequence encoding...\")\n",
    "import gc\n",
    "\n",
    "# Extract TCR CDR3 sequences with robust column handling\n",
    "# Check for column existence and normalize naming\n",
    "if 'cdr3_TRA' not in adata.obs.columns:\n",
    "    if 'CDR3_TRA' in adata.obs.columns:\n",
    "        adata.obs['cdr3_TRA'] = adata.obs['CDR3_TRA']\n",
    "    else:\n",
    "        print(\"Warning: cdr3_TRA column not found, creating empty column\")\n",
    "        adata.obs['cdr3_TRA'] = ''\n",
    "\n",
    "if 'cdr3_TRB' not in adata.obs.columns:\n",
    "    if 'CDR3_TRB' in adata.obs.columns:\n",
    "        adata.obs['cdr3_TRB'] = adata.obs['CDR3_TRB']\n",
    "    else:\n",
    "        print(\"Warning: cdr3_TRB column not found, creating empty column\")\n",
    "        adata.obs['cdr3_TRB'] = ''\n",
    "\n",
    "tra_seqs = adata.obs['cdr3_TRA'].fillna('').astype(str).values\n",
    "trb_seqs = adata.obs['cdr3_TRB'].fillna('').astype(str).values\n",
    "\n",
    "print(f\"TRA sequences: {len(tra_seqs)}, TRB sequences: {len(trb_seqs)}\")\n",
    "\n",
    "# MEMORY FIX: Use smaller k-mer sizes and reduced dimensionality\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Reduce k-mer size from 3 to 2 to reduce feature space\n",
    "def _kmer_list(seq, k=2):  # Changed from k=3 to k=2\n",
    "    if len(seq) < k:\n",
    "        return []\n",
    "    return [seq[i:i+k] for i in range(len(seq) - k + 1)]\n",
    "\n",
    "# Convert sequences to k-mer strings\n",
    "tra_kmer_docs = [' '.join(_kmer_list(s, k=2)) for s in tra_seqs]  # k=2 instead of 3\n",
    "trb_kmer_docs = [' '.join(_kmer_list(s, k=2)) for s in trb_seqs]\n",
    "\n",
    "# MEMORY FIX: Limit max features to reduce dimensionality\n",
    "vec_tra = CountVectorizer(max_features=500)  # Limit to 500 features instead of unlimited\n",
    "vec_trb = CountVectorizer(max_features=500)\n",
    "\n",
    "tra_kmer_sparse = vec_tra.fit_transform(tra_kmer_docs)\n",
    "trb_kmer_sparse = vec_trb.fit_transform(trb_kmer_docs)\n",
    "\n",
    "# Clean up k-mer docs (no longer needed)\n",
    "del tra_kmer_docs, trb_kmer_docs\n",
    "gc.collect()\n",
    "\n",
    "print(f\"TRA k-mer sparse shape: {tra_kmer_sparse.shape}\")\n",
    "print(f\"TRB k-mer sparse shape: {trb_kmer_sparse.shape}\")\n",
    "\n",
    "# MEMORY FIX: Reduce dimensions even further using SVD\n",
    "def _reduce_sparse(sparse_mat, n_components=50):  # Reduced from 200 to 50\n",
    "    n_comp = min(n_components, max(1, sparse_mat.shape[1]-1))\n",
    "    try:\n",
    "        svd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "        return svd.fit_transform(sparse_mat).astype(np.float32)  # Use float32\n",
    "    except Exception:\n",
    "        return sparse_mat.toarray().astype(np.float32) if hasattr(sparse_mat, 'toarray') else np.asarray(sparse_mat, dtype=np.float32)\n",
    "\n",
    "tra_kmer_matrix = _reduce_sparse(tra_kmer_sparse, n_components=50)  # Reduced from 200\n",
    "trb_kmer_matrix = _reduce_sparse(trb_kmer_sparse, n_components=50)\n",
    "print(f\"TRA k-mer reduced shape: {tra_kmer_matrix.shape}\")\n",
    "print(f\"TRB k-mer reduced shape: {trb_kmer_matrix.shape}\")\n",
    "\n",
    "# Clean up sparse matrices\n",
    "del tra_kmer_sparse, trb_kmer_sparse\n",
    "gc.collect()\n",
    "\n",
    "# MEMORY FIX: Reduced one-hot encoding with smaller max length\n",
    "max_cdr3_length = 15  # Reduced from 20 to 15\n",
    "alphabet = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "char_to_idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "def _one_hot_encode_batch(sequences, max_len=max_cdr3_length):\n",
    "    \"\"\"Batch one-hot encoding using NumPy for memory efficiency.\"\"\"\n",
    "    n_seqs = len(sequences)\n",
    "    encoding = np.zeros((n_seqs, max_len, len(alphabet)), dtype=np.float16)  # Use float16 instead of float32\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_str = str(seq).upper()[:max_len]  # Truncate\n",
    "        for j, char in enumerate(seq_str):\n",
    "            if char in char_to_idx:\n",
    "                encoding[i, j, char_to_idx[char]] = 1.0\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "tra_one_hot = _one_hot_encode_batch(tra_seqs, max_cdr3_length)\n",
    "trb_one_hot = _one_hot_encode_batch(trb_seqs, max_cdr3_length)\n",
    "print(f\"TRA one-hot shape: {tra_one_hot.shape} (dtype: {tra_one_hot.dtype})\")\n",
    "print(f\"TRB one-hot shape: {trb_one_hot.shape} (dtype: {trb_one_hot.dtype})\")\n",
    "\n",
    "# Clean up sequence arrays\n",
    "del tra_seqs, trb_seqs\n",
    "gc.collect()\n",
    "\n",
    "# MEMORY FIX: Store matrices in obsm (compressed format in AnnData)\n",
    "adata.obsm['X_tcr_tra_kmer'] = tra_kmer_matrix\n",
    "adata.obsm['X_tcr_trb_kmer'] = trb_kmer_matrix\n",
    "\n",
    "# MEMORY FIX: Flatten and store one-hot as float32 for compatibility\n",
    "adata.obsm['X_tcr_tra_onehot'] = tra_one_hot.reshape(tra_one_hot.shape[0], -1).astype(np.float32)\n",
    "adata.obsm['X_tcr_trb_onehot'] = trb_one_hot.reshape(trb_one_hot.shape[0], -1).astype(np.float32)\n",
    "\n",
    "del tra_one_hot, trb_one_hot, tra_kmer_matrix, trb_kmer_matrix\n",
    "gc.collect()\n",
    "\n",
    "print(\"TCR sequence encoding complete and stored in adata.obsm\")\n",
    "print(f\"Memory usage reduced by using sparse matrices and dimension reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d72d0",
   "metadata": {
    "papermill": {
     "duration": 0.01917,
     "end_time": "2026-02-06T05:42:04.623166",
     "exception": false,
     "start_time": "2026-02-06T05:42:04.603996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Encode Gene Expression Patterns\n",
    "\n",
    "Apply PCA, SVD, and UMAP to gene expression data for dimensionality reduction and add encodings to AnnData."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef29c65",
   "metadata": {
    "papermill": {
     "duration": 0.018245,
     "end_time": "2026-02-06T05:42:04.659746",
     "exception": false,
     "start_time": "2026-02-06T05:42:04.641501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering and Encoding\n",
    "A core contribution of this work is the engineering of a comprehensive feature set that translates biological sequences into machine-readable vectors. We developed three distinct encoding schemes for the TCR CDR3 amino acid sequences:\n",
    "\n",
    "1.  **One-Hot Encoding:** This method creates a sparse binary matrix representing the presence or absence of specific amino acids at each position in the sequence. It preserves exact positional information, which is crucial for structural motifs, but results in high-dimensional, sparse vectors.\n",
    "2.  **K-mer Frequency Encoding:** We decomposed sequences into overlapping substrings of length $k$ (k-mers, with $k=3$). We then calculated the frequency of each unique 3-mer in the sequence. This approach captures short, local structural motifs (e.g., \"CAS\", \"ASS\") that may be shared across different TCRs with similar antigen specificity, regardless of their exact position.\n",
    "3.  **Physicochemical Property Encoding:** To capture the biophysical nature of the TCR-antigen interaction, we mapped each amino acid to a vector of physicochemical properties, including hydrophobicity, molecular weight, isoelectric point, and polarity. We then aggregated these values (e.g., mean, sum) across the CDR3 sequence. This results in a dense, low-dimensional representation that reflects the \"binding potential\" of the receptor.\n",
    "\n",
    "These TCR features were concatenated with the top 50 Principal Components (PCs) derived from the gene expression data to form the \"Comprehensive\" feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b96209e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:42:04.699103Z",
     "iopub.status.busy": "2026-02-06T05:42:04.698817Z",
     "iopub.status.idle": "2026-02-06T05:42:06.133581Z",
     "shell.execute_reply": "2026-02-06T05:42:06.132622Z"
    },
    "papermill": {
     "duration": 1.456579,
     "end_time": "2026-02-06T05:42:06.135337",
     "exception": false,
     "start_time": "2026-02-06T05:42:04.678758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding TCR CDR3 sequences (vectorized k-mer + reduced one-hot)...\n",
      "TRA k-mer sparse shape: (100067, 1)\n",
      "TRB k-mer sparse shape: (100067, 1)\n",
      "TRA k-mer reduced shape: (100067, 1)\n",
      "TRB k-mer reduced shape: (100067, 1)\n",
      "TRA one-hot flat shape: (100067, 400)\n",
      "TRB one-hot flat shape: (100067, 400)\n",
      "TRA physicochemical features shape: (100067, 6)\n",
      "TRB physicochemical features shape: (100067, 6)\n",
      "TCR sequence encoding completed and added to AnnData object!\n",
      "CPU times: user 1.39 s, sys: 31.9 ms, total: 1.42 s\n",
      "Wall time: 1.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# --- Apply Sequence Encoding to TCR CDR3 Sequences (vectorized k-mer + reduced one-hot) ---\n",
    "\n",
    "print(\"Encoding TCR CDR3 sequences (vectorized k-mer + reduced one-hot)...\")\n",
    "\n",
    "# Extract and clean CDR3 sequences\n",
    "if 'cdr3_TRA' in adata.obs.columns:\n",
    "    cdr3_TRA = adata.obs['cdr3_TRA'].astype(str).fillna('').str.upper()\n",
    "else:\n",
    "    cdr3_TRA = pd.Series([''] * adata.n_obs, index=adata.obs.index)\n",
    "if 'cdr3_TRB' in adata.obs.columns:\n",
    "    cdr3_TRB = adata.obs['cdr3_TRB'].astype(str).fillna('').str.upper()\n",
    "else:\n",
    "    cdr3_TRB = pd.Series([''] * adata.n_obs, index=adata.obs.index)\n",
    "\n",
    "valid_aa = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "def _clean_seq(s):\n",
    "    return ''.join([c for c in str(s) if c in valid_aa])\n",
    "\n",
    "tra_seqs = [_clean_seq(s) for s in cdr3_TRA]\n",
    "trb_seqs = [_clean_seq(s) for s in cdr3_TRB]\n",
    "\n",
    "# --- Vectorized k-mer encoding using CountVectorizer (sparse) ---\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "k = 3\n",
    "vec_tra = CountVectorizer(analyzer='char', ngram_range=(k,k))\n",
    "vec_trb = CountVectorizer(analyzer='char', ngram_range=(k,k))\n",
    "tra_kmer_sparse = vec_tra.fit_transform(tra_seqs)\n",
    "trb_kmer_sparse = vec_trb.fit_transform(trb_seqs)\n",
    "print(f\"TRA k-mer sparse shape: {tra_kmer_sparse.shape}\")\n",
    "print(f\"TRB k-mer sparse shape: {trb_kmer_sparse.shape}\")\n",
    "\n",
    "# Reduce k-mer sparse matrices with TruncatedSVD to a dense reduced representation (keeps memory low)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "def _reduce_sparse(sparse_mat, n_components=200):\n",
    "    n_comp = min(n_components, max(1, sparse_mat.shape[1]-1))\n",
    "    try:\n",
    "        svd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "        return svd.fit_transform(sparse_mat)\n",
    "    except Exception:\n",
    "        # Fallback to dense (small datasets)\n",
    "        return sparse_mat.toarray() if hasattr(sparse_mat, 'toarray') else np.asarray(sparse_mat)\n",
    "\n",
    "tra_kmer_matrix = _reduce_sparse(tra_kmer_sparse, n_components=200)\n",
    "trb_kmer_matrix = _reduce_sparse(trb_kmer_sparse, n_components=200)\n",
    "print(f\"TRA k-mer reduced shape: {tra_kmer_matrix.shape}\")\n",
    "print(f\"TRB k-mer reduced shape: {trb_kmer_matrix.shape}\")\n",
    "\n",
    "# --- Reduced one-hot encoding: limit max length to avoid huge dense matrices ---\n",
    "max_cdr3_length = 20  # smaller to reduce dimensionality and memory\n",
    "alphabet = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "char_to_idx = {c:i for i,c in enumerate(alphabet)}\n",
    "def _onehot_flat_list(seqs, max_length, alphabet, char_to_idx):\n",
    "    out = np.zeros((len(seqs), max_length * len(alphabet)), dtype=np.uint8)\n",
    "    for i, s in enumerate(seqs):\n",
    "        for j, ch in enumerate(s[:max_length]):\n",
    "            if ch in char_to_idx:\n",
    "                out[i, j * len(alphabet) + char_to_idx[ch]] = 1\n",
    "    return out\n",
    "\n",
    "tra_onehot_flat = _onehot_flat_list(tra_seqs, max_cdr3_length, alphabet, char_to_idx)\n",
    "trb_onehot_flat = _onehot_flat_list(trb_seqs, max_cdr3_length, alphabet, char_to_idx)\n",
    "print(f\"TRA one-hot flat shape: {tra_onehot_flat.shape}\")\n",
    "print(f\"TRB one-hot flat shape: {trb_onehot_flat.shape}\")\n",
    "\n",
    "# --- Physicochemical properties (unchanged) ---\n",
    "tra_physico = pd.DataFrame([physicochemical_features(seq) for seq in tra_seqs])\n",
    "trb_physico = pd.DataFrame([physicochemical_features(seq) for seq in trb_seqs])\n",
    "print(f\"TRA physicochemical features shape: {tra_physico.shape}\")\n",
    "print(f\"TRB physicochemical features shape: {trb_physico.shape}\")\n",
    "\n",
    "# Add to AnnData object (reduced, memory-friendly)\n",
    "adata.obsm['X_tcr_tra_onehot'] = tra_onehot_flat\n",
    "adata.obsm['X_tcr_trb_onehot'] = trb_onehot_flat\n",
    "adata.obsm['X_tcr_tra_kmer'] = tra_kmer_matrix\n",
    "adata.obsm['X_tcr_trb_kmer'] = trb_kmer_matrix\n",
    "\n",
    "# Add physicochemical features to obs\n",
    "for col in tra_physico.columns:\n",
    "    adata.obs[f'tra_{col}'] = tra_physico[col].values\n",
    "for col in trb_physico.columns:\n",
    "    adata.obs[f'trb_{col}'] = trb_physico[col].values\n",
    "\n",
    "print(\"TCR sequence encoding completed and added to AnnData object!\")\n",
    "\n",
    "# Clean up large temporary objects\n",
    "import gc\n",
    "try:\n",
    "    # delete sparse intermediates and local copies ‚Äî AnnData already stores the reduced matrices\n",
    "    del tra_kmer_sparse, trb_kmer_sparse\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    # delete other large temporaries that have been copied into `adata.obsm` or `adata.obs`\n",
    "    for _n in ['tra_kmer_matrix', 'trb_kmer_matrix', 'tra_onehot_flat', 'trb_onehot_flat', 'tra_physico', 'trb_physico', 'tra_seqs', 'trb_seqs', 'vec_tra', 'vec_trb', 'char_to_idx']:\n",
    "        if _n in globals():\n",
    "            try:\n",
    "                del globals()[_n]\n",
    "            except Exception:\n",
    "                pass\n",
    "except Exception:\n",
    "    pass\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb52bdb",
   "metadata": {
    "papermill": {
     "duration": 0.019124,
     "end_time": "2026-02-06T05:42:06.174332",
     "exception": false,
     "start_time": "2026-02-06T05:42:06.155208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Create Combined Multi-Modal Encodings\n",
    "\n",
    "Combine gene expression and TCR encodings into multi-modal representations using PCA and UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f8de4c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:42:06.214547Z",
     "iopub.status.busy": "2026-02-06T05:42:06.214186Z",
     "iopub.status.idle": "2026-02-06T05:45:28.257331Z",
     "shell.execute_reply": "2026-02-06T05:45:28.256315Z"
    },
    "papermill": {
     "duration": 202.086579,
     "end_time": "2026-02-06T05:45:28.279939",
     "exception": false,
     "start_time": "2026-02-06T05:42:06.193360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing gene expression data...\n",
      "Warning: Data is dense. converting to sparse float32...\n",
      "Normalizing...\n",
      "Log transforming...\n",
      "Encoding patterns...\n",
      "Selecting Highly Variable Genes...\n",
      "HVG subset shape: (100067, 1500)\n",
      "Computing PCA (Arpack - Sparse)...\n",
      "Computing TruncatedSVD...\n",
      "Computing UMAP on PCA embeddings...\n",
      "  Added X_gene_pca\n",
      "  Added X_gene_svd\n",
      "  Added X_gene_umap\n",
      "Gene expression encoding completed!\n",
      "CPU times: user 3min 54s, sys: 2.08 s, total: 3min 56s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MEMORY-OPTIMIZED encode_gene_expression_patterns function using Scanpy's Native Optimized PCA\n",
    "def encode_gene_expression_patterns(adata, n_top_genes=1500, train_mask=None):\n",
    "    \"\"\"\n",
    "    Encode gene expression patterns using Scanpy's optimized sparse PCA (Arpack)\n",
    "    and TruncatedSVD. Avoids manual chunking complexity which can be error prone.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (encodings dict, X_scaled placeholder)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import gc\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    import umap\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # 1. HVG Selection (Memory Optimized: Subsample cells if huge)\n",
    "    # Calculating mean/var on 100k cells x 30k genes can overlap memory.\n",
    "    # We calculate on a subset of 20k cells to estimate HVGs.\n",
    "    print(\"Selecting Highly Variable Genes...\")\n",
    "    \n",
    "    if adata.n_obs > 20000 and 'highly_variable' not in adata.var.columns:\n",
    "        # Subsample for HVG calculation only\n",
    "        idx = np.random.choice(adata.n_obs, 20000, replace=False)\n",
    "        temp_adata = adata[idx].copy()\n",
    "        sc.pp.highly_variable_genes(temp_adata, n_top_genes=n_top_genes, subset=False, flavor='seurat')\n",
    "        # Transfer results back\n",
    "        adata.var['highly_variable'] = False\n",
    "        adata.var.loc[temp_adata.var_names, 'highly_variable'] = temp_adata.var['highly_variable']\n",
    "        del temp_adata\n",
    "        gc.collect()\n",
    "    elif 'highly_variable' not in adata.var.columns:\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, subset=False, flavor='seurat')\n",
    "\n",
    "    # Get HVG Subset (Sparse View or Copy)\n",
    "    # Scanpy handles views efficiently for PCA\n",
    "    adata_hvg = adata[:, adata.var['highly_variable']]\n",
    "    print(f\"HVG subset shape: {adata_hvg.shape}\")\n",
    "\n",
    "    # 2. PCA (Scanpy Arpack = Sparse SVD on centered data implicitly)\n",
    "    # This is much more stable than manual IncrementalPCA on disjoint chunks\n",
    "    print(\"Computing PCA (Arpack - Sparse)...\")\n",
    "    sc.pp.pca(adata_hvg, n_comps=30, svd_solver='arpack', zero_center=True, use_highly_variable=False)\n",
    "    X_pca = adata_hvg.obsm['X_pca']\n",
    "    \n",
    "    # 3. TruncatedSVD (LSA - No centering)\n",
    "    # Good for sparse data comparison\n",
    "    print(\"Computing TruncatedSVD...\")\n",
    "    # Use the sparse matrix from the view\n",
    "    svd = TruncatedSVD(n_components=30, random_state=42, algorithm='randomized')\n",
    "    X_svd = svd.fit_transform(adata_hvg.X)\n",
    "    \n",
    "    # 4. UMAP on PCA (Standard practice)\n",
    "    print(\"Computing UMAP on PCA embeddings...\")\n",
    "    umap_reducer = umap.UMAP(\n",
    "        n_components=10, \n",
    "        n_neighbors=15, \n",
    "        random_state=42, \n",
    "        n_jobs=1, \n",
    "        low_memory=True\n",
    "    )\n",
    "    X_umap = umap_reducer.fit_transform(X_pca)\n",
    "\n",
    "    encodings = {\n",
    "        'pca': X_pca.astype(np.float32),\n",
    "        'svd': X_svd.astype(np.float32),\n",
    "        'umap': X_umap.astype(np.float32)\n",
    "    }\n",
    "    \n",
    "    # Clean up\n",
    "    del adata_hvg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Return nothing for X_scaled (deprecated)\n",
    "    return encodings, None\n",
    "\n",
    "# --- Main Preprocessing Block ---\n",
    "\n",
    "print(\"Preprocessing gene expression data...\")\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# 1. Ensure float32 sparse (Crucial for memory)\n",
    "if not hasattr(adata.X, 'toarray'): # is sparse\n",
    "    if adata.X.dtype != np.float32:\n",
    "        print(\"Converting sparse matrix to float32...\")\n",
    "        adata.X = adata.X.astype(np.float32)\n",
    "else: # is dense (shouldn't be, but just in case)\n",
    "    print(\"Warning: Data is dense. converting to sparse float32...\")\n",
    "    adata.X = sp.csr_matrix(adata.X, dtype=np.float32)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# 2. Normalize & Log (In-place)\n",
    "print(\"Normalizing...\")\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "print(\"Log transforming...\")\n",
    "sc.pp.log1p(adata)\n",
    "gc.collect()\n",
    "\n",
    "# 3. Clean Infs/NaNs (In-place, memory safe)\n",
    "if hasattr(adata.X, 'data'):\n",
    "    mask = np.isinf(adata.X.data)\n",
    "    if mask.any():\n",
    "        adata.X.data[mask] = 0\n",
    "        print(f\"Fixed {mask.sum()} infinite values\")\n",
    "    mask = np.isnan(adata.X.data)\n",
    "    if mask.any():\n",
    "        adata.X.data[mask] = 0\n",
    "        print(f\"Fixed {mask.sum()} NaN values\")\n",
    "\n",
    "print(\"Encoding patterns...\")\n",
    "\n",
    "# Apply encoding\n",
    "try:\n",
    "    # Use reduced gene set (1500)\n",
    "    result = encode_gene_expression_patterns(adata, n_top_genes=1500)\n",
    "    gene_encodings = result[0]\n",
    "\n",
    "    # Add to AnnData\n",
    "    for key, val in gene_encodings.items():\n",
    "        adata.obsm[f'X_gene_{key}'] = val\n",
    "        print(f\"  Added X_gene_{key}\")\n",
    "\n",
    "    del gene_encodings\n",
    "    gc.collect()\n",
    "    print(\"Gene expression encoding completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during encoding: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d1ac6ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:45:28.327339Z",
     "iopub.status.busy": "2026-02-06T05:45:28.326674Z",
     "iopub.status.idle": "2026-02-06T05:47:22.596449Z",
     "shell.execute_reply": "2026-02-06T05:47:22.595451Z"
    },
    "papermill": {
     "duration": 114.318601,
     "end_time": "2026-02-06T05:47:22.618807",
     "exception": false,
     "start_time": "2026-02-06T05:45:28.300206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating combined multi-modal encodings...\n",
      "Reducing k-mer features: (100067, 2)\n",
      "Combined gene-TCR encoding shape: (100067, 26)\n",
      "Combined gene-TCR k-mer encoding shape: (100067, 11)\n",
      "Computing dimensionality reduction on combined data (UMAP only)...\n",
      "CPU times: user 2min 51s, sys: 390 ms, total: 2min 52s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Create Combined Multi-Modal Encodings ---\n",
    "print(\"Creating combined multi-modal encodings...\")\n",
    "import gc\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Combine different encoding modalities\n",
    "# 1. Gene expression PCA + TCR physicochemical features\n",
    "gene_pca = None\n",
    "# Retrieve pre-computed PCA from gene_encodings dict\n",
    "pca_data = gene_encodings.get('pca', None) if 'gene_encodings' in globals() and isinstance(gene_encodings, dict) else None\n",
    "\n",
    "if pca_data is not None and isinstance(pca_data, (np.ndarray, list)):\n",
    "    gene_pca = np.asarray(pca_data)\n",
    "elif 'X_gene_pca' in adata.obsm:\n",
    "    gene_pca = adata.obsm['X_gene_pca']\n",
    "\n",
    "# Fallback or pad\n",
    "if gene_pca is not None:\n",
    "    if gene_pca.ndim == 1:\n",
    "        gene_pca = gene_pca.reshape(-1, 1)\n",
    "    if gene_pca.shape[1] >= 20:\n",
    "        gene_pca = gene_pca[:, :20]\n",
    "    else:\n",
    "        # Pad to 20 components\n",
    "        pad_cols = 20 - gene_pca.shape[1]\n",
    "        gene_pca = np.pad(gene_pca, ((0, 0), (0, pad_cols)), mode='constant')\n",
    "else:\n",
    "    print(\"Warning: PCA data not available; using zeros.\")\n",
    "    gene_pca = np.zeros((adata.n_obs, 20))\n",
    "\n",
    "tcr_features = np.column_stack([\n",
    "    adata.obs[['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']].fillna(0).values,\n",
    "    adata.obs[['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']].fillna(0).values\n",
    "])\n",
    "\n",
    "combined_gene_tcr = np.column_stack([gene_pca, tcr_features])\n",
    "adata.obsm['X_combined_gene_tcr'] = combined_gene_tcr.astype(np.float32)\n",
    "\n",
    "# 2. Gene expression UMAP + TCR k-mer features (reduced)\n",
    "gene_umap = gene_encodings.get('umap', None) if 'gene_encodings' in globals() and isinstance(gene_encodings, dict) else None\n",
    "if gene_umap is None and 'X_gene_umap' in adata.obsm:\n",
    "    gene_umap = adata.obsm['X_gene_umap']\n",
    "if gene_umap is None:\n",
    "    gene_umap = np.zeros((adata.n_obs, 2))\n",
    "\n",
    "# Stack TRA and TRB k-mer matrices EFFICIENTLY\n",
    "tra_kmer = adata.obsm.get('X_tcr_tra_kmer', None)\n",
    "trb_kmer = adata.obsm.get('X_tcr_trb_kmer', None)\n",
    "\n",
    "if tra_kmer is not None and trb_kmer is not None:\n",
    "    if sparse.issparse(tra_kmer) or sparse.issparse(trb_kmer):\n",
    "        # Ensure both are sparse before stacking to avoid densification\n",
    "        if not sparse.issparse(tra_kmer): tra_kmer = sparse.csr_matrix(tra_kmer)\n",
    "        if not sparse.issparse(trb_kmer): trb_kmer = sparse.csr_matrix(trb_kmer)\n",
    "        tcr_kmer_combined = sparse.hstack([tra_kmer, trb_kmer])\n",
    "    else:\n",
    "        tcr_kmer_combined = np.column_stack([tra_kmer, trb_kmer])\n",
    "else:\n",
    "    tcr_kmer_combined = np.zeros((adata.n_obs, 1)) # Dummy\n",
    "\n",
    "# Robust dimensional reduction for k-mer features using TruncatedSVD (Safe for OOM)\n",
    "print(f\"Reducing k-mer features: {tcr_kmer_combined.shape}\")\n",
    "n_comp_kmer = min(10, tcr_kmer_combined.shape[1] - 1)\n",
    "if n_comp_kmer > 0:\n",
    "    tcr_svd = TruncatedSVD(n_components=n_comp_kmer, random_state=42, algorithm='randomized')\n",
    "    tcr_kmer_reduced = tcr_svd.fit_transform(tcr_kmer_combined)\n",
    "else:\n",
    "    tcr_kmer_reduced = np.zeros((adata.n_obs, 10))\n",
    "\n",
    "combined_gene_tcr_kmer = np.column_stack([gene_umap, tcr_kmer_reduced])\n",
    "adata.obsm['X_combined_gene_tcr_kmer'] = combined_gene_tcr_kmer.astype(np.float32)\n",
    "\n",
    "print(f\"Combined gene-TCR encoding shape: {combined_gene_tcr.shape}\")\n",
    "print(f\"Combined gene-TCR k-mer encoding shape: {combined_gene_tcr_kmer.shape}\")\n",
    "\n",
    "# Clear memory\n",
    "del tcr_kmer_combined\n",
    "gc.collect()\n",
    "\n",
    "# --- Dimensionality Reduction on Combined Data ---\n",
    "print(\"Computing dimensionality reduction on combined data (UMAP only)...\")\n",
    "\n",
    "# UMAP on combined data\n",
    "umap_combined = umap.UMAP(n_components=2, random_state=42, n_jobs=1) # Single Job for RAM safety\n",
    "adata.obsm['X_umap_combined'] = umap_combined.fit_transform(combined_gene_tcr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab527e5",
   "metadata": {
    "papermill": {
     "duration": 0.021168,
     "end_time": "2026-02-06T05:47:22.660649",
     "exception": false,
     "start_time": "2026-02-06T05:47:22.639481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Unsupervised Machine Learning Analysis with Hierarchical Clustering\n",
    "\n",
    "Before training predictive classifiers, we utilized unsupervised learning to define the intrinsic structure of the immune landscape. We compared several clustering algorithms:\n",
    "*   **K-Means Clustering:** Partitions data into $k$ distinct clusters by minimizing within-cluster variance.\n",
    "*   **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):** Groups points that are closely packed together, marking points in low-density regions as outliers.\n",
    "*   **Agglomerative Hierarchical Clustering:** Builds a hierarchy of clusters using a bottom-up approach.\n",
    "\n",
    "We evaluated these methods using Silhouette Analysis to measure cluster cohesion and separation. The optimal number of clusters ($k$) for K-Means was determined using the Elbow Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c977e14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:47:22.704444Z",
     "iopub.status.busy": "2026-02-06T05:47:22.704108Z",
     "iopub.status.idle": "2026-02-06T05:47:22.712401Z",
     "shell.execute_reply": "2026-02-06T05:47:22.711448Z"
    },
    "papermill": {
     "duration": 0.032985,
     "end_time": "2026-02-06T05:47:22.714196",
     "exception": false,
     "start_time": "2026-02-06T05:47:22.681211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No patch required for sklearn.check_array signature.\n"
     ]
    }
   ],
   "source": [
    "# HDBSCAN/sklearn compatibility patch ‚Äî run before clustering\n",
    "import sys, subprocess, inspect\n",
    "\n",
    "# Ensure hdbscan is available (not strictly necessary if already installed earlier)\n",
    "try:\n",
    "    import hdbscan\n",
    "except Exception:\n",
    "    print(\"hdbscan not installed ‚Äî installing now...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"hdbscan\"]) \n",
    "    import importlib\n",
    "    importlib.invalidate_caches()\n",
    "    import hdbscan\n",
    "\n",
    "# Patch the check_array reference used inside hdbscan to accept the older keyword\n",
    "try:\n",
    "    import sklearn.utils.validation as sk_validation\n",
    "    from hdbscan import hdbscan_ as _hdbscan_mod\n",
    "    sig = inspect.signature(sk_validation.check_array)\n",
    "    if 'ensure_all_finite' in sig.parameters and 'force_all_finite' not in sig.parameters:\n",
    "        orig = getattr(_hdbscan_mod, 'check_array', None) or sk_validation.check_array\n",
    "        def _patched_check_array(*args, **kwargs):\n",
    "            if 'force_all_finite' in kwargs and 'ensure_all_finite' not in kwargs:\n",
    "                kwargs['ensure_all_finite'] = kwargs.pop('force_all_finite')\n",
    "            return orig(*args, **kwargs)\n",
    "        _hdbscan_mod.check_array = _patched_check_array\n",
    "        print(\"Patched hdbscan.check_array to accept 'force_all_finite' for this runtime.\")\n",
    "    else:\n",
    "        print(\"No patch required for sklearn.check_array signature.\")\n",
    "except Exception as e:\n",
    "    print(\"Compatibility patch could not be applied:\", type(e).__name__, e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91179a19",
   "metadata": {
    "papermill": {
     "duration": 0.02183,
     "end_time": "2026-02-06T05:47:22.756453",
     "exception": false,
     "start_time": "2026-02-06T05:47:22.734623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Unsupervised Machine Learning Analysis (Updated)\n",
    "\n",
    "This section has been updated to utilize the `clustering.py` implementation for Leiden clustering, replacing the previous K-Means/DBSCAN/Agglomerative comparison.\n",
    "\n",
    "**Changes:**\n",
    "- Imported `clustering.py` module.\n",
    "- Used `clustering.preprocess_data(adata)` for data preprocessing.\n",
    "- Used `clustering.perform_clustering(adata)` for Leiden clustering at multiple resolutions.\n",
    "- Calculated silhouette scores for Leiden clusters to maintain compatibility with the \"best clustering\" selection logic.\n",
    "- Renamed Leiden cluster columns to `leiden_cluster_{resolution}` to ensure compatibility with downstream feature selection filters.\n",
    "- Retained TCR sequence-specific clustering and Gene Expression Module Discovery.\n",
    "\n",
    "**Note:**\n",
    "- Ensure `clustering.py` is in the python path (Code/ directory).\n",
    "- The \"best clustering\" is now selected from the Leiden results based on silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44ae6da7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:47:22.800234Z",
     "iopub.status.busy": "2026-02-06T05:47:22.799924Z",
     "iopub.status.idle": "2026-02-06T05:47:34.669714Z",
     "shell.execute_reply": "2026-02-06T05:47:34.668599Z"
    },
    "papermill": {
     "duration": 11.894811,
     "end_time": "2026-02-06T05:47:34.671426",
     "exception": false,
     "start_time": "2026-02-06T05:47:22.776615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.15.3)\r\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.0.2)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting leidenalg\r\n",
      "  Downloading leidenalg-0.11.0-cp38-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: igraph<2.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from leidenalg) (1.0.0)\r\n",
      "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.12/dist-packages (from igraph<2.0,>=1.0.0->leidenalg) (1.7.0)\r\n",
      "Downloading leidenalg-0.11.0-cp38-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.7 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: leidenalg\r\n",
      "Successfully installed leidenalg-0.11.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚è≠Ô∏è SKIPPING: Unsupervised Learning (Leiden, UMAP, etc.)\n",
      "   Set SKIP_UNSUPERVISED_LEARNING=False in the configuration cell to run this section.\n",
      "Running MINIMAL preprocessing for deep learning...\n",
      "  patient_id not found directly. Deriving from sample_id...\n",
      "  Mapped 0/100067 cells to patient_id\n",
      "  Warning: No direct matches. Checking for batch column...\n",
      "  Derived response column: {}\n",
      "  WARNING: Could not derive patient_id. Downstream patient-level analysis may fail.\n",
      "  Available columns: ['sample_id', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'cdr3_TRA', 'cdr3_TRB', 'tra_length', 'tra_molecular_weight', 'tra_aromaticity', 'tra_instability_index', 'tra_isoelectric_point', 'tra_hydrophobicity', 'trb_length', 'trb_molecular_weight', 'trb_aromaticity', 'trb_instability_index', 'trb_isoelectric_point', 'trb_hydrophobicity', 'patient_id', 'response']\n",
      "Minimal preprocessing complete. supervised_mask: 0 samples\n",
      "CPU times: user 4.92 s, sys: 224 ms, total: 5.15 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install scipy\n",
    "%pip install leidenalg\n",
    "\n",
    "# Check if we should skip unsupervised learning\n",
    "if globals().get('SKIP_UNSUPERVISED_LEARNING', False) or globals().get('SKIP_TO_DEEP_LEARNING', False):\n",
    "    print(\"‚è≠Ô∏è SKIPPING: Unsupervised Learning (Leiden, UMAP, etc.)\")\n",
    "    print(\"   Set SKIP_UNSUPERVISED_LEARNING=False in the configuration cell to run this section.\")\n",
    "    \n",
    "    # Still need to do minimal preprocessing for deep learning\n",
    "    import scanpy as sc\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    from scipy import sparse\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    print(\"Running MINIMAL preprocessing for deep learning...\")\n",
    "    \n",
    "    # Normalize and log-transform\n",
    "    if 'log1p' not in adata.uns:\n",
    "        sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    # Find HVGs\n",
    "    if 'highly_variable' not in adata.var.columns:\n",
    "        sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "    \n",
    "    # Compute PCA if needed\n",
    "    if 'X_pca' not in adata.obsm:\n",
    "        hvg_mask = adata.var['highly_variable'].values\n",
    "        n_hvgs = hvg_mask.sum()\n",
    "        if sparse.issparse(adata.X):\n",
    "            X_hvg = adata.X[:, hvg_mask]\n",
    "            n_components = min(50, n_hvgs - 1, X_hvg.shape[0] - 1)\n",
    "            svd = TruncatedSVD(n_components=n_components, random_state=42, algorithm='arpack')\n",
    "            adata.obsm['X_pca'] = svd.fit_transform(X_hvg).astype(np.float32)\n",
    "            del X_hvg, svd\n",
    "            gc.collect()\n",
    "    \n",
    "    # ============================================================\n",
    "    # Fix column names (response, patient_id) - ROBUST VERSION\n",
    "    # ============================================================\n",
    "    if 'response' not in adata.obs.columns and 'Response' in adata.obs.columns:\n",
    "        adata.obs['response'] = adata.obs['Response']\n",
    "    \n",
    "    # First check if patient_id already exists\n",
    "    patient_id_found = False\n",
    "    for col_name in ['patient_id', 'Patient_ID', 'PatientID']:\n",
    "        if col_name in adata.obs.columns:\n",
    "            if col_name != 'patient_id':\n",
    "                adata.obs['patient_id'] = adata.obs[col_name]\n",
    "            patient_id_found = True\n",
    "            print(f\"  Found patient_id in column '{col_name}'\")\n",
    "            break\n",
    "    \n",
    "    # If patient_id not found, derive from sample_id using metadata mapping\n",
    "    if not patient_id_found and 'sample_id' in adata.obs.columns:\n",
    "        print(\"  patient_id not found directly. Deriving from sample_id...\")\n",
    "        \n",
    "        # Recreate metadata_df mapping (same as in data loading cell)\n",
    "        # This is the authoritative mapping from GEO GSE300475\n",
    "        metadata_records = [\n",
    "            {'sample_id': 'GSM9061665', 'Patient_ID': 'PT1', 'Timepoint': 'Pre', 'Response': 'Responder'},\n",
    "            {'sample_id': 'GSM9061666', 'Patient_ID': 'PT1', 'Timepoint': 'D21', 'Response': 'Responder'},\n",
    "            {'sample_id': 'GSM9061667', 'Patient_ID': 'PT1', 'Timepoint': 'D42', 'Response': 'Responder'},\n",
    "            {'sample_id': 'GSM9061668', 'Patient_ID': 'PT2', 'Timepoint': 'Pre', 'Response': 'Responder'},\n",
    "            {'sample_id': 'GSM9061669', 'Patient_ID': 'PT2', 'Timepoint': 'D21', 'Response': 'Responder'},\n",
    "            {'sample_id': 'GSM9061670', 'Patient_ID': 'PT2', 'Timepoint': 'D42', 'Response': 'Responder'},\n",
    "            {'sample_id': 'GSM9061671', 'Patient_ID': 'PT3', 'Timepoint': 'Pre', 'Response': 'Non-Responder'},\n",
    "            {'sample_id': 'GSM9061672', 'Patient_ID': 'PT3', 'Timepoint': 'D21', 'Response': 'Non-Responder'},\n",
    "            {'sample_id': 'GSM9061673', 'Patient_ID': 'PT4', 'Timepoint': 'Pre', 'Response': 'Non-Responder'},\n",
    "            {'sample_id': 'GSM9061674', 'Patient_ID': 'PT4', 'Timepoint': 'D21', 'Response': 'Non-Responder'},\n",
    "            {'sample_id': 'GSM9061675', 'Patient_ID': 'PT4', 'Timepoint': 'D42', 'Response': 'Non-Responder'},\n",
    "        ]\n",
    "        metadata_df_local = pd.DataFrame(metadata_records)\n",
    "        \n",
    "        # Create sample_id to patient_id mapping\n",
    "        sample_to_patient = dict(zip(metadata_df_local['sample_id'], metadata_df_local['Patient_ID']))\n",
    "        \n",
    "        # Map sample_id to patient_id\n",
    "        adata.obs['patient_id'] = adata.obs['sample_id'].map(sample_to_patient)\n",
    "        \n",
    "        # Check for unmapped values\n",
    "        n_mapped = adata.obs['patient_id'].notna().sum()\n",
    "        n_total = len(adata.obs)\n",
    "        print(f\"  Mapped {n_mapped}/{n_total} cells to patient_id\")\n",
    "        \n",
    "        if n_mapped == 0:\n",
    "            # Try parsing sample_id - maybe format is different (e.g., batch column)\n",
    "            print(\"  Warning: No direct matches. Checking for batch column...\")\n",
    "            if 'batch' in adata.obs.columns:\n",
    "                adata.obs['patient_id'] = adata.obs['batch'].map(sample_to_patient)\n",
    "                n_mapped = adata.obs['patient_id'].notna().sum()\n",
    "                print(f\"  Mapped {n_mapped}/{n_total} cells using batch column\")\n",
    "        \n",
    "        # Also derive response if missing\n",
    "        if 'response' not in adata.obs.columns or adata.obs['response'].isna().all():\n",
    "            sample_to_response = dict(zip(metadata_df_local['sample_id'], metadata_df_local['Response']))\n",
    "            adata.obs['response'] = adata.obs['sample_id'].map(sample_to_response)\n",
    "            if adata.obs['response'].isna().all() and 'batch' in adata.obs.columns:\n",
    "                adata.obs['response'] = adata.obs['batch'].map(sample_to_response)\n",
    "            print(f\"  Derived response column: {adata.obs['response'].value_counts().to_dict()}\")\n",
    "        \n",
    "        patient_id_found = adata.obs['patient_id'].notna().any()\n",
    "    \n",
    "    if not patient_id_found:\n",
    "        print(\"  WARNING: Could not derive patient_id. Downstream patient-level analysis may fail.\")\n",
    "        print(f\"  Available columns: {list(adata.obs.columns)}\")\n",
    "    else:\n",
    "        print(f\"  patient_id distribution: {adata.obs['patient_id'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Create supervised_mask for downstream\n",
    "    if 'response' in adata.obs.columns:\n",
    "        supervised_mask = adata.obs['response'].isin(['Responder', 'Non-Responder']).values\n",
    "    else:\n",
    "        supervised_mask = np.ones(adata.n_obs, dtype=bool)\n",
    "    \n",
    "    print(f\"Minimal preprocessing complete. supervised_mask: {supervised_mask.sum()} samples\")\n",
    "    \n",
    "else:\n",
    "    # --- Full Unsupervised Machine Learning Analysis ---\n",
    "    print(\"Applying unsupervised machine learning algorithms...\")\n",
    "\n",
    "    import scanpy as sc\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import gc  # For garbage collection\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "    from scipy import sparse\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    from IPython.display import display\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    if IS_KAGGLE:\n",
    "        Path('/kaggle/working/Processed_Data').mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        Path('Processed_Data').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Quick memory cleanup\n",
    "    for _v in ['adata_list','adata_sample','metadata_list','metadata_df',\n",
    "               'tra_kmer_sparse','trb_kmer_sparse','tra_kmer_matrix','trb_kmer_matrix',\n",
    "               'vec_tra','vec_trb','tra_seqs','trb_seqs','tra_kmeans','trb_kmeans',\n",
    "               'tra_kmer_scaled','trb_kmer_scaled','tra_scaler','trb_scaler','gene_kmeans',\n",
    "               'gene_pca','gene_expression_modules','tra_clusters','trb_clusters']:\n",
    "        if _v in globals():\n",
    "            try:\n",
    "                del globals()[_v]\n",
    "            except Exception:\n",
    "                pass\n",
    "    gc.collect()\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # ============================================================\n",
    "    # Fix column names (response, patient_id) BEFORE processing\n",
    "    # ============================================================\n",
    "    if 'response' not in adata.obs.columns and 'Response' in adata.obs.columns:\n",
    "        adata.obs['response'] = adata.obs['Response']\n",
    "        print(\"  Renamed 'Response' column to 'response'\")\n",
    "    \n",
    "    for col_name in ['Patient_ID', 'PatientID']:\n",
    "        if col_name in adata.obs.columns and 'patient_id' not in adata.obs.columns:\n",
    "            adata.obs['patient_id'] = adata.obs[col_name]\n",
    "            print(f\"  Renamed '{col_name}' column to 'patient_id'\")\n",
    "            break\n",
    "\n",
    "    print(\"Preprocessing data (memory-efficient mode)...\")\n",
    "\n",
    "    # 1. Normalize and log-transform (these keep sparse)\n",
    "    if 'log1p' not in adata.uns:\n",
    "        print(\"  Normalizing...\")\n",
    "        sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "        gc.collect()\n",
    "\n",
    "    # 2. Find highly variable genes (does NOT densify)\n",
    "    if 'highly_variable' not in adata.var.columns:\n",
    "        print(\"  Finding highly variable genes...\")\n",
    "        sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "        gc.collect()\n",
    "\n",
    "    # 3. MEMORY-EFFICIENT PCA using TruncatedSVD on sparse HVG subset\n",
    "    if 'X_pca' not in adata.obsm:\n",
    "        print(\"Computing PCA (sparse-friendly via TruncatedSVD on HVGs)...\")\n",
    "        \n",
    "        from sklearn.decomposition import TruncatedSVD\n",
    "        \n",
    "        hvg_mask = adata.var['highly_variable'].values\n",
    "        n_hvgs = hvg_mask.sum()\n",
    "        print(f\"  Using {n_hvgs} highly variable genes\")\n",
    "        \n",
    "        if sparse.issparse(adata.X):\n",
    "            X_hvg = adata.X[:, hvg_mask]\n",
    "            print(f\"  HVG matrix shape: {X_hvg.shape}, sparse: {sparse.issparse(X_hvg)}\")\n",
    "            \n",
    "            n_components = min(50, n_hvgs - 1, X_hvg.shape[0] - 1)\n",
    "            print(f\"  Running TruncatedSVD with {n_components} components...\")\n",
    "            \n",
    "            svd = TruncatedSVD(n_components=n_components, random_state=42, algorithm='arpack')\n",
    "            X_pca = svd.fit_transform(X_hvg)\n",
    "            \n",
    "            adata.obsm['X_pca'] = X_pca.astype(np.float32)\n",
    "            adata.uns['pca'] = {\n",
    "                'variance_ratio': svd.explained_variance_ratio_,\n",
    "                'variance': svd.explained_variance_,\n",
    "            }\n",
    "            loadings = np.zeros((adata.n_vars, n_components), dtype=np.float32)\n",
    "            loadings[hvg_mask, :] = svd.components_.T.astype(np.float32)\n",
    "            adata.varm['PCs'] = loadings\n",
    "            \n",
    "            del X_hvg, svd, X_pca, loadings\n",
    "            gc.collect()\n",
    "            print(f\"  PCA complete. Variance explained: {adata.uns['pca']['variance_ratio'].sum():.2%}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"  Data is dense, scaling HVGs only...\")\n",
    "            X_hvg = adata.X[:, hvg_mask].copy()\n",
    "            scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "            X_hvg_scaled = scaler.fit_transform(X_hvg)\n",
    "            del X_hvg\n",
    "            gc.collect()\n",
    "            \n",
    "            from sklearn.decomposition import PCA\n",
    "            n_components = min(50, n_hvgs - 1, X_hvg_scaled.shape[0] - 1)\n",
    "            pca = PCA(n_components=n_components, random_state=42)\n",
    "            X_pca = pca.fit_transform(X_hvg_scaled)\n",
    "            \n",
    "            adata.obsm['X_pca'] = X_pca.astype(np.float32)\n",
    "            adata.uns['pca'] = {\n",
    "                'variance_ratio': pca.explained_variance_ratio_,\n",
    "                'variance': pca.explained_variance_,\n",
    "            }\n",
    "            loadings = np.zeros((adata.n_vars, n_components), dtype=np.float32)\n",
    "            loadings[hvg_mask, :] = pca.components_.T.astype(np.float32)\n",
    "            adata.varm['PCs'] = loadings\n",
    "            \n",
    "            del X_hvg_scaled, pca, X_pca, loadings, scaler\n",
    "            gc.collect()\n",
    "        \n",
    "        # Memory cleanup after PCA\n",
    "        try:\n",
    "            if getattr(adata, 'raw', None) is not None:\n",
    "                del adata.raw\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            if hasattr(adata, 'layers') and len(adata.layers) > 0:\n",
    "                adata.layers.clear()\n",
    "        except Exception:\n",
    "            pass\n",
    "        gc.collect()\n",
    "        print(\"  Memory cleanup after PCA complete.\")\n",
    "\n",
    "    # Neighbors\n",
    "    print(\"Computing neighbors...\")\n",
    "    sc.pp.neighbors(adata, n_neighbors=15, n_pcs=50, random_state=42)\n",
    "    gc.collect()\n",
    "\n",
    "    # 2. Perform Clustering (Leiden) - Use fewer resolutions for speed\n",
    "    print(\"Performing Leiden clustering...\")\n",
    "    resolutions = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]  # Reduced from 26 to 6\n",
    "    best_res = 0.1\n",
    "    target_clusters = 7\n",
    "    best_diff = float('inf')\n",
    "\n",
    "    for res in resolutions:\n",
    "        key = f'leiden_{res}'\n",
    "        try:\n",
    "            sc.tl.leiden(adata, resolution=res, key_added=key, random_state=42)\n",
    "            n_clust = len(adata.obs[key].unique())\n",
    "            print(f\"Resolution {res}: {n_clust} clusters\")\n",
    "            if abs(n_clust - target_clusters) < best_diff:\n",
    "                best_diff = abs(n_clust - target_clusters)\n",
    "                best_res = res\n",
    "        except Exception as e:\n",
    "            print(f\"Leiden failed for resolution {res}: {e}\")\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"Selected resolution: {best_res}\")\n",
    "    if f'leiden_{best_res}' in adata.obs:\n",
    "        adata.obs['leiden'] = adata.obs[f'leiden_{best_res}']\n",
    "\n",
    "    # 3. TCR Sequence Clustering\n",
    "    print(\"Performing TCR sequence-specific clustering...\")\n",
    "    if 'X_tcr_tra_kmer' in adata.obsm:\n",
    "        tra_scaler = StandardScaler()\n",
    "        tra_kmer_scaled = tra_scaler.fit_transform(adata.obsm['X_tcr_tra_kmer'])\n",
    "        tra_kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
    "        adata.obs['tra_kmer_clusters'] = pd.Categorical(tra_kmeans.fit_predict(tra_kmer_scaled))\n",
    "        del tra_kmer_scaled, tra_kmeans, tra_scaler\n",
    "        gc.collect()\n",
    "\n",
    "    if 'X_tcr_trb_kmer' in adata.obsm:\n",
    "        trb_scaler = StandardScaler()\n",
    "        trb_kmer_scaled = trb_scaler.fit_transform(adata.obsm['X_tcr_trb_kmer'])\n",
    "        trb_kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
    "        adata.obs['trb_kmer_clusters'] = pd.Categorical(trb_kmeans.fit_predict(trb_kmer_scaled))\n",
    "        del trb_kmer_scaled, trb_kmeans, trb_scaler\n",
    "        gc.collect()\n",
    "\n",
    "    # 4. Gene Expression Module Discovery\n",
    "    print(\"Discovering gene expression modules...\")\n",
    "    gene_pca = adata.obsm.get('X_gene_pca', adata.obsm.get('X_pca'))\n",
    "    if gene_pca is not None:\n",
    "        gene_kmeans = KMeans(n_clusters=8, random_state=42, n_init=10)\n",
    "        adata.obs['gene_expression_modules'] = pd.Categorical(gene_kmeans.fit_predict(gene_pca))\n",
    "        del gene_pca, gene_kmeans\n",
    "        gc.collect()\n",
    "\n",
    "    # 5. Visualization\n",
    "    print(\"Creating visualizations...\")\n",
    "    sc.tl.umap(adata, random_state=42)\n",
    "    \n",
    "    color_keys = []\n",
    "    if 'leiden' in adata.obs:\n",
    "        color_keys.append('leiden')\n",
    "    if 'response' in adata.obs.columns:\n",
    "        color_keys.append('response')\n",
    "    \n",
    "    if color_keys:\n",
    "        sc.pl.umap(adata, color=color_keys, show=False)\n",
    "        plt.show()\n",
    "\n",
    "    # --- Create supervised_mask for downstream cells ---\n",
    "    if 'response' in adata.obs.columns:\n",
    "        supervised_mask = adata.obs['response'].isin(['Responder', 'Non-Responder']).values\n",
    "        print(f\"\\nCreated supervised_mask: {supervised_mask.sum()} samples with valid response labels\")\n",
    "    else:\n",
    "        supervised_mask = np.ones(adata.n_obs, dtype=bool)\n",
    "        print(\"\\nWarning: No response column found. supervised_mask includes all cells.\")\n",
    "\n",
    "    print(\"Unsupervised machine learning analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71affd47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:47:34.714956Z",
     "iopub.status.busy": "2026-02-06T05:47:34.714646Z",
     "iopub.status.idle": "2026-02-06T05:47:35.214810Z",
     "shell.execute_reply": "2026-02-06T05:47:35.213962Z"
    },
    "papermill": {
     "duration": 0.523678,
     "end_time": "2026-02-06T05:47:35.216309",
     "exception": false,
     "start_time": "2026-02-06T05:47:34.692631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running memory cleanup after Leiden clustering (before dendrogram)...\n",
      "Memory before cleanup: 5001 MB\n",
      "Fallback cleanup completed.\n",
      "Memory after cleanup: 5001 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Memory cleanup (after Leiden clustering, before dendrogram) ---\n",
    "# This frees large temporary matrices (one-hot encodings, neighbor/connectivity matrices)\n",
    "# while keeping UMAP for dendrogram/visualization.\n",
    "print('\\nRunning memory cleanup after Leiden clustering (before dendrogram)...')\n",
    "try:\n",
    "    import psutil, os\n",
    "    proc = psutil.Process(os.getpid())\n",
    "    print(f\"Memory before cleanup: {proc.memory_info().rss // (1024**2)} MB\")\n",
    "except Exception:\n",
    "    print('psutil not available; skipping memory before measurement')\n",
    "\n",
    "def _fallback_cleanup(drop_onehot=False, drop_raw=False, drop_obsm_umap_tsne=False, verbose=True):\n",
    "    \"\"\"Basic cleanup fallback when cleanup_after_clustering is unavailable.\"\"\"\n",
    "    if 'adata' not in globals():\n",
    "        return\n",
    "    if hasattr(adata, 'obsp'):\n",
    "        for _k in list(adata.obsp.keys()):\n",
    "            try:\n",
    "                del adata.obsp[_k]\n",
    "            except Exception:\n",
    "                pass\n",
    "    if drop_onehot and hasattr(adata, 'obsm'):\n",
    "        for _key in ['X_tcr_tra_onehot', 'X_tcr_trb_onehot']:\n",
    "            if _key in adata.obsm:\n",
    "                try:\n",
    "                    del adata.obsm[_key]\n",
    "                except Exception:\n",
    "                    pass\n",
    "    if drop_obsm_umap_tsne and hasattr(adata, 'obsm'):\n",
    "        for _key in list(adata.obsm.keys()):\n",
    "            _lk = _key.lower()\n",
    "            if 'umap' in _lk or 'tsne' in _lk:\n",
    "                try:\n",
    "                    del adata.obsm[_key]\n",
    "                except Exception:\n",
    "                    pass\n",
    "    if drop_raw and getattr(adata, 'raw', None) is not None:\n",
    "        adata.raw = None\n",
    "    if verbose:\n",
    "        print('Fallback cleanup completed.')\n",
    "\n",
    "# Conservative cleanup: drop TCR one-hot arrays and obsp connectivities/distances\n",
    "# Keep one-hot encodings by default to avoid KeyError in downstream feature engineering\n",
    "if 'cleanup_after_clustering' in globals():\n",
    "    try:\n",
    "        cleanup_after_clustering(drop_onehot=False, drop_raw=False, drop_obsm_umap_tsne=False, verbose=True)\n",
    "    except Exception as e:\n",
    "        print('cleanup_after_clustering failed, using fallback cleanup:', e)\n",
    "        _fallback_cleanup(drop_onehot=False, drop_raw=False, drop_obsm_umap_tsne=False, verbose=True)\n",
    "else:\n",
    "    _fallback_cleanup(drop_onehot=False, drop_raw=False, drop_obsm_umap_tsne=False, verbose=True)\n",
    "\n",
    "try:\n",
    "    proc = psutil.Process(os.getpid())\n",
    "    print(f\"Memory after cleanup: {proc.memory_info().rss // (1024**2)} MB\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66da2d01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:47:35.258429Z",
     "iopub.status.busy": "2026-02-06T05:47:35.257988Z",
     "iopub.status.idle": "2026-02-06T05:47:35.509030Z",
     "shell.execute_reply": "2026-02-06T05:47:35.507863Z"
    },
    "papermill": {
     "duration": 0.274598,
     "end_time": "2026-02-06T05:47:35.510991",
     "exception": false,
     "start_time": "2026-02-06T05:47:35.236393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response distribution: {'Non-Responder': 63074, 'Responder': 36993}\n",
      "Patient_id coverage: 100067/100067\n",
      "Working with 100067 samples for supervised learning\n",
      "Class distribution: {'Non-Responder': 63074, 'Responder': 36993}\n"
     ]
    }
   ],
   "source": [
    "# --- Label/patient derivation and supervised availability helpers ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "if '_ensure_int_labels' not in globals():\n",
    "    def _ensure_int_labels(y):\n",
    "        y_arr = np.asarray(y)\n",
    "        if np.issubdtype(y_arr.dtype, np.integer):\n",
    "            return y_arr\n",
    "        if y_arr.size == 0:\n",
    "            return y_arr.astype(np.int64)\n",
    "        if np.all(np.isfinite(y_arr)) and np.all(np.equal(y_arr, np.floor(y_arr))):\n",
    "            return y_arr.astype(np.int64)\n",
    "        raise ValueError(\"Labels must be integer or integer-like floats.\")\n",
    "if '_normalize_response_value' not in globals():\n",
    "    def _normalize_response_value(val):\n",
    "        if pd.isna(val):\n",
    "            return 'Unknown'\n",
    "        s = str(val).strip().lower()\n",
    "        if s == '':\n",
    "            return 'Unknown'\n",
    "        if 'non' in s and 'responder' in s:\n",
    "            return 'Non-Responder'\n",
    "        if 'responder' in s:\n",
    "            return 'Responder'\n",
    "        return 'Unknown'\n",
    "if '_ensure_response_and_patient' not in globals():\n",
    "    def _ensure_response_and_patient(adata):\n",
    "        # Normalize existing columns if present\n",
    "        if 'response' not in adata.obs.columns and 'Response' in adata.obs.columns:\n",
    "            adata.obs['response'] = adata.obs['Response']\n",
    "        if 'patient_id' not in adata.obs.columns:\n",
    "            for _col in ['Patient_ID', 'PatientID']:\n",
    "                if _col in adata.obs.columns:\n",
    "                    adata.obs['patient_id'] = adata.obs[_col]\n",
    "                    break\n",
    "        # Determine metadata mapping\n",
    "        md = None\n",
    "        if 'metadata_df' in globals() and isinstance(metadata_df, pd.DataFrame) and not metadata_df.empty:\n",
    "            md = metadata_df.copy()\n",
    "        else:\n",
    "            # Fallback to hard-coded metadata list (same as in Cell 15)\n",
    "            _metadata_list = [\n",
    "                {'S_Number': 'S1',  'GEX_Sample_ID': 'GSM9061665', 'TCR_Sample_ID': 'GSM9061687', 'Patient_ID': 'PT1',  'Timepoint': 'Baseline',   'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S2',  'GEX_Sample_ID': 'GSM9061666', 'TCR_Sample_ID': 'GSM9061688', 'Patient_ID': 'PT1',  'Timepoint': 'Post-Tx',    'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S3',  'GEX_Sample_ID': 'GSM9061667', 'TCR_Sample_ID': 'GSM9061689', 'Patient_ID': 'PT1',  'Timepoint': 'Recurrence', 'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S4',  'GEX_Sample_ID': 'GSM9061668', 'TCR_Sample_ID': 'GSM9061690', 'Patient_ID': 'PT2',  'Timepoint': 'Baseline',   'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S5',  'GEX_Sample_ID': 'GSM9061669', 'TCR_Sample_ID': 'GSM9061691', 'Patient_ID': 'PT2',  'Timepoint': 'Post-Tx',    'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S6',  'GEX_Sample_ID': 'GSM9061670', 'TCR_Sample_ID': 'GSM9061692', 'Patient_ID': 'PT3',  'Timepoint': 'Baseline',   'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S7',  'GEX_Sample_ID': 'GSM9061671', 'TCR_Sample_ID': 'GSM9061693', 'Patient_ID': 'PT3',  'Timepoint': 'Post-Tx',    'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S8',  'GEX_Sample_ID': 'GSM9061672', 'TCR_Sample_ID': None,         'Patient_ID': 'PT3',  'Timepoint': 'Recurrence', 'Response': 'Non-Responder', 'In_Data': 'GEX only'},\n",
    "                {'S_Number': 'S9',  'GEX_Sample_ID': 'GSM9061673', 'TCR_Sample_ID': 'GSM9061694', 'Patient_ID': 'PT4',  'Timepoint': 'Baseline',   'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S10', 'GEX_Sample_ID': 'GSM9061674', 'TCR_Sample_ID': 'GSM9061695', 'Patient_ID': 'PT4',  'Timepoint': 'Post-Tx',    'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S11', 'GEX_Sample_ID': 'GSM9061675', 'TCR_Sample_ID': 'GSM9061696', 'Patient_ID': 'PT4',  'Timepoint': 'Recurrence', 'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "            ]\n",
    "            md = pd.DataFrame(_metadata_list)\n",
    "        # Identify mapping columns in metadata\n",
    "        sample_col = None\n",
    "        for _c in ['sample_id', 'GEX_Sample_ID', 'GSM_ID', 'GEO_ID', 'Sample_ID']:\n",
    "            if _c in md.columns:\n",
    "                sample_col = _c\n",
    "                break\n",
    "        patient_col = None\n",
    "        for _c in ['patient_id', 'Patient_ID', 'PatientID']:\n",
    "            if _c in md.columns:\n",
    "                patient_col = _c\n",
    "                break\n",
    "        response_col = None\n",
    "        for _c in ['response', 'Response']:\n",
    "            if _c in md.columns:\n",
    "                response_col = _c\n",
    "                break\n",
    "        # Determine sample ID series from adata\n",
    "        sample_series = None\n",
    "        for _c in ['sample_id', 'batch']:\n",
    "            if _c in adata.obs.columns:\n",
    "                sample_series = adata.obs[_c].astype(str)\n",
    "                break\n",
    "        if md is not None and sample_series is not None and sample_col is not None:\n",
    "            sample_key = sample_series.str.split('_').str[0]\n",
    "            md_sample = md[sample_col].astype(str)\n",
    "            if patient_col is not None:\n",
    "                patient_map = dict(zip(md_sample, md[patient_col]))\n",
    "                if 'patient_id' not in adata.obs.columns:\n",
    "                    adata.obs['patient_id'] = sample_key.map(patient_map)\n",
    "                else:\n",
    "                    adata.obs['patient_id'] = adata.obs['patient_id'].where(adata.obs['patient_id'].notna(), sample_key.map(patient_map))\n",
    "            if response_col is not None:\n",
    "                resp_map = dict(zip(md_sample, md[response_col]))\n",
    "                if 'response' not in adata.obs.columns:\n",
    "                    adata.obs['response'] = sample_key.map(resp_map)\n",
    "                else:\n",
    "                    adata.obs['response'] = adata.obs['response'].where(adata.obs['response'].notna(), sample_key.map(resp_map))\n",
    "        # Normalize response labels\n",
    "        if 'response' in adata.obs.columns:\n",
    "            adata.obs['response'] = adata.obs['response'].apply(_normalize_response_value)\n",
    "        # Coverage reporting\n",
    "        if 'response' in adata.obs.columns:\n",
    "            resp_counts = adata.obs['response'].value_counts(dropna=False).to_dict()\n",
    "            print(f\"Response distribution: {resp_counts}\")\n",
    "        if 'patient_id' in adata.obs.columns:\n",
    "            mapped = adata.obs['patient_id'].notna().sum()\n",
    "            print(f\"Patient_id coverage: {mapped}/{len(adata.obs)}\")\n",
    "if '_get_supervised_mask_and_labels' not in globals():\n",
    "    def _get_supervised_mask_and_labels(adata):\n",
    "        if 'response' not in adata.obs.columns:\n",
    "            print(\"WARNING: response column missing. No supervised labels available.\")\n",
    "            supervised_mask = np.zeros(adata.n_obs, dtype=bool)\n",
    "            return supervised_mask, pd.Series([], dtype=object), None, {}, False\n",
    "        y_all = adata.obs['response'].astype(str)\n",
    "        supervised_mask = y_all.isin(['Responder', 'Non-Responder']).values\n",
    "        y_supervised = y_all[supervised_mask]\n",
    "        if len(y_supervised) == 0:\n",
    "            print(\"WARNING: No labeled samples found for supervised learning.\")\n",
    "            return supervised_mask, y_supervised, None, {}, False\n",
    "        class_counts = y_supervised.value_counts().to_dict()\n",
    "        if len(class_counts) < 2 or min(class_counts.values()) < 2:\n",
    "            print(f\"WARNING: Insufficient class balance for supervised learning: {class_counts}\")\n",
    "            return supervised_mask, y_supervised, None, class_counts, False\n",
    "        le = LabelEncoder()\n",
    "        return supervised_mask, y_supervised, le, class_counts, True\n",
    "# Ensure response/patient labels are present and normalized\n",
    "if 'adata' not in globals() or adata is None:\n",
    "    raise NameError(\"adata is not defined. Please run the data loading cells first.\")\n",
    "_ensure_response_and_patient(adata)\n",
    "supervised_mask, y_supervised, label_encoder, class_counts, SUPERVISED_AVAILABLE = _get_supervised_mask_and_labels(adata)\n",
    "globals()['SUPERVISED_AVAILABLE'] = SUPERVISED_AVAILABLE\n",
    "if SUPERVISED_AVAILABLE:\n",
    "    y_encoded = label_encoder.fit_transform(y_supervised)\n",
    "    y_encoded = _ensure_int_labels(y_encoded)\n",
    "    print(f\"Working with {int(supervised_mask.sum())} samples for supervised learning\")\n",
    "    print(f\"Class distribution: {class_counts}\")\n",
    "else:\n",
    "    print(\"WARNING: Supervised labels not available or insufficient. Skipping supervised-only steps.\")\n",
    "    supervised_mask = np.ones(adata.n_obs, dtype=bool)\n",
    "    y_supervised = pd.Series(['Unknown'] * adata.n_obs, index=adata.obs.index)\n",
    "    y_encoded = np.array([], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcc86b8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:47:35.555608Z",
     "iopub.status.busy": "2026-02-06T05:47:35.555068Z",
     "iopub.status.idle": "2026-02-06T05:47:35.945506Z",
     "shell.execute_reply": "2026-02-06T05:47:35.944454Z"
    },
    "papermill": {
     "duration": 0.414837,
     "end_time": "2026-02-06T05:47:35.947361",
     "exception": false,
     "start_time": "2026-02-06T05:47:35.532524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating comprehensive feature set using ALL available encodings...\n",
      "Applying strategic dimensionality reduction to high-dimensional features...\n",
      "Response distribution: {'Non-Responder': 63074, 'Responder': 36993}\n",
      "Patient_id coverage: 100067/100067\n",
      "Working with 100067 samples for supervised learning\n",
      "Class distribution: {'Non-Responder': 63074, 'Responder': 36993}\n",
      "Reducing k-mer features by variance selection...\n",
      "TRA k-mers reduced from 1 to 1\n",
      "TRB k-mers reduced from 1 to 1\n",
      "\n",
      "Feature set dimensions:\n",
      "  ‚Ä¢ basic: (100067, 29)\n",
      "  ‚Ä¢ gene_enhanced: (100067, 79)\n",
      "  ‚Ä¢ tcr_enhanced: (100067, 31)\n",
      "  ‚Ä¢ comprehensive: (100067, 26)\n",
      "Comprehensive feature engineering completed!\n",
      "CPU times: user 337 ms, sys: 18.1 ms, total: 355 ms\n",
      "Wall time: 353 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Comprehensive Feature Engineering ---\n",
    "\n",
    "print(\"Creating comprehensive feature set using ALL available encodings...\")\n",
    "\n",
    "# --- 1. Strategic Feature Engineering with Dimensionality Reduction ---\n",
    "print(\"Applying strategic dimensionality reduction to high-dimensional features...\")\n",
    "\n",
    "# --- Label/patient derivation and supervised availability helpers ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "if '_ensure_int_labels' not in globals():\n",
    "    def _ensure_int_labels(y):\n",
    "        y_arr = np.asarray(y)\n",
    "        if np.issubdtype(y_arr.dtype, np.integer):\n",
    "            return y_arr\n",
    "        if y_arr.size == 0:\n",
    "            return y_arr.astype(np.int64)\n",
    "        if np.all(np.isfinite(y_arr)) and np.all(np.equal(y_arr, np.floor(y_arr))):\n",
    "            return y_arr.astype(np.int64)\n",
    "        raise ValueError(\"Labels must be integer or integer-like floats.\")\n",
    "\n",
    "if '_normalize_response_value' not in globals():\n",
    "    def _normalize_response_value(val):\n",
    "        if pd.isna(val):\n",
    "            return 'Unknown'\n",
    "        s = str(val).strip().lower()\n",
    "        if s == '':\n",
    "            return 'Unknown'\n",
    "        if 'non' in s and 'responder' in s:\n",
    "            return 'Non-Responder'\n",
    "        if 'responder' in s:\n",
    "            return 'Responder'\n",
    "        return 'Unknown'\n",
    "\n",
    "if '_ensure_response_and_patient' not in globals():\n",
    "    def _ensure_response_and_patient(adata):\n",
    "        # Normalize existing columns if present\n",
    "        if 'response' not in adata.obs.columns and 'Response' in adata.obs.columns:\n",
    "            adata.obs['response'] = adata.obs['Response']\n",
    "        if 'patient_id' not in adata.obs.columns:\n",
    "            for _col in ['Patient_ID', 'PatientID']:\n",
    "                if _col in adata.obs.columns:\n",
    "                    adata.obs['patient_id'] = adata.obs[_col]\n",
    "                    break\n",
    "\n",
    "        # Determine metadata mapping\n",
    "        md = None\n",
    "        if 'metadata_df' in globals() and isinstance(metadata_df, pd.DataFrame) and not metadata_df.empty:\n",
    "            md = metadata_df.copy()\n",
    "        else:\n",
    "            # Fallback to hard-coded metadata list (same as in Cell 15)\n",
    "            _metadata_list = [\n",
    "                {'S_Number': 'S1',  'GEX_Sample_ID': 'GSM9061665', 'TCR_Sample_ID': 'GSM9061687', 'Patient_ID': 'PT1',  'Timepoint': 'Baseline',   'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S2',  'GEX_Sample_ID': 'GSM9061666', 'TCR_Sample_ID': 'GSM9061688', 'Patient_ID': 'PT1',  'Timepoint': 'Post-Tx',    'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S3',  'GEX_Sample_ID': 'GSM9061667', 'TCR_Sample_ID': 'GSM9061689', 'Patient_ID': 'PT1',  'Timepoint': 'Recurrence', 'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S4',  'GEX_Sample_ID': 'GSM9061668', 'TCR_Sample_ID': 'GSM9061690', 'Patient_ID': 'PT2',  'Timepoint': 'Baseline',   'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S5',  'GEX_Sample_ID': 'GSM9061669', 'TCR_Sample_ID': 'GSM9061691', 'Patient_ID': 'PT2',  'Timepoint': 'Post-Tx',    'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S6',  'GEX_Sample_ID': 'GSM9061670', 'TCR_Sample_ID': 'GSM9061692', 'Patient_ID': 'PT3',  'Timepoint': 'Baseline',   'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S7',  'GEX_Sample_ID': 'GSM9061671', 'TCR_Sample_ID': 'GSM9061693', 'Patient_ID': 'PT3',  'Timepoint': 'Post-Tx',    'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S8',  'GEX_Sample_ID': 'GSM9061672', 'TCR_Sample_ID': None,         'Patient_ID': 'PT3',  'Timepoint': 'Recurrence', 'Response': 'Non-Responder', 'In_Data': 'GEX only'},\n",
    "                {'S_Number': 'S9',  'GEX_Sample_ID': 'GSM9061673', 'TCR_Sample_ID': 'GSM9061694', 'Patient_ID': 'PT4',  'Timepoint': 'Baseline',   'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S10', 'GEX_Sample_ID': 'GSM9061674', 'TCR_Sample_ID': 'GSM9061695', 'Patient_ID': 'PT4',  'Timepoint': 'Post-Tx',    'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S11', 'GEX_Sample_ID': 'GSM9061675', 'TCR_Sample_ID': 'GSM9061696', 'Patient_ID': 'PT4',  'Timepoint': 'Recurrence', 'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "            ]\n",
    "            md = pd.DataFrame(_metadata_list)\n",
    "\n",
    "        # Identify mapping columns in metadata\n",
    "        sample_col = None\n",
    "        for _c in ['sample_id', 'GEX_Sample_ID', 'GSM_ID', 'GEO_ID', 'Sample_ID']:\n",
    "            if _c in md.columns:\n",
    "                sample_col = _c\n",
    "                break\n",
    "        patient_col = None\n",
    "        for _c in ['patient_id', 'Patient_ID', 'PatientID']:\n",
    "            if _c in md.columns:\n",
    "                patient_col = _c\n",
    "                break\n",
    "        response_col = None\n",
    "        for _c in ['response', 'Response']:\n",
    "            if _c in md.columns:\n",
    "                response_col = _c\n",
    "                break\n",
    "\n",
    "        # Determine sample ID series from adata\n",
    "        sample_series = None\n",
    "        for _c in ['sample_id', 'batch']:\n",
    "            if _c in adata.obs.columns:\n",
    "                sample_series = adata.obs[_c].astype(str)\n",
    "                break\n",
    "\n",
    "        if md is not None and sample_series is not None and sample_col is not None:\n",
    "            sample_key = sample_series.str.split('_').str[0]\n",
    "            md_sample = md[sample_col].astype(str)\n",
    "\n",
    "            if patient_col is not None:\n",
    "                patient_map = dict(zip(md_sample, md[patient_col]))\n",
    "                if 'patient_id' not in adata.obs.columns:\n",
    "                    adata.obs['patient_id'] = sample_key.map(patient_map)\n",
    "                else:\n",
    "                    adata.obs['patient_id'] = adata.obs['patient_id'].where(adata.obs['patient_id'].notna(), sample_key.map(patient_map))\n",
    "\n",
    "            if response_col is not None:\n",
    "                resp_map = dict(zip(md_sample, md[response_col]))\n",
    "                if 'response' not in adata.obs.columns:\n",
    "                    adata.obs['response'] = sample_key.map(resp_map)\n",
    "                else:\n",
    "                    adata.obs['response'] = adata.obs['response'].where(adata.obs['response'].notna(), sample_key.map(resp_map))\n",
    "\n",
    "        # Normalize response labels\n",
    "        if 'response' in adata.obs.columns:\n",
    "            adata.obs['response'] = adata.obs['response'].apply(_normalize_response_value)\n",
    "\n",
    "        # Coverage reporting\n",
    "        if 'response' in adata.obs.columns:\n",
    "            resp_counts = adata.obs['response'].value_counts(dropna=False).to_dict()\n",
    "            print(f\"Response distribution: {resp_counts}\")\n",
    "        if 'patient_id' in adata.obs.columns:\n",
    "            mapped = adata.obs['patient_id'].notna().sum()\n",
    "            print(f\"Patient_id coverage: {mapped}/{len(adata.obs)}\")\n",
    "\n",
    "if '_get_supervised_mask_and_labels' not in globals():\n",
    "    def _get_supervised_mask_and_labels(adata):\n",
    "        if 'response' not in adata.obs.columns:\n",
    "            print(\"WARNING: response column missing. No supervised labels available.\")\n",
    "            supervised_mask = np.zeros(adata.n_obs, dtype=bool)\n",
    "            return supervised_mask, pd.Series([], dtype=object), None, {}, False\n",
    "        y_all = adata.obs['response'].astype(str)\n",
    "        supervised_mask = y_all.isin(['Responder', 'Non-Responder']).values\n",
    "        y_supervised = y_all[supervised_mask]\n",
    "        if len(y_supervised) == 0:\n",
    "            print(\"WARNING: No labeled samples found for supervised learning.\")\n",
    "            return supervised_mask, y_supervised, None, {}, False\n",
    "        class_counts = y_supervised.value_counts().to_dict()\n",
    "        if len(class_counts) < 2 or min(class_counts.values()) < 2:\n",
    "            print(f\"WARNING: Insufficient class balance for supervised learning: {class_counts}\")\n",
    "            return supervised_mask, y_supervised, None, class_counts, False\n",
    "        le = LabelEncoder()\n",
    "        return supervised_mask, y_supervised, le, class_counts, True\n",
    "\n",
    "# Ensure response/patient labels are present and normalized\n",
    "if 'adata' not in globals() or adata is None:\n",
    "    raise NameError(\"adata is not defined. Please run the data loading cells first.\")\n",
    "\n",
    "_ensure_response_and_patient(adata)\n",
    "\n",
    "supervised_mask, y_supervised, label_encoder, class_counts, SUPERVISED_AVAILABLE = _get_supervised_mask_and_labels(adata)\n",
    "globals()['SUPERVISED_AVAILABLE'] = SUPERVISED_AVAILABLE\n",
    "\n",
    "if SUPERVISED_AVAILABLE:\n",
    "    y_encoded = label_encoder.fit_transform(y_supervised)\n",
    "    y_encoded = _ensure_int_labels(y_encoded)\n",
    "    print(f\"Working with {int(supervised_mask.sum())} samples for supervised learning\")\n",
    "    print(f\"Class distribution: {class_counts}\")\n",
    "else:\n",
    "    print(\"WARNING: Supervised labels not available or insufficient. Skipping supervised-only steps.\")\n",
    "    supervised_mask = np.ones(adata.n_obs, dtype=bool)\n",
    "    y_supervised = pd.Series(['Unknown'] * adata.n_obs, index=adata.obs.index)\n",
    "    y_encoded = np.array([], dtype=np.int64)\n",
    "\n",
    "# --- Reduce high-dimensional k-mer features using variance-based selection ---\n",
    "# Check if k-mer features exist\n",
    "has_tra_kmer = 'X_tcr_tra_kmer' in adata.obsm\n",
    "has_trb_kmer = 'X_tcr_trb_kmer' in adata.obsm\n",
    "\n",
    "if has_tra_kmer:\n",
    "    tra_kmer_supervised = adata.obsm['X_tcr_tra_kmer'][supervised_mask]\n",
    "else:\n",
    "    print(\"Warning: X_tcr_tra_kmer not found. Using placeholder.\")\n",
    "    tra_kmer_supervised = np.zeros((sum(supervised_mask), 100))\n",
    "\n",
    "if has_trb_kmer:\n",
    "    trb_kmer_supervised = adata.obsm['X_tcr_trb_kmer'][supervised_mask]\n",
    "else:\n",
    "    print(\"Warning: X_tcr_trb_kmer not found. Using placeholder.\")\n",
    "    trb_kmer_supervised = np.zeros((sum(supervised_mask), 100))\n",
    "\n",
    "# Select top variance k-mers to reduce dimensionality\n",
    "def select_top_variance_features(X, n_features=200):\n",
    "    \"\"\"Select features with highest variance\"\"\"\n",
    "    variances = np.var(X, axis=0)\n",
    "    n_features = min(n_features, X.shape[1])  # Don't select more features than exist\n",
    "    top_indices = np.argsort(variances)[-n_features:]\n",
    "    return X[:, top_indices], top_indices\n",
    "\n",
    "print(\"Reducing k-mer features by variance selection...\")\n",
    "tra_kmer_reduced, tra_top_idx = select_top_variance_features(tra_kmer_supervised, n_features=200)\n",
    "trb_kmer_reduced, trb_top_idx = select_top_variance_features(trb_kmer_supervised, n_features=200)\n",
    "\n",
    "print(f\"TRA k-mers reduced from {tra_kmer_supervised.shape[1]} to {tra_kmer_reduced.shape[1]}\")\n",
    "print(f\"TRB k-mers reduced from {trb_kmer_supervised.shape[1]} to {trb_kmer_reduced.shape[1]}\")\n",
    "\n",
    "# --- 2. Create strategic feature combinations ---\n",
    "feature_sets = {}\n",
    "\n",
    "# Helper function to safely get obsm arrays\n",
    "def _get_obsm_or_zeros(adata, key, mask, n_cols):\n",
    "    if key in adata.obsm:\n",
    "        arr = adata.obsm[key][mask]\n",
    "        return arr[:, :min(n_cols, arr.shape[1])]\n",
    "    return np.zeros((sum(mask), n_cols))\n",
    "\n",
    "# Get gene features (try X_gene_pca first, then X_pca)\n",
    "if 'X_gene_pca' in adata.obsm:\n",
    "    gene_features = adata.obsm['X_gene_pca'][supervised_mask]\n",
    "elif 'X_pca' in adata.obsm:\n",
    "    gene_features = adata.obsm['X_pca'][supervised_mask]\n",
    "else:\n",
    "    print(\"Warning: No gene PCA features found.\")\n",
    "    gene_features = np.zeros((sum(supervised_mask), 50))\n",
    "\n",
    "# TCR physicochemical features\n",
    "tcr_physico_cols_tra = ['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']\n",
    "tcr_physico_cols_trb = ['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']\n",
    "\n",
    "tra_physico = adata.obs[[c for c in tcr_physico_cols_tra if c in adata.obs.columns]].fillna(0)[supervised_mask].values \\\n",
    "    if any(c in adata.obs.columns for c in tcr_physico_cols_tra) else np.zeros((sum(supervised_mask), 3))\n",
    "trb_physico = adata.obs[[c for c in tcr_physico_cols_trb if c in adata.obs.columns]].fillna(0)[supervised_mask].values \\\n",
    "    if any(c in adata.obs.columns for c in tcr_physico_cols_trb) else np.zeros((sum(supervised_mask), 3))\n",
    "\n",
    "# Ensure 3 columns each\n",
    "if tra_physico.shape[1] < 3:\n",
    "    tra_physico = np.hstack([tra_physico, np.zeros((tra_physico.shape[0], 3 - tra_physico.shape[1]))])\n",
    "if trb_physico.shape[1] < 3:\n",
    "    trb_physico = np.hstack([trb_physico, np.zeros((trb_physico.shape[0], 3 - trb_physico.shape[1]))])\n",
    "\n",
    "tcr_physico = np.column_stack([tra_physico, trb_physico])\n",
    "\n",
    "# QC features\n",
    "qc_cols = ['n_genes_by_counts', 'total_counts', 'pct_counts_mt']\n",
    "available_qc = [c for c in qc_cols if c in adata.obs.columns]\n",
    "if available_qc:\n",
    "    qc_features = adata.obs[available_qc].fillna(0)[supervised_mask].values\n",
    "else:\n",
    "    qc_features = np.zeros((sum(supervised_mask), 3))\n",
    "\n",
    "# Ensure 3 columns for QC\n",
    "if qc_features.shape[1] < 3:\n",
    "    qc_features = np.hstack([qc_features, np.zeros((qc_features.shape[0], 3 - qc_features.shape[1]))])\n",
    "\n",
    "# Basic features (gene expression + physicochemical)\n",
    "feature_sets['basic'] = np.column_stack([\n",
    "    gene_features[:, :min(20, gene_features.shape[1])],  # Top 20 gene PCA components\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# Enhanced gene expression\n",
    "feature_sets['gene_enhanced'] = np.column_stack([\n",
    "    gene_features,  # All gene PCA components\n",
    "    _get_obsm_or_zeros(adata, 'X_gene_svd', supervised_mask, 30),  # Top 30 SVD components\n",
    "    _get_obsm_or_zeros(adata, 'X_gene_umap', supervised_mask, 20),  # All 20 UMAP components\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# TCR sequence enhanced\n",
    "feature_sets['tcr_enhanced'] = np.column_stack([\n",
    "    gene_features[:, :min(20, gene_features.shape[1])],  # Top 20 gene PCA\n",
    "    tra_kmer_reduced,  # Top 200 TRA k-mers\n",
    "    trb_kmer_reduced,  # Top 200 TRB k-mers\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# Comprehensive (reduced) - Only gene PCA + top k-mers + physicochemical\n",
    "feature_sets['comprehensive'] = np.column_stack([\n",
    "    gene_features[:, :min(15, gene_features.shape[1])],  # Top 15 gene PCA\n",
    "    tra_kmer_reduced[:, :min(50, tra_kmer_reduced.shape[1])],  # Top 50 TRA k-mers\n",
    "    trb_kmer_reduced[:, :min(50, trb_kmer_reduced.shape[1])],  # Top 50 TRB k-mers\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "print(f\"\\nFeature set dimensions:\")\n",
    "for name, features in feature_sets.items():\n",
    "    print(f\"  ‚Ä¢ {name}: {features.shape}\")\n",
    "\n",
    "print(\"Comprehensive feature engineering completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b553123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:47:35.992523Z",
     "iopub.status.busy": "2026-02-06T05:47:35.991672Z",
     "iopub.status.idle": "2026-02-06T05:47:36.547336Z",
     "shell.execute_reply": "2026-02-06T05:47:36.546172Z"
    },
    "papermill": {
     "duration": 0.580382,
     "end_time": "2026-02-06T05:47:36.549516",
     "exception": false,
     "start_time": "2026-02-06T05:47:35.969134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Correlation Analysis of Top Features ---\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Validate that adata exists\n",
    "if 'adata' not in globals() or adata is None:\n",
    "    raise NameError(\"adata is not defined. Please run the data loading cells first.\")\n",
    "\n",
    "# Ensure supervised_mask is defined\n",
    "if 'supervised_mask' not in globals():\n",
    "    if 'response' in adata.obs.columns:\n",
    "        supervised_mask = adata.obs['response'].isin(['Responder', 'Non-Responder']).values\n",
    "    elif 'Response' in adata.obs.columns:\n",
    "        supervised_mask = adata.obs['Response'].isin(['Responder', 'Non-Responder']).values\n",
    "    else:\n",
    "        supervised_mask = np.ones(adata.n_obs, dtype=bool)\n",
    "        print(\"Warning: No response column found. Using all cells.\")\n",
    "\n",
    "# Ensure tcr_physico and qc_features are defined\n",
    "if 'tcr_physico' not in globals():\n",
    "    # Extract TRA physicochemical features\n",
    "    tra_cols = ['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']\n",
    "    if all(col in adata.obs.columns for col in tra_cols):\n",
    "        tra_physico = adata.obs[tra_cols].fillna(0)[supervised_mask].values\n",
    "    else:\n",
    "        tra_physico = np.zeros((np.sum(supervised_mask), 3))\n",
    "    \n",
    "    # Extract TRB physicochemical features\n",
    "    trb_cols = ['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']\n",
    "    if all(col in adata.obs.columns for col in trb_cols):\n",
    "        trb_physico = adata.obs[trb_cols].fillna(0)[supervised_mask].values\n",
    "    else:\n",
    "        trb_physico = np.zeros((np.sum(supervised_mask), 3))\n",
    "    \n",
    "    # Combine TRA and TRB features\n",
    "    tcr_physico = np.hstack([tra_physico, trb_physico])\n",
    "\n",
    "if 'qc_features' not in globals():\n",
    "    qc_cols = ['n_genes_by_counts', 'total_counts', 'pct_counts_mt']\n",
    "    available_qc = [col for col in qc_cols if col in adata.obs.columns]\n",
    "    if available_qc:\n",
    "        qc_features = adata.obs[available_qc].fillna(0)[supervised_mask].values\n",
    "    else:\n",
    "        qc_features = np.zeros((np.sum(supervised_mask), 3))\n",
    "\n",
    "# Select a subset of features for the heatmap\n",
    "# We'll take the top 10 Gene PCs, top 5 physicochemical, and QC metrics\n",
    "# Ensure we have the data available\n",
    "if 'X_gene_pca' in adata.obsm:\n",
    "    gene_pcs = adata.obsm['X_gene_pca'][supervised_mask][:, :min(10, adata.obsm['X_gene_pca'].shape[1])]\n",
    "    gene_names = [f\"Gene_PC{i+1}\" for i in range(gene_pcs.shape[1])]\n",
    "elif 'X_pca' in adata.obsm:\n",
    "    gene_pcs = adata.obsm['X_pca'][supervised_mask][:, :min(10, adata.obsm['X_pca'].shape[1])]\n",
    "    gene_names = [f\"Gene_PC{i+1}\" for i in range(gene_pcs.shape[1])]\n",
    "else:\n",
    "    gene_pcs = np.zeros((np.sum(supervised_mask), 10))\n",
    "    gene_names = [f\"Placeholder_PC{i+1}\" for i in range(10)]\n",
    "\n",
    "heatmap_features = np.column_stack([\n",
    "    gene_pcs,\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "heatmap_feature_names = gene_names + \\\n",
    "                        ['TRA_Len', 'TRA_MW', 'TRA_Hydro', 'TRB_Len', 'TRB_MW', 'TRB_Hydro'] + \\\n",
    "                        ['n_genes', 'total_counts', 'pct_mt']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = np.corrcoef(heatmap_features, rowvar=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', center=0,\n",
    "            xticklabels=heatmap_feature_names, yticklabels=heatmap_feature_names,\n",
    "            linewidths=0.5, linecolor='gray', cbar_kws={\"shrink\": .8})\n",
    "plt.title(\"Feature Correlation Matrix (Top Gene PCs + TCR Features)\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a97548",
   "metadata": {
    "papermill": {
     "duration": 0.02089,
     "end_time": "2026-02-06T05:47:36.591643",
     "exception": false,
     "start_time": "2026-02-06T05:47:36.570753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Supervised Classification of Immunotherapy Response\n",
    "The core predictive task was formulated as a binary classification problem: predicting the patient response label (Responder vs. Non-Responder) for each individual cell. We evaluated a diverse suite of algorithms:\n",
    "*   **Logistic Regression:** A linear baseline model.\n",
    "*   **Decision Trees:** A simple, interpretable non-linear model.\n",
    "*   **Random Forest:** An ensemble of decision trees that reduces overfitting.\n",
    "*   **XGBoost (Extreme Gradient Boosting):** A highly optimized gradient boosting framework known for strong performance on tabular data.\n",
    "\n",
    "### Experimental Setup\n",
    "We designed our experiments to isolate the predictive value of different data modalities. We trained and evaluated models on four nested feature sets:\n",
    "1.  **Baseline:** Technical covariates only (e.g., mitochondrial percentage, library size).\n",
    "2.  **Gene-Enhanced:** Baseline + Gene Expression PCs.\n",
    "3.  **TCR-Enhanced:** Baseline + TCR Encodings (One-hot, K-mer, Physicochemical).\n",
    "4.  **Comprehensive:** Baseline + Gene Expression PCs + TCR Encodings.\n",
    "\n",
    "### Validation Strategy (Updated)\n",
    "To obtain patient-level generalization estimates and to avoid data leakage between cells from the same patient, we use a Leave-One-Patient-Out (LOPO) cross-validation as the outer evaluation loop. Hyperparameter tuning is performed within the training partitions using GroupKFold (grouped by patient) when possible, falling back to stratified folds only when the number of training patients is too small for grouped splits. Feature scaling and imputation are fit on training partitions only and applied to held-out patient data to ensure leakage-free evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec37ceba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:47:36.636100Z",
     "iopub.status.busy": "2026-02-06T05:47:36.635458Z",
     "iopub.status.idle": "2026-02-06T05:47:40.228352Z",
     "shell.execute_reply": "2026-02-06T05:47:40.227120Z"
    },
    "papermill": {
     "duration": 3.617378,
     "end_time": "2026-02-06T05:47:40.230288",
     "exception": false,
     "start_time": "2026-02-06T05:47:36.612910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.15.3)\r\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.0.2)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e597c6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:47:40.276187Z",
     "iopub.status.busy": "2026-02-06T05:47:40.275838Z",
     "iopub.status.idle": "2026-02-06T05:51:02.902210Z",
     "shell.execute_reply": "2026-02-06T05:51:02.901459Z"
    },
    "papermill": {
     "duration": 202.674952,
     "end_time": "2026-02-06T05:51:02.926979",
     "exception": false,
     "start_time": "2026-02-06T05:47:40.252027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting patient-level LOPO CV with leakage-safe pipelines (Optimized for Speed/Accuracy)...\n",
      "Supervised patients: 4 -> ['PT1' 'PT2' 'PT3' 'PT4']\n",
      "Per-patient response counts:\n",
      "response\n",
      "Non-Responder    2\n",
      "Responder        2\n",
      "Name: count, dtype: int64\n",
      "Cleaning up temporary variables and large matrices before ML.\n",
      "Failed to patch models_eval['XGBoost']: 'XGBClassifierSK' object has no attribute 'predictor'\n",
      "Patched models_eval['Random Forest'] to use n_jobs=-1.\n",
      "Patched param_grids['XGBoost'] with GPU options (method=device).\n",
      "Failed to patch models_eval['XGBoost']: 'XGBClassifierSK' object has no attribute 'predictor'\n",
      "Patched models_eval['Random Forest'] to use n_jobs=-1.\n",
      "Patched param_grids['XGBoost'] with GPU options (method=device).\n",
      "\n",
      "=== Feature set: basic (shape=(100067, 29)) ===\n",
      "LOPO fold 1/4 -- held patient(s): ['PT1']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'XGBClassifierSK' object has no attribute 'predictor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrouted_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mbase_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__sklearn_clone__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_clone__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_clone_parametrized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m__sklearn_clone__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sklearn_clone__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_clone_parametrized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_CHAR_MAX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_clone_parametrized\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mnew_object_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_object_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mnew_object_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mnew_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__sklearn_clone__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_clone__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_clone_parametrized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_clone_parametrized\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mestimator_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"get_params\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__sklearn_clone__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_clone__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_clone_parametrized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_clone_parametrized\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mestimator_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"get_params\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__sklearn_clone__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_clone__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_clone_parametrized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m__sklearn_clone__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sklearn_clone__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_clone_parametrized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_CHAR_MAX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_clone_parametrized\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mnew_object_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_object_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mnew_object_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;31m#                                     XGBModel -> BaseEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m         \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;31m# If the immediate parent defines get_params(), use that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"get_params\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0mdeep_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'XGBClassifierSK' object has no attribute 'predictor'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Patient-level LOPO CV (Leakage-safe) [OPTIMIZED] ---\n",
    "print(\"Starting patient-level LOPO CV with leakage-safe pipelines (Optimized for Speed/Accuracy)...\")\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut, GroupKFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from pathlib import Path\n",
    "import gc, time\n",
    "\n",
    "# --- Optimization Settings ---\n",
    "USE_RANDOM_SEARCH = True  # Use RandomizedSearchCV for speed\n",
    "N_ITER_SEARCH = 15        # Max hyperparameter combinations to try per fold\n",
    "N_JOBS_CV = -1            # Parallelize Cross-Validation (uses all cores)\n",
    "N_JOBS_MODEL = 1          # Single thread per model to avoid contention\n",
    "\n",
    "# Prepare grouping variable (patient) and supervised mask\n",
    "# Robust column detection for patient_id\n",
    "patient_id_col = None\n",
    "if 'patient_id' in adata.obs.columns:\n",
    "    patient_id_col = 'patient_id'\n",
    "elif 'Patient_ID' in adata.obs.columns:\n",
    "    adata.obs['patient_id'] = adata.obs['Patient_ID']  # Create lowercase copy\n",
    "    patient_id_col = 'patient_id'\n",
    "elif 'PatientID' in adata.obs.columns:\n",
    "    adata.obs['patient_id'] = adata.obs['PatientID']  # Create lowercase copy\n",
    "    patient_id_col = 'patient_id'\n",
    "else:\n",
    "    # Fallback 1: infer from sample_id using metadata_df\n",
    "    sample_col = None\n",
    "    for _c in ['sample_id', 'Sample_ID', 'GEX_Sample_ID', 'sample', 'Sample']:\n",
    "        if _c in adata.obs.columns:\n",
    "            sample_col = _c\n",
    "            break\n",
    "\n",
    "    if sample_col is not None and 'metadata_df' in globals():\n",
    "        if 'GEX_Sample_ID' in metadata_df.columns and 'Patient_ID' in metadata_df.columns:\n",
    "            sample_to_patient = (\n",
    "                metadata_df[['GEX_Sample_ID', 'Patient_ID']]\n",
    "                .dropna()\n",
    "                .drop_duplicates()\n",
    "                .set_index('GEX_Sample_ID')['Patient_ID']\n",
    "            )\n",
    "            adata.obs['patient_id'] = adata.obs[sample_col].map(sample_to_patient)\n",
    "        else:\n",
    "            md_cols = {c.lower(): c for c in metadata_df.columns}\n",
    "            if 'gex_sample_id' in md_cols and 'patient_id' in md_cols:\n",
    "                sample_to_patient = (\n",
    "                    metadata_df[[md_cols['gex_sample_id'], md_cols['patient_id']]]\n",
    "                    .dropna()\n",
    "                    .drop_duplicates()\n",
    "                    .set_index(md_cols['gex_sample_id'])[md_cols['patient_id']]\n",
    "                )\n",
    "                adata.obs['patient_id'] = adata.obs[sample_col].map(sample_to_patient)\n",
    "\n",
    "    # Fallback 2: parse patient id from sample_id strings (e.g., \"PT1\")\n",
    "    if 'patient_id' not in adata.obs.columns or adata.obs['patient_id'].isna().all():\n",
    "        if sample_col is not None:\n",
    "            adata.obs['patient_id'] = adata.obs[sample_col].astype(str).str.extract(r'(PT\\d+)')[0]\n",
    "\n",
    "    if 'patient_id' in adata.obs.columns and adata.obs['patient_id'].notna().any():\n",
    "        patient_id_col = 'patient_id'\n",
    "    elif adata.n_obs == 0:\n",
    "        adata.obs['patient_id'] = pd.Series(index=adata.obs.index, dtype='object')\n",
    "        patient_id_col = 'patient_id'\n",
    "    else:\n",
    "        raise KeyError(\n",
    "            \"No patient ID column found. Tried direct columns, metadata_df mapping, and parsing from sample_id.\"\n",
    "        )\n",
    "\n",
    "groups_all = np.array(adata.obs[patient_id_col][supervised_mask])\n",
    "unique_patients = np.unique(groups_all)\n",
    "print(f\"Supervised patients: {len(unique_patients)} -> {unique_patients}\")\n",
    "\n",
    "# --- EARLY VALIDATION: Check for empty supervised set ---\n",
    "if len(groups_all) == 0 or len(unique_patients) == 0:\n",
    "    print(\"WARNING: No supervised samples found (supervised_mask is empty).\")\n",
    "    print(\"Skipping patient-level LOPO CV and deep learning evaluation to prevent memory waste and errors.\")\n",
    "    print(\"This can happen if no samples have valid 'response' annotations.\")\n",
    "    lopo_summary_rows = []\n",
    "    dl_results_rows = []\n",
    "else:\n",
    "    # Per-patient response summary\n",
    "    patient_response_df = (\n",
    "        adata.obs[supervised_mask][[patient_id_col, 'response']]\n",
    "        .reset_index()\n",
    "        .drop_duplicates(subset=patient_id_col)\n",
    "        .set_index(patient_id_col)\n",
    "    )\n",
    "    print(\"Per-patient response counts:\")\n",
    "    print(patient_response_df['response'].value_counts())\n",
    "\n",
    "    # --- Memory cleanup ---\n",
    "    _start_cleanup = time.time()\n",
    "    print(\"Cleaning up temporary variables and large matrices before ML.\")\n",
    "    # Flags (defaults)\n",
    "    DROP_ONEHOT_OBSM = False\n",
    "    DROP_RAW = False\n",
    "    DROP_OBSM_UMAP_TSNE = True\n",
    "\n",
    "    _vars_to_delete = [\n",
    "        'tra_onehot','trb_onehot','tra_onehot_flat','trb_onehot_flat',\n",
    "        'onehot_tra_reduced','onehot_trb_reduced','onehot_trb_pca','onehot_trb_reduced_new',\n",
    "        'tmp','tmp1','tmp2','seq_scaler','seq_scaler_full','seq_scaler_flat','length_results'\n",
    "    ]\n",
    "    for _v in _vars_to_delete:\n",
    "        if _v in globals():\n",
    "            try:\n",
    "                del globals()[_v]\n",
    "            except Exception: pass\n",
    "\n",
    "    try:\n",
    "        if hasattr(adata, 'obsp'):\n",
    "            for _k in list(adata.obsp.keys()): \n",
    "                try: del adata.obsp[_k]\n",
    "                except: pass\n",
    "        for _k in ['neighbors', 'umap']:\n",
    "            if _k in adata.uns: \n",
    "                try: del adata.uns[_k]\n",
    "                except: pass\n",
    "        if DROP_OBSM_UMAP_TSNE:\n",
    "            for _key in list(adata.obsm.keys()):\n",
    "                _lk = _key.lower()\n",
    "                if 'umap' in _lk or 'tsne' in _lk or (_lk == 'x_pca' and 'x_gene_pca' not in _lk):\n",
    "                    try: del adata.obsm[_key]\n",
    "                    except: pass\n",
    "        if DROP_ONEHOT_OBSM:\n",
    "            for _key in ['X_tcr_tra_onehot', 'X_tcr_trb_onehot']:\n",
    "                 if _key in adata.obsm: \n",
    "                     try: del adata.obsm[_key]\n",
    "                     except: pass\n",
    "        if DROP_RAW and getattr(adata, 'raw', None) is not None:\n",
    "             adata.raw = None\n",
    "    except Exception as _e:\n",
    "        print('Error while pruning adata structures:', _e)\n",
    "\n",
    "    try:\n",
    "        import tensorflow.keras.backend as K\n",
    "        K.clear_session()\n",
    "    except Exception: pass\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Define Models & Optimized Hyperparameters ---\n",
    "    # Defined here to ensure robust execution without dependency on other cells\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {'C': [0.1, 1, 10], 'penalty': ['l2'], 'solver': ['liblinear']},\n",
    "        'Decision Tree': {'max_depth': [5, 10], 'min_samples_split': [5, 10], 'min_samples_leaf': [2, 4]},\n",
    "        'Random Forest': {'n_estimators': [100], 'max_depth': [10, 20], 'min_samples_split': [5, 10]}, # Reduced grid\n",
    "        'XGBoost': {\n",
    "            'max_depth': [3, 5], \n",
    "            'learning_rate': [0.05, 0.1], \n",
    "            'subsample': [0.8, 1.0], \n",
    "            'colsample_bytree': [0.8, 1.0], \n",
    "            'n_estimators': [100]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    models_eval = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, solver='liblinear'),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=N_JOBS_MODEL),\n",
    "        'XGBoost': (lambda: (globals().get('XGBClassifierSK', xgb.XGBClassifier)(\n",
    "            random_state=42, \n",
    "            use_label_encoder=False, \n",
    "            eval_metric='logloss',\n",
    "            n_jobs=N_JOBS_MODEL,\n",
    "            **({'tree_method':'gpu_hist','predictor':'gpu_predictor'} \n",
    "               if globals().get('XGBOOST_GPU_AVAILABLE', False) \n",
    "               else {'tree_method':'hist'}) # Optimization: Use 'hist' on CPU which is much faster than 'exact'\n",
    "        )))()\n",
    "    }\n",
    "    _apply_gpu_patches()\n",
    "\n",
    "    # Adapt param_grids to pipeline format (prefix 'clf__')\n",
    "    param_grid_pipeline = {m: {f'clf__{k}': v for k, v in g.items()} for m, g in param_grids.items()}\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "    lopo_summary_rows = []\n",
    "\n",
    "    # Iterate feature sets\n",
    "    for feature_name, X_features in feature_sets.items():\n",
    "        print(f\"\\n=== Feature set: {feature_name} (shape={X_features.shape}) ===\")\n",
    "        X = X_features\n",
    "        y = y_encoded\n",
    "        groups = groups_all\n",
    "\n",
    "        accum = {m: {'y_true': [], 'y_pred': [], 'y_proba': [], 'groups': []} for m in models_eval.keys()}\n",
    "\n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(logo.split(X, y, groups)):\n",
    "            held_patient = np.unique(groups[test_idx])\n",
    "            print(f\"LOPO fold {fold_idx+1}/{len(unique_patients)} -- held patient(s): {held_patient}\")\n",
    "\n",
    "            X_tr, X_te = X[train_idx], X[test_idx]\n",
    "            y_tr, y_te = y[train_idx], y[test_idx]\n",
    "            groups_tr = groups[train_idx]\n",
    "            \n",
    "            n_train_groups = len(np.unique(groups_tr))\n",
    "            inner_n_splits = min(3, n_train_groups) if n_train_groups >= 2 else 1\n",
    "\n",
    "            for model_name, base_model in models_eval.items():\n",
    "                pipeline = Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='mean')),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('clf', base_model)\n",
    "                ])\n",
    "\n",
    "                # Hyperparameter tuning\n",
    "                # Use RandomizedSearchCV to cap the maximum time spent on regular algorithms\n",
    "                if model_name in param_grid_pipeline:\n",
    "                    # Determine strategy\n",
    "                    grid_params = param_grid_pipeline[model_name]\n",
    "                    grid_size = np.prod([len(v) for v in grid_params.values()])\n",
    "                    \n",
    "                    # If grid is small enough, use GridSearch. If large, use RandomizedSearchCV\n",
    "                    if USE_RANDOM_SEARCH and grid_size > N_ITER_SEARCH:\n",
    "                        search_impl = RandomizedSearchCV(pipeline, grid_params, n_iter=N_ITER_SEARCH, \n",
    "                                                       cv=inner_n_splits if inner_n_splits > 1 else StratifiedKFold(3),\n",
    "                                                       scoring='accuracy', n_jobs=N_JOBS_CV, random_state=42)\n",
    "                    else:\n",
    "                        search_impl = GridSearchCV(pipeline, grid_params, \n",
    "                                                 cv=inner_n_splits if inner_n_splits > 1 else StratifiedKFold(3),\n",
    "                                                 scoring='accuracy', n_jobs=N_JOBS_CV)\n",
    "\n",
    "                    # Fit\n",
    "                    if inner_n_splits >= 2:\n",
    "                        search_impl.fit(X_tr, y_tr, groups=groups_tr)\n",
    "                    else: \n",
    "                        # Fallback for few groups\n",
    "                        search_impl.fit(X_tr, y_tr)\n",
    "                        \n",
    "                    best_model = search_impl.best_estimator_\n",
    "                else:\n",
    "                    best_model = pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "                # Predict\n",
    "                y_pred = best_model.predict(X_te)\n",
    "                try:\n",
    "                    y_pred_proba = best_model.predict_proba(X_te)[:, 1]\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        d = best_model.decision_function(X_te)\n",
    "                        y_pred_proba = d[:, 1] if d.ndim > 1 else d\n",
    "                    except:\n",
    "                        y_pred_proba = np.zeros(len(y_pred))\n",
    "\n",
    "                # Accumulate\n",
    "                accum[model_name]['y_true'].extend(y_te.tolist())\n",
    "                accum[model_name]['y_pred'].extend(y_pred.tolist())\n",
    "                accum[model_name]['y_proba'].extend(y_pred_proba.tolist())\n",
    "                accum[model_name]['groups'].extend(groups[test_idx].tolist())\n",
    "\n",
    "        # --- Aggregation & Reporting ---\n",
    "        for model_name, data_dict in accum.items():\n",
    "            y_true_all = np.array(data_dict['y_true'])\n",
    "            y_pred_all = np.array(data_dict['y_pred'])\n",
    "            y_proba_all = np.array(data_dict['y_proba'])\n",
    "            groups_all_pred = np.array(data_dict.get('groups', []), dtype=object)\n",
    "\n",
    "            if len(y_true_all) == 0: continue\n",
    "\n",
    "            # Cell-level metrics\n",
    "            acc = accuracy_score(y_true_all, y_pred_all)\n",
    "            prec = precision_score(y_true_all, y_pred_all, zero_division=0)\n",
    "            rec = recall_score(y_true_all, y_pred_all, zero_division=0)\n",
    "            f1s = f1_score(y_true_all, y_pred_all, zero_division=0)\n",
    "            try: auc = roc_auc_score(y_true_all, y_proba_all)\n",
    "            except: auc = float('nan')\n",
    "            cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "            if cm.size == 4:\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "                spec = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "                npv = tn / (tn + fn) if (tn + fn) > 0 else float('nan')\n",
    "            else: spec, npv = float('nan'), float('nan')\n",
    "\n",
    "            lopo_summary_rows.append({\n",
    "                'feature_set': feature_name, 'model': model_name, 'evaluation_level': 'cell',\n",
    "                'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1s, 'auc': auc,\n",
    "                'specificity': spec, 'npv': npv, 'n_patients': len(unique_patients), 'n_cells': X_features.shape[0]\n",
    "            })\n",
    "\n",
    "            # Patient-level aggregation\n",
    "            try:\n",
    "                pred_df = pd.DataFrame({'patient': groups_all_pred, 'y_true': y_true_all, 'y_proba': y_proba_all})\n",
    "                patient_summary = pred_df.groupby('patient').agg({'y_proba': 'mean', 'y_true': 'first'}).reset_index()\n",
    "                patient_summary['y_pred'] = (patient_summary['y_proba'] >= 0.5).astype(int)\n",
    "\n",
    "                y_t, y_p = patient_summary['y_true'], patient_summary['y_pred']\n",
    "                try: auc_p = roc_auc_score(y_t, patient_summary['y_proba'])\n",
    "                except: auc_p = float('nan')\n",
    "                \n",
    "                lopo_summary_rows.append({\n",
    "                    'feature_set': feature_name, 'model': model_name, 'evaluation_level': 'patient',\n",
    "                    'accuracy': accuracy_score(y_t, y_p), 'precision': precision_score(y_t, y_p, zero_division=0),\n",
    "                    'recall': recall_score(y_t, y_p, zero_division=0), 'f1': f1_score(y_t, y_p, zero_division=0),\n",
    "                    'auc': auc_p, 'n_patients': len(patient_summary), 'n_cells': X_features.shape[0]\n",
    "                })\n",
    "                \n",
    "                p_out = Path('Processed_Data') / f'lopo_patient_predictions_{feature_name}_{model_name}.csv'\n",
    "                patient_summary.to_csv(p_out, index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed patient-level metrics: {e}\")\n",
    "\n",
    "    lopo_df = pd.DataFrame(lopo_summary_rows)\n",
    "    output_path = Path('Processed_Data') / 'lopo_results.csv'\n",
    "    Path('Processed_Data').mkdir(exist_ok=True)\n",
    "    lopo_df.to_csv(output_path, index=False)\n",
    "    print(f\"LOPO results saved to: {output_path}\")\n",
    "    display(lopo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5321defb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:51:02.971857Z",
     "iopub.status.busy": "2026-02-06T05:51:02.971224Z",
     "iopub.status.idle": "2026-02-06T05:51:02.979288Z",
     "shell.execute_reply": "2026-02-06T05:51:02.978422Z"
    },
    "papermill": {
     "duration": 0.032751,
     "end_time": "2026-02-06T05:51:02.981004",
     "exception": false,
     "start_time": "2026-02-06T05:51:02.948253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXED: param_grids defined with reduced hyperparameter space: ['Logistic Regression', 'Decision Tree', 'Random Forest', 'XGBoost']\n",
      "  Logistic Regression: 3 combinations\n",
      "  Decision Tree: 8 combinations\n",
      "  Random Forest: 4 combinations\n",
      "  XGBoost: 8 combinations\n"
     ]
    }
   ],
   "source": [
    "# === FIX 1.4: CONSTRAIN HYPERPARAMETER GRID ===\n",
    "# PREVIOUS: 162 XGBoost combinations for 7 patients caused overfitting\n",
    "# IMPROVED: Reduced to 16 combinations to prevent hyperparameter overfitting\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10],  # Reduced from 5 to 3 options\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [5, 10],  # Reduced: removed 20 and None (prone to overfitting)\n",
    "        'min_samples_split': [5, 10],  # Removed 2 (too permissive)\n",
    "        'min_samples_leaf': [2, 4]  # Removed 1 (too permissive)\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100],  # Fixed value (vs [50, 100, 200])\n",
    "        'max_depth': [10, 20],  # Removed None (unconstrained depth)\n",
    "        'min_samples_split': [5, 10]  # Removed 2\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'max_depth': [3, 5],  # Reduced from [3, 6, 9]\n",
    "        'learning_rate': [0.05, 0.1],  # Reduced from [0.01, 0.1, 0.3]\n",
    "        'subsample': [0.8, 1.0],  # Kept same\n",
    "        'colsample_bytree': [0.8, 1.0],  # Reduced from [0.6, 0.8, 1.0]\n",
    "        'n_estimators': [100]  # Fixed (vs [50, 100, 200])\n",
    "    }\n",
    "}\n",
    "# Total: LR=3, DT=2√ó2√ó2=8, RF=1√ó2√ó2=4, XGB=2√ó2√ó2√ó1=8 (manageable grid)\n",
    "print('FIXED: param_grids defined with reduced hyperparameter space:', list(param_grids.keys()))\n",
    "print('  Logistic Regression: 3 combinations')\n",
    "print('  Decision Tree: 8 combinations')\n",
    "print('  Random Forest: 4 combinations')\n",
    "print('  XGBoost: 8 combinations')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556769e",
   "metadata": {
    "papermill": {
     "duration": 0.02327,
     "end_time": "2026-02-06T05:51:03.026270",
     "exception": false,
     "start_time": "2026-02-06T05:51:03.003000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Advanced Deep Learning: Multimodal RNN\n",
    "To better capture the sequential nature of TCR data, we implement a **Multimodal Recurrent Neural Network (RNN)**. This architecture processes the heterogeneous input data using specialized sub-networks:\n",
    "1.  **Gene Expression Branch:** A Dense network processes the PCA-reduced gene expression features.\n",
    "2.  **TCR Sequence Branches:** Two separate LSTM (Long Short-Term Memory) networks process the raw amino acid sequences of the TRA and TRB chains, respectively. LSTMs are well-suited for capturing sequential dependencies and motifs in protein sequences.\n",
    "3.  **Fusion Layer:** The outputs of these branches are concatenated and passed through a final dense classification head.\n",
    "\n",
    "This approach allows the model to learn complex interactions between the transcriptomic state of the T-cell and its specific antigen receptor sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optimization Helper: Streaming Data Generator & Memory Config ---\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# 1. Enable Mixed Precision to reduce memory usage (Float16)\n",
    "try:\n",
    "    if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print(\"Mixed precision enabled (mixed_float16).\")\n",
    "    else:\n",
    "        print(\"GPU not found; skipping mixed precision (using float32).\")\n",
    "except Exception as e:\n",
    "    print(f\"Mixed precision setup failed: {e}\")\n",
    "\n",
    "# 2. Lazy Generator to prevent OOM\n",
    "class LazySequenceGenerator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Generates batches of data from AnnData sparse matrices on the fly.\n",
    "    Avoids densifying the entire dataset in memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, adata, indices, y, batch_size=32, shuffle=True, \n",
    "                 use_gene=False, X_gene=None, \n",
    "                 use_seq=False, \n",
    "                 tra_key='X_tcr_tra_onehot', trb_key='X_tcr_trb_onehot',\n",
    "                 arch='MLP', n_channels=20):\n",
    "        self.adata = adata\n",
    "        self.indices = indices  # Global indices into adata\n",
    "        self.y = y              # Labels corresponding to indices\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.use_gene = use_gene\n",
    "        self.X_gene = X_gene    # Pre-scaled gene data corresponding to indices (0..N relative)\n",
    "        self.use_seq = use_seq\n",
    "        self.tra_key = tra_key\n",
    "        self.trb_key = trb_key\n",
    "        self.arch = arch\n",
    "        self.n_channels = n_channels\n",
    "        self.indexes = np.arange(len(self.indices))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Indices for this batch in the local subset\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Global indices for sparse data lookup\n",
    "        global_indices = self.indices[batch_indexes]\n",
    "        \n",
    "        inputs = []\n",
    "        \n",
    "        # Sequence Data (Heavy - Handle Lazily)\n",
    "        if self.use_seq:\n",
    "            # Slicing sparse matrix is fast and low-memory\n",
    "            tra = self.adata.obsm[self.tra_key][global_indices]\n",
    "            trb = self.adata.obsm[self.trb_key][global_indices]\n",
    "            \n",
    "            # Densify only this micro-batch\n",
    "            tra = tra.toarray() if hasattr(tra, 'toarray') else tra\n",
    "            trb = trb.toarray() if hasattr(trb, 'toarray') else trb\n",
    "            \n",
    "            # Ensure float32 (or 16 via mixed precision policy interaction)\n",
    "            tra = tra.astype(np.float32)\n",
    "            trb = trb.astype(np.float32)\n",
    "\n",
    "            if self.arch == 'MLP':\n",
    "                # For MLP, concatenate flattened (Batch, SeqLen*20)\n",
    "                # Inputs are already flat (Batch, Features)\n",
    "                X_seq = np.concatenate([tra, trb], axis=1)\n",
    "            else:\n",
    "                # For CNN/RNN, reshape to 3D (Batch, SeqLen, Channels)\n",
    "                # Infer seq_len\n",
    "                seq_len = tra.shape[1] // self.n_channels\n",
    "                tra_seq = tra.reshape(-1, seq_len, self.n_channels)\n",
    "                trb_seq = trb.reshape(-1, seq_len, self.n_channels)\n",
    "                X_seq = np.concatenate([tra_seq, trb_seq], axis=2)\n",
    "            \n",
    "            inputs.append(X_seq)\n",
    "            \n",
    "        # Gene Data (Pre-loaded, small)\n",
    "        if self.use_gene:\n",
    "            # Slice the pre-scaled array\n",
    "            X_g = self.X_gene[batch_indexes]\n",
    "            inputs.append(X_g)\n",
    "            \n",
    "        # Keras Model input format\n",
    "        if len(inputs) == 1:\n",
    "            final_X = inputs[0]\n",
    "        else:\n",
    "            final_X = inputs # [X_seq, X_gene] order typically\n",
    "            \n",
    "        final_y = self.y[batch_indexes]\n",
    "        return final_X, final_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78116a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T05:51:03.074006Z",
     "iopub.status.busy": "2026-02-06T05:51:03.073648Z"
    },
    "papermill": {
     "duration": 15082.781147,
     "end_time": "2026-02-06T10:02:25.830562",
     "exception": false,
     "start_time": "2026-02-06T05:51:03.049415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Found 1 GPU(s): [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow will use: GPU\n",
      "DL hyperparameter combinations: 32\n",
      "Failed to patch models_eval['XGBoost']: 'XGBClassifierSK' object has no attribute 'predictor'\n",
      "Patched models_eval['Random Forest'] to use n_jobs=-1.\n",
      "Patched param_grids['XGBoost'] with GPU options (method=device).\n",
      "\n",
      "=== DL evaluation using feature set: sequence_structure ===\n",
      "LOPO fold 1/4 -- held patient: ['PT1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1770357065.906960     284 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15511 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1770357077.314546     300 service.cc:152] XLA service 0x7c261000eaa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1770357077.314597     300 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1770357078.109484     300 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "I0000 00:00:1770357082.405076     299 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 2793 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7c2902026200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-06 05:53:25.316906: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 05:56:59.189785: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:09:19.586217: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:09:19.774051: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:09:19.995502: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:09:20.168027: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:09:20.379310: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:09:20.581462: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:09:21.068832: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:09:21.292358: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:09:21.626626: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:09:21.808068: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:09:21.959879: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:11:23.914819: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:11:39.845730: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:11:40.275501: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:15:33.773475: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:30:58.865627: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:30:59.458686: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:30:59.823383: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:31:00.296914: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:31:12.129841: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:31:12.678038: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:33:09.693347: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:33:10.144818: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:33:10.312699: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:33:10.476789: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:33:10.642008: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:33:10.899642: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:33:23.115558: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:37:07.486820: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:40:29.510874: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:54:02.386245: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:54:02.567698: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 06:56:08.762300: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Selected best inner config: ('BiLSTM', 128, 0.3, 0.0001, 32, 30) with mean val acc=0.7840\n",
      "\u001b[1m793/793\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
      "LOPO fold 2/4 -- held patient: ['PT2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-06 07:27:14.241572: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 07:32:56.672610: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 07:46:58.520700: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 07:49:06.638878: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 07:49:06.851073: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 08:11:17.551474: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-06 08:13:30.039994: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Selected best inner config: ('BiLSTM', 128, 0.2, 0.001, 32, 30) with mean val acc=0.5855\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "LOPO fold 3/4 -- held patient: ['PT3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-06 08:33:53.886339: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Selected best inner config: ('MLP', 128, 0.3, 0.001, 32, 30) with mean val acc=0.5910\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "LOPO fold 4/4 -- held patient: ['PT4']\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Advanced Multimodal Deep Learning (MLP / CNN / BiLSTM / Transformer) [OPTIMIZED]\n",
    "# Optimization: Streaming Data Inspection, Mixed Precision, BatchNormalization, Serial Execution\n",
    "\n",
    "if 'supervised_mask' not in globals() or supervised_mask.sum() == 0:\n",
    "    print(\"WARNING: No supervised samples available.\")\n",
    "    dl_results_rows = []\n",
    "else:\n",
    "    import itertools\n",
    "    import time\n",
    "    import math\n",
    "    import random\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, regularizers\n",
    "    from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "    from pathlib import Path\n",
    "    from joblib import Parallel, delayed\n",
    "\n",
    "    # Deterministic seeds\n",
    "    SEED = 42\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "    # 1. Device Config\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            try:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "    \n",
    "    # 2. Metadata Inspection (No Data copy)\n",
    "    def invoke_gc():\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    def get_seq_len(adata, n_channels=20):\n",
    "        # Infer sequence length from sparse matrix shape\n",
    "        if 'X_tcr_tra_onehot' in adata.obsm:\n",
    "            shape = adata.obsm['X_tcr_tra_onehot'].shape\n",
    "            return shape[1] // n_channels\n",
    "        return None\n",
    "\n",
    "    # 3. Model Builders with Integrated Scaling (BatchNormalization)\n",
    "    def compile_model(model, lr):\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                      loss='binary_crossentropy', \n",
    "                      metrics=[keras.metrics.AUC(name='auc'), 'accuracy'])\n",
    "        return model\n",
    "\n",
    "    def build_mlp(input_dim, hidden1=128, hidden2=64, dropout=0.3, l2_reg=1e-3, lr=1e-3):\n",
    "        inp = keras.Input(shape=(input_dim,), name='gene_input')\n",
    "        # Use BatchNormalization instead of manual StandardScaler for efficiency\n",
    "        x = layers.BatchNormalization()(inp)\n",
    "        x = layers.Dense(hidden1, kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Dense(hidden2, kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        out = layers.Dense(1, activation='sigmoid')(x)\n",
    "        model = keras.Model(inputs=inp, outputs=out)\n",
    "        return compile_model(model, lr)\n",
    "\n",
    "    def build_cnn(seq_len, n_channels, gene_dim=None, conv_filters=64, kernel_size=5, dropout=0.3, l2_reg=1e-3, lr=1e-3):\n",
    "        seq_in = keras.Input(shape=(seq_len, n_channels), name='seq_input')\n",
    "        x = layers.Conv1D(conv_filters, kernel_size, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(seq_in)\n",
    "        x = layers.Conv1D(conv_filters, kernel_size, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "        x = layers.GlobalMaxPooling1D()(x)\n",
    "        if gene_dim is not None:\n",
    "            gene_in = keras.Input(shape=(gene_dim,), name='gene_input')\n",
    "            g = layers.Dense(64, activation='relu')(gene_in)\n",
    "            x = layers.concatenate([x, g])\n",
    "            out_in = [seq_in, gene_in]\n",
    "        else:\n",
    "            out_in = seq_in\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        out = layers.Dense(1, activation='sigmoid')(x)\n",
    "        model = keras.Model(inputs=out_in, outputs=out)\n",
    "        return compile_model(model, lr)\n",
    "\n",
    "    def build_bilstm(seq_len, n_channels, gene_dim=None, lstm_units=128, dropout=0.3, l2_reg=1e-3, lr=1e-3):\n",
    "        seq_in = keras.Input(shape=(seq_len, n_channels), name='seq_input')\n",
    "        # LSTM\n",
    "        x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=False, kernel_regularizer=regularizers.l2(l2_reg)))(seq_in)\n",
    "        if gene_dim is not None:\n",
    "            gene_in = keras.Input(shape=(gene_dim,), name='gene_input')\n",
    "            g = layers.Dense(64, activation='relu')(gene_in)\n",
    "            x = layers.concatenate([x, g])\n",
    "            out_in = [seq_in, gene_in]\n",
    "        else:\n",
    "            out_in = seq_in\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        out = layers.Dense(1, activation='sigmoid')(x)\n",
    "        return compile_model(keras.Model(inputs=out_in, outputs=out), lr)\n",
    "\n",
    "    class TransformerBlock(layers.Layer):\n",
    "        def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.embed_dim = embed_dim\n",
    "            self.num_heads = num_heads\n",
    "            self.ff_dim = ff_dim\n",
    "            self.rate = rate\n",
    "            self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "            self.ffn = keras.Sequential([layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim)])\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "        def call(self, inputs, training=None):\n",
    "            attn_output = self.att(inputs, inputs)\n",
    "            attn_output = self.dropout1(attn_output, training=training)\n",
    "            out1 = self.layernorm1(inputs + attn_output)\n",
    "            ffn_output = self.ffn(out1)\n",
    "            ffn_output = self.dropout2(ffn_output, training=training)\n",
    "            return self.layernorm2(out1 + ffn_output)\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\"embed_dim\": self.embed_dim, \"num_heads\": self.num_heads, \"ff_dim\": self.ff_dim, \"rate\": self.rate})\n",
    "            return config\n",
    "\n",
    "    def build_transformer(seq_len, n_channels, gene_dim=None, embed_dim=64, num_heads=4, ff_dim=128, dropout=0.1, lr=1e-3):\n",
    "        seq_in = keras.Input(shape=(seq_len, n_channels), name='seq_input')\n",
    "        x = layers.Dense(embed_dim)(seq_in)\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim, rate=dropout)(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        if gene_dim is not None:\n",
    "            gene_in = keras.Input(shape=(gene_dim,), name='gene_input')\n",
    "            g = layers.Dense(64, activation='relu')(gene_in)\n",
    "            x = layers.concatenate([x, g])\n",
    "            inputs_list = [seq_in, gene_in]\n",
    "        else:\n",
    "            inputs_list = seq_in\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        out = layers.Dense(1, activation='sigmoid')(x)\n",
    "        return compile_model(keras.Model(inputs=inputs_list, outputs=out), lr)\n",
    "\n",
    "    # 4. Optimized Training Function (Uses Lazy Generators)\n",
    "    def train_eval_single_config(cfg_idx, config, use_gene, use_seq,\n",
    "                                 inner_train_inds_global, inner_val_inds_global,\n",
    "                                 y_inner_tr, y_inner_val, class_weights,\n",
    "                                 X_inner_tr_gene_scaled, X_inner_val_gene_scaled,\n",
    "                                 seq_len):\n",
    "        invoke_gc() # Clean up before training\n",
    "        arch, hu, dr, lr, bs, epochs = config\n",
    "        \n",
    "        # Generator Creation\n",
    "        train_gen = LazySequenceGenerator(adata, inner_train_inds_global, y_inner_tr, batch_size=bs,\n",
    "                                          use_gene=use_gene, X_gene=X_inner_tr_gene_scaled,\n",
    "                                          use_seq=use_seq, arch=arch, shuffle=True)\n",
    "        val_gen = LazySequenceGenerator(adata, inner_val_inds_global, y_inner_val, batch_size=bs,\n",
    "                                        use_gene=use_gene, X_gene=X_inner_val_gene_scaled,\n",
    "                                        use_seq=use_seq, arch=arch, shuffle=False)\n",
    "        \n",
    "        input_dim_gene = X_inner_tr_gene_scaled.shape[1] if use_gene else 0\n",
    "        input_dim_seq_flat = seq_len * 20 * 2 # approx\n",
    "        \n",
    "        try:\n",
    "            if arch == 'MLP':\n",
    "                # MLP Input Dim calculation\n",
    "                # If generator concatenates flat seq + gene\n",
    "                # Seq: (20 * seq_len)*2. Gene: input_dim_gene.\n",
    "                # Use a dummy batch to check shape\n",
    "                x_sample, _ = train_gen[0]\n",
    "                total_dim = x_sample.shape[1] if hasattr(x_sample, 'shape') else x_sample[0].shape[1]\n",
    "                model = build_mlp(total_dim, hidden1=hu, hidden2=max(32, hu//2), dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "                \n",
    "            elif arch == 'CNN':\n",
    "                model = build_cnn(seq_len, 20, gene_dim=(input_dim_gene if use_gene else None), conv_filters=hu, dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "            elif arch == 'BiLSTM':\n",
    "                model = build_bilstm(seq_len, 20, gene_dim=(input_dim_gene if use_gene else None), lstm_units=hu, dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "            else: # Transformer\n",
    "                model = build_transformer(seq_len, 20, gene_dim=(input_dim_gene if use_gene else None), embed_dim=hu//2, num_heads=4, ff_dim=hu, dropout=dr, lr=lr)\n",
    "\n",
    "            es = keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=5, restore_best_weights=True, verbose=0)\n",
    "            \n",
    "            # FIT using GENERATOR\n",
    "            model.fit(train_gen, validation_data=val_gen, epochs=epochs, \n",
    "                      class_weight=class_weights, callbacks=[es], verbose=0)\n",
    "            \n",
    "            # Evaluate (use batch processing via generator/predict)\n",
    "            y_val_pred = model.predict(val_gen, verbose=0).flatten()\n",
    "            # Truncate to match length (drop last batch remainder padding if any? No, gen handles it)\n",
    "            # Generator length matches len(y_inner_val)? \n",
    "            # predict returns data for all batches.\n",
    "            # Make sure labels match.\n",
    "            y_val_label = (y_val_pred > 0.5).astype(int)\n",
    "            val_acc = accuracy_score(y_inner_val[:len(y_val_label)], y_val_label)\n",
    "            \n",
    "            del model\n",
    "            return cfg_idx, val_acc\n",
    "        except Exception as e:\n",
    "            print(f\"Err {arch}: {e}\")\n",
    "            return cfg_idx, -1.0\n",
    "\n",
    "    # 5. Main Execution Flow\n",
    "    dl_param_grid = {\n",
    "        'arch': ['MLP', 'CNN', 'BiLSTM', 'Transformer'],\n",
    "        'hidden_units': [64, 128], # Reduced grid for speed test\n",
    "        'dropout': [0.3],\n",
    "        'lr': [1e-3],\n",
    "        'batch_size': [32],\n",
    "        'epochs': [20] # Reduced epochs\n",
    "    }\n",
    "    grid_items = list(itertools.product(dl_param_grid['arch'], dl_param_grid['hidden_units'], dl_param_grid['dropout'], dl_param_grid['lr'], dl_param_grid['batch_size'], dl_param_grid['epochs']))\n",
    "    \n",
    "    # Global Setup\n",
    "    supervised_mask_local = supervised_mask\n",
    "    X_gene_all = adata.obsm['X_gene_pca'][supervised_mask_local]\n",
    "    seq_len = get_seq_len(adata)\n",
    "    use_sequence = (seq_len is not None)\n",
    "    \n",
    "    y_all = y_encoded\n",
    "    # Patient ID logic\n",
    "    patient_id_col_local = next((c for c in ['patient_id', 'Patient_ID', 'PatientID'] if c in adata.obs.columns), None)\n",
    "    groups_all_local = np.array(adata.obs[patient_id_col_local][supervised_mask_local])\n",
    "    unique_patients = np.unique(groups_all_local)\n",
    "    global_indices_all = np.where(supervised_mask_local)[0] # Global indices corresponding to y_all\n",
    "\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    logo = LeaveOneGroupOut()\n",
    "    dl_results_rows = []\n",
    "\n",
    "    for feature_name in ['sequence_structure', 'comprehensive']:\n",
    "        print(f\"\\n=== DL: {feature_name} ===\")\n",
    "        use_gene = True\n",
    "        use_seq = (feature_name != 'gene_only') and use_sequence\n",
    "        \n",
    "        accum_arch = {arch: {'y_true': [], 'y_pred': [], 'y_proba': [], 'groups': []} for arch in ['MLP','CNN','BiLSTM','Transformer']}\n",
    "        \n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(logo.split(X_gene_all, y_all, groups_all_local)):\n",
    "            print(f\"Fold {fold_idx+1}/{len(unique_patients)}\")\n",
    "            invoke_gc()\n",
    "            \n",
    "            # Split Data\n",
    "            X_tr_gene = X_gene_all[train_idx]\n",
    "            scaler = StandardScaler().fit(X_tr_gene)\n",
    "            X_tr_gene_scaled = scaler.transform(X_tr_gene)\n",
    "            X_te_gene_scaled = scaler.transform(X_gene_all[test_idx])\n",
    "            \n",
    "            y_tr = y_all[train_idx]\n",
    "            y_te = y_all[test_idx]\n",
    "            groups_tr = groups_all_local[train_idx]\n",
    "            \n",
    "            # Map fold indices to global indices\n",
    "            train_global_inds = global_indices_all[train_idx]\n",
    "            test_global_inds = global_indices_all[test_idx]\n",
    "\n",
    "            classes = np.unique(y_tr)\n",
    "            cw = compute_class_weight(class_weight='balanced', classes=classes, y=y_tr)\n",
    "            class_weight_dict = {int(c): float(w) for c,w in zip(classes, cw)}\n",
    "            \n",
    "            # Inner CV\n",
    "            n_train_groups = len(np.unique(groups_tr))\n",
    "            inner_splits = min(3, n_train_groups) if n_train_groups >= 2 else 1\n",
    "            \n",
    "            best_cfg = list(grid_items)[0] # Default\n",
    "            if inner_splits >= 2:\n",
    "                inner_cv = GroupKFold(n_splits=inner_splits)\n",
    "                config_scores = {i: [] for i in range(len(grid_items))}\n",
    "                \n",
    "                # SERIAL EXECUTION TO SAVE MEMORY (n_jobs=1)\n",
    "                for inner_train_idx, inner_val_idx in inner_cv.split(X_tr_gene_scaled, y_tr, groups_tr):\n",
    "                    # Map to global\n",
    "                    inner_tr_global = train_global_inds[inner_train_idx]\n",
    "                    inner_val_global = train_global_inds[inner_val_idx]\n",
    "                    \n",
    "                    for cfg_idx, config in enumerate(grid_items):\n",
    "                        idx, score = train_eval_single_config(\n",
    "                            cfg_idx, config, use_gene, use_seq,\n",
    "                            inner_tr_global, inner_val_global,\n",
    "                            y_tr[inner_train_idx], y_tr[inner_val_idx], class_weight_dict,\n",
    "                            X_tr_gene_scaled[inner_train_idx], X_tr_gene_scaled[inner_val_idx],\n",
    "                            seq_len\n",
    "                        )\n",
    "                        if score >= 0: config_scores[idx].append(score)\n",
    "                \n",
    "                # Select best\n",
    "                best_avg = -math.inf\n",
    "                for i, scores in config_scores.items():\n",
    "                    if scores and np.mean(scores) > best_avg:\n",
    "                        best_avg = np.mean(scores)\n",
    "                        best_cfg = grid_items[i]\n",
    "                print(f\"  Best Config: {best_cfg} (Acc: {best_avg:.3f})\")\n",
    "\n",
    "            # Retrain Best\n",
    "            invoke_gc()\n",
    "            arch, hu, dr, lr, bs, epochs = best_cfg\n",
    "            \n",
    "            # Generators for final training\n",
    "            train_gen_final = LazySequenceGenerator(adata, train_global_inds, y_tr, batch_size=bs,\n",
    "                                                    use_gene=use_gene, X_gene=X_tr_gene_scaled,\n",
    "                                                    use_seq=use_seq, arch=arch, shuffle=True)\n",
    "            test_gen_final = LazySequenceGenerator(adata, test_global_inds, y_te, batch_size=bs,\n",
    "                                                   use_gene=use_gene, X_gene=X_te_gene_scaled,\n",
    "                                                   use_seq=use_seq, arch=arch, shuffle=False)\n",
    "            \n",
    "            # Build & Train\n",
    "            x_sample, _ = train_gen_final[0]\n",
    "            if arch == 'MLP':\n",
    "                 total_dim = x_sample.shape[1] if hasattr(x_sample, 'shape') else x_sample[0].shape[1]\n",
    "                 model = build_mlp(total_dim, hidden1=hu, hidden2=max(32, hu//2), dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "            elif arch == 'CNN':\n",
    "                 model = build_cnn(seq_len, 20, gene_dim=(X_tr_gene.shape[1] if use_gene else None), conv_filters=hu, dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "            elif arch == 'BiLSTM':\n",
    "                 model = build_bilstm(seq_len, 20, gene_dim=(X_tr_gene.shape[1] if use_gene else None), lstm_units=hu, dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "            else:\n",
    "                 model = build_transformer(seq_len, 20, gene_dim=(X_tr_gene.shape[1] if use_gene else None), embed_dim=hu//2, num_heads=4, ff_dim=hu, dropout=dr, lr=lr)\n",
    "            \n",
    "            es = keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=8, restore_best_weights=True, verbose=0)\n",
    "            model.fit(train_gen_final, validation_data=test_gen_final, epochs=epochs, \n",
    "                      class_weight=class_weight_dict, callbacks=[es], verbose=0)\n",
    "            \n",
    "            y_test_proba = model.predict(test_gen_final, verbose=0).flatten()\n",
    "            y_test_pred = (y_test_proba > 0.5).astype(int)\n",
    "            \n",
    "            accum_arch[arch]['y_true'].extend(y_te.tolist())\n",
    "            accum_arch[arch]['y_pred'].extend(y_test_pred.tolist())\n",
    "            accum_arch[arch]['y_proba'].extend(y_test_proba.tolist())\n",
    "            accum_arch[arch]['groups'].extend(groups_all_local[test_idx].tolist())\n",
    "            \n",
    "            del model\n",
    "            invoke_gc()\n",
    "\n",
    "        # Save Results logic (abbreviated)\n",
    "        for arch, data in accum_arch.items():\n",
    "            if not data['y_true']: continue\n",
    "            acc = accuracy_score(data['y_true'], data['y_pred'])\n",
    "            dl_results_rows.append({'feature_set': feature_name, 'architecture': arch, 'accuracy': acc, 'n_patients': len(unique_patients)})\n",
    "    \n",
    "    if dl_results_rows:\n",
    "        dl_df = pd.DataFrame(dl_results_rows)\n",
    "        print(\"Done. Saved results.\")\n",
    "        display(dl_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SMOKE TEST: Benchmark & Verification ---\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "print(\"=== SMOKE TEST: Memory & Performance Check ===\")\n",
    "\n",
    "def get_mem_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "mem_start = get_mem_usage()\n",
    "start_time = time.time()\n",
    "\n",
    "# 1. Setup Test Data (Subset of Real or Mock)\n",
    "print(\"Setting up smoke test data...\")\n",
    "try:\n",
    "    if 'adata' in globals() and 'supervised_mask' in globals():\n",
    "        indices = np.where(supervised_mask)[0]\n",
    "        limit = min(len(indices), 128)\n",
    "        subset_indices = indices[:limit]\n",
    "        y_subset = y_encoded[:limit]\n",
    "        dataset_mode = \"REAL\"\n",
    "    else:\n",
    "        raise NameError(\"Data not loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"Real data unavailable ({e}), creating MOCK data...\")\n",
    "    dataset_mode = \"MOCK\"\n",
    "    class MockAdata:\n",
    "        def __init__(self): self.obsm = {}\n",
    "        class obs_attr:\n",
    "            columns = []\n",
    "        obs = obs_attr()\n",
    "    \n",
    "    adata = MockAdata()\n",
    "    # Create random sparse matrices (simulating one-hot)\n",
    "    # 20AA * 30 Len = 600 cols\n",
    "    adata.obsm['X_tcr_tra_onehot'] = sparse.random(500, 600, density=0.05, format='csr')\n",
    "    adata.obsm['X_tcr_trb_onehot'] = sparse.random(500, 600, density=0.05, format='csr')\n",
    "    subset_indices = np.arange(128)\n",
    "    y_subset = np.random.randint(0, 2, 128)\n",
    "\n",
    "# 2. Test Generator & Training\n",
    "try:\n",
    "    # Instantiate Generator\n",
    "    gen_test = LazySequenceGenerator(adata, subset_indices, y_subset, batch_size=32, \n",
    "                                     use_seq=True, use_gene=False, n_channels=20)\n",
    "    \n",
    "    # Fetch one batch to verify shapes\n",
    "    X_batch, y_batch = gen_test[0]\n",
    "    print(f\"Generator Batch Shape: {X_batch.shape if hasattr(X_batch, 'shape') else [x.shape for x in X_batch]}\")\n",
    "    \n",
    "    # Build a small BiLSTM model\n",
    "    seq_len = X_batch.shape[1]\n",
    "    n_channels = X_batch.shape[2]\n",
    "    \n",
    "    print(f\"Building BiLSTM (SeqLen={seq_len}, Channels={n_channels})...\")\n",
    "    # Need to access build_bilstm from previous cell\n",
    "    model_test = build_bilstm(seq_len, n_channels, gene_dim=None, lstm_units=32)\n",
    "    \n",
    "    print(\"Running 1 training epoch...\")\n",
    "    h = model_test.fit(gen_test, epochs=1, verbose=1)\n",
    "    \n",
    "    mem_end = get_mem_usage()\n",
    "    print(\"\\n--- SMOKE TEST PASSED ---\")\n",
    "    print(f\"Dataset Mode: {dataset_mode}\")\n",
    "    print(f\"Memory Increase: {mem_end - mem_start:.2f} MB\")\n",
    "    print(f\"Execution Time: {time.time() - start_time:.2f} s\")\n",
    "    print(f\"Final Loss: {h.history['loss'][0]:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n!!! SMOKE TEST FAILED !!!\")\n",
    "    print(e)\n",
    "    if 'model_test' in locals():\n",
    "        del model_test\n",
    "    # Traceback\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad91994",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Experiment with Sequence Length Cutoffs ---\n",
    "print(\"Experimenting with sequence length cutoffs...\")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "# Ensure adata exists\n",
    "if 'adata' not in globals() or adata is None:\n",
    "    raise NameError(\"adata is not defined. Please run the data loading cells first.\")\n",
    "# Ensure label/patient helpers have run\n",
    "if '_ensure_response_and_patient' in globals():\n",
    "    _ensure_response_and_patient(adata)\n",
    "if '_get_supervised_mask_and_labels' in globals():\n",
    "    supervised_mask, y_supervised, _le, class_counts, SUPERVISED_AVAILABLE = _get_supervised_mask_and_labels(adata)\n",
    "else:\n",
    "    # Fallback logic\n",
    "    if 'response' in adata.obs.columns:\n",
    "        y_all = adata.obs['response'].astype(str)\n",
    "        supervised_mask = y_all.isin(['Responder', 'Non-Responder']).values\n",
    "        y_supervised = y_all[supervised_mask]\n",
    "        class_counts = y_supervised.value_counts().to_dict()\n",
    "        SUPERVISED_AVAILABLE = len(class_counts) >= 2 and min(class_counts.values()) >= 2\n",
    "        _le = LabelEncoder() if SUPERVISED_AVAILABLE else None\n",
    "    else:\n",
    "        supervised_mask = np.zeros(adata.n_obs, dtype=bool)\n",
    "        y_supervised = pd.Series([], dtype=object)\n",
    "        class_counts = {}\n",
    "        SUPERVISED_AVAILABLE = False\n",
    "        _le = None\n",
    "globals()['SUPERVISED_AVAILABLE'] = SUPERVISED_AVAILABLE\n",
    "if not SUPERVISED_AVAILABLE:\n",
    "    print(\"WARNING: No supervised samples available. Skipping sequence length cutoff experiment.\")\n",
    "else:\n",
    "    supervised_mask = np.asarray(supervised_mask)\n",
    "    if '_ensure_int_labels' in globals():\n",
    "        y_encoded = _ensure_int_labels(_le.fit_transform(y_supervised))\n",
    "    else:\n",
    "        y_encoded = np.asarray(_le.fit_transform(y_supervised), dtype=np.int64)\n",
    "    min_class = min(class_counts.values()) if class_counts else 0\n",
    "    stratify = y_encoded if min_class >= 2 else None\n",
    "    cv_folds = min(3, min_class) if min_class >= 2 else 0\n",
    "    # Ensure cdr3_sequences exists\n",
    "    if 'cdr3_sequences' not in globals():\n",
    "        cdr3_sequences = {\n",
    "            'TRA': adata.obs['cdr3_TRA'].astype(str).fillna('').str.upper().tolist() if 'cdr3_TRA' in adata.obs.columns else [''] * adata.n_obs,\n",
    "            'TRB': adata.obs['cdr3_TRB'].astype(str).fillna('').str.upper().tolist() if 'cdr3_TRB' in adata.obs.columns else [''] * adata.n_obs\n",
    "        }\n",
    "    # Ensure gene features exist\n",
    "    if 'gene_features' not in globals():\n",
    "        if 'X_gene_pca' in adata.obsm:\n",
    "            gene_features = adata.obsm['X_gene_pca'][supervised_mask]\n",
    "        elif 'X_pca' in adata.obsm:\n",
    "            gene_features = adata.obsm['X_pca'][supervised_mask]\n",
    "        else:\n",
    "            gene_features = np.zeros((int(supervised_mask.sum()), 30))\n",
    "    if gene_features.shape[1] < 30:\n",
    "        gene_features = np.pad(gene_features, ((0, 0), (0, 30 - gene_features.shape[1])), mode='constant')\n",
    "    # Ensure TCR physico features exist\n",
    "    if 'tcr_physico' not in globals():\n",
    "        if all(c in adata.obs.columns for c in ['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity','trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']):\n",
    "            tra_physico = adata.obs[['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']].fillna(0)[supervised_mask].values.astype(np.float32)\n",
    "            trb_physico = adata.obs[['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']].fillna(0)[supervised_mask].values.astype(np.float32)\n",
    "            tcr_physico = np.column_stack([tra_physico, trb_physico]).astype(np.float32)\n",
    "        else:\n",
    "            tcr_physico = np.zeros((int(supervised_mask.sum()), 6), dtype=np.float32)\n",
    "    # Ensure QC features exist\n",
    "    if 'qc_features' not in globals():\n",
    "        if all(c in adata.obs.columns for c in ['n_genes_by_counts', 'total_counts', 'pct_counts_mt']):\n",
    "            qc_features = adata.obs[['n_genes_by_counts', 'total_counts', 'pct_counts_mt']].fillna(0)[supervised_mask].values\n",
    "        else:\n",
    "            qc_features = np.zeros((int(supervised_mask.sum()), 3))\n",
    "    # Define length cutoffs to test\n",
    "    length_cutoffs = [10, 15, 20, 25, 30, 35, 40, 50]\n",
    "    length_results = []\n",
    "    for max_length in length_cutoffs:\n",
    "        print(f\"\\nTesting max sequence length: {max_length}\")\n",
    "        # Re-encode sequences with new length - ENSURE FLOAT32 DTYPE\n",
    "        tra_onehot_new = np.array([one_hot_encode_sequence(seq, max_length, 'ACDEFGHIKLMNPQRSTVWY')\n",
    "                                   for seq in cdr3_sequences['TRA']], dtype=np.float32)\n",
    "        tra_onehot_flat_new = tra_onehot_new.reshape(tra_onehot_new.shape[0], -1).astype(np.float32)\n",
    "        trb_onehot_new = np.array([one_hot_encode_sequence(seq, max_length, 'ACDEFGHIKLMNPQRSTVWY')\n",
    "                                   for seq in cdr3_sequences['TRB']], dtype=np.float32)\n",
    "        trb_onehot_flat_new = trb_onehot_new.reshape(trb_onehot_new.shape[0], -1).astype(np.float32)\n",
    "        # Update AnnData\n",
    "        adata.obsm['X_tcr_tra_onehot'] = tra_onehot_flat_new\n",
    "        adata.obsm['X_tcr_trb_onehot'] = trb_onehot_flat_new\n",
    "        # Re-create feature sets with new encodings using robust PCA\n",
    "        # Use robust PCA reduction with fallback to TruncatedSVD\n",
    "        try:\n",
    "            n_comp_onehot = min(50, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[1], max(1, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[0]-1))\n",
    "            onehot_tra_reduced = PCA(n_components=n_comp_onehot, svd_solver='randomized', random_state=42).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "        except Exception as e:\n",
    "            print(f\"  PCA failed for TRA ({e}), using TruncatedSVD\")\n",
    "            n_comp = max(1, min(50, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[1]-1))\n",
    "            onehot_tra_reduced = TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "        try:\n",
    "            n_comp_onehot = min(50, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[1], max(1, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[0]-1))\n",
    "            onehot_trb_reduced = PCA(n_components=n_comp_onehot, svd_solver='randomized', random_state=42).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "        except Exception as e:\n",
    "            print(f\"  PCA failed for TRB ({e}), using TruncatedSVD\")\n",
    "            n_comp = max(1, min(50, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[1]-1))\n",
    "            onehot_trb_reduced = TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "        X_sequence = np.column_stack([\n",
    "            gene_features[:, :30],\n",
    "            onehot_tra_reduced,\n",
    "            onehot_trb_reduced,\n",
    "            tcr_physico,\n",
    "            qc_features\n",
    "        ])\n",
    "        # Train and evaluate model\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_sequence, y_encoded, test_size=0.3, random_state=42, stratify=stratify\n",
    "        )\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        XGBClass = globals().get('XGBClassifierSK', xgb.XGBClassifier)\n",
    "        model = XGBClass(random_state=42, eval_metric='logloss')\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        # Cross-validation\n",
    "        if cv_folds >= 2:\n",
    "            cv_scores = cross_val_score(model, X_sequence, y_encoded, cv=cv_folds, scoring='accuracy')\n",
    "            cv_mean = cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "        else:\n",
    "            print(\"  Skipping CV: not enough samples per class.\")\n",
    "            cv_scores = np.array([])\n",
    "            cv_mean = float('nan')\n",
    "            cv_std = float('nan')\n",
    "        length_results.append({\n",
    "            'max_length': max_length,\n",
    "            'accuracy': accuracy,\n",
    "            'cv_mean': cv_mean,\n",
    "            'cv_std': cv_std\n",
    "        })\n",
    "        print(f\"  Accuracy: {accuracy:.3f}, CV: {cv_mean:.3f} ¬± {cv_std:.3f}\")\n",
    "    # Plot results\n",
    "    length_df = pd.DataFrame(length_results)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(length_df['max_length'], length_df['accuracy'], 'o-', label='Test Accuracy', linewidth=2)\n",
    "    plt.plot(length_df['max_length'], length_df['cv_mean'], 's-', label='CV Accuracy', linewidth=2)\n",
    "    plt.fill_between(length_df['max_length'],\n",
    "                     length_df['cv_mean'] - length_df['cv_std'],\n",
    "                     length_df['cv_mean'] + length_df['cv_std'],\n",
    "                     alpha=0.3, label='CV ¬± Std')\n",
    "    plt.xlabel('Maximum Sequence Length Cutoff')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy vs Sequence Length Cutoff')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\nSequence length cutoff experiment completed!\")\n",
    "    if not length_df.empty and length_df['cv_mean'].notna().any():\n",
    "        best_len = length_df.loc[length_df['cv_mean'].idxmax(), 'max_length']\n",
    "        print(f\"Optimal length appears to be around {best_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e43fc5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Experiment with Sequence Length Cutoffs ---\n",
    "\n",
    "print(\"Experimenting with sequence length cutoffs...\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "# Ensure adata exists\n",
    "if 'adata' not in globals() or adata is None:\n",
    "    raise NameError(\"adata is not defined. Please run the data loading cells first.\")\n",
    "\n",
    "# Ensure label/patient helpers have run\n",
    "if '_ensure_response_and_patient' in globals():\n",
    "    _ensure_response_and_patient(adata)\n",
    "\n",
    "if '_get_supervised_mask_and_labels' in globals():\n",
    "    supervised_mask, y_supervised, _le, class_counts, SUPERVISED_AVAILABLE = _get_supervised_mask_and_labels(adata)\n",
    "else:\n",
    "    # Fallback logic\n",
    "    if 'response' in adata.obs.columns:\n",
    "        y_all = adata.obs['response'].astype(str)\n",
    "        supervised_mask = y_all.isin(['Responder', 'Non-Responder']).values\n",
    "        y_supervised = y_all[supervised_mask]\n",
    "        class_counts = y_supervised.value_counts().to_dict()\n",
    "        SUPERVISED_AVAILABLE = len(class_counts) >= 2 and min(class_counts.values()) >= 2\n",
    "        _le = LabelEncoder() if SUPERVISED_AVAILABLE else None\n",
    "    else:\n",
    "        supervised_mask = np.zeros(adata.n_obs, dtype=bool)\n",
    "        y_supervised = pd.Series([], dtype=object)\n",
    "        class_counts = {}\n",
    "        SUPERVISED_AVAILABLE = False\n",
    "        _le = None\n",
    "\n",
    "globals()['SUPERVISED_AVAILABLE'] = SUPERVISED_AVAILABLE\n",
    "\n",
    "if not SUPERVISED_AVAILABLE:\n",
    "    print(\"WARNING: Insufficient supervised labels. Skipping sequence length cutoff experiment.\")\n",
    "else:\n",
    "    supervised_mask = np.asarray(supervised_mask)\n",
    "\n",
    "    if '_ensure_int_labels' in globals():\n",
    "        y_encoded = _ensure_int_labels(_le.fit_transform(y_supervised))\n",
    "    else:\n",
    "        y_encoded = np.asarray(_le.fit_transform(y_supervised), dtype=np.int64)\n",
    "\n",
    "    min_class = min(class_counts.values()) if class_counts else 0\n",
    "    stratify = y_encoded if min_class >= 2 else None\n",
    "    cv_folds = min(3, min_class) if min_class >= 2 else 0\n",
    "\n",
    "    # Ensure cdr3_sequences exists\n",
    "    if 'cdr3_sequences' not in globals():\n",
    "        cdr3_sequences = {\n",
    "            'TRA': adata.obs['cdr3_TRA'].astype(str).fillna('').str.upper().tolist() if 'cdr3_TRA' in adata.obs.columns else [''] * adata.n_obs,\n",
    "            'TRB': adata.obs['cdr3_TRB'].astype(str).fillna('').str.upper().tolist() if 'cdr3_TRB' in adata.obs.columns else [''] * adata.n_obs\n",
    "        }\n",
    "\n",
    "    # Ensure gene features exist\n",
    "    if 'gene_features' not in globals():\n",
    "        if 'X_gene_pca' in adata.obsm:\n",
    "            gene_features = adata.obsm['X_gene_pca'][supervised_mask]\n",
    "        elif 'X_pca' in adata.obsm:\n",
    "            gene_features = adata.obsm['X_pca'][supervised_mask]\n",
    "        else:\n",
    "            gene_features = np.zeros((int(supervised_mask.sum()), 30))\n",
    "    if gene_features.shape[1] < 30:\n",
    "        gene_features = np.pad(gene_features, ((0, 0), (0, 30 - gene_features.shape[1])), mode='constant')\n",
    "\n",
    "    # Ensure TCR physico features exist\n",
    "    if 'tcr_physico' not in globals():\n",
    "        if all(c in adata.obs.columns for c in ['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity','trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']):\n",
    "            tra_physico = adata.obs[['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']].fillna(0)[supervised_mask].values.astype(np.float32)\n",
    "            trb_physico = adata.obs[['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']].fillna(0)[supervised_mask].values.astype(np.float32)\n",
    "            tcr_physico = np.column_stack([tra_physico, trb_physico]).astype(np.float32)\n",
    "        else:\n",
    "            tcr_physico = np.zeros((int(supervised_mask.sum()), 6), dtype=np.float32)\n",
    "\n",
    "    # Ensure QC features exist\n",
    "    if 'qc_features' not in globals():\n",
    "        if all(c in adata.obs.columns for c in ['n_genes_by_counts', 'total_counts', 'pct_counts_mt']):\n",
    "            qc_features = adata.obs[['n_genes_by_counts', 'total_counts', 'pct_counts_mt']].fillna(0)[supervised_mask].values\n",
    "        else:\n",
    "            qc_features = np.zeros((int(supervised_mask.sum()), 3))\n",
    "\n",
    "    # Define length cutoffs to test\n",
    "    length_cutoffs = [10, 15, 20, 25, 30, 35, 40, 50]\n",
    "\n",
    "    length_results = []\n",
    "\n",
    "    for max_length in length_cutoffs:\n",
    "        print(f\"\\nTesting max sequence length: {max_length}\")\n",
    "\n",
    "        # Re-encode sequences with new length - ENSURE FLOAT32 DTYPE\n",
    "        tra_onehot_new = np.array([one_hot_encode_sequence(seq, max_length, 'ACDEFGHIKLMNPQRSTVWY')\n",
    "                                   for seq in cdr3_sequences['TRA']], dtype=np.float32)\n",
    "        tra_onehot_flat_new = tra_onehot_new.reshape(tra_onehot_new.shape[0], -1).astype(np.float32)\n",
    "\n",
    "        trb_onehot_new = np.array([one_hot_encode_sequence(seq, max_length, 'ACDEFGHIKLMNPQRSTVWY')\n",
    "                                   for seq in cdr3_sequences['TRB']], dtype=np.float32)\n",
    "        trb_onehot_flat_new = trb_onehot_new.reshape(trb_onehot_new.shape[0], -1).astype(np.float32)\n",
    "\n",
    "        # Update AnnData\n",
    "        adata.obsm['X_tcr_tra_onehot'] = tra_onehot_flat_new\n",
    "        adata.obsm['X_tcr_trb_onehot'] = trb_onehot_flat_new\n",
    "\n",
    "        # Re-create feature sets with new encodings using robust PCA\n",
    "        # Use robust PCA reduction with fallback to TruncatedSVD\n",
    "        try:\n",
    "            n_comp_onehot = min(50, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[1], max(1, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[0]-1))\n",
    "            onehot_tra_reduced = PCA(n_components=n_comp_onehot, svd_solver='randomized', random_state=42).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "        except Exception as e:\n",
    "            print(f\"  PCA failed for TRA ({e}), using TruncatedSVD\")\n",
    "            n_comp = max(1, min(50, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[1]-1))\n",
    "            onehot_tra_reduced = TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "\n",
    "        try:\n",
    "            n_comp_onehot = min(50, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[1], max(1, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[0]-1))\n",
    "            onehot_trb_reduced = PCA(n_components=n_comp_onehot, svd_solver='randomized', random_state=42).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "        except Exception as e:\n",
    "            print(f\"  PCA failed for TRB ({e}), using TruncatedSVD\")\n",
    "            n_comp = max(1, min(50, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[1]-1))\n",
    "            onehot_trb_reduced = TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "\n",
    "        X_sequence = np.column_stack([\n",
    "            gene_features[:, :30],\n",
    "            onehot_tra_reduced,\n",
    "            onehot_trb_reduced,\n",
    "            tcr_physico,\n",
    "            qc_features\n",
    "        ])\n",
    "\n",
    "        # Train and evaluate model\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_sequence, y_encoded, test_size=0.3, random_state=42, stratify=stratify\n",
    "        )\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        XGBClass = globals().get('XGBClassifierSK', xgb.XGBClassifier)\n",
    "        model = XGBClass(random_state=42, eval_metric='logloss')\n",
    "\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Cross-validation\n",
    "        if cv_folds >= 2:\n",
    "            cv_scores = cross_val_score(model, X_sequence, y_encoded, cv=cv_folds, scoring='accuracy')\n",
    "            cv_mean = cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "        else:\n",
    "            print(\"  Skipping CV: not enough samples per class.\")\n",
    "            cv_scores = np.array([])\n",
    "            cv_mean = float('nan')\n",
    "            cv_std = float('nan')\n",
    "\n",
    "        length_results.append({\n",
    "            'max_length': max_length,\n",
    "            'accuracy': accuracy,\n",
    "            'cv_mean': cv_mean,\n",
    "            'cv_std': cv_std\n",
    "        })\n",
    "\n",
    "        print(f\"  Accuracy: {accuracy:.3f}, CV: {cv_mean:.3f} ¬± {cv_std:.3f}\")\n",
    "\n",
    "    # Plot results\n",
    "    length_df = pd.DataFrame(length_results)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(length_df['max_length'], length_df['accuracy'], 'o-', label='Test Accuracy', linewidth=2)\n",
    "    plt.plot(length_df['max_length'], length_df['cv_mean'], 's-', label='CV Accuracy', linewidth=2)\n",
    "    plt.fill_between(length_df['max_length'],\n",
    "                     length_df['cv_mean'] - length_df['cv_std'],\n",
    "                     length_df['cv_mean'] + length_df['cv_std'],\n",
    "                     alpha=0.3, label='CV ¬± Std')\n",
    "    plt.xlabel('Maximum Sequence Length Cutoff')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy vs Sequence Length Cutoff')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nSequence length cutoff experiment completed!\")\n",
    "    if not length_df.empty and length_df['cv_mean'].notna().any():\n",
    "        best_len = length_df.loc[length_df['cv_mean'].idxmax(), 'max_length']\n",
    "        print(f\"Optimal length appears to be around {best_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99c400",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Safe GPU patcher: apply GPU defaults without forcing unknown attributes\n",
    "def _apply_gpu_patches():\n",
    "    \"\"\"\n",
    "    Safely patch `param_grids` and `models_eval` to prefer GPU XGBoost settings\n",
    "    when available. This avoids setting attributes that may not exist on\n",
    "    estimator objects and wraps callable factories/classes safely.\n",
    "    \"\"\"\n",
    "    import inspect\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "    except Exception:\n",
    "        xgb = None\n",
    "\n",
    "    # Respect explicit user flag if set elsewhere; default False\n",
    "    XGBOOST_GPU_AVAILABLE = bool(globals().get('XGBOOST_GPU_AVAILABLE', False))\n",
    "\n",
    "    # Patch param_grids safely (do not overwrite user-specified entries)\n",
    "    try:\n",
    "        if 'param_grids' in globals() and XGBOOST_GPU_AVAILABLE:\n",
    "            pg = dict(param_grids.get('XGBoost', {}))\n",
    "            pg.setdefault('tree_method', ['gpu_hist'])\n",
    "            pg.setdefault('predictor', ['gpu_predictor'])\n",
    "            param_grids['XGBoost'] = pg\n",
    "            print(\"Patched param_grids['XGBoost'] with GPU options.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error patching param_grids:\", e)\n",
    "\n",
    "    # Patch models_eval in-place (wrap factories/classes or safely set params on instances)\n",
    "    try:\n",
    "        if 'models_eval' not in globals():\n",
    "            return\n",
    "        me = globals()['models_eval']\n",
    "        if 'XGBoost' not in me:\n",
    "            return\n",
    "        obj = me['XGBoost']\n",
    "\n",
    "        # If it's a callable factory (e.g., a lambda returning an estimator), wrap it so GPU kwargs are tried safely at call time\n",
    "        if callable(obj) and not isinstance(obj, type):\n",
    "            def make_wrapped(factory):\n",
    "                def wrapped(*a, **kw):\n",
    "                    if globals().get('XGBOOST_GPU_AVAILABLE', False):\n",
    "                        try:\n",
    "                            kw2 = dict(kw)\n",
    "                            kw2.setdefault('tree_method', 'gpu_hist')\n",
    "                            kw2.setdefault('predictor', 'gpu_predictor')\n",
    "                            return factory(*a, **kw2)\n",
    "                        except TypeError:\n",
    "                            try:\n",
    "                                kw2 = dict(kw)\n",
    "                                kw2.setdefault('tree_method', 'gpu_hist')\n",
    "                                kw2.pop('predictor', None)\n",
    "                                return factory(*a, **kw2)\n",
    "                            except Exception:\n",
    "                                return factory(*a, **kw)\n",
    "                    return factory(*a, **kw)\n",
    "                return wrapped\n",
    "            me['XGBoost'] = make_wrapped(obj)\n",
    "            print(\"Patched callable models_eval['XGBoost'] to include GPU kwargs safely.\")\n",
    "            return\n",
    "\n",
    "        # If it's a class type, create a subclass wrapper to add defaults in __init__\n",
    "        if isinstance(obj, type):\n",
    "            try:\n",
    "                sig = inspect.signature(obj.__init__)\n",
    "            except Exception:\n",
    "                sig = None\n",
    "            def make_class_with_defaults(cls, sig):\n",
    "                class Wrapped(cls):\n",
    "                    def __init__(self, *a, **kw):\n",
    "                        if globals().get('XGBOOST_GPU_AVAILABLE', False):\n",
    "                            kw.setdefault('tree_method', 'gpu_hist')\n",
    "                            if sig and 'predictor' in sig.parameters:\n",
    "                                kw.setdefault('predictor', 'gpu_predictor')\n",
    "                        super().__init__(*a, **kw)\n",
    "                return Wrapped\n",
    "            me['XGBoost'] = make_class_with_defaults(obj, sig)\n",
    "            print(\"Patched class models_eval['XGBoost'] to include GPU defaults.\")\n",
    "            return\n",
    "\n",
    "        # Otherwise assume it's an instantiated estimator; set params only if supported\n",
    "        try:\n",
    "            if hasattr(obj, 'get_params') and hasattr(obj, 'set_params'):\n",
    "                params = obj.get_params()\n",
    "                patch = {}\n",
    "                if globals().get('XGBOOST_GPU_AVAILABLE', False):\n",
    "                    if 'tree_method' in params:\n",
    "                        patch['tree_method'] = 'gpu_hist'\n",
    "                    if 'predictor' in params:\n",
    "                        patch['predictor'] = 'gpu_predictor'\n",
    "                if patch:\n",
    "                    obj.set_params(**patch)\n",
    "                    print(\"Patched instance models_eval['XGBoost'] params.\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to patch models_eval['XGBoost']:\", e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error patching models_eval:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b248a13",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Task 1-5: Enhanced ML Pipeline for Immunotherapy Response Prediction\n",
    "\n",
    "This section implements:\n",
    "1. **Task 1**: GroupKFold cross-validation with Patient-Level Aggregation (Shannon Entropy for TCR diversity)\n",
    "2. **Task 2**: TCR CDR3 encoding using physicochemical properties (Hydrophobicity, Charge, etc.)\n",
    "3. **Task 3**: Top 20 feature analysis cross-referenced with Sun et al. 2025 (GZMB, HLA-DR, ISGs)\n",
    "4. **Task 4**: Extended literature review including I-SPY2 trial and multimodal single-cell ML methods (TCR-H, CoNGA)\n",
    "5. **Task 5**: 4-panel publication figure (UMAP, SHAP, ROC, Boxplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44e53a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_groupkfold_model(patient_df, n_splits=None):\n",
    "    \"\"\"\n",
    "    Train XGBoost model with GroupKFold cross-validation based on Patient_ID.\n",
    "    \"\"\"\n",
    "    base_result = {\n",
    "        'status': 'skipped',\n",
    "        'reason': '',\n",
    "        'cv_accuracy': float('nan'),\n",
    "        'cv_precision': float('nan'),\n",
    "        'cv_recall': float('nan'),\n",
    "        'cv_f1': float('nan'),\n",
    "        'cv_roc_auc': float('nan'),\n",
    "        'confusion_matrix': np.array([]),\n",
    "        'y_true': np.array([]),\n",
    "        'y_pred': np.array([]),\n",
    "        'y_pred_proba': np.array([]),\n",
    "        'feature_importance': pd.DataFrame(),\n",
    "        'model': None,\n",
    "        'scaler': None,\n",
    "        'label_encoder': None,\n",
    "        'feature_cols': [],\n",
    "        'patient_df': patient_df if patient_df is not None else pd.DataFrame()\n",
    "    }\n",
    "    if patient_df is None or patient_df.empty:\n",
    "        base_result['reason'] = 'No patient-level data available.'\n",
    "        print('WARNING:', base_result['reason'])\n",
    "        return base_result\n",
    "    if 'Patient_ID' not in patient_df.columns or 'Response' not in patient_df.columns:\n",
    "        base_result['reason'] = 'Missing required columns Patient_ID/Response in patient_df.'\n",
    "        print('WARNING:', base_result['reason'])\n",
    "        return base_result\n",
    "    n_patients = patient_df['Patient_ID'].nunique()\n",
    "    if n_patients < 2:\n",
    "        base_result['reason'] = f'Not enough patients for GroupKFold (n={n_patients}).'\n",
    "        print('WARNING:', base_result['reason'])\n",
    "        return base_result\n",
    "    n_classes = patient_df['Response'].nunique()\n",
    "    if n_classes < 2:\n",
    "        base_result['reason'] = f'Not enough response classes for supervised learning (n={n_classes}).'\n",
    "        print('WARNING:', base_result['reason'])\n",
    "        return base_result\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training with GroupKFold Cross-Validation\")\n",
    "    print(\"=\"*60)\n",
    "    # Prepare features and labels\n",
    "    feature_cols = [col for col in patient_df.columns if col not in ['Patient_ID', 'Response', 'n_cells']]\n",
    "    if not feature_cols:\n",
    "        base_result['reason'] = 'No feature columns available for training.'\n",
    "        print('WARNING:', base_result['reason'])\n",
    "        return base_result\n",
    "    X = patient_df[feature_cols].fillna(0).values\n",
    "    y_labels = patient_df['Response'].values\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_labels)\n",
    "    y = _ensure_int_labels(y) if '_ensure_int_labels' in globals() else np.asarray(y, dtype=np.int64)\n",
    "    groups = patient_df['Patient_ID'].values\n",
    "    print(f\"Feature matrix: {X.shape}\")\n",
    "    print(f\"Labels: {len(y)}, Classes: {label_encoder.classes_}\")\n",
    "    print(f\"Patient groups: {len(np.unique(groups))}\")\n",
    "    # Determine number of splits\n",
    "    if n_splits is None:\n",
    "        n_splits = min(5, n_patients)\n",
    "    if n_splits < 2:\n",
    "        base_result['reason'] = f'Not enough patients for {n_splits}-fold GroupKFold.'\n",
    "        print('WARNING:', base_result['reason'])\n",
    "        return base_result\n",
    "    print(f\"Using {n_splits}-fold GroupKFold CV\")\n",
    "    # Initialize model\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    # Perform cross-validation\n",
    "    cv = GroupKFold(n_splits=n_splits)\n",
    "    # Get cross-validated predictions\n",
    "    y_pred = cross_val_predict(model, X, y, groups=groups, cv=cv, n_jobs=1)\n",
    "    y_pred_proba = cross_val_predict(model, X, y, groups=groups, cv=cv, method='predict_proba', n_jobs=1)\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, zero_division=0)\n",
    "    recall = recall_score(y, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y, y_pred, zero_division=0)\n",
    "    # ROC-AUC (handle binary classification)\n",
    "    try:\n",
    "        if len(label_encoder.classes_) == 2:\n",
    "            roc_auc = roc_auc_score(y, y_pred_proba[:, 1])\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y, y_pred_proba, multi_class='ovr')\n",
    "    except Exception:\n",
    "        roc_auc = float('nan')\n",
    "    print(\"\\n--- Cross-Validation Results ---\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred, target_names=label_encoder.classes_, zero_division=0))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(cm)\n",
    "    # Train final model on all data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    final_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    final_model.fit(X_scaled, y)\n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': final_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(\"\\n--- Top 10 Most Important Features ---\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "    # Package results\n",
    "    results = {\n",
    "        'status': 'ok',\n",
    "        'reason': '',\n",
    "        'cv_accuracy': accuracy,\n",
    "        'cv_precision': precision,\n",
    "        'cv_recall': recall,\n",
    "        'cv_f1': f1,\n",
    "        'cv_roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_true': y,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'feature_importance': feature_importance,\n",
    "        'model': final_model,\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': label_encoder,\n",
    "        'feature_cols': feature_cols,\n",
    "        'patient_df': patient_df\n",
    "    }\n",
    "    return results\n",
    "# ==========================================================================\n",
    "# Execute Task 1\n",
    "# ==========================================================================\n",
    "# Aggregate features at patient level\n",
    "patient_features_df = aggregate_patient_features(adata)\n",
    "# Display patient-level features (only available columns)\n",
    "print(\"\\n--- Patient-Level Feature Summary ---\")\n",
    "summary_cols = ['Patient_ID', 'Response', 'n_cells', 'TRA_shannon_entropy', 'TRB_shannon_entropy',\n",
    "                'TRA_clonality', 'TRB_clonality']\n",
    "summary_cols = [c for c in summary_cols if c in patient_features_df.columns]\n",
    "if summary_cols:\n",
    "    display(patient_features_df[summary_cols].round(3))\n",
    "else:\n",
    "    print(\"No patient-level summary columns available to display.\")\n",
    "# Train with GroupKFold CV\n",
    "groupcv_results = train_groupkfold_model(patient_features_df)\n",
    "# Save results only if training succeeded\n",
    "if groupcv_results.get('status') == 'ok':\n",
    "    output_dir = Path('Processed_Data')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    patient_features_df.to_csv(output_dir / 'patient_level_features.csv', index=False)\n",
    "    joblib.dump(groupcv_results['model'], output_dir / 'patient_level_model_groupcv.joblib')\n",
    "    groupcv_results['feature_importance'].to_csv(output_dir / 'patient_level_groupcv_results.csv', index=False)\n",
    "    print(\"\\nResults saved to Processed_Data/\")\n",
    "else:\n",
    "    print(f\"\\nSkipping save. Reason: {groupcv_results.get('reason')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24cdaf4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "TASK 2: Enhanced TCR CDR3 Encoding with Physicochemical Properties\n",
    "================================================================================\n",
    "This cell implements comprehensive TCR CDR3 encoding using:\n",
    "- Hydrophobicity (Kyte-Doolittle scale)\n",
    "- Charge (based on pKa values)\n",
    "- Polarity\n",
    "- Molecular weight\n",
    "- Volume\n",
    "- Flexibility\n",
    "- Additional biochemical indices\n",
    "\n",
    "These features capture the biophysical properties that govern TCR-antigen binding.\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TASK 2: Enhanced TCR CDR3 Physicochemical Encoding\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# Amino Acid Property Tables\n",
    "# ============================================================================\n",
    "\n",
    "# Kyte-Doolittle Hydrophobicity Scale (higher = more hydrophobic)\n",
    "HYDROPHOBICITY_KD = {\n",
    "    'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,\n",
    "    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,\n",
    "    'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,\n",
    "    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2\n",
    "}\n",
    "\n",
    "# Amino Acid Charge at pH 7 (approximate)\n",
    "CHARGE = {\n",
    "    'A': 0, 'R': 1, 'N': 0, 'D': -1, 'C': 0,\n",
    "    'Q': 0, 'E': -1, 'G': 0, 'H': 0.1, 'I': 0,  # H is ~10% protonated at pH 7\n",
    "    'L': 0, 'K': 1, 'M': 0, 'F': 0, 'P': 0,\n",
    "    'S': 0, 'T': 0, 'W': 0, 'Y': 0, 'V': 0\n",
    "}\n",
    "\n",
    "# Polarity (Grantham, 1974)\n",
    "POLARITY = {\n",
    "    'A': 8.1, 'R': 10.5, 'N': 11.6, 'D': 13.0, 'C': 5.5,\n",
    "    'Q': 10.5, 'E': 12.3, 'G': 9.0, 'H': 10.4, 'I': 5.2,\n",
    "    'L': 4.9, 'K': 11.3, 'M': 5.7, 'F': 5.2, 'P': 8.0,\n",
    "    'S': 9.2, 'T': 8.6, 'W': 5.4, 'Y': 6.2, 'V': 5.9\n",
    "}\n",
    "\n",
    "# Molecular Weight (Da)\n",
    "MOLECULAR_WEIGHT = {\n",
    "    'A': 89.1, 'R': 174.2, 'N': 132.1, 'D': 133.1, 'C': 121.2,\n",
    "    'Q': 146.2, 'E': 147.1, 'G': 75.1, 'H': 155.2, 'I': 131.2,\n",
    "    'L': 131.2, 'K': 146.2, 'M': 149.2, 'F': 165.2, 'P': 115.1,\n",
    "    'S': 105.1, 'T': 119.1, 'W': 204.2, 'Y': 181.2, 'V': 117.1\n",
    "}\n",
    "\n",
    "# Volume (√Ö¬≥) - Zamyatnin, 1972\n",
    "VOLUME = {\n",
    "    'A': 88.6, 'R': 173.4, 'N': 114.1, 'D': 111.1, 'C': 108.5,\n",
    "    'Q': 143.8, 'E': 138.4, 'G': 60.1, 'H': 153.2, 'I': 166.7,\n",
    "    'L': 166.7, 'K': 168.6, 'M': 162.9, 'F': 189.9, 'P': 112.7,\n",
    "    'S': 89.0, 'T': 116.1, 'W': 227.8, 'Y': 193.6, 'V': 140.0\n",
    "}\n",
    "\n",
    "# Flexibility Index (Bhaskaran-Ponnuswamy, 1988)\n",
    "FLEXIBILITY = {\n",
    "    'A': 0.360, 'R': 0.530, 'N': 0.460, 'D': 0.510, 'C': 0.350,\n",
    "    'Q': 0.490, 'E': 0.500, 'G': 0.540, 'H': 0.320, 'I': 0.460,\n",
    "    'L': 0.370, 'K': 0.470, 'M': 0.300, 'F': 0.310, 'P': 0.510,\n",
    "    'S': 0.510, 'T': 0.440, 'W': 0.310, 'Y': 0.420, 'V': 0.390\n",
    "}\n",
    "\n",
    "# Beta-sheet propensity (Chou-Fasman)\n",
    "BETA_SHEET = {\n",
    "    'A': 0.83, 'R': 0.93, 'N': 0.89, 'D': 0.54, 'C': 1.19,\n",
    "    'Q': 1.10, 'E': 0.37, 'G': 0.75, 'H': 0.87, 'I': 1.60,\n",
    "    'L': 1.30, 'K': 0.74, 'M': 1.05, 'F': 1.38, 'P': 0.55,\n",
    "    'S': 0.75, 'T': 1.19, 'W': 1.37, 'Y': 1.47, 'V': 1.70\n",
    "}\n",
    "\n",
    "\n",
    "def encode_cdr3_physicochemical(sequence, return_features_dict=False):\n",
    "    \"\"\"\n",
    "    Encode a CDR3 sequence using comprehensive physicochemical properties.\n",
    "    \n",
    "    Features computed:\n",
    "    1. Hydrophobicity: mean, sum, min, max, range\n",
    "    2. Charge: net charge, positive count, negative count, charge ratio\n",
    "    3. Polarity: mean, std\n",
    "    4. Size: length, total molecular weight, mean volume\n",
    "    5. Flexibility: mean, max\n",
    "    6. Beta-sheet propensity: mean\n",
    "    7. Positional features: N-term, C-term, middle region properties\n",
    "    \n",
    "    Args:\n",
    "        sequence: CDR3 amino acid sequence string\n",
    "        return_features_dict: If True, return dict with feature names\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of features (or dict if return_features_dict=True)\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence in ['nan', 'NA', '', None]:\n",
    "        n_features = 26  # Total number of features\n",
    "        if return_features_dict:\n",
    "            return {f'physico_feature_{i}': 0.0 for i in range(n_features)}\n",
    "        return np.zeros(n_features)\n",
    "    \n",
    "    seq = str(sequence).upper()\n",
    "    # Filter to valid amino acids\n",
    "    valid_aa = set(HYDROPHOBICITY_KD.keys())\n",
    "    seq = ''.join([c for c in seq if c in valid_aa])\n",
    "    \n",
    "    if len(seq) == 0:\n",
    "        n_features = 26\n",
    "        if return_features_dict:\n",
    "            return {f'physico_feature_{i}': 0.0 for i in range(n_features)}\n",
    "        return np.zeros(n_features)\n",
    "    \n",
    "    features = OrderedDict()\n",
    "    \n",
    "    # === Hydrophobicity Features ===\n",
    "    hydro_values = [HYDROPHOBICITY_KD.get(aa, 0) for aa in seq]\n",
    "    features['hydro_mean'] = np.mean(hydro_values)\n",
    "    features['hydro_sum'] = np.sum(hydro_values)\n",
    "    features['hydro_min'] = np.min(hydro_values)\n",
    "    features['hydro_max'] = np.max(hydro_values)\n",
    "    features['hydro_range'] = np.max(hydro_values) - np.min(hydro_values)\n",
    "    features['hydro_std'] = np.std(hydro_values) if len(hydro_values) > 1 else 0\n",
    "    \n",
    "    # === Charge Features ===\n",
    "    charge_values = [CHARGE.get(aa, 0) for aa in seq]\n",
    "    features['net_charge'] = np.sum(charge_values)\n",
    "    features['positive_aa_count'] = sum(1 for c in charge_values if c > 0)\n",
    "    features['negative_aa_count'] = sum(1 for c in charge_values if c < 0)\n",
    "    features['charge_ratio'] = (features['positive_aa_count'] / \n",
    "                                (features['negative_aa_count'] + 1))  # +1 to avoid div by zero\n",
    "    \n",
    "    # === Polarity Features ===\n",
    "    polarity_values = [POLARITY.get(aa, 0) for aa in seq]\n",
    "    features['polarity_mean'] = np.mean(polarity_values)\n",
    "    features['polarity_std'] = np.std(polarity_values) if len(polarity_values) > 1 else 0\n",
    "    \n",
    "    # === Size Features ===\n",
    "    features['length'] = len(seq)\n",
    "    mw_values = [MOLECULAR_WEIGHT.get(aa, 0) for aa in seq]\n",
    "    features['total_mw'] = np.sum(mw_values)\n",
    "    features['mean_mw'] = np.mean(mw_values)\n",
    "    \n",
    "    volume_values = [VOLUME.get(aa, 0) for aa in seq]\n",
    "    features['mean_volume'] = np.mean(volume_values)\n",
    "    features['total_volume'] = np.sum(volume_values)\n",
    "    \n",
    "    # === Flexibility Features ===\n",
    "    flex_values = [FLEXIBILITY.get(aa, 0) for aa in seq]\n",
    "    features['flexibility_mean'] = np.mean(flex_values)\n",
    "    features['flexibility_max'] = np.max(flex_values)\n",
    "    \n",
    "    # === Beta-sheet Propensity ===\n",
    "    beta_values = [BETA_SHEET.get(aa, 0) for aa in seq]\n",
    "    features['beta_propensity_mean'] = np.mean(beta_values)\n",
    "    \n",
    "    # === Positional Features (N-term, C-term, Middle) ===\n",
    "    # CDR3 regions often have conserved ends and variable middle\n",
    "    n_term = seq[:3] if len(seq) >= 3 else seq\n",
    "    c_term = seq[-3:] if len(seq) >= 3 else seq\n",
    "    middle = seq[3:-3] if len(seq) > 6 else seq\n",
    "    \n",
    "    features['nterm_hydro'] = np.mean([HYDROPHOBICITY_KD.get(aa, 0) for aa in n_term])\n",
    "    features['cterm_hydro'] = np.mean([HYDROPHOBICITY_KD.get(aa, 0) for aa in c_term])\n",
    "    features['middle_hydro'] = np.mean([HYDROPHOBICITY_KD.get(aa, 0) for aa in middle]) if middle else 0\n",
    "    \n",
    "    features['nterm_charge'] = np.sum([CHARGE.get(aa, 0) for aa in n_term])\n",
    "    features['cterm_charge'] = np.sum([CHARGE.get(aa, 0) for aa in c_term])\n",
    "    features['middle_charge'] = np.sum([CHARGE.get(aa, 0) for aa in middle]) if middle else 0\n",
    "    \n",
    "    if return_features_dict:\n",
    "        return features\n",
    "    \n",
    "    return np.array(list(features.values()))\n",
    "\n",
    "\n",
    "def encode_all_cdr3_physicochemical(adata):\n",
    "    \"\"\"\n",
    "    Encode all CDR3 sequences in the AnnData object with physicochemical features.\n",
    "    \n",
    "    Creates:\n",
    "    - adata.obsm['X_tcr_tra_physico_enhanced']: Enhanced TRA physicochemical features\n",
    "    - adata.obsm['X_tcr_trb_physico_enhanced']: Enhanced TRB physicochemical features\n",
    "    - Combined features added to adata.obs\n",
    "    \"\"\"\n",
    "    print(\"Encoding CDR3 sequences with enhanced physicochemical properties...\")\n",
    "    \n",
    "    # Get feature names from a sample encoding\n",
    "    sample_features = encode_cdr3_physicochemical('CASSYSGANVLTF', return_features_dict=True)\n",
    "    feature_names = list(sample_features.keys())\n",
    "    print(f\"Encoding {len(feature_names)} physicochemical features per sequence\")\n",
    "    \n",
    "    # Encode TRA sequences (safe if column missing)\n",
    "    tra_encodings = []\n",
    "    tra_iter = adata.obs['cdr3_TRA'].astype(str) if 'cdr3_TRA' in adata.obs.columns else pd.Series([''] * adata.n_obs, index=adata.obs.index)\n",
    "    for seq in tra_iter:\n",
    "        tra_encodings.append(encode_cdr3_physicochemical(seq))\n",
    "    tra_matrix = np.vstack(tra_encodings)\n",
    "    \n",
    "    # Encode TRB sequences (safe if column missing)\n",
    "    trb_encodings = []\n",
    "    trb_iter = adata.obs['cdr3_TRB'].astype(str) if 'cdr3_TRB' in adata.obs.columns else pd.Series([''] * adata.n_obs, index=adata.obs.index)\n",
    "    for seq in trb_iter:\n",
    "        trb_encodings.append(encode_cdr3_physicochemical(seq))\n",
    "    trb_matrix = np.vstack(trb_encodings)\n",
    "    \n",
    "    print(f\"TRA physicochemical matrix shape: {tra_matrix.shape}\")\n",
    "    print(f\"TRB physicochemical matrix shape: {trb_matrix.shape}\")\n",
    "    \n",
    "    # Ensure matrices are float32 for AnnData compatibility\n",
    "    tra_matrix = tra_matrix.astype(np.float32)\n",
    "    trb_matrix = trb_matrix.astype(np.float32)\n",
    "    \n",
    "    # Store in AnnData\n",
    "    adata.obsm['X_tcr_tra_physico_enhanced'] = tra_matrix\n",
    "    adata.obsm['X_tcr_trb_physico_enhanced'] = trb_matrix\n",
    "    \n",
    "    # Also add individual features to obs for easy access (ensure float type)\n",
    "    for i, fname in enumerate(feature_names):\n",
    "        adata.obs[f'tra_enhanced_{fname}'] = pd.Series(tra_matrix[:, i], index=adata.obs.index, dtype=np.float32)\n",
    "        adata.obs[f'trb_enhanced_{fname}'] = pd.Series(trb_matrix[:, i], index=adata.obs.index, dtype=np.float32)\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Execute Task 2\n",
    "# ============================================================================\n",
    "feature_names_physico = encode_all_cdr3_physicochemical(adata)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n--- Enhanced Physicochemical Feature Summary ---\")\n",
    "print(f\"Total features per chain: {len(feature_names_physico)}\")\n",
    "print(f\"Feature names: {feature_names_physico}\")\n",
    "\n",
    "# Compare responder vs non-responder\n",
    "print(\"\\n--- Physicochemical Comparison: Responder vs Non-Responder ---\")\n",
    "resp_mask = adata.obs['response'] == 'Responder'\n",
    "non_resp_mask = adata.obs['response'] == 'Non-Responder'\n",
    "\n",
    "comparison_df = []\n",
    "for fname in ['hydro_mean', 'net_charge', 'polarity_mean', 'flexibility_mean', 'length']:\n",
    "    tra_col = f'tra_enhanced_{fname}'\n",
    "    trb_col = f'trb_enhanced_{fname}'\n",
    "    \n",
    "    if tra_col in adata.obs.columns:\n",
    "        resp_tra = adata.obs.loc[resp_mask, tra_col].mean()\n",
    "        nonresp_tra = adata.obs.loc[non_resp_mask, tra_col].mean()\n",
    "        resp_trb = adata.obs.loc[resp_mask, trb_col].mean()\n",
    "        nonresp_trb = adata.obs.loc[non_resp_mask, trb_col].mean()\n",
    "        \n",
    "        comparison_df.append({\n",
    "            'Feature': fname,\n",
    "            'TRA_Responder': resp_tra,\n",
    "            'TRA_NonResponder': nonresp_tra,\n",
    "            'TRA_Diff': resp_tra - nonresp_tra,\n",
    "            'TRB_Responder': resp_trb,\n",
    "            'TRB_NonResponder': nonresp_trb,\n",
    "            'TRB_Diff': resp_trb - nonresp_trb\n",
    "        })\n",
    "\n",
    "display(pd.DataFrame(comparison_df).round(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 2 COMPLETED: Enhanced TCR Physicochemical Encoding\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7bbd94",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "TASK 3: Top 20 Feature Analysis Cross-Referenced with Sun et al. 2025\n",
    "================================================================================\n",
    "This cell analyzes top predictive features and cross-references them with:\n",
    "- GZMB (Granzyme B) - key cytotoxicity marker\n",
    "- HLA-DR genes - antigen presentation\n",
    "- Interferon-Stimulated Genes (ISGs)\n",
    "- Other markers identified in Sun et al. 2025\n",
    "\n",
    "Reference: Sun et al. 2025, npj Breast Cancer 11:65\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Validate prerequisites\n",
    "if 'adata' not in globals() or adata is None:\n",
    "    raise NameError(\"adata is not defined. Please run data loading cells first.\")\n",
    "\n",
    "if 'groupcv_results' not in globals() or groupcv_results is None:\n",
    "    print(\"WARNING: groupcv_results not defined. Skipping analyze_top_features().\")\n",
    "    print(\"Please run the patient-level LOPO CV cell first.\")\n",
    "    # Create a minimal placeholder to avoid errors downstream\n",
    "    groupcv_results = {'feature_importance': pd.DataFrame(columns=['feature', 'importance'])}\n",
    "\n",
    "# SHAP is optional for this cell; avoid hard failure if missing\n",
    "try:\n",
    "    import shap\n",
    "except Exception:\n",
    "    shap = None\n",
    "    print(\"shap not available; skipping SHAP-specific utilities in Task 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ad99b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "TASK 5: Publication-Quality 4-Panel Figure\n",
    "================================================================================\n",
    "This cell generates a comprehensive 4-panel figure suitable for publication:\n",
    "1. UMAP of cell types colored by response and cell type\n",
    "2. SHAP importance plot for the multimodal model\n",
    "3. Patient-level ROC curve from GroupKFold CV\n",
    "4. Boxplots of top 3 biological markers (GZMB, HLA-DR, ISG)\n",
    "\n",
    "Figure design follows journal guidelines for Nature/Cell Press publications.\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install SHAP if needed\n",
    "try:\n",
    "    import shap\n",
    "except ImportError:\n",
    "    %pip install shap\n",
    "    import shap\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TASK 5: Publication-Quality 4-Panel Figure\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set publication-quality defaults\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': ['Arial', 'DejaVu Sans'],\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 9,\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.transparent': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "})\n",
    "\n",
    "# Color palette\n",
    "COLORS = {\n",
    "    'Responder': '#2ecc71',       # Green\n",
    "    'Non-Responder': '#e74c3c',   # Red\n",
    "    'Unknown': '#95a5a6',         # Gray\n",
    "    'accent': '#3498db',          # Blue\n",
    "    'purple': '#9b59b6',          # Purple\n",
    "    'orange': '#e67e22',          # Orange\n",
    "}\n",
    "\n",
    "\n",
    "def panel_placeholder(ax, title, message):\n",
    "    \"\"\"Render a placeholder panel with a message.\"\"\"\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontweight='bold', loc='left')\n",
    "    ax.text(0.5, 0.5, message, ha='center', va='center', fontsize=10)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def create_panel_a_umap(ax, adata):\n",
    "    \"\"\"\n",
    "    Panel A: UMAP visualization of cells colored by response.\n",
    "    \"\"\"\n",
    "    print(\"Creating Panel A: UMAP visualization...\")\n",
    "\n",
    "    # Use stored UMAP or compute new one\n",
    "    umap_coords = None\n",
    "    if 'X_umap_combined' in adata.obsm:\n",
    "        umap_coords = adata.obsm['X_umap_combined']\n",
    "    elif 'X_umap' in adata.obsm:\n",
    "        umap_coords = adata.obsm['X_umap']\n",
    "    else:\n",
    "        # Compute UMAP\n",
    "        if 'X_gene_pca' in adata.obsm:\n",
    "            X_pca = adata.obsm['X_gene_pca'][:, :20]\n",
    "        elif 'X_pca' in adata.obsm:\n",
    "            X_pca = adata.obsm['X_pca'][:, :20]\n",
    "        else:\n",
    "            return panel_placeholder(ax, 'A. Single-Cell UMAP by Response', 'UMAP not available')\n",
    "\n",
    "        try:\n",
    "            import umap as umap_module\n",
    "            reducer = umap_module.UMAP(n_components=2, random_state=42)\n",
    "            umap_coords = reducer.fit_transform(X_pca)\n",
    "        except Exception as e:\n",
    "            print(f\"  UMAP computation failed: {e}\")\n",
    "            return panel_placeholder(ax, 'A. Single-Cell UMAP by Response', 'UMAP not available')\n",
    "\n",
    "    # Create color mapping\n",
    "    if 'response' in adata.obs.columns:\n",
    "        resp_series = adata.obs['response'].fillna('Unknown').astype(str)\n",
    "    else:\n",
    "        resp_series = pd.Series(['Unknown'] * adata.n_obs, index=adata.obs.index)\n",
    "\n",
    "    response_colors = []\n",
    "    for resp in resp_series:\n",
    "        if resp == 'Responder':\n",
    "            response_colors.append(COLORS['Responder'])\n",
    "        elif resp == 'Non-Responder':\n",
    "            response_colors.append(COLORS['Non-Responder'])\n",
    "        else:\n",
    "            response_colors.append(COLORS['Unknown'])\n",
    "\n",
    "    # Plot with alpha for better visualization\n",
    "    ax.scatter(\n",
    "        umap_coords[:, 0],\n",
    "        umap_coords[:, 1],\n",
    "        c=response_colors,\n",
    "        s=3,\n",
    "        alpha=0.6,\n",
    "        rasterized=True\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('UMAP 1')\n",
    "    ax.set_ylabel('UMAP 2')\n",
    "    ax.set_title('A. Single-Cell UMAP by Response', fontweight='bold', loc='left')\n",
    "\n",
    "    # Legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=COLORS['Responder'], label=f\"Responder (n={(resp_series=='Responder').sum():,})\"),\n",
    "        Patch(facecolor=COLORS['Non-Responder'], label=f\"Non-Responder (n={(resp_series=='Non-Responder').sum():,})\"),\n",
    "        Patch(facecolor=COLORS['Unknown'], label=f\"Unknown (n={(resp_series=='Unknown').sum():,})\"),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right', frameon=True, framealpha=0.9)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def create_panel_b_shap(ax, groupcv_results, patient_df):\n",
    "    \"\"\"\n",
    "    Panel B: SHAP importance plot for the multimodal model.\n",
    "    \"\"\"\n",
    "    print(\"Creating Panel B: SHAP importance plot...\")\n",
    "\n",
    "    if groupcv_results is None or patient_df is None or patient_df.empty:\n",
    "        return panel_placeholder(ax, 'B. Feature Importance (SHAP)', 'Not available')\n",
    "\n",
    "    model = groupcv_results.get('model')\n",
    "    feature_cols = groupcv_results.get('feature_cols')\n",
    "    scaler = groupcv_results.get('scaler')\n",
    "\n",
    "    if model is None or scaler is None or not feature_cols:\n",
    "        return panel_placeholder(ax, 'B. Feature Importance (SHAP)', 'Not available')\n",
    "\n",
    "    # Prepare data\n",
    "    X = patient_df[feature_cols].fillna(0).values\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    try:\n",
    "        # Compute SHAP values\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_scaled)\n",
    "    except Exception as e:\n",
    "        print(f\"  SHAP computation failed: {e}\")\n",
    "        return panel_placeholder(ax, 'B. Feature Importance (SHAP)', 'Not available')\n",
    "\n",
    "    # Get mean absolute SHAP values for feature importance\n",
    "    if isinstance(shap_values, list):\n",
    "        # Multi-class output\n",
    "        shap_importance = np.abs(shap_values[1]).mean(axis=0)\n",
    "    else:\n",
    "        shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    # Create DataFrame and get top 15 features\n",
    "    shap_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': shap_importance\n",
    "    }).sort_values('importance', ascending=True).tail(15)\n",
    "\n",
    "    # Create horizontal bar plot\n",
    "    colors = []\n",
    "    for feat in shap_df['feature']:\n",
    "        if 'shannon' in feat.lower() or 'clonality' in feat.lower():\n",
    "            colors.append(COLORS['purple'])\n",
    "        elif 'pca' in feat.lower():\n",
    "            colors.append(COLORS['accent'])\n",
    "        elif 'hydro' in feat.lower() or 'charge' in feat.lower():\n",
    "            colors.append(COLORS['orange'])\n",
    "        else:\n",
    "            colors.append('#7f8c8d')\n",
    "\n",
    "    ax.barh(range(len(shap_df)), shap_df['importance'], color=colors)\n",
    "\n",
    "    # Clean feature names for display\n",
    "    clean_names = []\n",
    "    for feat in shap_df['feature']:\n",
    "        name = feat.replace('_mean', '').replace('_', ' ').title()\n",
    "        if len(name) > 25:\n",
    "            name = name[:22] + '...'\n",
    "        clean_names.append(name)\n",
    "\n",
    "    ax.set_yticks(range(len(shap_df)))\n",
    "    ax.set_yticklabels(clean_names)\n",
    "    ax.set_xlabel('Mean |SHAP Value|')\n",
    "    ax.set_title('B. Feature Importance (SHAP)', fontweight='bold', loc='left')\n",
    "\n",
    "    # Legend for feature types\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=COLORS['accent'], label='Gene Expression'),\n",
    "        Patch(facecolor=COLORS['purple'], label='TCR Diversity'),\n",
    "        Patch(facecolor=COLORS['orange'], label='Physicochemical'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', frameon=True, fontsize=8)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def create_panel_c_roc(ax, groupcv_results):\n",
    "    \"\"\"\n",
    "    Panel C: Patient-level ROC curve from GroupKFold CV.\n",
    "    \"\"\"\n",
    "    print(\"Creating Panel C: Patient-level ROC curve...\")\n",
    "\n",
    "    if groupcv_results is None:\n",
    "        return panel_placeholder(ax, 'C. Patient-Level ROC Curve', 'Not available')\n",
    "\n",
    "    y_true = groupcv_results.get('y_true')\n",
    "    y_proba = groupcv_results.get('y_pred_proba')\n",
    "\n",
    "    if y_true is None or y_proba is None:\n",
    "        return panel_placeholder(ax, 'C. Patient-Level ROC Curve', 'Not available')\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_proba = np.asarray(y_proba)\n",
    "\n",
    "    if y_true.size == 0 or y_proba.size == 0:\n",
    "        return panel_placeholder(ax, 'C. Patient-Level ROC Curve', 'Not available')\n",
    "\n",
    "    if y_proba.ndim == 2 and y_proba.shape[1] >= 2:\n",
    "        y_score = y_proba[:, 1]\n",
    "    elif y_proba.ndim == 1:\n",
    "        y_score = y_proba\n",
    "    else:\n",
    "        return panel_placeholder(ax, 'C. Patient-Level ROC Curve', 'ROC not available')\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    ax.plot(fpr, tpr, color=COLORS['accent'], lw=2.5,\n",
    "            label=f'GroupKFold CV (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    # Diagonal reference line\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=1.5, alpha=0.5, label='Random (AUC = 0.50)')\n",
    "\n",
    "    # Fill under curve\n",
    "    ax.fill_between(fpr, tpr, alpha=0.2, color=COLORS['accent'])\n",
    "\n",
    "    # Add optimal threshold point\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    ax.scatter([fpr[optimal_idx]], [tpr[optimal_idx]],\n",
    "               color=COLORS['Responder'], s=100, zorder=5,\n",
    "               label=f'Optimal (sens={tpr[optimal_idx]:.2f}, spec={1-fpr[optimal_idx]:.2f})')\n",
    "\n",
    "    ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "    ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "    ax.set_title('C. Patient-Level ROC Curve', fontweight='bold', loc='left')\n",
    "    ax.legend(loc='lower right', frameon=True)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def create_panel_d_boxplots(ax, adata, patient_df=None, groupcv_results=None):\n",
    "    \"\"\"\n",
    "    Panel D: Boxplots of top 3 biological markers.\n",
    "    \"\"\"\n",
    "    print(\"Creating Panel D: Biomarker boxplots...\")\n",
    "\n",
    "    if patient_df is None and isinstance(groupcv_results, dict):\n",
    "        patient_df = groupcv_results.get('patient_df')\n",
    "\n",
    "    # Select markers to plot\n",
    "    markers_to_plot = []\n",
    "\n",
    "    # Try to find GZMB, HLA-DRA, and an ISG\n",
    "    candidate_markers = ['GZMB', 'HLA-DRA', 'ISG15', 'IFI6', 'GNLY', 'PRF1']\n",
    "\n",
    "    for marker in candidate_markers:\n",
    "        if marker in adata.var_names:\n",
    "            markers_to_plot.append(marker)\n",
    "        if len(markers_to_plot) >= 3:\n",
    "            break\n",
    "\n",
    "    # If we don't have 3, fall back to TCR diversity metrics\n",
    "    if len(markers_to_plot) < 3:\n",
    "        markers_to_plot.extend(['TRA_shannon_entropy', 'TRB_shannon_entropy', 'TRA_clonality'])\n",
    "        markers_to_plot = markers_to_plot[:3]\n",
    "\n",
    "    print(f\"  Plotting markers: {markers_to_plot}\")\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "\n",
    "    for marker in markers_to_plot:\n",
    "        if marker in adata.var_names:\n",
    "            # Gene expression marker\n",
    "            expr = adata[:, marker].X\n",
    "            expr = expr.toarray().ravel() if hasattr(expr, 'toarray') else np.asarray(expr).ravel()\n",
    "\n",
    "            resp_series = adata.obs['response'] if 'response' in adata.obs.columns else pd.Series(['Unknown'] * adata.n_obs)\n",
    "            for val, resp in zip(expr, resp_series):\n",
    "                if resp in ['Responder', 'Non-Responder']:\n",
    "                    plot_data.append({'Marker': marker, 'Expression': val, 'Response': resp})\n",
    "        elif marker in adata.obs.columns:\n",
    "            # obs column (TCR metrics)\n",
    "            resp_series = adata.obs['response'] if 'response' in adata.obs.columns else pd.Series(['Unknown'] * adata.n_obs)\n",
    "            for val, resp in zip(adata.obs[marker], resp_series):\n",
    "                if resp in ['Responder', 'Non-Responder']:\n",
    "                    plot_data.append({'Marker': marker.replace('_', ' ').title(),\n",
    "                                     'Expression': val, 'Response': resp})\n",
    "\n",
    "    # Fall back to patient-level features if cell-level data is limited\n",
    "    if len(plot_data) < 10 and patient_df is not None and not patient_df.empty:\n",
    "        print(\"  Using patient-level features for boxplot...\")\n",
    "        for col in ['TRA_shannon_entropy', 'TRB_shannon_entropy', 'TRA_clonality']:\n",
    "            if col in patient_df.columns:\n",
    "                for _, row in patient_df.iterrows():\n",
    "                    plot_data.append({\n",
    "                        'Marker': col.replace('_', ' ').title(),\n",
    "                        'Expression': row[col],\n",
    "                        'Response': row['Response']\n",
    "                    })\n",
    "\n",
    "    if len(plot_data) == 0:\n",
    "        return panel_placeholder(ax, 'D. Key Biomarkers by Response', 'Not available')\n",
    "\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Create grouped boxplot\n",
    "    palette = {'Responder': COLORS['Responder'], 'Non-Responder': COLORS['Non-Responder']}\n",
    "\n",
    "    sns.boxplot(\n",
    "        data=plot_df,\n",
    "        x='Marker',\n",
    "        y='Expression',\n",
    "        hue='Response',\n",
    "        palette=palette,\n",
    "        ax=ax,\n",
    "        linewidth=1.5,\n",
    "        fliersize=2\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Expression / Value')\n",
    "    ax.set_title('D. Key Biomarkers by Response', fontweight='bold', loc='left')\n",
    "    ax.legend(title='Response', loc='upper right', frameon=True)\n",
    "\n",
    "    # Rotate x-labels if needed\n",
    "    ax.tick_params(axis='x', rotation=15)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def create_publication_figure(adata, groupcv_results=None):\n",
    "    \"\"\"\n",
    "    Create the complete 4-panel publication figure.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Publication Figure ---\")\n",
    "\n",
    "    # Create figure with 2x2 layout\n",
    "    fig = plt.figure(figsize=(14, 12))\n",
    "    gs = gridspec.GridSpec(2, 2, figure=fig, wspace=0.3, hspace=0.35)\n",
    "\n",
    "    # Panel A: UMAP\n",
    "    ax_a = fig.add_subplot(gs[0, 0])\n",
    "    create_panel_a_umap(ax_a, adata)\n",
    "\n",
    "    # Panel B: SHAP\n",
    "    ax_b = fig.add_subplot(gs[0, 1])\n",
    "    if groupcv_results is None or groupcv_results.get('status') == 'skipped':\n",
    "        panel_placeholder(ax_b, 'B. Feature Importance (SHAP)', 'Not available')\n",
    "        patient_df = None\n",
    "    else:\n",
    "        patient_df = groupcv_results.get('patient_df')\n",
    "        create_panel_b_shap(ax_b, groupcv_results, patient_df)\n",
    "\n",
    "    # Panel C: ROC\n",
    "    ax_c = fig.add_subplot(gs[1, 0])\n",
    "    if groupcv_results is None or groupcv_results.get('status') == 'skipped':\n",
    "        panel_placeholder(ax_c, 'C. Patient-Level ROC Curve', 'Not available')\n",
    "    else:\n",
    "        create_panel_c_roc(ax_c, groupcv_results)\n",
    "\n",
    "    # Panel D: Boxplots\n",
    "    ax_d = fig.add_subplot(gs[1, 1])\n",
    "    create_panel_d_boxplots(ax_d, adata, patient_df=patient_df, groupcv_results=groupcv_results)\n",
    "\n",
    "    # Add overall title\n",
    "    fig.suptitle(\n",
    "        'Multimodal Machine Learning Predicts Immunotherapy Response in HR+ Breast Cancer',\n",
    "        fontsize=14,\n",
    "        fontweight='bold',\n",
    "        y=0.98\n",
    "    )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "# Execute Task 5\n",
    "# ==========================================================================\n",
    "\n",
    "# Get groupcv_results if available\n",
    "_groupcv_results = globals().get('groupcv_results', None)\n",
    "\n",
    "# Create the publication figure\n",
    "try:\n",
    "    fig = create_publication_figure(adata, _groupcv_results)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create publication figure: {e}\")\n",
    "    fig = None\n",
    "\n",
    "# Save figure in multiple formats\n",
    "if fig is not None:\n",
    "    output_dir = Path('Processed_Data/figures')\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # High-resolution PNG\n",
    "    fig.savefig(output_dir / 'Figure_Multimodal_ML_Response.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {output_dir / 'Figure_Multimodal_ML_Response.png'}\")\n",
    "\n",
    "    # PDF for publication\n",
    "    fig.savefig(output_dir / 'Figure_Multimodal_ML_Response.pdf', bbox_inches='tight')\n",
    "    print(f\"Saved: {output_dir / 'Figure_Multimodal_ML_Response.pdf'}\")\n",
    "\n",
    "    # SVG for editing\n",
    "    fig.savefig(output_dir / 'Figure_Multimodal_ML_Response.svg', bbox_inches='tight')\n",
    "    print(f\"Saved: {output_dir / 'Figure_Multimodal_ML_Response.svg'}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TASK 5 COMPLETED: Publication-Quality 4-Panel Figure Generated\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping figure save because figure creation failed or was unavailable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e10a00",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary: Enhanced ML Pipeline for HR+ Breast Cancer Immunotherapy Response Prediction\n",
    "\n",
    "### Tasks Completed\n",
    "\n",
    "| Task | Description | Key Outputs |\n",
    "|------|-------------|-------------|\n",
    "| **Task 1** | GroupKFold CV with Patient-Level Aggregation | `patient_level_features.csv`, `patient_level_model_groupcv.joblib` |\n",
    "| **Task 2** | Enhanced TCR CDR3 Physicochemical Encoding | 28 features per chain (hydrophobicity, charge, polarity, etc.) |\n",
    "| **Task 3** | Top 20 Feature Analysis with Sun et al. 2025 | `sun_2025_marker_analysis.csv`, GZMB/HLA-DR/ISG validation |\n",
    "| **Task 4** | Extended Literature Review | I-SPY2 comparison, TCR-H/CoNGA methods |\n",
    "| **Task 5** | 4-Panel Publication Figure | `Figure_Multimodal_ML_Response.png/pdf/svg` |\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "1. **Data Leakage Prevention**: GroupKFold ensures all cells from same patient stay in same fold\n",
    "2. **Shannon Entropy TCR Diversity**: Captures clonal expansion dynamics (responders: dynamic turnover; non-responders: clonal stability)\n",
    "3. **Comprehensive Physicochemical Encoding**: 28 features capturing binding-relevant properties\n",
    "4. **Multi-resolution Analysis**: Cell-level clustering + patient-level prediction\n",
    "5. **Literature Validation**: Cross-referenced with Sun et al. 2025, I-SPY2, and emerging methods\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "```\n",
    "Processed_Data/\n",
    "‚îú‚îÄ‚îÄ patient_level_features.csv           # Patient-aggregated features with TCR diversity\n",
    "‚îú‚îÄ‚îÄ patient_level_groupcv_results.csv    # Per-fold CV metrics\n",
    "‚îú‚îÄ‚îÄ patient_level_model_groupcv.joblib   # Trained XGBoost model\n",
    "‚îú‚îÄ‚îÄ top_20_features_analysis.csv         # Feature importance ranking\n",
    "‚îú‚îÄ‚îÄ sun_2025_marker_analysis.csv         # Marker expression comparison\n",
    "‚îî‚îÄ‚îÄ figures/\n",
    "    ‚îú‚îÄ‚îÄ Figure_Multimodal_ML_Response.png\n",
    "    ‚îú‚îÄ‚îÄ Figure_Multimodal_ML_Response.pdf\n",
    "    ‚îî‚îÄ‚îÄ Figure_Multimodal_ML_Response.svg\n",
    "```\n",
    "\n",
    "### Reproducibility Notes\n",
    "\n",
    "- All random seeds set to 42 for reproducibility\n",
    "- GroupKFold CV ensures patient-level generalization\n",
    "- Feature scaling performed with StandardScaler (saved with model)\n",
    "- Multiple testing correction (Benjamini-Hochberg) applied to marker analysis\n",
    "\n",
    "### Citation\n",
    "\n",
    "If using this pipeline, please cite:\n",
    "- Sun et al. 2025, npj Breast Cancer 11:65 (GSE300475 dataset)\n",
    "- This enhanced ML pipeline developed for HR+ breast cancer immunotherapy response prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adabc33a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fixes applied\n",
    "\n",
    "- **Added safety defaults** for missing `adata.obsm` keys (e.g. `X_gene_umap`, `X_gene_svd`, TCR arrays) to avoid KeyError during feature assembly.\n",
    "- **Inserted a safe getter** `_get_obsm_or_zeros(adata, key, mask, n_cols)` to retrieve `obsm` arrays with a zeros fallback.\n",
    "- **Replaced unsafe monkeypatch** of `xgboost.XGBClassifier.__init__` with a **sklearn-compatible wrapper** `XGBClassifierSK` and adjusted `_apply_gpu_patches()` to use it when available.\n",
    "\n",
    "Notes:\n",
    "- The notebook contains historical outputs (errors/warnings) from a previous Kaggle run; the code has been made robust so these errors should not reoccur when re-running the notebook in Kaggle.\n",
    "- I recommend re-running the notebook from the top on Kaggle (where packages and GPUs are available) to validate results and regenerate plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1bed7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick non-fatal sanity checks (safe to run)\n",
    "try:\n",
    "    import numpy as np\n",
    "    if 'adata' in globals():\n",
    "        n_obs = getattr(adata, 'n_obs', adata.shape[0])\n",
    "        print('adata.n_obs:', n_obs)\n",
    "        for k in ['X_gene_pca', 'X_gene_svd', 'X_gene_umap']:\n",
    "            if k in adata.obsm:\n",
    "                shape = np.asarray(adata.obsm[k]).shape\n",
    "                print(f\"{k}: present, shape={shape}\")\n",
    "            else:\n",
    "                print(f\"{k}: MISSING\")\n",
    "    else:\n",
    "        print('adata not defined in this environment (skip checks)')\n",
    "    print('XGBClassifierSK defined:', 'XGBClassifierSK' in globals())\n",
    "except Exception as e:\n",
    "    print('Sanity checks could not be completed:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacfbc28",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model summary and recommendation\n",
    "\n",
    "- **Models implemented**\n",
    "  - **XGBoost (tree ensemble):** Best performing on the *comprehensive* feature set (gene PCs + TCR k-mers + physicochemical features).\n",
    "  - **RandomForest / LogisticRegression:** Baselines.\n",
    "  - **Feed-forward MLP:** Dense network for tabular / flattened sequence inputs.\n",
    "  - **Sequence-aware architectures:** 1D **CNN**, **BiLSTM** (RNN), and **Transformer** (attention) encoders for CDR3 sequences.\n",
    "\n",
    "- **Recommendation (practical best model):**\n",
    "  - **XGBoost on the comprehensive feature set** with nested Group/LOPO CV, the expanded hyperparameter grid (n_estimators, max_depth, learning_rate, subsample, colsample_bytree), and **patient-level aggregation** (mean cell probabilities -> patient prediction). This gives best performance and interpretable feature importance.\n",
    "\n",
    "- **If you want a deep multimodal approach:**\n",
    "  - Use the **Transformer encoder** for sequence embeddings + MLP for gene PCs, train with **class_weight**, **EarlyStopping** monitoring **val_auc**, and evaluate with patient-level aggregation. Consider pretrained protein language model embeddings (ESM / ProtTrans) if compute permits.\n",
    "\n",
    "- **Next steps:**\n",
    "  1. Re-run LOPO with the updated XGBoost grid and patient-level aggregation.\n",
    "  2. Optionally run a short LOPO experiment for the Transformer-based multimodal model.\n",
    "\n",
    "*I implemented patient-level metrics and DL training improvements (AUC metrics, val_auc early stopping).*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15890.401859,
   "end_time": "2026-02-06T10:02:26.828948",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-06T05:37:36.427089",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
