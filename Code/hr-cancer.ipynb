{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c42c0fd5",
   "metadata": {
    "papermill": {
     "duration": 0.016204,
     "end_time": "2026-02-13T17:09:59.215105",
     "exception": false,
     "start_time": "2026-02-13T17:09:59.198901",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Unsupervised Learning for HR+ Breast Cancer RNA Sequencing\n",
    "This notebook performs a comprehensive analysis of single-cell RNA sequencing data to predict immunotherapy response in high-risk HR+/HER2- breast cancer patients. It includes data loading, quality control, TCR sequence integration, unsupervised clustering, and multi-modal machine learning classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64138369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:09:59.244846Z",
     "iopub.status.busy": "2026-02-13T17:09:59.244572Z",
     "iopub.status.idle": "2026-02-13T17:09:59.253690Z",
     "shell.execute_reply": "2026-02-13T17:09:59.252967Z"
    },
    "papermill": {
     "duration": 0.025294,
     "end_time": "2026-02-13T17:09:59.255041",
     "exception": false,
     "start_time": "2026-02-13T17:09:59.229747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initial Memory Check ===\n",
      "Current memory usage: 100.70 MB\n",
      "System memory: 3.6% used (0.68 GB / 31.35 GB)\n",
      "\n",
      "Tip: Run this cell periodically to monitor memory usage\n"
     ]
    }
   ],
   "source": [
    "# Memory monitoring utility\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    mem_mb = mem_info.rss / 1024 / 1024\n",
    "    print(f\"Current memory usage: {mem_mb:.2f} MB\")\n",
    "    \n",
    "    # System memory info\n",
    "    vm = psutil.virtual_memory()\n",
    "    print(f\"System memory: {vm.percent}% used ({vm.used / 1024**3:.2f} GB / {vm.total / 1024**3:.2f} GB)\")\n",
    "    return mem_mb\n",
    "\n",
    "# Check initial memory\n",
    "print(\"=== Initial Memory Check ===\")\n",
    "initial_mem = print_memory_usage()\n",
    "print(\"\\nTip: Run this cell periodically to monitor memory usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb1304b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:09:59.283076Z",
     "iopub.status.busy": "2026-02-13T17:09:59.282780Z",
     "iopub.status.idle": "2026-02-13T17:09:59.287596Z",
     "shell.execute_reply": "2026-02-13T17:09:59.286962Z"
    },
    "papermill": {
     "duration": 0.020322,
     "end_time": "2026-02-13T17:09:59.288937",
     "exception": false,
     "start_time": "2026-02-13T17:09:59.268615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL MODE: Running all analysis sections\n",
      "  - Skip Unsupervised Learning: False\n",
      "  - Skip XGBoost/LOPO CV: False\n",
      "  - Skip Traditional ML: False\n"
     ]
    }
   ],
   "source": [
    "# Set these flags to True to SKIP the corresponding sections\n",
    "# This is useful for debugging or if you only want to run deep learning\n",
    "\n",
    "SKIP_UNSUPERVISED_LEARNING = False    # Skip Leiden clustering, UMAP, etc. (Cell 44+)\n",
    "SKIP_XGBOOST_LOPO_CV = False          # Skip XGBoost and traditional ML LOPO CV (Cell 51+)\n",
    "SKIP_TRADITIONAL_ML = False           # Skip Logistic Regression, Random Forest, etc.\n",
    "SKIP_TO_DEEP_LEARNING = False         # Master switch: Skip everything except data loading and deep learning\n",
    "\n",
    "# If SKIP_TO_DEEP_LEARNING is True, it overrides the other flags\n",
    "if SKIP_TO_DEEP_LEARNING:\n",
    "    SKIP_UNSUPERVISED_LEARNING = True\n",
    "    SKIP_XGBOOST_LOPO_CV = True\n",
    "    SKIP_TRADITIONAL_ML = True\n",
    "    print(\"FAST MODE: Skipping unsupervised learning and traditional ML, going straight to deep learning!\")\n",
    "else:\n",
    "    print(\"FULL MODE: Running all analysis sections\")\n",
    "    \n",
    "print(f\"  - Skip Unsupervised Learning: {SKIP_UNSUPERVISED_LEARNING}\")\n",
    "print(f\"  - Skip XGBoost/LOPO CV: {SKIP_XGBOOST_LOPO_CV}\")\n",
    "print(f\"  - Skip Traditional ML: {SKIP_TRADITIONAL_ML}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f608b4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:09:59.317285Z",
     "iopub.status.busy": "2026-02-13T17:09:59.316836Z",
     "iopub.status.idle": "2026-02-13T17:10:29.028409Z",
     "shell.execute_reply": "2026-02-13T17:10:29.027509Z"
    },
    "papermill": {
     "duration": 29.727737,
     "end_time": "2026-02-13T17:10:29.030178",
     "exception": false,
     "start_time": "2026-02-13T17:09:59.302441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.6/176.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.1/284.1 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install anndata scanpy scikit-learn umap-learn --quiet\n",
    "%pip install biopython --quiet\n",
    "%pip install scikit-learn --quiet\n",
    "%pip install umap-learn --quiet\n",
    "%pip install hdbscan --quiet\n",
    "%pip install plotly --quiet\n",
    "%pip install xgboost --quiet\n",
    "%pip install tensorflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d93a145c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:10:29.061219Z",
     "iopub.status.busy": "2026-02-13T17:10:29.060952Z",
     "iopub.status.idle": "2026-02-13T17:10:31.134305Z",
     "shell.execute_reply": "2026-02-13T17:10:31.133666Z"
    },
    "papermill": {
     "duration": 2.090816,
     "end_time": "2026-02-13T17:10:31.136028",
     "exception": false,
     "start_time": "2026-02-13T17:10:29.045212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure non-interactive Matplotlib backend to avoid font import issues\n",
    "import matplotlib\n",
    "try:\n",
    "    matplotlib.use('Agg')\n",
    "except Exception as e:\n",
    "    print(\"Could not set Agg backend:\", e)\n",
    "\n",
    "# Set memory optimization flags\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'  # Limit parallel threads to save memory\n",
    "\n",
    "# --- Idempotent monkeypatch CountVectorizer.fit_transform to handle empty vocabulary errors ---\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import scipy.sparse as _sps\n",
    "\n",
    "    # Only patch once; store original on the class to avoid double-wrapping\n",
    "    if not hasattr(CountVectorizer, '_orig_fit_transform'):\n",
    "        CountVectorizer._orig_fit_transform = CountVectorizer.fit_transform\n",
    "\n",
    "        def _safe_cv_fit(self, raw_docs, *args, **kwargs):\n",
    "            try:\n",
    "                return CountVectorizer._orig_fit_transform(self, raw_docs, *args, **kwargs)\n",
    "            except ValueError as e:\n",
    "                # Handle sklearn's \"empty vocabulary\" error by returning an all-zero matrix\n",
    "                if 'empty vocabulary' in str(e).lower():\n",
    "                    n = len(raw_docs) if raw_docs is not None else 0\n",
    "                    return _sps.csr_matrix((n, 1))\n",
    "                raise\n",
    "\n",
    "        CountVectorizer.fit_transform = _safe_cv_fit\n",
    "    else:\n",
    "        # Already patched; do nothing\n",
    "        pass\n",
    "except Exception as e:\n",
    "    # If sklearn/scipy are not available at import time, skip patching and log reason\n",
    "    print(\"CountVectorizer monkeypatch skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35cb2482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:10:31.167158Z",
     "iopub.status.busy": "2026-02-13T17:10:31.166509Z",
     "iopub.status.idle": "2026-02-13T17:10:31.257099Z",
     "shell.execute_reply": "2026-02-13T17:10:31.256281Z"
    },
    "papermill": {
     "duration": 0.107458,
     "end_time": "2026-02-13T17:10:31.258471",
     "exception": false,
     "start_time": "2026-02-13T17:10:31.151013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Kaggle: True\n"
     ]
    }
   ],
   "source": [
    "# --- Initial Setup & Imports ---\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install critical dependencies if missing\n",
    "try:\n",
    "    import Bio\n",
    "except ImportError:\n",
    "    print(\"Installing biopython...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"biopython\"])\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# BioPython Imports\n",
    "try:\n",
    "    from Bio.Seq import Seq\n",
    "    from Bio.SeqUtils import ProtParam\n",
    "except ImportError:\n",
    "    # If install just happened, might need re-import logic or kernel restart, \n",
    "    # but usually works in same session after import\n",
    "    pass\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Environment Detection ---\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input') or os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None\n",
    "print(f\"Running on Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # Ensure standard directories exist\n",
    "    os.makedirs('/kaggle/working/Data', exist_ok=True)\n",
    "    os.makedirs('/kaggle/working/Output', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba6100",
   "metadata": {
    "papermill": {
     "duration": 0.014357,
     "end_time": "2026-02-13T17:10:31.288227",
     "exception": false,
     "start_time": "2026-02-13T17:10:31.273870",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Loading and Preparation\n",
    "We analyze a single-cell dataset recently published by Sun et al. (2025) (GEO accession GSE300475). The data originates from the DFCI 16-466 clinical trial (NCT02999477), a randomized phase II study evaluating neoadjuvant nab-paclitaxel in combination with pembrolizumab for high-risk, early-stage HR+/HER2- breast cancer. The specific cohort analyzed consists of longitudinal peripheral blood mononuclear cell (PBMC) samples from patients in the chemotherapy-first arm.\n",
    "\n",
    "Patients were classified into binary response categories based on Residual Cancer Burden (RCB) index assessed at surgery:\n",
    "*   **Responders:** Patients achieving Pathologic Complete Response (pCR, RCB-0) or minimal residual disease (RCB-I).\n",
    "*   **Non-Responders:** Patients with moderate (RCB-II) or extensive (RCB-III) residual disease.\n",
    "\n",
    "The following code handles the downloading and extraction of the raw data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea57af28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:10:31.318546Z",
     "iopub.status.busy": "2026-02-13T17:10:31.317556Z",
     "iopub.status.idle": "2026-02-13T17:10:31.321960Z",
     "shell.execute_reply": "2026-02-13T17:10:31.321332Z"
    },
    "papermill": {
     "duration": 0.020999,
     "end_time": "2026-02-13T17:10:31.323325",
     "exception": false,
     "start_time": "2026-02-13T17:10:31.302326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "files_to_fetch = [\n",
    "    {\n",
    "        \"name\": \"GSE300475_RAW.tar\",\n",
    "        \"size\": \"565.5 Mb\",\n",
    "        \"download_url\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE300475&format=file\",\n",
    "        \"type\": \"TAR (of CSV, MTX, TSV)\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GSE300475_feature_ref.xlsx\",\n",
    "        \"size\": \"5.4 Kb\",\n",
    "        \"download_url\": \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE300nnn/GSE300475/suppl/GSE300475%5Ffeature%5Fref.xlsx\",\n",
    "        \"type\": \"XLSX\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e286caa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:10:31.353029Z",
     "iopub.status.busy": "2026-02-13T17:10:31.352769Z",
     "iopub.status.idle": "2026-02-13T17:10:31.360577Z",
     "shell.execute_reply": "2026-02-13T17:10:31.359837Z"
    },
    "papermill": {
     "duration": 0.024392,
     "end_time": "2026-02-13T17:10:31.361966",
     "exception": false,
     "start_time": "2026-02-13T17:10:31.337574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloads will be saved in: /kaggle/working/Data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set download directory based on environment\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_project_root():\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for candidate in [cwd, *cwd.parents]:\n",
    "        if (candidate / \"README.md\").exists() or (candidate / \"Code\").exists():\n",
    "            return candidate\n",
    "    return cwd\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # On Kaggle, use /kaggle/working which is writable\n",
    "    download_dir = Path(\"/kaggle/working/Data\")\n",
    "else:\n",
    "    # Local (VS Code / Windows): use project-root/Data\n",
    "    project_root = _find_project_root()\n",
    "    download_dir = project_root / \"Data\"\n",
    "\n",
    "download_dir = Path(download_dir)\n",
    "download_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Downloads will be saved in: {download_dir.resolve()}\\n\")\n",
    "\n",
    "def download_file(url, filename, destination_folder):\n",
    "    \"\"\"\n",
    "    Downloads a file from a given URL to a specified destination folder.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(destination_folder, filename)\n",
    "    print(f\"Attempting to download {filename} from {url}...\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        print(f\"Successfully downloaded {filename} to {filepath}\")\n",
    "        return filepath\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "661aaf04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:10:31.392209Z",
     "iopub.status.busy": "2026-02-13T17:10:31.391480Z",
     "iopub.status.idle": "2026-02-13T17:11:43.311097Z",
     "shell.execute_reply": "2026-02-13T17:11:43.310377Z"
    },
    "papermill": {
     "duration": 71.936321,
     "end_time": "2026-02-13T17:11:43.312616",
     "exception": false,
     "start_time": "2026-02-13T17:10:31.376295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download GSE300475_RAW.tar from https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE300475&format=file...\n",
      "Successfully downloaded GSE300475_RAW.tar to /kaggle/working/Data/GSE300475_RAW.tar\n",
      "Extracting GSE300475_RAW.tar...\n",
      "\n",
      "Files contained in GSE300475_RAW.tar:\n",
      " - GSM9061665_S1_barcodes.tsv.gz\n",
      " - GSM9061665_S1_features.tsv.gz\n",
      " - GSM9061665_S1_matrix.mtx.gz\n",
      " - GSM9061666_S2_barcodes.tsv.gz\n",
      " - GSM9061666_S2_features.tsv.gz\n",
      " - GSM9061666_S2_matrix.mtx.gz\n",
      " - GSM9061667_S3_barcodes.tsv.gz\n",
      " - GSM9061667_S3_features.tsv.gz\n",
      " - GSM9061667_S3_matrix.mtx.gz\n",
      " - GSM9061668_S4_barcodes.tsv.gz\n",
      " - GSM9061668_S4_features.tsv.gz\n",
      " - GSM9061668_S4_matrix.mtx.gz\n",
      " - GSM9061669_S5_barcodes.tsv.gz\n",
      " - GSM9061669_S5_features.tsv.gz\n",
      " - GSM9061669_S5_matrix.mtx.gz\n",
      " - GSM9061670_S6_barcodes.tsv.gz\n",
      " - GSM9061670_S6_features.tsv.gz\n",
      " - GSM9061670_S6_matrix.mtx.gz\n",
      " - GSM9061671_S7_barcodes.tsv.gz\n",
      " - GSM9061671_S7_features.tsv.gz\n",
      " - GSM9061671_S7_matrix.mtx.gz\n",
      " - GSM9061672_S8_barcodes.tsv.gz\n",
      " - GSM9061672_S8_features.tsv.gz\n",
      " - GSM9061672_S8_matrix.mtx.gz\n",
      " - GSM9061673_S9_barcodes.tsv.gz\n",
      " - GSM9061673_S9_features.tsv.gz\n",
      " - GSM9061673_S9_matrix.mtx.gz\n",
      " - GSM9061674_S10_barcodes.tsv.gz\n",
      " - GSM9061674_S10_features.tsv.gz\n",
      " - GSM9061674_S10_matrix.mtx.gz\n",
      " - GSM9061675_S11_barcodes.tsv.gz\n",
      " - GSM9061675_S11_features.tsv.gz\n",
      " - GSM9061675_S11_matrix.mtx.gz\n",
      " - GSM9061687_S1_all_contig_annotations.csv.gz\n",
      " - GSM9061688_S2_all_contig_annotations.csv.gz\n",
      " - GSM9061689_S3_all_contig_annotations.csv.gz\n",
      " - GSM9061690_S4_all_contig_annotations.csv.gz\n",
      " - GSM9061691_S5_all_contig_annotations.csv.gz\n",
      " - GSM9061692_S6_all_contig_annotations.csv.gz\n",
      " - GSM9061693_S7_all_contig_annotations.csv.gz\n",
      " - GSM9061694_S9_all_contig_annotations.csv.gz\n",
      " - GSM9061695_S10_all_contig_annotations.csv.gz\n",
      " - GSM9061696_S11_all_contig_annotations.csv.gz\n",
      "\n",
      "Extracted to: /kaggle/working/Data/GSE300475_RAW\n",
      "--------------------------------------------------\n",
      "\n",
      "Attempting to download GSE300475_feature_ref.xlsx from https://ftp.ncbi.nlm.nih.gov/geo/series/GSE300nnn/GSE300475/suppl/GSE300475%5Ffeature%5Fref.xlsx...\n",
      "Successfully downloaded GSE300475_feature_ref.xlsx to /kaggle/working/Data/GSE300475_feature_ref.xlsx\n"
     ]
    }
   ],
   "source": [
    "for file_info in files_to_fetch:\n",
    "    filename = file_info[\"name\"]\n",
    "    url = file_info[\"download_url\"]\n",
    "    file_type = file_info[\"type\"]\n",
    "\n",
    "    downloaded_filepath = download_file(url, filename, download_dir)\n",
    "\n",
    "    # If the file is a TAR archive, extract it and list the contents\n",
    "    if downloaded_filepath and filename.endswith(\".tar\"):\n",
    "        print(f\"Extracting {filename}...\\n\")\n",
    "        try:\n",
    "            with tarfile.open(downloaded_filepath, \"r\") as tar:\n",
    "                # List contents\n",
    "                members = tar.getnames()\n",
    "                print(f\"Files contained in {filename}:\")\n",
    "                for member in members:\n",
    "                    print(f\" - {member}\")\n",
    "\n",
    "                # Extract to a subdirectory within download_dir\n",
    "                extract_path = os.path.join(download_dir, filename.replace(\".tar\", \"\"))\n",
    "                os.makedirs(extract_path, exist_ok=True)\n",
    "                tar.extractall(path=extract_path, filter='data')\n",
    "                print(f\"\\nExtracted to: {extract_path}\")\n",
    "        except tarfile.TarError as e:\n",
    "            print(f\"Error extracting {filename}: {e}\")\n",
    "\n",
    "        print(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0634932f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:11:43.343768Z",
     "iopub.status.busy": "2026-02-13T17:11:43.343502Z",
     "iopub.status.idle": "2026-02-13T17:11:43.379870Z",
     "shell.execute_reply": "2026-02-13T17:11:43.378984Z"
    },
    "papermill": {
     "duration": 0.053597,
     "end_time": "2026-02-13T17:11:43.381474",
     "exception": false,
     "start_time": "2026-02-13T17:11:43.327877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data directory set to: /kaggle/working/Data/GSE300475_RAW\n",
      "Found 43 .gz files. Ready for processing (Decompression skipped).\n",
      "\n",
      "--- Preview of GSM9061673_S9_barcodes.tsv.gz ---\n",
      "   AAACCTGAGAGAGCTC-1\n",
      "0  AAACCTGAGCGTAGTG-1\n",
      "1  AAACCTGAGCGTGAGT-1\n",
      "2  AAACCTGAGGCATGTG-1\n",
      "3  AAACCTGAGTCACGCC-1\n",
      "4  AAACCTGAGTCCCACG-1\n",
      "\n",
      "--- Preview of GSM9061674_S10_barcodes.tsv.gz ---\n",
      "   AAACCTGAGATCCTGT-1\n",
      "0  AAACCTGAGCTCTCGG-1\n",
      "1  AAACCTGAGTAGGTGC-1\n",
      "2  AAACCTGAGTGGTAAT-1\n",
      "3  AAACCTGAGTTAAGTG-1\n",
      "4  AAACCTGCAAGGTTTC-1\n",
      "\n",
      "--- Preview of GSM9061665_S1_features.tsv.gz ---\n",
      "   ENSG00000243485 MIR1302-2HG  Gene Expression\n",
      "0  ENSG00000237613     FAM138A  Gene Expression\n",
      "1  ENSG00000186092       OR4F5  Gene Expression\n",
      "2  ENSG00000238009  AL627309.1  Gene Expression\n",
      "3  ENSG00000239945  AL627309.3  Gene Expression\n",
      "4  ENSG00000239906  AL627309.2  Gene Expression\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# NOTE: We SKIP explicit decompression to avoid consuming disk space/memory.\n",
    "# Scanpy's read_10x_mtx and other tools can read .gz files directly.\n",
    "\n",
    "def preview_file(file_path):\n",
    "    \"\"\"\n",
    "    Display the first few lines of a file (supports .gz automatically)\n",
    "    \"\"\"\n",
    "    if file_path is None: return\n",
    "    \n",
    "    print(f\"\\n--- Preview of {os.path.basename(file_path)} ---\")\n",
    "    try:\n",
    "        # Handle gzip if extension matches\n",
    "        opener = gzip.open if str(file_path).endswith('.gz') else open\n",
    "        \n",
    "        if str(file_path).endswith(\".tsv\") or str(file_path).endswith(\".csv\") or str(file_path).endswith(\".tsv.gz\") or str(file_path).endswith(\".csv.gz\"):\n",
    "            # Use pandas with nrows \n",
    "            sep = '\\t' if 'tsv' in str(file_path) else ','\n",
    "            comp = 'gzip' if str(file_path).endswith('.gz') else None\n",
    "            try:\n",
    "                # Try reading with header inference\n",
    "                df = pd.read_csv(file_path, sep=sep, nrows=5, compression=comp)\n",
    "                print(df)\n",
    "            except:\n",
    "                print(\"Could not read as CSV/TSV\")\n",
    "        elif 'matrix.mtx' in str(file_path):\n",
    "            # Read as text stream\n",
    "            with opener(file_path, 'rt') as f: # 'rt' for text mode\n",
    "                print(\"First 10 lines (header and data):\")\n",
    "                for _ in range(10):\n",
    "                    line = f.readline()\n",
    "                    if not line: break\n",
    "                    print(line.strip())\n",
    "        else:\n",
    "            print(f\"File type {file_path} preview not customized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not preview {file_path}: {e}\")\n",
    "\n",
    "# Define extract_dir based on download_dir from previous cell\n",
    "extract_dir = os.path.join(download_dir, \"GSE300475_RAW\")\n",
    "raw_data_dir = Path(extract_dir) # Explicitly define this for downstream cells\n",
    "print(f\"Raw data directory set to: {raw_data_dir}\")\n",
    "\n",
    "gz_files = []\n",
    "for root, _, files in os.walk(extract_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".gz\"):\n",
    "            gz_files.append((os.path.join(root, file), root))\n",
    "\n",
    "print(f\"Found {len(gz_files)} .gz files. Ready for processing (Decompression skipped).\")\n",
    "\n",
    "# Just preview a few to ensure they are readable\n",
    "for path, _ in gz_files[:3]:\n",
    "    preview_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bc71d23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:11:43.412047Z",
     "iopub.status.busy": "2026-02-13T17:11:43.411804Z",
     "iopub.status.idle": "2026-02-13T17:11:46.728877Z",
     "shell.execute_reply": "2026-02-13T17:11:46.727867Z"
    },
    "papermill": {
     "duration": 3.334314,
     "end_time": "2026-02-13T17:11:46.730636",
     "exception": false,
     "start_time": "2026-02-13T17:11:43.396322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scanpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf28cc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:11:46.762158Z",
     "iopub.status.busy": "2026-02-13T17:11:46.761838Z",
     "iopub.status.idle": "2026-02-13T17:11:46.768184Z",
     "shell.execute_reply": "2026-02-13T17:11:46.767427Z"
    },
    "papermill": {
     "duration": 0.023482,
     "end_time": "2026-02-13T17:11:46.769460",
     "exception": false,
     "start_time": "2026-02-13T17:11:46.745978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows in all contig annotation files: 0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Find all \"all_contig_annotations.csv\" files in the extracted directory and sum their lengths (number of rows)\n",
    "\n",
    "all_contig_files = glob.glob(os.path.join(extract_dir, \"*_all_contig_annotations.csv\"))\n",
    "total_rows = 0\n",
    "\n",
    "for file in all_contig_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        num_rows = len(df)\n",
    "        print(f\"{os.path.basename(file)}: {num_rows} rows\")\n",
    "        total_rows += num_rows\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read {file}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal rows in all contig annotation files: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42019d",
   "metadata": {
    "papermill": {
     "duration": 0.014913,
     "end_time": "2026-02-13T17:11:46.799030",
     "exception": false,
     "start_time": "2026-02-13T17:11:46.784117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Metadata Construction and Sample Mapping\n",
    "\n",
    "We manually define the metadata mapping for the 11 samples included in this analysis. This ensures accurate association of sample IDs with patient identifiers, timepoints (Baseline, Post-Treatment, Recurrence), and clinical response status (Responder vs. Non-Responder), as detailed in the study's supplementary materials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38e68085",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:11:46.830151Z",
     "iopub.status.busy": "2026-02-13T17:11:46.829521Z",
     "iopub.status.idle": "2026-02-13T17:11:54.514863Z",
     "shell.execute_reply": "2026-02-13T17:11:54.513882Z"
    },
    "papermill": {
     "duration": 7.702837,
     "end_time": "2026-02-13T17:11:54.516450",
     "exception": false,
     "start_time": "2026-02-13T17:11:46.813613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Scanpy version: 1.12\n",
      "Pandas version: 2.2.2\n",
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "%pip install scanpy pandas numpy --quiet\n",
    "# Import required libraries for single-cell RNA-seq analysis and data handling\n",
    "import scanpy as sc  # Main library for single-cell analysis, provides AnnData structure and many tools\n",
    "import pandas as pd  # For tabular data manipulation and metadata handling\n",
    "import numpy as np   # For numerical operations and array handling\n",
    "import os            # For operating system interactions (file paths, etc.)\n",
    "from pathlib import Path  # For robust and readable file path management\n",
    "\n",
    "# Print versions to ensure reproducibility and compatibility\n",
    "print(f\"Scanpy version: {sc.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76cd7b3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:11:54.548511Z",
     "iopub.status.busy": "2026-02-13T17:11:54.547986Z",
     "iopub.status.idle": "2026-02-13T17:11:54.560469Z",
     "shell.execute_reply": "2026-02-13T17:11:54.559745Z"
    },
    "papermill": {
     "duration": 0.029783,
     "end_time": "2026-02-13T17:11:54.561765",
     "exception": false,
     "start_time": "2026-02-13T17:11:54.531982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping explicit decompression to save disk space and IO.\n",
      "Scanpy handles .gz files directly during loading.\n",
      "Found 43 compressed files ready for loading.\n",
      "Example: /kaggle/working/Data/GSE300475_RAW/GSM9061673_S9_barcodes.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def preview_file(file_path):\n",
    "    \"\"\"\n",
    "    Display the first few lines of a decompressed file without loading the whole file into memory.\n",
    "    \"\"\"\n",
    "    if file_path is None: return\n",
    "    \n",
    "    print(f\"\\n--- Preview of {os.path.basename(file_path)} ---\")\n",
    "    try:\n",
    "        if file_path.endswith(\".tsv\") or file_path.endswith(\".csv\"):\n",
    "            # Use pandas with nrows to avoid loading full file\n",
    "            sep = '\\t' if file_path.endswith(\".tsv\") else ','\n",
    "            df = pd.read_csv(file_path, sep=sep, nrows=5) \n",
    "            print(df)\n",
    "        elif file_path.endswith(\".mtx\"):\n",
    "            # Read as text stream to avoid loading massive matrix into memory\n",
    "            with open(file_path, 'r') as f:\n",
    "                print(\"First 10 lines (header and data):\")\n",
    "                for _ in range(10):\n",
    "                    line = f.readline()\n",
    "                    if not line: break\n",
    "                    print(line.strip())\n",
    "        elif str(file_path).endswith(\".gz\"):\n",
    "             print(f\"File is compressed ({file_path}). Scanpy will handle decompression automatically.\")\n",
    "        else:\n",
    "            print(\"Unsupported file type for preview.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not preview {file_path}: {e}\")\n",
    "\n",
    "# Ensure download_dir exists (fallback protection)\n",
    "if 'download_dir' not in globals():\n",
    "     # Fallback logic if variable not in scope\n",
    "     if 'IS_KAGGLE' in globals() and IS_KAGGLE:\n",
    "         download_dir = \"/kaggle/working/Data\"\n",
    "     else:\n",
    "         def _find_project_root():\n",
    "             cwd = Path.cwd().resolve()\n",
    "             for candidate in [cwd, *cwd.parents]:\n",
    "                 if (candidate / \"README.md\").exists() or (candidate / \"Code\").exists():\n",
    "                     return candidate\n",
    "             return cwd\n",
    "         download_dir = str(_find_project_root() / \"Data\")\n",
    "\n",
    "# Normalize download_dir to string for os.path usage\n",
    "if isinstance(download_dir, Path):\n",
    "    download_dir = str(download_dir)\n",
    "\n",
    "extract_dir = os.path.join(download_dir, \"GSE300475_RAW\")\n",
    "\n",
    "# --- PATH CORRECTION LOGIC ---\n",
    "# If extract_dir is empty or missing, but files are in download_dir, use download_dir\n",
    "if not os.path.exists(extract_dir) or not any(f.endswith('.gz') for f in os.listdir(extract_dir) if os.path.isfile(os.path.join(extract_dir, f))):\n",
    "    if os.path.exists(download_dir) and any(f.endswith('.gz') for f in os.listdir(download_dir) if os.path.isfile(os.path.join(download_dir, f))):\n",
    "         print(f\"Detecting files in {download_dir} directly. Adjusting path.\")\n",
    "         extract_dir = download_dir\n",
    "\n",
    "# --- MEMORY OPTIMIZATION ---\n",
    "# We SKIP explicit decompression here because Scanpy's read_10x_mtx can read .gz files directly.\n",
    "# Decompressing large sparse matrices to dense text files on disk is unnecessary and wastes storage/IO.\n",
    "print(\"Skipping explicit decompression to save disk space and IO.\")\n",
    "print(\"Scanpy handles .gz files directly during loading.\")\n",
    "\n",
    "# Just preview one GZ file to show it exists\n",
    "gz_files = []\n",
    "for root, _, files in os.walk(extract_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".gz\"):\n",
    "            gz_files.append(os.path.join(root, file))\n",
    "\n",
    "if gz_files:\n",
    "    print(f\"Found {len(gz_files)} compressed files ready for loading.\")\n",
    "    print(f\"Example: {gz_files[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdd3e789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:11:54.592550Z",
     "iopub.status.busy": "2026-02-13T17:11:54.592328Z",
     "iopub.status.idle": "2026-02-13T17:11:54.628235Z",
     "shell.execute_reply": "2026-02-13T17:11:54.627477Z"
    },
    "papermill": {
     "duration": 0.053137,
     "end_time": "2026-02-13T17:11:54.629641",
     "exception": false,
     "start_time": "2026-02-13T17:11:54.576504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory set to: /kaggle/working/Data/GSE300475_RAW\n",
      "Metadata table now matches the requested specification:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_Number</th>\n",
       "      <th>GEX_Sample_ID</th>\n",
       "      <th>TCR_Sample_ID</th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Response</th>\n",
       "      <th>In_Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>GSM9061687</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2</td>\n",
       "      <td>GSM9061666</td>\n",
       "      <td>GSM9061688</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S3</td>\n",
       "      <td>GSM9061667</td>\n",
       "      <td>GSM9061689</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Recurrence</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S4</td>\n",
       "      <td>GSM9061668</td>\n",
       "      <td>GSM9061690</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S5</td>\n",
       "      <td>GSM9061669</td>\n",
       "      <td>GSM9061691</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S6</td>\n",
       "      <td>GSM9061670</td>\n",
       "      <td>GSM9061692</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S7</td>\n",
       "      <td>GSM9061671</td>\n",
       "      <td>GSM9061693</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S8</td>\n",
       "      <td>GSM9061672</td>\n",
       "      <td>None</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Recurrence</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>GEX only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S9</td>\n",
       "      <td>GSM9061673</td>\n",
       "      <td>GSM9061694</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S10</td>\n",
       "      <td>GSM9061674</td>\n",
       "      <td>GSM9061695</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S11</td>\n",
       "      <td>GSM9061675</td>\n",
       "      <td>GSM9061696</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Recurrence</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S_Number GEX_Sample_ID TCR_Sample_ID Patient_ID   Timepoint       Response  \\\n",
       "0        S1    GSM9061665    GSM9061687        PT1    Baseline      Responder   \n",
       "1        S2    GSM9061666    GSM9061688        PT1     Post-Tx      Responder   \n",
       "2        S3    GSM9061667    GSM9061689        PT1  Recurrence      Responder   \n",
       "3        S4    GSM9061668    GSM9061690        PT2    Baseline      Responder   \n",
       "4        S5    GSM9061669    GSM9061691        PT2     Post-Tx      Responder   \n",
       "5        S6    GSM9061670    GSM9061692        PT3    Baseline  Non-Responder   \n",
       "6        S7    GSM9061671    GSM9061693        PT3     Post-Tx  Non-Responder   \n",
       "7        S8    GSM9061672          None        PT3  Recurrence  Non-Responder   \n",
       "8        S9    GSM9061673    GSM9061694        PT4    Baseline  Non-Responder   \n",
       "9       S10    GSM9061674    GSM9061695        PT4     Post-Tx  Non-Responder   \n",
       "10      S11    GSM9061675    GSM9061696        PT4  Recurrence  Non-Responder   \n",
       "\n",
       "     In_Data  \n",
       "0        Yes  \n",
       "1        Yes  \n",
       "2        Yes  \n",
       "3        Yes  \n",
       "4        Yes  \n",
       "5        Yes  \n",
       "6        Yes  \n",
       "7   GEX only  \n",
       "8        Yes  \n",
       "9        Yes  \n",
       "10       Yes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-check In_Data column (based on files found in raw_data_dir):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_Number</th>\n",
       "      <th>GEX_Sample_ID</th>\n",
       "      <th>TCR_Sample_ID</th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Response</th>\n",
       "      <th>In_Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>GSM9061687</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2</td>\n",
       "      <td>GSM9061666</td>\n",
       "      <td>GSM9061688</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S3</td>\n",
       "      <td>GSM9061667</td>\n",
       "      <td>GSM9061689</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Recurrence</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S4</td>\n",
       "      <td>GSM9061668</td>\n",
       "      <td>GSM9061690</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S5</td>\n",
       "      <td>GSM9061669</td>\n",
       "      <td>GSM9061691</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S6</td>\n",
       "      <td>GSM9061670</td>\n",
       "      <td>GSM9061692</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S7</td>\n",
       "      <td>GSM9061671</td>\n",
       "      <td>GSM9061693</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S8</td>\n",
       "      <td>GSM9061672</td>\n",
       "      <td>None</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Recurrence</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>GEX only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S9</td>\n",
       "      <td>GSM9061673</td>\n",
       "      <td>GSM9061694</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S10</td>\n",
       "      <td>GSM9061674</td>\n",
       "      <td>GSM9061695</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Post-Tx</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S11</td>\n",
       "      <td>GSM9061675</td>\n",
       "      <td>GSM9061696</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Recurrence</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S_Number GEX_Sample_ID TCR_Sample_ID Patient_ID   Timepoint       Response  \\\n",
       "0        S1    GSM9061665    GSM9061687        PT1    Baseline      Responder   \n",
       "1        S2    GSM9061666    GSM9061688        PT1     Post-Tx      Responder   \n",
       "2        S3    GSM9061667    GSM9061689        PT1  Recurrence      Responder   \n",
       "3        S4    GSM9061668    GSM9061690        PT2    Baseline      Responder   \n",
       "4        S5    GSM9061669    GSM9061691        PT2     Post-Tx      Responder   \n",
       "5        S6    GSM9061670    GSM9061692        PT3    Baseline  Non-Responder   \n",
       "6        S7    GSM9061671    GSM9061693        PT3     Post-Tx  Non-Responder   \n",
       "7        S8    GSM9061672          None        PT3  Recurrence  Non-Responder   \n",
       "8        S9    GSM9061673    GSM9061694        PT4    Baseline  Non-Responder   \n",
       "9       S10    GSM9061674    GSM9061695        PT4     Post-Tx  Non-Responder   \n",
       "10      S11    GSM9061675    GSM9061696        PT4  Recurrence  Non-Responder   \n",
       "\n",
       "     In_Data  \n",
       "0        Yes  \n",
       "1        Yes  \n",
       "2        Yes  \n",
       "3        Yes  \n",
       "4        Yes  \n",
       "5        Yes  \n",
       "6        Yes  \n",
       "7   GEX only  \n",
       "8        Yes  \n",
       "9        Yes  \n",
       "10       Yes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.2 ms, sys: 1.61 ms, total: 22.8 ms\n",
      "Wall time: 22.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Setup data paths ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import requests\n",
    "\n",
    "# Use existing IS_KAGGLE flag or detect\n",
    "if 'IS_KAGGLE' not in globals():\n",
    "    IS_KAGGLE = os.path.exists('/kaggle/input') or os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None\n",
    "\n",
    "def _find_project_root():\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for candidate in [cwd, *cwd.parents]:\n",
    "        if (candidate / \"README.md\").exists() or (candidate / \"Code\").exists():\n",
    "            return candidate\n",
    "    return cwd\n",
    "\n",
    "def _has_matrix_files(path: Path) -> bool:\n",
    "    return path.exists() and any(path.rglob(\"*matrix.mtx*\"))\n",
    "\n",
    "# Determine candidate base directories\n",
    "candidate_dirs = []\n",
    "if IS_KAGGLE:\n",
    "    candidate_dirs = [Path('/kaggle/working/Data'), Path('/Data'), Path('/kaggle/input')]\n",
    "else:\n",
    "    project_root = _find_project_root()\n",
    "    if 'download_dir' in globals() and download_dir:\n",
    "        candidate_dirs.append(Path(download_dir))\n",
    "    candidate_dirs += [project_root / 'Data', project_root / 'data', project_root]\n",
    "\n",
    "raw_data_dir = None\n",
    "for base in candidate_dirs:\n",
    "    if base is None:\n",
    "        continue\n",
    "    base = Path(base)\n",
    "    if base.name == 'GSE300475_RAW' and _has_matrix_files(base):\n",
    "        raw_data_dir = base\n",
    "        break\n",
    "    if _has_matrix_files(base / 'GSE300475_RAW'):\n",
    "        raw_data_dir = base / 'GSE300475_RAW'\n",
    "        break\n",
    "    if _has_matrix_files(base):\n",
    "        raw_data_dir = base\n",
    "        break\n",
    "\n",
    "# Fallback: search under project root (local only)\n",
    "if raw_data_dir is None and not IS_KAGGLE:\n",
    "    project_root = _find_project_root()\n",
    "    for match in project_root.rglob('GSE300475_RAW'):\n",
    "        if _has_matrix_files(match):\n",
    "            raw_data_dir = match\n",
    "            break\n",
    "\n",
    "# Auto-download if still missing\n",
    "if raw_data_dir is None:\n",
    "    print(\"Raw data not found locally. Attempting download...\")\n",
    "    if IS_KAGGLE:\n",
    "        download_dir = Path('/kaggle/working/Data')\n",
    "    else:\n",
    "        project_root = _find_project_root()\n",
    "        if 'download_dir' in globals() and download_dir:\n",
    "            download_dir = Path(download_dir)\n",
    "        else:\n",
    "            download_dir = project_root / 'Data'\n",
    "    download_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tar_path = download_dir / 'GSE300475_RAW.tar'\n",
    "    if not tar_path.exists():\n",
    "        url = 'https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE300475&format=file'\n",
    "        try:\n",
    "            with requests.get(url, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(tar_path, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            print(f\"Downloaded {tar_path}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Download failed: {e}\")\n",
    "    extract_path = download_dir / 'GSE300475_RAW'\n",
    "    if not extract_path.exists():\n",
    "        print(f\"Extracting {tar_path} to {extract_path}...\")\n",
    "        try:\n",
    "            with tarfile.open(tar_path, 'r') as tar:\n",
    "                try:\n",
    "                    tar.extractall(path=extract_path, filter='data')\n",
    "                except TypeError:\n",
    "                    tar.extractall(path=extract_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Extraction failed: {e}\")\n",
    "    raw_data_dir = extract_path\n",
    "\n",
    "print(f\"Data directory set to: {raw_data_dir}\")\n",
    "\n",
    "# --- Manually create the metadata mapping ---\n",
    "# This list contains information about each sample, including GEO IDs, patient IDs, timepoints, and treatment response.\n",
    "# Note: S8 (GSM9061672) has GEX files but no corresponding TCR file.\n",
    "metadata_list = [\n",
    "    # Patient 1 (Responder)\n",
    "    {'S_Number': 'S1',  'GEX_Sample_ID': 'GSM9061665', 'TCR_Sample_ID': 'GSM9061687', 'Patient_ID': 'PT1',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S2',  'GEX_Sample_ID': 'GSM9061666', 'TCR_Sample_ID': 'GSM9061688', 'Patient_ID': 'PT1',  'Timepoint': 'Post-Tx',      'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S3',  'GEX_Sample_ID': 'GSM9061667', 'TCR_Sample_ID': 'GSM9061689', 'Patient_ID': 'PT1',  'Timepoint': 'Recurrence',   'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "    \n",
    "    # Patient 2 (Responder)\n",
    "    {'S_Number': 'S4',  'GEX_Sample_ID': 'GSM9061668', 'TCR_Sample_ID': 'GSM9061690', 'Patient_ID': 'PT2',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S5',  'GEX_Sample_ID': 'GSM9061669', 'TCR_Sample_ID': 'GSM9061691', 'Patient_ID': 'PT2',  'Timepoint': 'Post-Tx',      'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "    \n",
    "    # Patient 3 (Non-Responder)\n",
    "    {'S_Number': 'S6',  'GEX_Sample_ID': 'GSM9061670', 'TCR_Sample_ID': 'GSM9061692', 'Patient_ID': 'PT3',  'Timepoint': 'Baseline',     'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S7',  'GEX_Sample_ID': 'GSM9061671', 'TCR_Sample_ID': 'GSM9061693', 'Patient_ID': 'PT3',  'Timepoint': 'Post-Tx',      'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S8',  'GEX_Sample_ID': 'GSM9061672', 'TCR_Sample_ID': None,         'Patient_ID': 'PT3',  'Timepoint': 'Recurrence',   'Response': 'Non-Responder', 'In_Data': 'GEX only'},\n",
    "    \n",
    "    # Patient 4 (Non-Responder)\n",
    "    {'S_Number': 'S9',  'GEX_Sample_ID': 'GSM9061673', 'TCR_Sample_ID': 'GSM9061694', 'Patient_ID': 'PT4',  'Timepoint': 'Baseline',     'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S10', 'GEX_Sample_ID': 'GSM9061674', 'TCR_Sample_ID': 'GSM9061695', 'Patient_ID': 'PT4',  'Timepoint': 'Post-Tx',      'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "    {'S_Number': 'S11', 'GEX_Sample_ID': 'GSM9061675', 'TCR_Sample_ID': 'GSM9061696', 'Patient_ID': 'PT4',  'Timepoint': 'Recurrence',   'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "]\n",
    "\n",
    "# Create pandas DataFrame for easy access\n",
    "metadata_df = pd.DataFrame(metadata_list)\n",
    "print(\"Metadata table now matches the requested specification:\")\n",
    "display(metadata_df)\n",
    "\n",
    "# --- Programmatic sanity-check for file presence ---\n",
    "# This loop checks if the expected files exist for each sample and updates the 'In_Data' column accordingly.\n",
    "for idx, row in metadata_df.iterrows():\n",
    "    s = row['S_Number']\n",
    "    g = row['GEX_Sample_ID']\n",
    "    t = row['TCR_Sample_ID']\n",
    "    # Check for gene expression matrix file (compressed or uncompressed)\n",
    "    # Check .mtx, .mtx.gz, and also potential file name variations or if they are in subfolders\n",
    "    # We look in raw_data_dir found above.\n",
    "    g_exists = (raw_data_dir / f\"{g}_{s}_matrix.mtx.gz\").exists() or (raw_data_dir / f\"{g}_{s}_matrix.mtx\").exists()\n",
    "    \n",
    "    # Also check if just the GSM id is present in some filename if strict match fails (fallback)\n",
    "    if not g_exists:\n",
    "         # Try simpler wildcard search\n",
    "         g_exists = len(list(raw_data_dir.glob(f\"*{g}*matrix*\"))) > 0\n",
    "\n",
    "    t_exists = False\n",
    "    # Check for TCR annotation file if TCR sample ID is present\n",
    "    if pd.notna(t) and t is not None:\n",
    "        t_exists = (raw_data_dir / f\"{t}_{s}_all_contig_annotations.csv.gz\").exists() or (raw_data_dir / f\"{t}_{s}_all_contig_annotations.csv\").exists()\n",
    "        if not t_exists:\n",
    "             t_exists = len(list(raw_data_dir.glob(f\"*{t}*all_contig_annotations*\"))) > 0\n",
    "             \n",
    "    # Update 'In_Data' column based on file presence\n",
    "    if g_exists and t_exists:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'Yes'\n",
    "    elif g_exists and not t_exists:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'GEX only'\n",
    "    else:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'No'\n",
    "\n",
    "print(\"\\nPost-check In_Data column (based on files found in raw_data_dir):\")\n",
    "display(metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d3ee384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:11:54.663119Z",
     "iopub.status.busy": "2026-02-13T17:11:54.662846Z",
     "iopub.status.idle": "2026-02-13T17:13:26.274899Z",
     "shell.execute_reply": "2026-02-13T17:13:26.274146Z"
    },
    "papermill": {
     "duration": 91.630211,
     "end_time": "2026-02-13T17:13:26.276496",
     "exception": false,
     "start_time": "2026-02-13T17:11:54.646285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Map Phase (Processing & Saving Chunks)...\n",
      "Processing 1/11: GSM9061665_S1\n",
      "  Saved chunk: 8804 cells. Memory cleared.\n",
      "Processing 2/11: GSM9061666_S2\n",
      "  Saved chunk: 9037 cells. Memory cleared.\n",
      "Processing 3/11: GSM9061667_S3\n",
      "  Saved chunk: 7343 cells. Memory cleared.\n",
      "Processing 4/11: GSM9061668_S4\n",
      "  Saved chunk: 8608 cells. Memory cleared.\n",
      "Processing 5/11: GSM9061669_S5\n",
      "  Saved chunk: 2887 cells. Memory cleared.\n",
      "Processing 6/11: GSM9061670_S6\n",
      "  Saved chunk: 10353 cells. Memory cleared.\n",
      "Processing 7/11: GSM9061671_S7\n",
      "  Saved chunk: 9186 cells. Memory cleared.\n",
      "Processing 8/11: GSM9061672_S8\n",
      "  Saved chunk: 12665 cells. Memory cleared.\n",
      "Processing 9/11: GSM9061673_S9\n",
      "  Saved chunk: 11216 cells. Memory cleared.\n",
      "Processing 10/11: GSM9061674_S10\n",
      "  Saved chunk: 9582 cells. Memory cleared.\n",
      "Processing 11/11: GSM9061675_S11\n",
      "  Saved chunk: 9286 cells. Memory cleared.\n",
      "\n",
      "Starting Reduce Phase (Merging 11 chunks)...\n",
      "Using on-disk concatenation (anndata.experimental.concat_on_disk)...\n",
      "Final Merged Data: 98967 cells x 14819 genes\n",
      "Full TCR Data: 162133 rows\n",
      "Temp chunks cleaned up.\n",
      "CPU times: user 1min 36s, sys: 8.8 s, total: 1min 45s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- DISK-BASED MAP-REDUCE STRATEGY TO SOLVE OOM ---\n",
    "# Strategy: \n",
    "# 1. Map: Process each sample -> QC -> Save to temp .h5ad on disk\n",
    "# 2. Reduce: Concatenate on disk (preferred) or in small batches\n",
    "# This keeps RAM usage low during processing and avoids the iterative reallocation spike.\n",
    "\n",
    "import gc\n",
    "import shutil\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "# Validate prerequisites\n",
    "if 'metadata_df' not in globals():\n",
    "    raise NameError(\"metadata_df is not defined. Please run the metadata creation cell first.\")\n",
    "if 'raw_data_dir' not in globals():\n",
    "    raise NameError(\"raw_data_dir is not defined. Please run the data path setup cell first.\")\n",
    "\n",
    "# Setup temp directory for chunks\n",
    "temp_chunk_dir = Path(\"temp_adata_chunks\")\n",
    "if temp_chunk_dir.exists():\n",
    "    shutil.rmtree(temp_chunk_dir)\n",
    "temp_chunk_dir.mkdir(exist_ok=True)\n",
    "\n",
    "chunk_files = []\n",
    "chunk_keys = []\n",
    "tcr_data_list = []  # TCR data is small enough to keep in memory\n",
    "\n",
    "print(\"Starting Map Phase (Processing & Saving Chunks)...\")\n",
    "\n",
    "# --- MAP PHASE: Process & Save ---\n",
    "for index, row in metadata_df.iterrows():\n",
    "    gex_sample_id = row['GEX_Sample_ID']\n",
    "    s_number = row['S_Number']\n",
    "    \n",
    "    # Construct sample-level prefix\n",
    "    sample_prefix = f\"{gex_sample_id}_{s_number}\"\n",
    "\n",
    "    # Use robust file finding logic from previous cells\n",
    "    matrix_file = None\n",
    "    for ext in ['matrix.mtx.gz', 'matrix.mtx']:\n",
    "        candidate = raw_data_dir / f\"{sample_prefix}_{ext}\"\n",
    "        if candidate.exists():\n",
    "            matrix_file = candidate\n",
    "            break\n",
    "            \n",
    "    if not matrix_file:\n",
    "         # Fallback search\n",
    "        for ext in ['matrix.mtx.gz', 'matrix.mtx']:\n",
    "            possible_files = list(raw_data_dir.glob(f\"*{gex_sample_id}*{ext}\"))\n",
    "            if possible_files:\n",
    "                matrix_file = possible_files[0]\n",
    "                break\n",
    "    \n",
    "    if not matrix_file:\n",
    "        print(f\"Skipping {sample_prefix}: Matrix file not found.\")\n",
    "        continue\n",
    "\n",
    "    sample_data_path = matrix_file.parent\n",
    "    matrix_prefix = matrix_file.name.replace('matrix.mtx', '').replace('.gz', '')\n",
    "\n",
    "    print(f\"Processing {index+1}/{len(metadata_df)}: {sample_prefix}\")\n",
    "    \n",
    "    try:\n",
    "        # Load GEX\n",
    "        adata_sample = sc.read_10x_mtx(\n",
    "            sample_data_path, \n",
    "            var_names='gene_symbols',\n",
    "            prefix=matrix_prefix,\n",
    "            cache=True\n",
    "        )\n",
    "        \n",
    "        # Ensure sparse float32 IMMEDIATELY\n",
    "        if not hasattr(adata_sample.X, 'toarray'):\n",
    "            adata_sample.X = sp.csr_matrix(adata_sample.X, dtype=np.float32)\n",
    "        else:\n",
    "            adata_sample.X = sp.csr_matrix(adata_sample.X, dtype=np.float32)\n",
    "            \n",
    "        # Add metadata\n",
    "        adata_sample.obs['sample_id'] = gex_sample_id \n",
    "        adata_sample.obs['patient_id'] = row['Patient_ID']\n",
    "        adata_sample.obs['timepoint'] = row['Timepoint']\n",
    "        adata_sample.obs['response'] = row['Response']\n",
    "        \n",
    "        # QC Filtering (Crucial reduction)\n",
    "        sc.pp.filter_cells(adata_sample, min_genes=200)\n",
    "        sc.pp.filter_genes(adata_sample, min_cells=3)\n",
    "        \n",
    "        # Ensure unique var names before saving\n",
    "        adata_sample.var_names_make_unique()\n",
    "        \n",
    "        # Save chunk\n",
    "        chunk_path = temp_chunk_dir / f\"chunk_{index}_{gex_sample_id}.h5ad\"\n",
    "        adata_sample.write_h5ad(chunk_path, compression='gzip')\n",
    "        chunk_files.append(chunk_path)\n",
    "        chunk_keys.append(sample_prefix)\n",
    "        \n",
    "        print(f\"  Saved chunk: {adata_sample.n_obs} cells. Memory cleared.\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del adata_sample\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {sample_prefix}: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # TCR Loading (Keep separate list)\n",
    "    tcr_sample_id = row['TCR_Sample_ID']\n",
    "    if pd.notna(tcr_sample_id):\n",
    "        tcr_file = raw_data_dir / f\"{tcr_sample_id}_{s_number}_all_contig_annotations.csv.gz\"\n",
    "        if not tcr_file.exists():\n",
    "             tcr_file = raw_data_dir / f\"{tcr_sample_id}_{s_number}_all_contig_annotations.csv\"\n",
    "             \n",
    "        if tcr_file.exists():\n",
    "            try:\n",
    "                # Load essential columns only\n",
    "                cols = ['barcode', 'is_cell', 'contig_id', 'high_confidence', 'length', \n",
    "                        'chain', 'v_gene', 'd_gene', 'j_gene', 'c_gene', 'full_length', \n",
    "                        'productive', 'cdr3']\n",
    "                        \n",
    "                # Check which columns actually exist in the file first to strictly avoid errors?\n",
    "                # Faster to just try/except or load all if cols obscure\n",
    "                # Let's try loading header first? No, pandas handling is fine.\n",
    "                tcr_df = pd.read_csv(tcr_file, usecols=lambda c: c in cols)\n",
    "                tcr_df['sample_id'] = gex_sample_id\n",
    "                tcr_data_list.append(tcr_df)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "# --- REDUCE PHASE: Disk-safe Concatenation ---\n",
    "print(f\"\\nStarting Reduce Phase (Merging {len(chunk_files)} chunks)...\")\n",
    "\n",
    "if not chunk_files:\n",
    "    raise ValueError(\"No chunks were saved! Check data paths.\")\n",
    "\n",
    "merged_path = temp_chunk_dir / \"merged.h5ad\"\n",
    "adata = None\n",
    "\n",
    "# Prefer on-disk concatenation if available (anndata>=0.9)\n",
    "try:\n",
    "    if hasattr(ad, \"experimental\") and hasattr(ad.experimental, \"concat_on_disk\"):\n",
    "        print(\"Using on-disk concatenation (anndata.experimental.concat_on_disk)...\")\n",
    "        # Use 'batch' as the label instead of 'sample_id' to preserve the original sample_id column\n",
    "        ad.experimental.concat_on_disk(\n",
    "            [str(p) for p in chunk_files],\n",
    "            str(merged_path),\n",
    "            join='inner',  # intersection avoids union blow-up\n",
    "            merge='same',\n",
    "            label='batch',  # Changed from 'sample_id' to preserve original sample_id\n",
    "            keys=chunk_keys,\n",
    "            index_unique='-'\n",
    "        )\n",
    "        adata = sc.read_h5ad(merged_path)\n",
    "    else:\n",
    "        raise AttributeError(\"concat_on_disk not available in this anndata version\")\n",
    "except Exception as e:\n",
    "    print(f\"Falling back to batch in-memory concat: {e}\")\n",
    "    batch_size = 4\n",
    "    adata = None\n",
    "    for i in range(0, len(chunk_files), batch_size):\n",
    "        batch_files = chunk_files[i:i+batch_size]\n",
    "        batch_adatas = [sc.read_h5ad(f) for f in batch_files]\n",
    "        batch = ad.concat(batch_adatas, join='inner', merge='same', index_unique='-')\n",
    "        del batch_adatas\n",
    "        gc.collect()\n",
    "        if adata is None:\n",
    "            adata = batch\n",
    "        else:\n",
    "            adata = ad.concat([adata, batch], join='inner', merge='same', index_unique='-')\n",
    "        del batch\n",
    "        gc.collect()\n",
    "\n",
    "# Final sparse enforcement\n",
    "if not sp.issparse(adata.X):\n",
    "    adata.X = sp.csr_matrix(adata.X, dtype=np.float32)\n",
    "\n",
    "print(f\"Final Merged Data: {adata.n_obs} cells x {adata.n_vars} genes\")\n",
    "\n",
    "# Merge TCR data\n",
    "if tcr_data_list:\n",
    "    full_tcr_df = pd.concat(tcr_data_list, ignore_index=True)\n",
    "    print(f\"Full TCR Data: {len(full_tcr_df)} rows\")\n",
    "    del tcr_data_list\n",
    "else:\n",
    "    print(\"No TCR data loaded.\")\n",
    "\n",
    "# Cleanup chunks\n",
    "shutil.rmtree(temp_chunk_dir)\n",
    "print(\"Temp chunks cleaned up.\")\n",
    "\n",
    "# Dummy adata_list for compatibility\n",
    "adata_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb04c24",
   "metadata": {
    "papermill": {
     "duration": 0.01855,
     "end_time": "2026-02-13T17:13:26.312503",
     "exception": false,
     "start_time": "2026-02-13T17:13:26.293953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Integrate TCR Data and Perform QC\n",
    "\n",
    "Next, we'll merge the TCR information into the `.obs` of our main `AnnData` object. We will keep only the cells that have corresponding TCR data and filter based on the `high_confidence` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9c573a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:13:26.347319Z",
     "iopub.status.busy": "2026-02-13T17:13:26.347037Z",
     "iopub.status.idle": "2026-02-13T17:13:26.355658Z",
     "shell.execute_reply": "2026-02-13T17:13:26.354867Z"
    },
    "papermill": {
     "duration": 0.02771,
     "end_time": "2026-02-13T17:13:26.357104",
     "exception": false,
     "start_time": "2026-02-13T17:13:26.329394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should_process_raw not set; defaulting to False (loaded_h5ad=False, adata_missing=False)\n"
     ]
    }
   ],
   "source": [
    "# 3. Raw Processing Branch (Only runs if needed)\n",
    "# Auto-define should_process_raw if missing to avoid NameError\n",
    "if 'should_process_raw' not in globals():\n",
    "    _loaded_h5ad = bool(globals().get('loaded_h5ad', False))\n",
    "    _adata_missing = ('adata' not in globals()) or (adata is None)\n",
    "    _metadata_ready = 'metadata_df' in globals()\n",
    "    should_process_raw = _metadata_ready and (not _loaded_h5ad) and _adata_missing\n",
    "    print(f\"should_process_raw not set; defaulting to {should_process_raw} (loaded_h5ad={_loaded_h5ad}, adata_missing={_adata_missing})\")\n",
    "\n",
    "if should_process_raw:\n",
    "    print(\"Starting raw data processing from metadata...\")\n",
    "\n",
    "    # Ensure raw_data_dir is defined\n",
    "    if 'raw_data_dir' not in globals():\n",
    "        base_dir = Path('/kaggle/working/Data') if (globals().get('IS_KAGGLE', False)) else Path('../Data')\n",
    "        raw_data_dir = base_dir / 'GSE300475_RAW'\n",
    "        print(f\"raw_data_dir undefined. Defaulting to: {raw_data_dir}\")\n",
    "    \n",
    "    # --- Initialize lists ---\n",
    "    adata_list = []  \n",
    "    tcr_data_list = []  \n",
    "\n",
    "    # --- Iterate through each sample ---\n",
    "    for index, row in metadata_df.iterrows():\n",
    "        gex_sample_id = row['GEX_Sample_ID']\n",
    "        tcr_sample_id = row['TCR_Sample_ID']\n",
    "        s_number = row['S_Number']\n",
    "        patient_id = row['Patient_ID']\n",
    "        timepoint = row['Timepoint']\n",
    "        response = row['Response']\n",
    "        \n",
    "        print(f\"Processing sample {index+1}/{len(metadata_df)}: {gex_sample_id} ({s_number})...\")\n",
    "        \n",
    "        # --- Robust File Finding (Fixing 'GEX data not found') ---\n",
    "        # Pattern: *GSM123*matrix.mtx* matches both .mtx and .mtx.gz\n",
    "        try:\n",
    "            found_gex_files = list(raw_data_dir.rglob(f\"*{gex_sample_id}*matrix.mtx*\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching {raw_data_dir}: {e}\")\n",
    "            found_gex_files = []\n",
    "        \n",
    "        if not found_gex_files:\n",
    "            print(f\"  Warning: GEX matrix file for {gex_sample_id} not found in {raw_data_dir}. Skipping.\")\n",
    "            try:\n",
    "                 print(\"  Debug: Listing first 5 files in raw_data_dir to help diagnose:\")\n",
    "                 for i, p in enumerate(raw_data_dir.rglob('*')):\n",
    "                     if i >= 5: break\n",
    "                     print(f\"    {p.name}\")\n",
    "            except: pass\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76282bd2",
   "metadata": {
    "papermill": {
     "duration": 0.018242,
     "end_time": "2026-02-13T17:13:26.391000",
     "exception": false,
     "start_time": "2026-02-13T17:13:26.372758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. TCR Integration and Data Saving\n",
    "\n",
    "We integrate the TCR sequencing data into the AnnData object by retrieving the corresponding TCR contig annotations (TRA/TRB chains) and merging them with the gene expression data based on cell barcodes. We also perform quality control to retain only cells with high-confidence TCR data.\n",
    "\n",
    "*Note: The code below also includes commented-out logic for saving the intermediate processed data to disk.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de1163a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:13:26.424247Z",
     "iopub.status.busy": "2026-02-13T17:13:26.423598Z",
     "iopub.status.idle": "2026-02-13T17:13:30.775272Z",
     "shell.execute_reply": "2026-02-13T17:13:30.774402Z"
    },
    "papermill": {
     "duration": 4.370207,
     "end_time": "2026-02-13T17:13:30.776866",
     "exception": false,
     "start_time": "2026-02-13T17:13:26.406659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrating TCR data into AnnData (TCR contigs: 162133, cells: 98967)...\n",
      "  DEBUG: adata.obs sample_id examples: ['GSM9061665', 'GSM9061666', 'GSM9061667']\n",
      "  DEBUG: TCR sample_id examples: ['GSM9061665', 'GSM9061666', 'GSM9061667']\n",
      "  DEBUG: adata barcode examples: ['AAACCTGAGAAGGGTA-1', 'AAACCTGAGACTGTAA-1', 'AAACCTGAGCAGCGTA-1']\n",
      "  DEBUG: TCR barcode examples: ['AAACCTGAGACTGTAA-1', 'AAACCTGAGCGTGAAC-1', 'AAACCTGAGCTACCTA-1']\n",
      "Successfully merged TCR data. Cells with TCR info: 38413 / 98967\n",
      "Filtered from 98967 to 38413 cells based on having high-confidence TCR data.\n",
      "\n",
      "Performing QC filtering (starting with 38413 cells, 14819 genes)...\n",
      "  After min_genes filter: 38413 cells\n",
      "  After min_cells filter: 14816 genes\n",
      "\n",
      "Post-QC AnnData object:\n",
      "AnnData object with n_obs × n_vars = 38413 × 14816\n",
      "    obs: 'sample_id', 'patient_id', 'timepoint', 'response', 'n_genes', 'batch', 'barcode_for_merge', 'barcode', 'cdr3_TRA', 'cdr3_TRB', 'j_gene_TRA', 'j_gene_TRB', 'v_gene_TRA', 'v_gene_TRB', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt'\n",
      "    var: 'gene_ids', 'feature_types', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'\n",
      "\n",
      "Sample metadata preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>timepoint</th>\n",
       "      <th>response</th>\n",
       "      <th>n_genes</th>\n",
       "      <th>batch</th>\n",
       "      <th>barcode_for_merge</th>\n",
       "      <th>barcode</th>\n",
       "      <th>cdr3_TRA</th>\n",
       "      <th>cdr3_TRB</th>\n",
       "      <th>j_gene_TRA</th>\n",
       "      <th>j_gene_TRB</th>\n",
       "      <th>v_gene_TRA</th>\n",
       "      <th>v_gene_TRB</th>\n",
       "      <th>n_genes_by_counts</th>\n",
       "      <th>total_counts</th>\n",
       "      <th>total_counts_mt</th>\n",
       "      <th>pct_counts_mt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGACTGTAA-1-GSM9061665_S1</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>1379</td>\n",
       "      <td>GSM9061665_S1</td>\n",
       "      <td>AAACCTGAGACTGTAA-1</td>\n",
       "      <td>AAACCTGAGACTGTAA-1</td>\n",
       "      <td>CAVEARNYKLTF</td>\n",
       "      <td>CASGTGLNTEAFF</td>\n",
       "      <td>TRAJ53</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>TRAV36/DV7</td>\n",
       "      <td>TRBV3-1</td>\n",
       "      <td>1379</td>\n",
       "      <td>4637.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>3.385810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGCGTGAAC-1-GSM9061665_S1</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>1275</td>\n",
       "      <td>GSM9061665_S1</td>\n",
       "      <td>AAACCTGAGCGTGAAC-1</td>\n",
       "      <td>AAACCTGAGCGTGAAC-1</td>\n",
       "      <td>CAASAVGNEKLTF</td>\n",
       "      <td>CAWSALLGTVNGYTF</td>\n",
       "      <td>TRAJ48</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>TRAV29/DV5</td>\n",
       "      <td>TRBV30</td>\n",
       "      <td>1275</td>\n",
       "      <td>4843.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>5.100144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGCTACCTA-1-GSM9061665_S1</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>886</td>\n",
       "      <td>GSM9061665_S1</td>\n",
       "      <td>AAACCTGAGCTACCTA-1</td>\n",
       "      <td>AAACCTGAGCTACCTA-1</td>\n",
       "      <td>CALSEAWGNARLMF</td>\n",
       "      <td>CASRSREETYEQYF</td>\n",
       "      <td>TRAJ31</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>TRAV19</td>\n",
       "      <td>TRBV2</td>\n",
       "      <td>886</td>\n",
       "      <td>3076.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>9.102731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGCTGTTCA-1-GSM9061665_S1</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>1628</td>\n",
       "      <td>GSM9061665_S1</td>\n",
       "      <td>AAACCTGAGCTGTTCA-1</td>\n",
       "      <td>AAACCTGAGCTGTTCA-1</td>\n",
       "      <td>CALLGLKGEGSARQLTF</td>\n",
       "      <td>CASSLPPWRANTEAFF</td>\n",
       "      <td>TRAJ22</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>TRAV9-2</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>1628</td>\n",
       "      <td>4914.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>5.860806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGGCATTGG-1-GSM9061665_S1</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>1313</td>\n",
       "      <td>GSM9061665_S1</td>\n",
       "      <td>AAACCTGAGGCATTGG-1</td>\n",
       "      <td>AAACCTGAGGCATTGG-1</td>\n",
       "      <td>CAVTGFSDGQKLLF</td>\n",
       "      <td>CASSLTGEVWDEQFF</td>\n",
       "      <td>TRAJ16</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>TRAV8-6</td>\n",
       "      <td>TRBV5-1</td>\n",
       "      <td>1313</td>\n",
       "      <td>4947.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>4.002426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   sample_id patient_id timepoint   response  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665_S1  GSM9061665        PT1  Baseline  Responder   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665_S1  GSM9061665        PT1  Baseline  Responder   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665_S1  GSM9061665        PT1  Baseline  Responder   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665_S1  GSM9061665        PT1  Baseline  Responder   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665_S1  GSM9061665        PT1  Baseline  Responder   \n",
       "\n",
       "                                  n_genes          batch   barcode_for_merge  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665_S1     1379  GSM9061665_S1  AAACCTGAGACTGTAA-1   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665_S1     1275  GSM9061665_S1  AAACCTGAGCGTGAAC-1   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665_S1      886  GSM9061665_S1  AAACCTGAGCTACCTA-1   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665_S1     1628  GSM9061665_S1  AAACCTGAGCTGTTCA-1   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665_S1     1313  GSM9061665_S1  AAACCTGAGGCATTGG-1   \n",
       "\n",
       "                                             barcode           cdr3_TRA  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665_S1  AAACCTGAGACTGTAA-1       CAVEARNYKLTF   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665_S1  AAACCTGAGCGTGAAC-1      CAASAVGNEKLTF   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665_S1  AAACCTGAGCTACCTA-1     CALSEAWGNARLMF   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665_S1  AAACCTGAGCTGTTCA-1  CALLGLKGEGSARQLTF   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665_S1  AAACCTGAGGCATTGG-1     CAVTGFSDGQKLLF   \n",
       "\n",
       "                                          cdr3_TRB j_gene_TRA j_gene_TRB  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665_S1     CASGTGLNTEAFF     TRAJ53    TRBJ1-1   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665_S1   CAWSALLGTVNGYTF     TRAJ48    TRBJ1-2   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665_S1    CASRSREETYEQYF     TRAJ31    TRBJ2-7   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665_S1  CASSLPPWRANTEAFF     TRAJ22    TRBJ1-1   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665_S1   CASSLTGEVWDEQFF     TRAJ16    TRBJ2-1   \n",
       "\n",
       "                                  v_gene_TRA v_gene_TRB  n_genes_by_counts  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665_S1  TRAV36/DV7    TRBV3-1               1379   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665_S1  TRAV29/DV5     TRBV30               1275   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665_S1      TRAV19      TRBV2                886   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665_S1     TRAV9-2   TRBV11-2               1628   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665_S1     TRAV8-6    TRBV5-1               1313   \n",
       "\n",
       "                                  total_counts  total_counts_mt  pct_counts_mt  \n",
       "AAACCTGAGACTGTAA-1-GSM9061665_S1        4637.0            157.0       3.385810  \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665_S1        4843.0            247.0       5.100144  \n",
       "AAACCTGAGCTACCTA-1-GSM9061665_S1        3076.0            280.0       9.102731  \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665_S1        4914.0            288.0       5.860806  \n",
       "AAACCTGAGGCATTGG-1-GSM9061665_S1        4947.0            198.0       4.002426  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.28 s, sys: 929 ms, total: 4.21 s\n",
      "Wall time: 4.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Integrate TCR data into AnnData.obs and perform quality control ---\n",
    "\n",
    "# Validate that adata exists\n",
    "if 'adata' not in globals():\n",
    "    raise NameError(\"adata is not defined. Please run the data loading cell first.\")\n",
    "if adata is None:\n",
    "    raise ValueError(\"adata is None. Data loading may have failed.\")\n",
    "\n",
    "# Check if TCR data exists and is not empty\n",
    "if 'full_tcr_df' in globals() and isinstance(full_tcr_df, pd.DataFrame) and not full_tcr_df.empty:\n",
    "    print(f\"Integrating TCR data into AnnData (TCR contigs: {len(full_tcr_df)}, cells: {adata.n_obs})...\")\n",
    "    \n",
    "    try:\n",
    "        # --- TCR Data Aggregation ---\n",
    "        # The previous join failed because one cell (barcode) can have multiple TCR contigs (e.g., TRA and TRB chains),\n",
    "        # creating a one-to-many join that increases the number of rows.\n",
    "        # The fix is to aggregate the TCR data to one row per cell *before* merging.\n",
    "\n",
    "        # 1. Filter for high-confidence, productive TRA/TRB chains.\n",
    "        # Only keep TCR contigs that are both high-confidence and productive, and are either TRA or TRB chains.\n",
    "        if 'high_confidence' not in full_tcr_df.columns or 'productive' not in full_tcr_df.columns or 'chain' not in full_tcr_df.columns:\n",
    "            print(\"WARNING: TCR dataframe missing required columns (high_confidence, productive, chain). Skipping TCR integration.\")\n",
    "            tcr_to_agg = pd.DataFrame()\n",
    "        else:\n",
    "            tcr_to_agg = full_tcr_df[\n",
    "                (full_tcr_df['high_confidence'] == True) &\n",
    "                (full_tcr_df['productive'] == True) &\n",
    "                (full_tcr_df['chain'].isin(['TRA', 'TRB']))\n",
    "            ].copy()\n",
    "\n",
    "        if not tcr_to_agg.empty:\n",
    "            # 2. Pivot the data to create one row per barcode, with columns for TRA and TRB data.\n",
    "            # This step ensures each cell (barcode) has its TRA and TRB info in separate columns.\n",
    "            tcr_aggregated = tcr_to_agg.pivot_table(\n",
    "                index=['sample_id', 'barcode'],\n",
    "                columns='chain',\n",
    "                values=['v_gene', 'j_gene', 'cdr3'],\n",
    "                aggfunc='first'  # 'first' is safe as we expect at most one productive TRA/TRB per cell\n",
    "            )\n",
    "\n",
    "            # 3. Flatten the multi-level column index (e.g., from ('v_gene', 'TRA') to 'v_gene_TRA')\n",
    "            tcr_aggregated.columns = ['_'.join(col).strip() for col in tcr_aggregated.columns.values]\n",
    "            tcr_aggregated.reset_index(inplace=True)\n",
    "\n",
    "            # --- DEBUG: Print sample formats to diagnose any mismatches ---\n",
    "            print(f\"  DEBUG: adata.obs sample_id examples: {adata.obs['sample_id'].unique()[:3].tolist()}\")\n",
    "            print(f\"  DEBUG: TCR sample_id examples: {tcr_aggregated['sample_id'].unique()[:3].tolist()}\")\n",
    "            \n",
    "            # 4. Prepare adata.obs for the merge by creating a matching barcode column.\n",
    "            # The index in adata.obs is like 'AGCCATGCAGCTGTTA-1-0' (barcode-concat_suffix).\n",
    "            # The barcode in TCR data is like 'AGCCATGCAGCTGTTA-1'.\n",
    "            adata.obs['barcode_for_merge'] = adata.obs.index.str.rsplit('-', n=1).str[0]\n",
    "            \n",
    "            # Handle case where sample_id might have been modified by concat (fallback fix)\n",
    "            # Extract just the GSM ID if sample_id contains underscores (e.g., \"GSM9061665_S1\" -> \"GSM9061665\")\n",
    "            if adata.obs['sample_id'].astype(str).str.contains('_').any():\n",
    "                print(\"  INFO: sample_id contains underscores, extracting GSM ID portion for merge...\")\n",
    "                adata.obs['sample_id_for_merge'] = adata.obs['sample_id'].astype(str).str.split('_').str[0]\n",
    "            else:\n",
    "                adata.obs['sample_id_for_merge'] = adata.obs['sample_id']\n",
    "            \n",
    "            print(f\"  DEBUG: adata barcode examples: {adata.obs['barcode_for_merge'].head(3).tolist()}\")\n",
    "            print(f\"  DEBUG: TCR barcode examples: {tcr_aggregated['barcode'].head(3).tolist()}\")\n",
    "\n",
    "            # 5. Perform a left merge. This keeps all cells from adata and adds TCR info where available.\n",
    "            # The number of rows will not change because tcr_aggregated has unique barcodes per sample.\n",
    "            original_obs = adata.obs.copy()\n",
    "            merged_obs = original_obs.merge(\n",
    "                tcr_aggregated,\n",
    "                left_on=['sample_id_for_merge', 'barcode_for_merge'],\n",
    "                right_on=['sample_id', 'barcode'],\n",
    "                how='left',\n",
    "                suffixes=('', '_tcr')\n",
    "            )\n",
    "            \n",
    "            # 6. Restore the original index to the merged dataframe.\n",
    "            merged_obs.index = original_obs.index\n",
    "            adata.obs = merged_obs\n",
    "            \n",
    "            # Clean up redundant columns from merge\n",
    "            cols_to_drop = [c for c in ['sample_id_tcr', 'sample_id_for_merge'] if c in adata.obs.columns]\n",
    "            if cols_to_drop:\n",
    "                adata.obs.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "            # Check how many cells got TCR info\n",
    "            tcr_col = 'v_gene_TRA' if 'v_gene_TRA' in adata.obs.columns else None\n",
    "            if tcr_col:\n",
    "                cells_with_tcr = (~adata.obs[tcr_col].isna()).sum()\n",
    "                print(f\"Successfully merged TCR data. Cells with TCR info: {cells_with_tcr} / {adata.n_obs}\")\n",
    "                \n",
    "                if cells_with_tcr == 0:\n",
    "                    print(\"WARNING: No cells matched TCR data! Check barcode/sample_id formats.\")\n",
    "                    print(\"  Skipping TCR filtering to preserve data.\")\n",
    "                else:\n",
    "                    # --- Filter for cells that have TCR information after the merge ---\n",
    "                    # Only keep cells with non-null v_gene_TRA (i.e., cells with high-confidence TCR data)\n",
    "                    initial_cells = adata.n_obs\n",
    "                    adata = adata[~adata.obs[tcr_col].isna()].copy()\n",
    "                    print(f\"Filtered from {initial_cells} to {adata.n_obs} cells based on having high-confidence TCR data.\")\n",
    "            else:\n",
    "                print(\"WARNING: TCR merge did not produce expected columns. Skipping TCR filtering.\")\n",
    "        else:\n",
    "            print(\"WARNING: No high-confidence productive TRA/TRB chains found in TCR data. Skipping TCR filtering.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"ERROR during TCR integration: {e}\")\n",
    "        traceback.print_exc()\n",
    "        print(\"Proceeding without TCR integration...\")\n",
    "else:\n",
    "    print(\"No TCR data available or full_tcr_df is empty. Proceeding without TCR integration...\")\n",
    "\n",
    "# --- Basic QC and filtering ---\n",
    "try:\n",
    "    print(f\"\\nPerforming QC filtering (starting with {adata.n_obs} cells, {adata.n_vars} genes)...\")\n",
    "    \n",
    "    # Filter out cells with fewer than 200 genes detected\n",
    "    sc.pp.filter_cells(adata, min_genes=200)\n",
    "    print(f\"  After min_genes filter: {adata.n_obs} cells\")\n",
    "    \n",
    "    # Filter out genes detected in fewer than 3 cells\n",
    "    sc.pp.filter_genes(adata, min_cells=3)\n",
    "    print(f\"  After min_cells filter: {adata.n_vars} genes\")\n",
    "\n",
    "    # Annotate mitochondrial genes for QC metrics\n",
    "    adata.var['mt'] = adata.var_names.str.startswith('MT-')\n",
    "    # Calculate QC metrics (e.g., percent mitochondrial genes)\n",
    "    sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "\n",
    "    print(\"\\nPost-QC AnnData object:\")\n",
    "    print(adata)\n",
    "    print(\"\\nSample metadata preview:\")\n",
    "    display(adata.obs.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR during QC filtering: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ab54002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:13:30.914271Z",
     "iopub.status.busy": "2026-02-13T17:13:30.913844Z",
     "iopub.status.idle": "2026-02-13T17:13:30.920874Z",
     "shell.execute_reply": "2026-02-13T17:13:30.920219Z"
    },
    "papermill": {
     "duration": 0.126706,
     "end_time": "2026-02-13T17:13:30.922294",
     "exception": false,
     "start_time": "2026-02-13T17:13:30.795588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding with TCR integration...\n"
     ]
    }
   ],
   "source": [
    "# MEMORY TIP: Save intermediate results to avoid reprocessing\n",
    "# Uncomment the lines below to save the concatenated data before TCR integration\n",
    "\n",
    "# output_dir = Path(\"/kaggle/working/Output\") if IS_KAGGLE else Path(\"../Output\")\n",
    "# output_dir.mkdir(exist_ok=True, parents=True)\n",
    "# checkpoint_file = output_dir / \"adata_concatenated_checkpoint.h5ad\"\n",
    "# \n",
    "# print(f\"Saving checkpoint to {checkpoint_file}...\")\n",
    "# adata.write_h5ad(checkpoint_file, compression='gzip')\n",
    "# print(f\"Checkpoint saved! File size: {checkpoint_file.stat().st_size / 1024**2:.2f} MB\")\n",
    "# \n",
    "# # To load this checkpoint later, use:\n",
    "# # adata = sc.read_h5ad(checkpoint_file)\n",
    "\n",
    "print(\"Proceeding with TCR integration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a797687f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:13:30.958640Z",
     "iopub.status.busy": "2026-02-13T17:13:30.957858Z",
     "iopub.status.idle": "2026-02-13T17:13:30.969503Z",
     "shell.execute_reply": "2026-02-13T17:13:30.968614Z"
    },
    "papermill": {
     "duration": 0.031345,
     "end_time": "2026-02-13T17:13:30.970970",
     "exception": false,
     "start_time": "2026-02-13T17:13:30.939625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Statistics ===\n",
      "Total cells: 38413\n",
      "Total genes: 14816\n",
      "\n",
      "Samples: 10\n",
      "sample_id\n",
      "GSM9061670    5310\n",
      "GSM9061674    5070\n",
      "GSM9061673    5045\n",
      "GSM9061671    4838\n",
      "GSM9061665    4008\n",
      "GSM9061666    3855\n",
      "GSM9061675    3774\n",
      "GSM9061667    3127\n",
      "GSM9061668    2471\n",
      "GSM9061669     915\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Patients: 4\n",
      "patient_id\n",
      "PT4    13889\n",
      "PT1    10990\n",
      "PT3    10148\n",
      "PT2     3386\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Response distribution:\n",
      "response\n",
      "Non-Responder    24037\n",
      "Responder        14376\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Timepoint distribution:\n",
      "timepoint\n",
      "Baseline      16834\n",
      "Post-Tx       14678\n",
      "Recurrence     6901\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Show basic statistics about the dataset ---\n",
    "\n",
    "# Validate that adata exists\n",
    "if 'adata' not in globals():\n",
    "    raise NameError(\"adata is not defined. Please run the data loading and QC cells first.\")\n",
    "if adata is None:\n",
    "    raise ValueError(\"adata is None. Data loading may have failed.\")\n",
    "\n",
    "print(\"=== Dataset Statistics ===\")\n",
    "print(f\"Total cells: {adata.n_obs}\")\n",
    "print(f\"Total genes: {adata.n_vars}\")\n",
    "\n",
    "if 'sample_id' in adata.obs.columns:\n",
    "    print(f\"\\nSamples: {adata.obs['sample_id'].nunique()}\")\n",
    "    print(adata.obs['sample_id'].value_counts())\n",
    "\n",
    "if 'patient_id' in adata.obs.columns:\n",
    "    print(f\"\\nPatients: {adata.obs['patient_id'].nunique()}\")\n",
    "    print(adata.obs['patient_id'].value_counts())\n",
    "\n",
    "if 'response' in adata.obs.columns:\n",
    "    print(f\"\\nResponse distribution:\")\n",
    "    print(adata.obs['response'].value_counts())\n",
    "\n",
    "if 'timepoint' in adata.obs.columns:\n",
    "    print(f\"\\nTimepoint distribution:\")\n",
    "    print(adata.obs['timepoint'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e31fba",
   "metadata": {
    "papermill": {
     "duration": 0.016132,
     "end_time": "2026-02-13T17:13:31.004599",
     "exception": false,
     "start_time": "2026-02-13T17:13:30.988467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Install Additional Libraries for Advanced ML and Visualization\n",
    "\n",
    "Install and import libraries such as XGBoost, TensorFlow/Keras, scipy, and additional visualization tools for comprehensive ML analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ecb9a49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:13:31.038469Z",
     "iopub.status.busy": "2026-02-13T17:13:31.038237Z",
     "iopub.status.idle": "2026-02-13T17:14:26.844900Z",
     "shell.execute_reply": "2026-02-13T17:14:26.844146Z"
    },
    "papermill": {
     "duration": 55.843309,
     "end_time": "2026-02-13T17:14:26.864188",
     "exception": false,
     "start_time": "2026-02-13T17:13:31.020879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 17:13:54.560726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771002834.719238      24 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771002834.766224      24 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771002835.152402      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771002835.152435      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771002835.152437      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771002835.152440      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional libraries installed!\n",
      "CPU times: user 24.6 s, sys: 2.45 s, total: 27.1 s\n",
      "Wall time: 55.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Install required packages for genetic sequence encoding and ML ---\n",
    "%pip install biopython --quiet\n",
    "%pip install scikit-learn --quiet\n",
    "%pip install umap-learn --quiet\n",
    "%pip install hdbscan --quiet\n",
    "%pip install plotly --quiet\n",
    "%pip install xgboost --quiet\n",
    "%pip install tensorflow --quiet\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import ProtParam\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Import scipy for hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import umap\n",
    "import hdbscan\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"Additional libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "231d00af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:14:26.899604Z",
     "iopub.status.busy": "2026-02-13T17:14:26.898851Z",
     "iopub.status.idle": "2026-02-13T17:14:27.383878Z",
     "shell.execute_reply": "2026-02-13T17:14:27.382888Z"
    },
    "papermill": {
     "duration": 0.505,
     "end_time": "2026-02-13T17:14:27.385662",
     "exception": false,
     "start_time": "2026-02-13T17:14:26.880662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPUs detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Set memory growth for TensorFlow GPUs.\n",
      "Enabled mixed precision (mixed_float16).\n",
      "XGBoost version: 3.1.0\n",
      "XGBoost GPU support detected (device='cuda' API).\n",
      "Default param_grids defined early (can be overridden later).\n",
      "TF_GPU_AVAILABLE=True, MIXED_PRECISION=True, XGBOOST_GPU_AVAILABLE=True, CUML_AVAILABLE=False\n",
      "If models or param_grids are defined later, call _apply_gpu_patches() to apply GPU settings.\n"
     ]
    }
   ],
   "source": [
    "# --- GPU acceleration helper (minimal, safe) ---\n",
    "# Detect GPUs for TensorFlow, enable memory growth and mixed precision if available.\n",
    "# Detect XGBoost GPU support and cuML availability.\n",
    "# Provide a function _apply_gpu_patches() that will patch `models_eval` and `param_grids` in-place when they exist.\n",
    "\n",
    "TF_GPU_AVAILABLE = False\n",
    "MIXED_PRECISION_AVAILABLE = False\n",
    "XGBOOST_GPU_AVAILABLE = False\n",
    "CUML_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    TF_GPU_AVAILABLE = len(gpus) > 0\n",
    "    if TF_GPU_AVAILABLE:\n",
    "        print(\"TensorFlow GPUs detected:\", gpus)\n",
    "        try:\n",
    "            for g in gpus:\n",
    "                tf.config.experimental.set_memory_growth(g, True)\n",
    "            print(\"Set memory growth for TensorFlow GPUs.\")\n",
    "        except Exception as e:\n",
    "            print(\"Could not set memory growth:\", e)\n",
    "        # Try enabling mixed precision for faster FP16 compute on modern GPUs\n",
    "        try:\n",
    "            from tensorflow.keras import mixed_precision\n",
    "            mixed_precision.set_global_policy('mixed_float16')\n",
    "            MIXED_PRECISION_AVAILABLE = True\n",
    "            print(\"Enabled mixed precision (mixed_float16).\")\n",
    "        except Exception as e:\n",
    "            print(\"Mixed precision policy not enabled:\", e)\n",
    "    else:\n",
    "        print(\"No TensorFlow GPU detected.\")\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow import failed or no GPUs:\", e)\n",
    "\n",
    "# XGBoost GPU detection - supports both old (gpu_hist) and new (device='cuda') APIs\n",
    "XGBOOST_GPU_METHOD = None  # Will be 'device' for XGBoost 2.0+, 'tree_method' for older versions\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgb_version = tuple(int(x) for x in xgb.__version__.split('.')[:2])\n",
    "    print(f\"XGBoost version: {xgb.__version__}\")\n",
    "    \n",
    "    # XGBoost 2.0+ uses device='cuda', older uses tree_method='gpu_hist'\n",
    "    if xgb_version >= (2, 0):\n",
    "        try:\n",
    "            # Test new API\n",
    "            _ = xgb.XGBClassifier(device='cuda', n_estimators=1)\n",
    "            XGBOOST_GPU_AVAILABLE = True\n",
    "            XGBOOST_GPU_METHOD = 'device'\n",
    "            print(\"XGBoost GPU support detected (device='cuda' API).\")\n",
    "        except Exception as e:\n",
    "            print(f\"XGBoost 2.0+ GPU not available: {e}\")\n",
    "    else:\n",
    "        try:\n",
    "            # Test old API\n",
    "            _ = xgb.XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', n_estimators=1)\n",
    "            XGBOOST_GPU_AVAILABLE = True\n",
    "            XGBOOST_GPU_METHOD = 'tree_method'\n",
    "            print(\"XGBoost GPU support detected (tree_method='gpu_hist' API).\")\n",
    "        except Exception as e:\n",
    "            print(f\"XGBoost GPU not available: {e}\")\n",
    "except Exception as e:\n",
    "    print(\"XGBoost not importable:\", e)\n",
    "\n",
    "# cuML detection\n",
    "try:\n",
    "    import cuml\n",
    "    CUML_AVAILABLE = True\n",
    "    print(\"cuML is available.\")\n",
    "except Exception:\n",
    "    CUML_AVAILABLE = False\n",
    "\n",
    "# Utility: robust getter for adata.obsm with mask and padding\n",
    "def _get_obsm_or_zeros(adata, key, mask=None, n_cols=0):\n",
    "    \"\"\"\n",
    "    Return adata.obsm[key][mask] if present, otherwise zeros(shape=(n_rows, n_cols)).\n",
    "    Ensures output is a dense numpy array with n_cols columns (pads with zeros if needed).\n",
    "    \"\"\"\n",
    "    import numpy as _np\n",
    "    # Determine number of rows requested\n",
    "    if mask is not None:\n",
    "        try:\n",
    "            n_rows = int(mask.sum()) if hasattr(mask, 'sum') else int(sum(1 for v in mask if v))\n",
    "        except Exception:\n",
    "            n_rows = int(sum(1 for v in mask if v))\n",
    "    else:\n",
    "        n_rows = getattr(adata, 'n_obs', adata.shape[0]) if 'adata' in globals() else 0\n",
    "\n",
    "    if key in getattr(adata, 'obsm', {}):\n",
    "        arr = adata.obsm[key]\n",
    "        try:\n",
    "            if hasattr(arr, 'toarray'):\n",
    "                arr = arr.toarray()\n",
    "            arr = _np.asarray(arr)\n",
    "        except Exception:\n",
    "            return _np.zeros((n_rows, n_cols))\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            try:\n",
    "                arr = arr[mask]\n",
    "            except Exception:\n",
    "                arr = _np.array(arr)[mask]\n",
    "        # Pad or trim columns to n_cols if requested\n",
    "        if n_cols:\n",
    "            if arr.shape[1] < n_cols:\n",
    "                pad = _np.zeros((arr.shape[0], n_cols - arr.shape[1]))\n",
    "                arr = _np.hstack([arr, pad])\n",
    "            elif arr.shape[1] > n_cols:\n",
    "                arr = arr[:, :n_cols]\n",
    "        return arr\n",
    "    else:\n",
    "        return _np.zeros((n_rows, n_cols))\n",
    "\n",
    "# Define sensible default param_grids early so LOPO can see them (will be overridden later if redefined)\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [5, 10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "print(\"Default param_grids defined early (can be overridden later).\")\n",
    "\n",
    "# Patching helper (improved with signature filtering and XGBoost 2.0+ support)\n",
    "def _apply_gpu_patches():\n",
    "    import inspect\n",
    "    try:\n",
    "        # Check if models_eval exists before trying to access it\n",
    "        if 'models_eval' not in globals():\n",
    "            return  # Nothing to patch yet\n",
    "            \n",
    "        models_eval_ref = globals()['models_eval']\n",
    "        \n",
    "        # Patch XGBoost model to use GPU params when available and supported\n",
    "        if 'XGBoost' in models_eval_ref and XGBOOST_GPU_AVAILABLE:\n",
    "            try:\n",
    "                import xgboost as xgb_mod\n",
    "                m = models_eval_ref['XGBoost']\n",
    "                params = m.get_params() if hasattr(m, 'get_params') else {}\n",
    "                # Determine class to instantiate (prefer wrapper if provided)\n",
    "                XGBClass = globals().get('XGBClassifierSK', getattr(xgb_mod, 'XGBClassifier', None))\n",
    "                if XGBClass is None:\n",
    "                    raise ImportError('xgboost.XGBClassifier not found')\n",
    "                # Build filtered params list based on constructor signature\n",
    "                sig = inspect.signature(XGBClass.__init__)\n",
    "                accepts_kwargs = any(p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values())\n",
    "                allowed = set(sig.parameters.keys())\n",
    "                filtered_params = {}\n",
    "                for k, v in params.items():\n",
    "                    if accepts_kwargs or k in allowed:\n",
    "                        filtered_params[k] = v\n",
    "                \n",
    "                # Add GPU params based on XGBoost version (2.0+ uses device, older uses tree_method)\n",
    "                xgb_gpu_method = globals().get('XGBOOST_GPU_METHOD', 'tree_method')\n",
    "                if xgb_gpu_method == 'device':\n",
    "                    # XGBoost 2.0+ API\n",
    "                    if accepts_kwargs or 'device' in allowed:\n",
    "                        filtered_params['device'] = 'cuda'\n",
    "                    # Remove old-style params if present\n",
    "                    filtered_params.pop('tree_method', None)\n",
    "                    filtered_params.pop('predictor', None)\n",
    "                else:\n",
    "                    # Old XGBoost API\n",
    "                    if accepts_kwargs or 'tree_method' in allowed:\n",
    "                        filtered_params['tree_method'] = 'gpu_hist'\n",
    "                    if accepts_kwargs or 'predictor' in allowed:\n",
    "                        filtered_params['predictor'] = 'gpu_predictor'\n",
    "                \n",
    "                # Remove unsupported keys\n",
    "                filtered_params.pop('gpu_id', None)\n",
    "                try:\n",
    "                    models_eval_ref['XGBoost'] = XGBClass(**filtered_params)\n",
    "                    print(f\"Patched models_eval['XGBoost'] to use GPU (method={xgb_gpu_method}).\")\n",
    "                except TypeError as e:\n",
    "                    # Fallback: try removing GPU-specific params and re-instantiate\n",
    "                    for k in ['tree_method', 'predictor', 'device']:\n",
    "                        filtered_params.pop(k, None)\n",
    "                    fallback_params = {k: v for k, v in filtered_params.items() if accepts_kwargs or k in allowed}\n",
    "                    models_eval_ref['XGBoost'] = XGBClass(**fallback_params)\n",
    "                    print(\"Patched models_eval['XGBoost'] without GPU params due to TypeError:\", e)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to patch models_eval['XGBoost']:\", e)\n",
    "            # Patch Random Forest to use n_jobs=-1 when possible\n",
    "            if 'Random Forest' in models_eval_ref:\n",
    "                try:\n",
    "                    from sklearn.ensemble import RandomForestClassifier\n",
    "                    m = models_eval_ref['Random Forest']\n",
    "                    params = m.get_params() if hasattr(m, 'get_params') else {}\n",
    "                    params.setdefault('n_jobs', -1)\n",
    "                    RFC = RandomForestClassifier\n",
    "                    sig_rfc = inspect.signature(RFC.__init__)\n",
    "                    accepts_kwargs_rfc = any(p.kind == inspect.Parameter.VAR_KEYWORD for p in sig_rfc.parameters.values())\n",
    "                    allowed_rfc = set(sig_rfc.parameters.keys())\n",
    "                    filtered_rfc_params = {k: v for k, v in params.items() if accepts_kwargs_rfc or k in allowed_rfc}\n",
    "                    models_eval_ref['Random Forest'] = RandomForestClassifier(**filtered_rfc_params)\n",
    "                    print(\"Patched models_eval['Random Forest'] to use n_jobs=-1.\")\n",
    "                except Exception as e:\n",
    "                    print(\"Failed to patch models_eval['Random Forest']:\", e)\n",
    "    except Exception as e:\n",
    "        print(\"Error patching models_eval:\", e)\n",
    "\n",
    "    # Patch param_grids for XGBoost if available\n",
    "    try:\n",
    "        if 'param_grids' in globals() and XGBOOST_GPU_AVAILABLE:\n",
    "            pg = param_grids.get('XGBoost', {})\n",
    "            xgb_gpu_method = globals().get('XGBOOST_GPU_METHOD', 'tree_method')\n",
    "            if xgb_gpu_method == 'device':\n",
    "                # XGBoost 2.0+ uses device parameter\n",
    "                if any(k.startswith('clf__') for k in pg.keys()):\n",
    "                    pg.setdefault('clf__device', ['cuda'])\n",
    "                else:\n",
    "                    pg.setdefault('device', ['cuda'])\n",
    "            else:\n",
    "                # Old XGBoost uses tree_method\n",
    "                if any(k.startswith('clf__') for k in pg.keys()):\n",
    "                    pg.setdefault('clf__tree_method', ['gpu_hist'])\n",
    "                    pg.setdefault('clf__predictor', ['gpu_predictor'])\n",
    "                else:\n",
    "                    pg.setdefault('tree_method', ['gpu_hist'])\n",
    "                    pg.setdefault('predictor', ['gpu_predictor'])\n",
    "            param_grids['XGBoost'] = pg\n",
    "            print(f\"Patched param_grids['XGBoost'] with GPU options (method={xgb_gpu_method}).\")\n",
    "    except Exception as e:\n",
    "        print(\"Error patching param_grids:\", e)\n",
    "\n",
    "# Apply patches now if models/param grids already defined\n",
    "_apply_gpu_patches()\n",
    "\n",
    "print(f\"TF_GPU_AVAILABLE={TF_GPU_AVAILABLE}, MIXED_PRECISION={MIXED_PRECISION_AVAILABLE}, XGBOOST_GPU_AVAILABLE={XGBOOST_GPU_AVAILABLE}, CUML_AVAILABLE={CUML_AVAILABLE}\")\n",
    "print(\"If models or param_grids are defined later, call _apply_gpu_patches() to apply GPU settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e86e024d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:14:27.422038Z",
     "iopub.status.busy": "2026-02-13T17:14:27.421188Z",
     "iopub.status.idle": "2026-02-13T17:14:27.429242Z",
     "shell.execute_reply": "2026-02-13T17:14:27.428376Z"
    },
    "papermill": {
     "duration": 0.02724,
     "end_time": "2026-02-13T17:14:27.430601",
     "exception": false,
     "start_time": "2026-02-13T17:14:27.403361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined XGBoost sklearn-compatible wrapper: XGBClassifierSK (XGBoost version 3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# --- Define a sklearn-compatible XGBoost wrapper (supports both old and new XGBoost APIs) ---\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgb_version = tuple(int(x) for x in xgb.__version__.split('.')[:2])\n",
    "    \n",
    "    class XGBClassifierSK(xgb.XGBClassifier):\n",
    "        \"\"\"XGBoost wrapper that handles both old (tree_method) and new (device) APIs.\"\"\"\n",
    "        def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=6, random_state=None,\n",
    "                     use_label_encoder=False, eval_metric='logloss',\n",
    "                     tree_method=None, predictor=None, device=None, **kwargs):\n",
    "            # Store parameters before passing to super().__init__\n",
    "            # This ensures they are available for sklearn's get_params()\n",
    "            self.tree_method = tree_method\n",
    "            self.predictor = predictor\n",
    "            self.device = device\n",
    "            \n",
    "            # Handle XGBoost 2.0+ API vs older versions\n",
    "            if xgb_version >= (2, 0):\n",
    "                # New API: use 'device' parameter\n",
    "                if device is not None:\n",
    "                    kwargs['device'] = device\n",
    "                # tree_method and predictor are deprecated in 2.0+\n",
    "            else:\n",
    "                # Old API: use tree_method/predictor\n",
    "                if tree_method is not None:\n",
    "                    kwargs.setdefault('tree_method', tree_method)\n",
    "                if predictor is not None:\n",
    "                    kwargs.setdefault('predictor', predictor)\n",
    "            \n",
    "            # Remove deprecated parameters that might cause warnings\n",
    "            self.use_label_encoder = use_label_encoder\n",
    "            kwargs.pop('use_label_encoder', None)\n",
    "            \n",
    "            super().__init__(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth,\n",
    "                             random_state=random_state, eval_metric=eval_metric, **kwargs)\n",
    "    \n",
    "    globals()['XGBClassifierSK'] = XGBClassifierSK\n",
    "    print(f'Defined XGBoost sklearn-compatible wrapper: XGBClassifierSK (XGBoost version {xgb.__version__})')\n",
    "except Exception as e:\n",
    "    print('Failed to define XGBClassifierSK:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bde3cdce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:14:27.466087Z",
     "iopub.status.busy": "2026-02-13T17:14:27.465433Z",
     "iopub.status.idle": "2026-02-13T17:14:27.471699Z",
     "shell.execute_reply": "2026-02-13T17:14:27.470967Z"
    },
    "papermill": {
     "duration": 0.025651,
     "end_time": "2026-02-13T17:14:27.473166",
     "exception": false,
     "start_time": "2026-02-13T17:14:27.447515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using raw_data_dir = /kaggle/working/Data/GSE300475_RAW\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Determine data directory consistently (prefer existing download_dir when present)\n",
    "if 'download_dir' in globals() and download_dir:\n",
    "    data_dir = Path(download_dir)\n",
    "elif IS_KAGGLE:\n",
    "    data_dir = Path('/kaggle/working/Data')\n",
    "else:\n",
    "    data_dir = Path('../Data')\n",
    "\n",
    "raw_data_dir = data_dir / 'GSE300475_RAW'\n",
    "raw_data_dir = raw_data_dir.resolve()\n",
    "\n",
    "# Ensure directory exists (no-op if not writing yet)\n",
    "os.makedirs(raw_data_dir, exist_ok=True)\n",
    "print(f\"Using raw_data_dir = {raw_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca8bcb0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:14:27.508354Z",
     "iopub.status.busy": "2026-02-13T17:14:27.507830Z",
     "iopub.status.idle": "2026-02-13T17:14:27.513505Z",
     "shell.execute_reply": "2026-02-13T17:14:27.512761Z"
    },
    "papermill": {
     "duration": 0.024802,
     "end_time": "2026-02-13T17:14:27.514845",
     "exception": false,
     "start_time": "2026-02-13T17:14:27.490043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched sklearn.model_selection.LeaveOneGroupOut to auto-apply GPU patches on init\n"
     ]
    }
   ],
   "source": [
    "# --- Auto-apply GPU patches when LOPO is instantiated ---\n",
    "try:\n",
    "    import sklearn.model_selection as _skms\n",
    "    if not getattr(_skms, '_LO_patched_applied', False):\n",
    "        _LO_orig = _skms.LeaveOneGroupOut\n",
    "        class _LO_patched(_LO_orig):\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                # Ensure GPU patches are applied just before LOPO is constructed\n",
    "                try:\n",
    "                    _apply_gpu_patches()\n",
    "                except Exception as _e:\n",
    "                    print('Warning: _apply_gpu_patches failed during LOPO patching:', _e)\n",
    "                super().__init__(*args, **kwargs)\n",
    "        _skms.LeaveOneGroupOut = _LO_patched\n",
    "        _skms._LO_patched_applied = True\n",
    "        print('Patched sklearn.model_selection.LeaveOneGroupOut to auto-apply GPU patches on init')\n",
    "    else:\n",
    "        print('LOPO patch already applied')\n",
    "except Exception as e:\n",
    "    print('Failed to apply LOPO patch:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "835644cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:14:27.550526Z",
     "iopub.status.busy": "2026-02-13T17:14:27.550326Z",
     "iopub.status.idle": "2026-02-13T17:15:06.806398Z",
     "shell.execute_reply": "2026-02-13T17:15:06.805214Z"
    },
    "papermill": {
     "duration": 39.276016,
     "end_time": "2026-02-13T17:15:06.808038",
     "exception": false,
     "start_time": "2026-02-13T17:14:27.532022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Loading...\n",
      "Searching for data in: /kaggle/working/Data/GSE300475_RAW\n",
      "Processing GSM9061668_S4_matrix.mtx.gz...\n",
      "Loaded 8723 cells from GSM9061668_S4_\n",
      "Processing GSM9061675_S11_matrix.mtx.gz...\n",
      "Loaded 9330 cells from GSM9061675_S11_\n",
      "Processing GSM9061669_S5_matrix.mtx.gz...\n",
      "Loaded 2912 cells from GSM9061669_S5_\n",
      "Processing GSM9061674_S10_matrix.mtx.gz...\n",
      "Loaded 9704 cells from GSM9061674_S10_\n",
      "Processing GSM9061670_S6_matrix.mtx.gz...\n",
      "Loaded 10398 cells from GSM9061670_S6_\n",
      "Processing GSM9061672_S8_matrix.mtx.gz...\n",
      "Loaded 12832 cells from GSM9061672_S8_\n",
      "Processing GSM9061665_S1_matrix.mtx.gz...\n",
      "Loaded 8931 cells from GSM9061665_S1_\n",
      "Processing GSM9061671_S7_matrix.mtx.gz...\n",
      "Loaded 9330 cells from GSM9061671_S7_\n",
      "Processing GSM9061666_S2_matrix.mtx.gz...\n",
      "Loaded 9069 cells from GSM9061666_S2_\n",
      "Processing GSM9061673_S9_matrix.mtx.gz...\n",
      "Loaded 11480 cells from GSM9061673_S9_\n",
      "Processing GSM9061667_S3_matrix.mtx.gz...\n",
      "Loaded 7358 cells from GSM9061667_S3_\n",
      "Combined AnnData object created: (100067, 36607)\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loading (Robust) ---\n",
    "import scanpy as sc\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def _first_existing(paths):\n",
    "    for p in paths:\n",
    "        if p and os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def _glob_pick(folder, patterns, key=None):\n",
    "    matches = []\n",
    "    for pat in patterns:\n",
    "        matches.extend(glob.glob(os.path.join(folder, pat)))\n",
    "    matches = sorted(set(matches))\n",
    "    if key:\n",
    "        key_matches = [m for m in matches if key in os.path.basename(m)]\n",
    "        if len(key_matches) == 1:\n",
    "            return key_matches[0]\n",
    "        if len(key_matches) > 1:\n",
    "            return key_matches[0]\n",
    "    if len(matches) == 1:\n",
    "        return matches[0]\n",
    "    return None\n",
    "\n",
    "print(\"Starting Data Loading...\")\n",
    "\n",
    "# Determine data directory (using extract_dir from Cell 7 if available)\n",
    "if 'extract_dir' not in globals():\n",
    "    # Fallback path logic matching Cell 7/8\n",
    "    base_dir = '/kaggle/working/Data' if IS_KAGGLE else '../Data'\n",
    "    extract_dir = os.path.join(base_dir, \"GSE300475_RAW\")\n",
    "\n",
    "if not os.path.exists(extract_dir):\n",
    "    print(f\"Warning: Directory {extract_dir} does not exist. Please ensure Cell 7 ran successfully.\")\n",
    "else:\n",
    "    print(f\"Searching for data in: {extract_dir}\")\n",
    "    # Find all matrix files\n",
    "    matrix_files = glob.glob(os.path.join(extract_dir, \"*matrix.mtx*\"))\n",
    "    # Also look recursively if structure is nested\n",
    "    if not matrix_files:\n",
    "        matrix_files = glob.glob(os.path.join(extract_dir, \"**\", \"*matrix.mtx*\"), recursive=True)\n",
    "\n",
    "    adata_list = []\n",
    "    \n",
    "    if not matrix_files:\n",
    "        print(\"No matrix.mtx files found or previously loaded.\")\n",
    "        # Check if we can proceed? If this is a re-run, adata might exist.\n",
    "    else:\n",
    "        for mat_file in matrix_files:\n",
    "            try:\n",
    "                print(f\"Processing {os.path.basename(mat_file)}...\")\n",
    "                # Handle formatted loading\n",
    "                # If file is standard 10x-like (matrix.mtx, genes.tsv, barcodes.tsv) in same folder\n",
    "                folder = os.path.dirname(mat_file)\n",
    "                prefix = os.path.basename(mat_file).replace('matrix.mtx', '').replace('.gz', '')\n",
    "                key = prefix.strip('_')\n",
    "                \n",
    "                # Check for accompanying files with same prefix\n",
    "                genes_path = _first_existing([\n",
    "                    os.path.join(folder, prefix + 'genes.tsv'),\n",
    "                    os.path.join(folder, prefix + 'features.tsv'),\n",
    "                    os.path.join(folder, prefix + 'genes.tsv.gz'),\n",
    "                    os.path.join(folder, prefix + 'features.tsv.gz'),\n",
    "                ])\n",
    "                \n",
    "                barcodes_path = _first_existing([\n",
    "                    os.path.join(folder, prefix + 'barcodes.tsv'),\n",
    "                    os.path.join(folder, prefix + 'barcodes.tsv.gz'),\n",
    "                ])\n",
    "\n",
    "                # Fallback to un-prefixed standard 10x naming\n",
    "                if not genes_path:\n",
    "                    genes_path = _first_existing([\n",
    "                        os.path.join(folder, 'genes.tsv'),\n",
    "                        os.path.join(folder, 'features.tsv'),\n",
    "                        os.path.join(folder, 'genes.tsv.gz'),\n",
    "                        os.path.join(folder, 'features.tsv.gz'),\n",
    "                    ])\n",
    "\n",
    "                if not barcodes_path:\n",
    "                    barcodes_path = _first_existing([\n",
    "                        os.path.join(folder, 'barcodes.tsv'),\n",
    "                        os.path.join(folder, 'barcodes.tsv.gz'),\n",
    "                    ])\n",
    "\n",
    "                # Fallback to any matching files in the folder (use key if present)\n",
    "                if not genes_path:\n",
    "                    genes_path = _glob_pick(folder, ['*genes.tsv*', '*features.tsv*'], key=key)\n",
    "                if not barcodes_path:\n",
    "                    barcodes_path = _glob_pick(folder, ['*barcodes.tsv*'], key=key)\n",
    "\n",
    "                if genes_path and barcodes_path and os.path.exists(genes_path) and os.path.exists(barcodes_path):\n",
    "                    # Load using read_mtx for flexibility with filenames\n",
    "                    adata_sample = sc.read_mtx(mat_file).T\n",
    "                    \n",
    "                    # Annotation\n",
    "                    genes = pd.read_csv(genes_path, sep='\\t', header=None)\n",
    "                    barcodes = pd.read_csv(barcodes_path, sep='\\t', header=None)\n",
    "                    \n",
    "                    # Assign var/obs names and sanitize whitespace\n",
    "                    if genes.shape[1] > 1:\n",
    "                        var_names = genes.iloc[:,1].astype(str).str.strip().values\n",
    "                        adata_sample.var['gene_ids'] = genes.iloc[:,0].astype(str).values\n",
    "                    else:\n",
    "                        var_names = genes.iloc[:,0].astype(str).str.strip().values\n",
    "                    adata_sample.var_names = pd.Index(var_names)\n",
    "                    adata_sample.obs_names = pd.Index(barcodes.iloc[:,0].astype(str).str.strip().values)\n",
    "                    adata_sample.obs['sample_id'] = prefix.strip('_') if prefix else os.path.basename(folder)\n",
    "                    \n",
    "                    # Ensure uniqueness within sample to avoid concat Index errors\n",
    "                    try:\n",
    "                        adata_sample.var_names_make_unique()\n",
    "                        adata_sample.obs_names_make_unique()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    \n",
    "                    adata_list.append(adata_sample)\n",
    "                    print(f\"Loaded {adata_sample.shape[0]} cells from {prefix or folder}\")\n",
    "                else:\n",
    "                    print(f\"Skipping {mat_file}: Missing genes/barcodes files (searched prefix '{prefix}' and fallbacks)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {mat_file}: {e}\")\n",
    "\n",
    "        # Pre-sanitize all adata samples before concatenation\n",
    "        for a in adata_list:\n",
    "            try:\n",
    "                a.var_names = pd.Index([str(v).strip() for v in a.var_names])\n",
    "                a.var_names_make_unique()\n",
    "                a.obs_names = pd.Index([str(v).strip() for v in a.obs_names])\n",
    "                a.obs_names_make_unique()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if adata_list:\n",
    "            # Concatenate all samples\n",
    "            try:\n",
    "                adata = sc.concat(adata_list, join='outer')\n",
    "            except Exception as e:\n",
    "                print('sc.concat failed:', e)\n",
    "                # Try fallback using AnnData.concatenate with batch info\n",
    "                try:\n",
    "                    loaded_batches = [a.obs['sample_id'].unique()[0] for a in adata_list]\n",
    "                except Exception:\n",
    "                    loaded_batches = None\n",
    "                try:\n",
    "                    if loaded_batches:\n",
    "                        adata = sc.AnnData.concatenate(*adata_list, join='outer', batch_key='sample_id', batch_categories=loaded_batches)\n",
    "                    else:\n",
    "                        adata = sc.AnnData.concatenate(*adata_list, join='outer', batch_key='sample_id')\n",
    "                except Exception as e2:\n",
    "                    raise RuntimeError(f\"Failed to concatenate AnnData objects: {e}; fallback failed: {e2}\")\n",
    "            adata.obs_names_make_unique()\n",
    "            # Basic fallback for mitochondrial genes logic (used later)\n",
    "            adata.var['mt'] = adata.var_names.str.startswith('MT-')\n",
    "            sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "            print(f\"Combined AnnData object created: {adata.shape}\")\n",
    "        else:\n",
    "            print(\"Warning: No valid data loaded into adata.\")\n",
    "\n",
    "# Ensure adata exists to prevent downstream crashes\n",
    "if 'adata' not in globals():\n",
    "    print(\"CRITICAL CHECK: adata variable not defined. Downstream cells will fail.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdddad1e",
   "metadata": {
    "papermill": {
     "duration": 0.017751,
     "end_time": "2026-02-13T17:15:06.844462",
     "exception": false,
     "start_time": "2026-02-13T17:15:06.826711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Genetic Sequence Encoding Functions\n",
    "\n",
    "Define functions for one-hot encoding, k-mer encoding, and physicochemical features extraction for TCR sequences and gene expression patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3713ca4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:15:06.881581Z",
     "iopub.status.busy": "2026-02-13T17:15:06.880816Z",
     "iopub.status.idle": "2026-02-13T17:15:06.893573Z",
     "shell.execute_reply": "2026-02-13T17:15:06.892769Z"
    },
    "papermill": {
     "duration": 0.032836,
     "end_time": "2026-02-13T17:15:06.894886",
     "exception": false,
     "start_time": "2026-02-13T17:15:06.862050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic sequence encoding functions defined successfully!\n",
      "CPU times: user 77 µs, sys: 0 ns, total: 77 µs\n",
      "Wall time: 82.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Genetic Sequence Encoding Functions ---\n",
    "\n",
    "def one_hot_encode_sequence(sequence, max_length=50, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    One-hot encode a protein/nucleotide sequence.\n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        max_length: Maximum sequence length (pad or truncate)\n",
    "        alphabet: Valid characters in the sequence\n",
    "    Returns:\n",
    "        2D numpy array of shape (max_length, len(alphabet))\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    sequence = str(sequence).upper()[:max_length]  # Truncate if too long\n",
    "    encoding = np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    for i, char in enumerate(sequence):\n",
    "        if char in alphabet:\n",
    "            char_idx = alphabet.index(char)\n",
    "            encoding[i, char_idx] = 1\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "def kmer_encode_sequence(sequence, k=3, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    K-mer encoding of sequences.\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {}\n",
    "    \n",
    "    sequence = str(sequence).upper()\n",
    "    kmers = [sequence[i:i+k] for i in range(len(sequence)-k+1)]\n",
    "    valid_kmers = [kmer for kmer in kmers if all(c in alphabet for c in kmer)]\n",
    "    \n",
    "    return Counter(valid_kmers)\n",
    "\n",
    "def physicochemical_features(sequence):\n",
    "    \"\"\"\n",
    "    Extract physicochemical properties from protein sequences.\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {\n",
    "            'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        seq = str(sequence).upper()\n",
    "        # Remove non-standard amino acids\n",
    "        seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "        \n",
    "        if len(seq) == 0:\n",
    "            return {\n",
    "                'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "                'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "            }\n",
    "        \n",
    "        bio_seq = Seq(seq)\n",
    "        analyzer = ProtParam.ProteinAnalysis(str(bio_seq))\n",
    "        \n",
    "        return {\n",
    "            'length': len(seq),\n",
    "            'molecular_weight': analyzer.molecular_weight(),\n",
    "            'aromaticity': analyzer.aromaticity(),\n",
    "            'instability_index': analyzer.instability_index(),\n",
    "            'isoelectric_point': analyzer.isoelectric_point(),\n",
    "            'hydrophobicity': analyzer.gravy()\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'length': len(str(sequence)) if not pd.isna(sequence) else 0,\n",
    "            'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "\n",
    "def encode_gene_expression_patterns(adata, n_top_genes=1000, train_mask=None):\n",
    "    \"\"\"\n",
    "    Encode gene expression patterns using various dimensionality reduction techniques.\n",
    "    \n",
    "    IMPORTANT: To avoid data leakage, pass train_mask to fit transformers only on training data.\n",
    "    If train_mask is None, fits on all data (use only for exploration, not CV).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (encodings dict, X_scaled array)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA, TruncatedSVD\n",
    "    import umap\n",
    "    \n",
    "    # Get highly variable genes if not already computed\n",
    "    if 'highly_variable' not in adata.var.columns:\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, subset=False)\n",
    "    \n",
    "    # Extract expression matrix for highly variable genes - ensure boolean mask\n",
    "    hvg_mask = np.array(adata.var['highly_variable'].values, dtype=bool)\n",
    "    \n",
    "    # Subset adata by HVG mask\n",
    "    X_full = adata.X\n",
    "    if hasattr(X_full, 'toarray'):\n",
    "        X_full = X_full.toarray()\n",
    "    else:\n",
    "        X_full = np.asarray(X_full)\n",
    "    \n",
    "    # Select HVG columns\n",
    "    X_hvg = X_full[:, hvg_mask]\n",
    "    \n",
    "    # Clean Infs/NaNs (robustness fix)\n",
    "    X_hvg = np.nan_to_num(X_hvg, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # Standardize the data - FIT ONLY ON TRAINING DATA if mask provided\n",
    "    scaler = StandardScaler()\n",
    "    if train_mask is not None:\n",
    "        scaler.fit(X_hvg[train_mask])\n",
    "        X_scaled = scaler.transform(X_hvg)\n",
    "    else:\n",
    "        X_scaled = scaler.fit_transform(X_hvg)\n",
    "    \n",
    "    encodings = {}\n",
    "    \n",
    "    # PCA encoding - FIT ONLY ON TRAINING DATA if mask provided\n",
    "    n_pca = min(50, X_scaled.shape[1], X_scaled.shape[0])\n",
    "    pca = PCA(n_components=n_pca)\n",
    "    if train_mask is not None:\n",
    "        pca.fit(X_scaled[train_mask])\n",
    "        encodings['pca'] = pca.transform(X_scaled)\n",
    "    else:\n",
    "        encodings['pca'] = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # TruncatedSVD for sparse matrices\n",
    "    n_svd = min(50, X_scaled.shape[1] - 1, X_scaled.shape[0] - 1)\n",
    "    if n_svd > 0:\n",
    "        svd = TruncatedSVD(n_components=n_svd, random_state=42)\n",
    "        if train_mask is not None:\n",
    "            svd.fit(X_scaled[train_mask])\n",
    "            encodings['svd'] = svd.transform(X_scaled)\n",
    "        else:\n",
    "            encodings['svd'] = svd.fit_transform(X_scaled)\n",
    "    else:\n",
    "        encodings['svd'] = np.zeros((X_scaled.shape[0], 1))\n",
    "    \n",
    "    # UMAP encoding - UMAP doesn't support clean fit/transform easily for this pipeline, usually unsupervised\n",
    "    try:\n",
    "        umap_encoder = umap.UMAP(n_components=20, random_state=42)\n",
    "        encodings['umap'] = umap_encoder.fit_transform(X_scaled)\n",
    "    except Exception as e:\n",
    "        print(f\"UMAP failed: {e}\")\n",
    "        encodings['umap'] = np.zeros((X_scaled.shape[0], 20))\n",
    "    \n",
    "    return encodings, X_scaled\n",
    "\n",
    "print(\"Genetic sequence encoding functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d11b4",
   "metadata": {
    "papermill": {
     "duration": 0.018007,
     "end_time": "2026-02-13T17:15:06.930776",
     "exception": false,
     "start_time": "2026-02-13T17:15:06.912769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Apply Sequence Encoding to TCR CDR3 Sequences\n",
    "\n",
    "Encode TRA and TRB CDR3 sequences using one-hot, k-mer, and physicochemical methods, and add to AnnData.obsm and obs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d3f6ce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:15:06.968139Z",
     "iopub.status.busy": "2026-02-13T17:15:06.967813Z",
     "iopub.status.idle": "2026-02-13T17:15:09.238282Z",
     "shell.execute_reply": "2026-02-13T17:15:09.237318Z"
    },
    "papermill": {
     "duration": 2.291234,
     "end_time": "2026-02-13T17:15:09.239876",
     "exception": false,
     "start_time": "2026-02-13T17:15:06.948642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-optimized TCR sequence encoding...\n",
      "Warning: cdr3_TRA column not found, creating empty column\n",
      "Warning: cdr3_TRB column not found, creating empty column\n",
      "TRA sequences: 100067, TRB sequences: 100067\n",
      "TRA k-mer sparse shape: (100067, 1)\n",
      "TRB k-mer sparse shape: (100067, 1)\n",
      "TRA k-mer reduced shape: (100067, 1)\n",
      "TRB k-mer reduced shape: (100067, 1)\n",
      "TRA one-hot shape: (100067, 15, 20) (dtype: float16)\n",
      "TRB one-hot shape: (100067, 15, 20) (dtype: float16)\n",
      "TCR sequence encoding complete and stored in adata.obsm\n",
      "Memory usage reduced by using sparse matrices and dimension reduction\n",
      "CPU times: user 2.22 s, sys: 48.1 ms, total: 2.26 s\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- MEMORY-OPTIMIZED TCR Sequence Encoding ---\n",
    "\n",
    "# Validate that adata exists\n",
    "if 'adata' not in globals():\n",
    "    raise NameError(\"adata is not defined. Please run the data loading and QC cells first.\")\n",
    "if adata is None:\n",
    "    raise ValueError(\"adata is None. Data loading may have failed.\")\n",
    "\n",
    "print(\"Starting memory-optimized TCR sequence encoding...\")\n",
    "import gc\n",
    "\n",
    "# Extract TCR CDR3 sequences with robust column handling\n",
    "# Check for column existence and normalize naming\n",
    "if 'cdr3_TRA' not in adata.obs.columns:\n",
    "    if 'CDR3_TRA' in adata.obs.columns:\n",
    "        adata.obs['cdr3_TRA'] = adata.obs['CDR3_TRA']\n",
    "    else:\n",
    "        print(\"Warning: cdr3_TRA column not found, creating empty column\")\n",
    "        adata.obs['cdr3_TRA'] = ''\n",
    "\n",
    "if 'cdr3_TRB' not in adata.obs.columns:\n",
    "    if 'CDR3_TRB' in adata.obs.columns:\n",
    "        adata.obs['cdr3_TRB'] = adata.obs['CDR3_TRB']\n",
    "    else:\n",
    "        print(\"Warning: cdr3_TRB column not found, creating empty column\")\n",
    "        adata.obs['cdr3_TRB'] = ''\n",
    "\n",
    "tra_seqs = adata.obs['cdr3_TRA'].fillna('').astype(str).values\n",
    "trb_seqs = adata.obs['cdr3_TRB'].fillna('').astype(str).values\n",
    "\n",
    "print(f\"TRA sequences: {len(tra_seqs)}, TRB sequences: {len(trb_seqs)}\")\n",
    "\n",
    "# MEMORY FIX: Use smaller k-mer sizes and reduced dimensionality\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Reduce k-mer size from 3 to 2 to reduce feature space\n",
    "def _kmer_list(seq, k=2):  # Changed from k=3 to k=2\n",
    "    if len(seq) < k:\n",
    "        return []\n",
    "    return [seq[i:i+k] for i in range(len(seq) - k + 1)]\n",
    "\n",
    "# Convert sequences to k-mer strings\n",
    "tra_kmer_docs = [' '.join(_kmer_list(s, k=2)) for s in tra_seqs]  # k=2 instead of 3\n",
    "trb_kmer_docs = [' '.join(_kmer_list(s, k=2)) for s in trb_seqs]\n",
    "\n",
    "# MEMORY FIX: Limit max features to reduce dimensionality\n",
    "vec_tra = CountVectorizer(max_features=500)  # Limit to 500 features instead of unlimited\n",
    "vec_trb = CountVectorizer(max_features=500)\n",
    "\n",
    "tra_kmer_sparse = vec_tra.fit_transform(tra_kmer_docs)\n",
    "trb_kmer_sparse = vec_trb.fit_transform(trb_kmer_docs)\n",
    "\n",
    "# Clean up k-mer docs (no longer needed)\n",
    "del tra_kmer_docs, trb_kmer_docs\n",
    "gc.collect()\n",
    "\n",
    "print(f\"TRA k-mer sparse shape: {tra_kmer_sparse.shape}\")\n",
    "print(f\"TRB k-mer sparse shape: {trb_kmer_sparse.shape}\")\n",
    "\n",
    "# MEMORY FIX: Reduce dimensions even further using SVD\n",
    "def _reduce_sparse(sparse_mat, n_components=50):  # Reduced from 200 to 50\n",
    "    n_comp = min(n_components, max(1, sparse_mat.shape[1]-1))\n",
    "    try:\n",
    "        svd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "        return svd.fit_transform(sparse_mat).astype(np.float32)  # Use float32\n",
    "    except Exception:\n",
    "        return sparse_mat.toarray().astype(np.float32) if hasattr(sparse_mat, 'toarray') else np.asarray(sparse_mat, dtype=np.float32)\n",
    "\n",
    "tra_kmer_matrix = _reduce_sparse(tra_kmer_sparse, n_components=50)  # Reduced from 200\n",
    "trb_kmer_matrix = _reduce_sparse(trb_kmer_sparse, n_components=50)\n",
    "print(f\"TRA k-mer reduced shape: {tra_kmer_matrix.shape}\")\n",
    "print(f\"TRB k-mer reduced shape: {trb_kmer_matrix.shape}\")\n",
    "\n",
    "# Clean up sparse matrices\n",
    "del tra_kmer_sparse, trb_kmer_sparse\n",
    "gc.collect()\n",
    "\n",
    "# MEMORY FIX: Reduced one-hot encoding with smaller max length\n",
    "max_cdr3_length = 15  # Reduced from 20 to 15\n",
    "alphabet = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "char_to_idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "def _one_hot_encode_batch(sequences, max_len=max_cdr3_length):\n",
    "    \"\"\"Batch one-hot encoding using NumPy for memory efficiency.\"\"\"\n",
    "    n_seqs = len(sequences)\n",
    "    encoding = np.zeros((n_seqs, max_len, len(alphabet)), dtype=np.float16)  # Use float16 instead of float32\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_str = str(seq).upper()[:max_len]  # Truncate\n",
    "        for j, char in enumerate(seq_str):\n",
    "            if char in char_to_idx:\n",
    "                encoding[i, j, char_to_idx[char]] = 1.0\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "tra_one_hot = _one_hot_encode_batch(tra_seqs, max_cdr3_length)\n",
    "trb_one_hot = _one_hot_encode_batch(trb_seqs, max_cdr3_length)\n",
    "print(f\"TRA one-hot shape: {tra_one_hot.shape} (dtype: {tra_one_hot.dtype})\")\n",
    "print(f\"TRB one-hot shape: {trb_one_hot.shape} (dtype: {trb_one_hot.dtype})\")\n",
    "\n",
    "# Clean up sequence arrays\n",
    "del tra_seqs, trb_seqs\n",
    "gc.collect()\n",
    "\n",
    "# MEMORY FIX: Store matrices in obsm (compressed format in AnnData)\n",
    "adata.obsm['X_tcr_tra_kmer'] = tra_kmer_matrix\n",
    "adata.obsm['X_tcr_trb_kmer'] = trb_kmer_matrix\n",
    "\n",
    "# MEMORY FIX: Flatten and store one-hot as float32 for compatibility\n",
    "adata.obsm['X_tcr_tra_onehot'] = tra_one_hot.reshape(tra_one_hot.shape[0], -1).astype(np.float32)\n",
    "adata.obsm['X_tcr_trb_onehot'] = trb_one_hot.reshape(trb_one_hot.shape[0], -1).astype(np.float32)\n",
    "\n",
    "del tra_one_hot, trb_one_hot, tra_kmer_matrix, trb_kmer_matrix\n",
    "gc.collect()\n",
    "\n",
    "print(\"TCR sequence encoding complete and stored in adata.obsm\")\n",
    "print(f\"Memory usage reduced by using sparse matrices and dimension reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8446cb0",
   "metadata": {
    "papermill": {
     "duration": 0.017575,
     "end_time": "2026-02-13T17:15:09.276020",
     "exception": false,
     "start_time": "2026-02-13T17:15:09.258445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Advanced TCR Feature Engineering\n",
    "\n",
    "We refine our TCR feature set by applying vectorized k-mer encoding and reduced one-hot encoding. This step creates the dense feature matrices stored in `adata.obsm` that will be used for downstream machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62543e10",
   "metadata": {
    "papermill": {
     "duration": 0.017423,
     "end_time": "2026-02-13T17:15:09.310933",
     "exception": false,
     "start_time": "2026-02-13T17:15:09.293510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering and Encoding\n",
    "A core contribution of this work is the engineering of a comprehensive feature set that translates biological sequences into machine-readable vectors. We developed three distinct encoding schemes for the TCR CDR3 amino acid sequences:\n",
    "\n",
    "1.  **One-Hot Encoding:** This method creates a sparse binary matrix representing the presence or absence of specific amino acids at each position in the sequence. It preserves exact positional information, which is crucial for structural motifs, but results in high-dimensional, sparse vectors.\n",
    "2.  **K-mer Frequency Encoding:** We decomposed sequences into overlapping substrings of length $k$ (k-mers, with $k=3$). We then calculated the frequency of each unique 3-mer in the sequence. This approach captures short, local structural motifs (e.g., \"CAS\", \"ASS\") that may be shared across different TCRs with similar antigen specificity, regardless of their exact position.\n",
    "3.  **Physicochemical Property Encoding:** To capture the biophysical nature of the TCR-antigen interaction, we mapped each amino acid to a vector of physicochemical properties, including hydrophobicity, molecular weight, isoelectric point, and polarity. We then aggregated these values (e.g., mean, sum) across the CDR3 sequence. This results in a dense, low-dimensional representation that reflects the \"binding potential\" of the receptor.\n",
    "\n",
    "These TCR features were concatenated with the top 50 Principal Components (PCs) derived from the gene expression data to form the \"Comprehensive\" feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d20ab29c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:15:09.347418Z",
     "iopub.status.busy": "2026-02-13T17:15:09.347118Z",
     "iopub.status.idle": "2026-02-13T17:15:10.645604Z",
     "shell.execute_reply": "2026-02-13T17:15:10.644854Z"
    },
    "papermill": {
     "duration": 1.318803,
     "end_time": "2026-02-13T17:15:10.647051",
     "exception": false,
     "start_time": "2026-02-13T17:15:09.328248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding TCR CDR3 sequences (vectorized k-mer + reduced one-hot)...\n",
      "TRA k-mer sparse shape: (100067, 1)\n",
      "TRB k-mer sparse shape: (100067, 1)\n",
      "TRA k-mer reduced shape: (100067, 1)\n",
      "TRB k-mer reduced shape: (100067, 1)\n",
      "TRA one-hot flat shape: (100067, 400)\n",
      "TRB one-hot flat shape: (100067, 400)\n",
      "TRA physicochemical features shape: (100067, 6)\n",
      "TRB physicochemical features shape: (100067, 6)\n",
      "TCR sequence encoding completed and added to AnnData object!\n",
      "CPU times: user 1.26 s, sys: 29.9 ms, total: 1.29 s\n",
      "Wall time: 1.29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# --- Apply Sequence Encoding to TCR CDR3 Sequences (vectorized k-mer + reduced one-hot) ---\n",
    "\n",
    "print(\"Encoding TCR CDR3 sequences (vectorized k-mer + reduced one-hot)...\")\n",
    "\n",
    "# Extract and clean CDR3 sequences\n",
    "if 'cdr3_TRA' in adata.obs.columns:\n",
    "    cdr3_TRA = adata.obs['cdr3_TRA'].astype(str).fillna('').str.upper()\n",
    "else:\n",
    "    cdr3_TRA = pd.Series([''] * adata.n_obs, index=adata.obs.index)\n",
    "if 'cdr3_TRB' in adata.obs.columns:\n",
    "    cdr3_TRB = adata.obs['cdr3_TRB'].astype(str).fillna('').str.upper()\n",
    "else:\n",
    "    cdr3_TRB = pd.Series([''] * adata.n_obs, index=adata.obs.index)\n",
    "\n",
    "valid_aa = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "def _clean_seq(s):\n",
    "    return ''.join([c for c in str(s) if c in valid_aa])\n",
    "\n",
    "tra_seqs = [_clean_seq(s) for s in cdr3_TRA]\n",
    "trb_seqs = [_clean_seq(s) for s in cdr3_TRB]\n",
    "\n",
    "# --- Vectorized k-mer encoding using CountVectorizer (sparse) ---\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "k = 3\n",
    "vec_tra = CountVectorizer(analyzer='char', ngram_range=(k,k))\n",
    "vec_trb = CountVectorizer(analyzer='char', ngram_range=(k,k))\n",
    "tra_kmer_sparse = vec_tra.fit_transform(tra_seqs)\n",
    "trb_kmer_sparse = vec_trb.fit_transform(trb_seqs)\n",
    "print(f\"TRA k-mer sparse shape: {tra_kmer_sparse.shape}\")\n",
    "print(f\"TRB k-mer sparse shape: {trb_kmer_sparse.shape}\")\n",
    "\n",
    "# Reduce k-mer sparse matrices with TruncatedSVD to a dense reduced representation (keeps memory low)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "def _reduce_sparse(sparse_mat, n_components=200):\n",
    "    n_comp = min(n_components, max(1, sparse_mat.shape[1]-1))\n",
    "    try:\n",
    "        svd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "        return svd.fit_transform(sparse_mat)\n",
    "    except Exception:\n",
    "        # Fallback to dense (small datasets)\n",
    "        return sparse_mat.toarray() if hasattr(sparse_mat, 'toarray') else np.asarray(sparse_mat)\n",
    "\n",
    "tra_kmer_matrix = _reduce_sparse(tra_kmer_sparse, n_components=200)\n",
    "trb_kmer_matrix = _reduce_sparse(trb_kmer_sparse, n_components=200)\n",
    "print(f\"TRA k-mer reduced shape: {tra_kmer_matrix.shape}\")\n",
    "print(f\"TRB k-mer reduced shape: {trb_kmer_matrix.shape}\")\n",
    "\n",
    "# --- Reduced one-hot encoding: limit max length to avoid huge dense matrices ---\n",
    "max_cdr3_length = 20  # smaller to reduce dimensionality and memory\n",
    "alphabet = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "char_to_idx = {c:i for i,c in enumerate(alphabet)}\n",
    "def _onehot_flat_list(seqs, max_length, alphabet, char_to_idx):\n",
    "    out = np.zeros((len(seqs), max_length * len(alphabet)), dtype=np.uint8)\n",
    "    for i, s in enumerate(seqs):\n",
    "        for j, ch in enumerate(s[:max_length]):\n",
    "            if ch in char_to_idx:\n",
    "                out[i, j * len(alphabet) + char_to_idx[ch]] = 1\n",
    "    return out\n",
    "\n",
    "tra_onehot_flat = _onehot_flat_list(tra_seqs, max_cdr3_length, alphabet, char_to_idx)\n",
    "trb_onehot_flat = _onehot_flat_list(trb_seqs, max_cdr3_length, alphabet, char_to_idx)\n",
    "print(f\"TRA one-hot flat shape: {tra_onehot_flat.shape}\")\n",
    "print(f\"TRB one-hot flat shape: {trb_onehot_flat.shape}\")\n",
    "\n",
    "# --- Physicochemical properties (unchanged) ---\n",
    "tra_physico = pd.DataFrame([physicochemical_features(seq) for seq in tra_seqs])\n",
    "trb_physico = pd.DataFrame([physicochemical_features(seq) for seq in trb_seqs])\n",
    "print(f\"TRA physicochemical features shape: {tra_physico.shape}\")\n",
    "print(f\"TRB physicochemical features shape: {trb_physico.shape}\")\n",
    "\n",
    "# Add to AnnData object (reduced, memory-friendly)\n",
    "adata.obsm['X_tcr_tra_onehot'] = tra_onehot_flat\n",
    "adata.obsm['X_tcr_trb_onehot'] = trb_onehot_flat\n",
    "adata.obsm['X_tcr_tra_kmer'] = tra_kmer_matrix\n",
    "adata.obsm['X_tcr_trb_kmer'] = trb_kmer_matrix\n",
    "\n",
    "# Add physicochemical features to obs\n",
    "for col in tra_physico.columns:\n",
    "    adata.obs[f'tra_{col}'] = tra_physico[col].values\n",
    "for col in trb_physico.columns:\n",
    "    adata.obs[f'trb_{col}'] = trb_physico[col].values\n",
    "\n",
    "print(\"TCR sequence encoding completed and added to AnnData object!\")\n",
    "\n",
    "# Clean up large temporary objects\n",
    "import gc\n",
    "try:\n",
    "    # delete sparse intermediates and local copies — AnnData already stores the reduced matrices\n",
    "    del tra_kmer_sparse, trb_kmer_sparse\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    # delete other large temporaries that have been copied into `adata.obsm` or `adata.obs`\n",
    "    for _n in ['tra_kmer_matrix', 'trb_kmer_matrix', 'tra_onehot_flat', 'trb_onehot_flat', 'tra_physico', 'trb_physico', 'tra_seqs', 'trb_seqs', 'vec_tra', 'vec_trb', 'char_to_idx']:\n",
    "        if _n in globals():\n",
    "            try:\n",
    "                del globals()[_n]\n",
    "            except Exception:\n",
    "                pass\n",
    "except Exception:\n",
    "    pass\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f2c314",
   "metadata": {
    "papermill": {
     "duration": 0.017791,
     "end_time": "2026-02-13T17:15:10.683335",
     "exception": false,
     "start_time": "2026-02-13T17:15:10.665544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Gene Expression Encoding\n",
    "\n",
    "We define and apply a memory-optimized function to encode gene expression patterns. This includes highly variable gene selection, normalization, log-transformation, and dimensionality reduction using PCA and UMAP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13e919f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:15:10.720312Z",
     "iopub.status.busy": "2026-02-13T17:15:10.720019Z",
     "iopub.status.idle": "2026-02-13T17:17:52.623303Z",
     "shell.execute_reply": "2026-02-13T17:17:52.622572Z"
    },
    "papermill": {
     "duration": 161.943206,
     "end_time": "2026-02-13T17:17:52.644271",
     "exception": false,
     "start_time": "2026-02-13T17:15:10.701065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing gene expression data...\n",
      "Warning: Data is dense. converting to sparse float32...\n",
      "Normalizing...\n",
      "Log transforming...\n",
      "Encoding patterns...\n",
      "Selecting Highly Variable Genes...\n",
      "HVG subset shape: (100067, 1500)\n",
      "Computing PCA (Arpack - Sparse)...\n",
      "Computing TruncatedSVD...\n",
      "Computing UMAP on PCA embeddings...\n",
      "  Added X_gene_pca\n",
      "  Added X_gene_svd\n",
      "  Added X_gene_umap\n",
      "Gene expression encoding completed!\n",
      "CPU times: user 3min 12s, sys: 1.85 s, total: 3min 14s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MEMORY-OPTIMIZED encode_gene_expression_patterns function using Scanpy's Native Optimized PCA\n",
    "def encode_gene_expression_patterns(adata, n_top_genes=1500, train_mask=None):\n",
    "    \"\"\"\n",
    "    Encode gene expression patterns using Scanpy's optimized sparse PCA (Arpack)\n",
    "    and TruncatedSVD. Avoids manual chunking complexity which can be error prone.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (encodings dict, X_scaled placeholder)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import gc\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    import umap\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # 1. HVG Selection (Memory Optimized: Subsample cells if huge)\n",
    "    # Calculating mean/var on 100k cells x 30k genes can overlap memory.\n",
    "    # We calculate on a subset of 20k cells to estimate HVGs.\n",
    "    print(\"Selecting Highly Variable Genes...\")\n",
    "    \n",
    "    if adata.n_obs > 20000 and 'highly_variable' not in adata.var.columns:\n",
    "        # Subsample for HVG calculation only\n",
    "        idx = np.random.choice(adata.n_obs, 20000, replace=False)\n",
    "        temp_adata = adata[idx].copy()\n",
    "        sc.pp.highly_variable_genes(temp_adata, n_top_genes=n_top_genes, subset=False, flavor='seurat')\n",
    "        # Transfer results back\n",
    "        adata.var['highly_variable'] = False\n",
    "        adata.var.loc[temp_adata.var_names, 'highly_variable'] = temp_adata.var['highly_variable']\n",
    "        del temp_adata\n",
    "        gc.collect()\n",
    "    elif 'highly_variable' not in adata.var.columns:\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, subset=False, flavor='seurat')\n",
    "\n",
    "    # Get HVG Subset (Sparse View or Copy)\n",
    "    # Scanpy handles views efficiently for PCA\n",
    "    adata_hvg = adata[:, adata.var['highly_variable']]\n",
    "    print(f\"HVG subset shape: {adata_hvg.shape}\")\n",
    "\n",
    "    # 2. PCA (Scanpy Arpack = Sparse SVD on centered data implicitly)\n",
    "    # This is much more stable than manual IncrementalPCA on disjoint chunks\n",
    "    print(\"Computing PCA (Arpack - Sparse)...\")\n",
    "    sc.pp.pca(adata_hvg, n_comps=30, svd_solver='arpack', zero_center=True, use_highly_variable=False)\n",
    "    X_pca = adata_hvg.obsm['X_pca']\n",
    "    \n",
    "    # 3. TruncatedSVD (LSA - No centering)\n",
    "    # Good for sparse data comparison\n",
    "    print(\"Computing TruncatedSVD...\")\n",
    "    # Use the sparse matrix from the view\n",
    "    svd = TruncatedSVD(n_components=30, random_state=42, algorithm='randomized')\n",
    "    X_svd = svd.fit_transform(adata_hvg.X)\n",
    "    \n",
    "    # 4. UMAP on PCA (Standard practice)\n",
    "    print(\"Computing UMAP on PCA embeddings...\")\n",
    "    umap_reducer = umap.UMAP(\n",
    "        n_components=10, \n",
    "        n_neighbors=15, \n",
    "        random_state=42, \n",
    "        n_jobs=1, \n",
    "        low_memory=True\n",
    "    )\n",
    "    X_umap = umap_reducer.fit_transform(X_pca)\n",
    "\n",
    "    encodings = {\n",
    "        'pca': X_pca.astype(np.float32),\n",
    "        'svd': X_svd.astype(np.float32),\n",
    "        'umap': X_umap.astype(np.float32)\n",
    "    }\n",
    "    \n",
    "    # Clean up\n",
    "    del adata_hvg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Return nothing for X_scaled (deprecated)\n",
    "    return encodings, None\n",
    "\n",
    "# --- Main Preprocessing Block ---\n",
    "\n",
    "print(\"Preprocessing gene expression data...\")\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# 1. Ensure float32 sparse (Crucial for memory)\n",
    "if not hasattr(adata.X, 'toarray'): # is sparse\n",
    "    if adata.X.dtype != np.float32:\n",
    "        print(\"Converting sparse matrix to float32...\")\n",
    "        adata.X = adata.X.astype(np.float32)\n",
    "else: # is dense (shouldn't be, but just in case)\n",
    "    print(\"Warning: Data is dense. converting to sparse float32...\")\n",
    "    adata.X = sp.csr_matrix(adata.X, dtype=np.float32)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# 2. Normalize & Log (In-place)\n",
    "print(\"Normalizing...\")\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "print(\"Log transforming...\")\n",
    "sc.pp.log1p(adata)\n",
    "gc.collect()\n",
    "\n",
    "# 3. Clean Infs/NaNs (In-place, memory safe)\n",
    "if hasattr(adata.X, 'data'):\n",
    "    mask = np.isinf(adata.X.data)\n",
    "    if mask.any():\n",
    "        adata.X.data[mask] = 0\n",
    "        print(f\"Fixed {mask.sum()} infinite values\")\n",
    "    mask = np.isnan(adata.X.data)\n",
    "    if mask.any():\n",
    "        adata.X.data[mask] = 0\n",
    "        print(f\"Fixed {mask.sum()} NaN values\")\n",
    "\n",
    "print(\"Encoding patterns...\")\n",
    "\n",
    "# Apply encoding\n",
    "try:\n",
    "    # Use reduced gene set (1500)\n",
    "    result = encode_gene_expression_patterns(adata, n_top_genes=1500)\n",
    "    gene_encodings = result[0]\n",
    "\n",
    "    # Add to AnnData\n",
    "    for key, val in gene_encodings.items():\n",
    "        adata.obsm[f'X_gene_{key}'] = val\n",
    "        print(f\"  Added X_gene_{key}\")\n",
    "\n",
    "    del gene_encodings\n",
    "    gc.collect()\n",
    "    print(\"Gene expression encoding completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during encoding: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a72cb1a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:17:52.686449Z",
     "iopub.status.busy": "2026-02-13T17:17:52.686180Z",
     "iopub.status.idle": "2026-02-13T17:19:30.153330Z",
     "shell.execute_reply": "2026-02-13T17:19:30.152479Z"
    },
    "papermill": {
     "duration": 97.511494,
     "end_time": "2026-02-13T17:19:30.173706",
     "exception": false,
     "start_time": "2026-02-13T17:17:52.662212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating combined multi-modal encodings...\n",
      "Reducing k-mer features: (100067, 2)\n",
      "Combined gene-TCR encoding shape: (100067, 26)\n",
      "Combined gene-TCR k-mer encoding shape: (100067, 11)\n",
      "Computing dimensionality reduction on combined data (UMAP only)...\n",
      "CPU times: user 2min 30s, sys: 154 ms, total: 2min 30s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Create Combined Multi-Modal Encodings ---\n",
    "print(\"Creating combined multi-modal encodings...\")\n",
    "import gc\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Combine different encoding modalities\n",
    "# 1. Gene expression PCA + TCR physicochemical features\n",
    "gene_pca = None\n",
    "# Retrieve pre-computed PCA from gene_encodings dict\n",
    "pca_data = gene_encodings.get('pca', None) if 'gene_encodings' in globals() and isinstance(gene_encodings, dict) else None\n",
    "\n",
    "if pca_data is not None and isinstance(pca_data, (np.ndarray, list)):\n",
    "    gene_pca = np.asarray(pca_data)\n",
    "elif 'X_gene_pca' in adata.obsm:\n",
    "    gene_pca = adata.obsm['X_gene_pca']\n",
    "\n",
    "# Fallback or pad\n",
    "if gene_pca is not None:\n",
    "    if gene_pca.ndim == 1:\n",
    "        gene_pca = gene_pca.reshape(-1, 1)\n",
    "    if gene_pca.shape[1] >= 20:\n",
    "        gene_pca = gene_pca[:, :20]\n",
    "    else:\n",
    "        # Pad to 20 components\n",
    "        pad_cols = 20 - gene_pca.shape[1]\n",
    "        gene_pca = np.pad(gene_pca, ((0, 0), (0, pad_cols)), mode='constant')\n",
    "else:\n",
    "    print(\"Warning: PCA data not available; using zeros.\")\n",
    "    gene_pca = np.zeros((adata.n_obs, 20))\n",
    "\n",
    "tcr_features = np.column_stack([\n",
    "    adata.obs[['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']].fillna(0).values,\n",
    "    adata.obs[['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']].fillna(0).values\n",
    "])\n",
    "\n",
    "combined_gene_tcr = np.column_stack([gene_pca, tcr_features])\n",
    "adata.obsm['X_combined_gene_tcr'] = combined_gene_tcr.astype(np.float32)\n",
    "\n",
    "# 2. Gene expression UMAP + TCR k-mer features (reduced)\n",
    "gene_umap = gene_encodings.get('umap', None) if 'gene_encodings' in globals() and isinstance(gene_encodings, dict) else None\n",
    "if gene_umap is None and 'X_gene_umap' in adata.obsm:\n",
    "    gene_umap = adata.obsm['X_gene_umap']\n",
    "if gene_umap is None:\n",
    "    gene_umap = np.zeros((adata.n_obs, 2))\n",
    "\n",
    "# Stack TRA and TRB k-mer matrices EFFICIENTLY\n",
    "tra_kmer = adata.obsm.get('X_tcr_tra_kmer', None)\n",
    "trb_kmer = adata.obsm.get('X_tcr_trb_kmer', None)\n",
    "\n",
    "if tra_kmer is not None and trb_kmer is not None:\n",
    "    if sparse.issparse(tra_kmer) or sparse.issparse(trb_kmer):\n",
    "        # Ensure both are sparse before stacking to avoid densification\n",
    "        if not sparse.issparse(tra_kmer): tra_kmer = sparse.csr_matrix(tra_kmer)\n",
    "        if not sparse.issparse(trb_kmer): trb_kmer = sparse.csr_matrix(trb_kmer)\n",
    "        tcr_kmer_combined = sparse.hstack([tra_kmer, trb_kmer])\n",
    "    else:\n",
    "        tcr_kmer_combined = np.column_stack([tra_kmer, trb_kmer])\n",
    "else:\n",
    "    tcr_kmer_combined = np.zeros((adata.n_obs, 1)) # Dummy\n",
    "\n",
    "# Robust dimensional reduction for k-mer features using TruncatedSVD (Safe for OOM)\n",
    "print(f\"Reducing k-mer features: {tcr_kmer_combined.shape}\")\n",
    "n_comp_kmer = min(10, tcr_kmer_combined.shape[1] - 1)\n",
    "if n_comp_kmer > 0:\n",
    "    tcr_svd = TruncatedSVD(n_components=n_comp_kmer, random_state=42, algorithm='randomized')\n",
    "    tcr_kmer_reduced = tcr_svd.fit_transform(tcr_kmer_combined)\n",
    "else:\n",
    "    tcr_kmer_reduced = np.zeros((adata.n_obs, 10))\n",
    "\n",
    "combined_gene_tcr_kmer = np.column_stack([gene_umap, tcr_kmer_reduced])\n",
    "adata.obsm['X_combined_gene_tcr_kmer'] = combined_gene_tcr_kmer.astype(np.float32)\n",
    "\n",
    "print(f\"Combined gene-TCR encoding shape: {combined_gene_tcr.shape}\")\n",
    "print(f\"Combined gene-TCR k-mer encoding shape: {combined_gene_tcr_kmer.shape}\")\n",
    "\n",
    "# Clear memory\n",
    "del tcr_kmer_combined\n",
    "gc.collect()\n",
    "\n",
    "# --- Dimensionality Reduction on Combined Data ---\n",
    "print(\"Computing dimensionality reduction on combined data (UMAP only)...\")\n",
    "\n",
    "# UMAP on combined data\n",
    "umap_combined = umap.UMAP(n_components=2, random_state=42, n_jobs=1) # Single Job for RAM safety\n",
    "adata.obsm['X_umap_combined'] = umap_combined.fit_transform(combined_gene_tcr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1dcaf0",
   "metadata": {
    "papermill": {
     "duration": 0.018005,
     "end_time": "2026-02-13T17:19:30.210062",
     "exception": false,
     "start_time": "2026-02-13T17:19:30.192057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Multi-Modal Feature Combination\n",
    "\n",
    "We combine the encoded gene expression features (PCA/UMAP) with the TCR features (physicochemical properties and k-mers) to create unified multi-modal representations for the learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38be236",
   "metadata": {
    "papermill": {
     "duration": 0.01802,
     "end_time": "2026-02-13T17:19:30.245880",
     "exception": false,
     "start_time": "2026-02-13T17:19:30.227860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11. Unsupervised Clustering with Leiden Algorithm\n",
    "\n",
    "To identify distinct cell populations within the immune landscape, we employ the Leiden algorithm. This community detection method improves upon the Louvain algorithm by guaranteeing well-connected communities and is the standard for single-cell RNA-seq analysis. We perform clustering at multiple resolutions to capture both broad cell types and fine-grained states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69f0e270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:19:30.283303Z",
     "iopub.status.busy": "2026-02-13T17:19:30.282787Z",
     "iopub.status.idle": "2026-02-13T17:19:30.289677Z",
     "shell.execute_reply": "2026-02-13T17:19:30.288944Z"
    },
    "papermill": {
     "duration": 0.027351,
     "end_time": "2026-02-13T17:19:30.290971",
     "exception": false,
     "start_time": "2026-02-13T17:19:30.263620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No patch required for sklearn.check_array signature.\n"
     ]
    }
   ],
   "source": [
    "# HDBSCAN/sklearn compatibility patch — run before clustering\n",
    "import sys, subprocess, inspect\n",
    "\n",
    "# Ensure hdbscan is available (not strictly necessary if already installed earlier)\n",
    "try:\n",
    "    import hdbscan\n",
    "except Exception:\n",
    "    print(\"hdbscan not installed — installing now...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"hdbscan\"]) \n",
    "    import importlib\n",
    "    importlib.invalidate_caches()\n",
    "    import hdbscan\n",
    "\n",
    "# Patch the check_array reference used inside hdbscan to accept the older keyword\n",
    "try:\n",
    "    import sklearn.utils.validation as sk_validation\n",
    "    from hdbscan import hdbscan_ as _hdbscan_mod\n",
    "    sig = inspect.signature(sk_validation.check_array)\n",
    "    if 'ensure_all_finite' in sig.parameters and 'force_all_finite' not in sig.parameters:\n",
    "        orig = getattr(_hdbscan_mod, 'check_array', None) or sk_validation.check_array\n",
    "        def _patched_check_array(*args, **kwargs):\n",
    "            if 'force_all_finite' in kwargs and 'ensure_all_finite' not in kwargs:\n",
    "                kwargs['ensure_all_finite'] = kwargs.pop('force_all_finite')\n",
    "            return orig(*args, **kwargs)\n",
    "        _hdbscan_mod.check_array = _patched_check_array\n",
    "        print(\"Patched hdbscan.check_array to accept 'force_all_finite' for this runtime.\")\n",
    "    else:\n",
    "        print(\"No patch required for sklearn.check_array signature.\")\n",
    "except Exception as e:\n",
    "    print(\"Compatibility patch could not be applied:\", type(e).__name__, e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d16637",
   "metadata": {
    "papermill": {
     "duration": 0.018413,
     "end_time": "2026-02-13T17:19:30.327643",
     "exception": false,
     "start_time": "2026-02-13T17:19:30.309230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Unsupervised Machine Learning Analysis (Updated)\n",
    "\n",
    "This section has been updated to utilize the `clustering.py` implementation for Leiden clustering, replacing the previous K-Means/DBSCAN/Agglomerative comparison.\n",
    "\n",
    "**Changes:**\n",
    "- Imported `clustering.py` module.\n",
    "- Used `clustering.preprocess_data(adata)` for data preprocessing.\n",
    "- Used `clustering.perform_clustering(adata)` for Leiden clustering at multiple resolutions.\n",
    "- Calculated silhouette scores for Leiden clusters to maintain compatibility with the \"best clustering\" selection logic.\n",
    "- Renamed Leiden cluster columns to `leiden_cluster_{resolution}` to ensure compatibility with downstream feature selection filters.\n",
    "- Retained TCR sequence-specific clustering and Gene Expression Module Discovery.\n",
    "\n",
    "**Note:**\n",
    "- Ensure `clustering.py` is in the python path (Code/ directory).\n",
    "- The \"best clustering\" is now selected from the Leiden results based on silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "526bafb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:19:30.368080Z",
     "iopub.status.busy": "2026-02-13T17:19:30.367575Z",
     "iopub.status.idle": "2026-02-13T17:31:24.179438Z",
     "shell.execute_reply": "2026-02-13T17:31:24.178453Z"
    },
    "papermill": {
     "duration": 713.835657,
     "end_time": "2026-02-13T17:31:24.181367",
     "exception": false,
     "start_time": "2026-02-13T17:19:30.345710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.15.3)\r\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.0.2)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting leidenalg\r\n",
      "  Downloading leidenalg-0.11.0-cp38-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: igraph<2.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from leidenalg) (1.0.0)\r\n",
      "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.12/dist-packages (from igraph<2.0,>=1.0.0->leidenalg) (1.7.0)\r\n",
      "Downloading leidenalg-0.11.0-cp38-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: leidenalg\r\n",
      "Successfully installed leidenalg-0.11.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Applying unsupervised machine learning algorithms...\n",
      "Preprocessing data (memory-efficient mode)...\n",
      "Computing PCA (sparse-friendly via TruncatedSVD on HVGs)...\n",
      "  Using 1500 highly variable genes\n",
      "  HVG matrix shape: (100067, 1500), sparse: True\n",
      "  Running TruncatedSVD with 50 components...\n",
      "  PCA complete. Variance explained: 69.56%\n",
      "  Memory cleanup after PCA complete.\n",
      "Computing neighbors...\n",
      "Performing Leiden clustering...\n",
      "Resolution 0.01: 4 clusters\n",
      "Resolution 0.05: 6 clusters\n",
      "Resolution 0.1: 7 clusters\n",
      "Resolution 0.2: 10 clusters\n",
      "Resolution 0.5: 14 clusters\n",
      "Resolution 1.0: 22 clusters\n",
      "Selected resolution: 0.1\n",
      "Performing TCR sequence-specific clustering...\n",
      "Discovering gene expression modules...\n",
      "Creating visualizations...\n",
      "\n",
      "Warning: No response column found. supervised_mask includes all cells.\n",
      "Unsupervised machine learning analysis completed!\n",
      "CPU times: user 12min 20s, sys: 3.32 s, total: 12min 24s\n",
      "Wall time: 11min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install scipy\n",
    "%pip install leidenalg\n",
    "\n",
    "# Check if we should skip unsupervised learning\n",
    "if globals().get('SKIP_UNSUPERVISED_LEARNING', False) or globals().get('SKIP_TO_DEEP_LEARNING', False):\n",
    "    print(\"⏭️ SKIPPING: Unsupervised Learning (Leiden, UMAP, etc.)\")\n",
    "    print(\"   Set SKIP_UNSUPERVISED_LEARNING=False in the configuration cell to run this section.\")\n",
    "    \n",
    "    # Still need to do minimal preprocessing for deep learning\n",
    "    import scanpy as sc\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    from scipy import sparse\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    print(\"Running MINIMAL preprocessing for deep learning...\")\n",
    "    \n",
    "    # Normalize and log-transform\n",
    "    if 'log1p' not in adata.uns:\n",
    "        sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    # Find HVGs\n",
    "    if 'highly_variable' not in adata.var.columns:\n",
    "        sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "    \n",
    "    # Compute PCA if needed\n",
    "    if 'X_pca' not in adata.obsm:\n",
    "        hvg_mask = adata.var['highly_variable'].values\n",
    "        n_hvgs = hvg_mask.sum()\n",
    "        if sparse.issparse(adata.X):\n",
    "            X_hvg = adata.X[:, hvg_mask]\n",
    "            n_components = min(50, n_hvgs - 1, X_hvg.shape[0] - 1)\n",
    "            svd = TruncatedSVD(n_components=n_components, random_state=42, algorithm='arpack')\n",
    "            adata.obsm['X_pca'] = svd.fit_transform(X_hvg).astype(np.float32)\n",
    "            del X_hvg, svd\n",
    "            gc.collect()\n",
    "    \n",
    "    # Fix column names (response, patient_id) - ROBUST VERSION\n",
    "    if 'response' not in adata.obs.columns and 'Response' in adata.obs.columns:\n",
    "        adata.obs['response'] = adata.obs['Response']\n",
    "    \n",
    "    # First check if patient_id already exists\n",
    "    patient_id_found = False\n",
    "    for col_name in ['patient_id', 'Patient_ID', 'PatientID']:\n",
    "        if col_name in adata.obs.columns:\n",
    "            if col_name != 'patient_id':\n",
    "                adata.obs['patient_id'] = adata.obs[col_name]\n",
    "            patient_id_found = True\n",
    "            print(f\"  Found patient_id in column '{col_name}'\")\n",
    "            break\n",
    "    \n",
    "    # If patient_id not found, derive from sample_id using metadata mapping\n",
    "    if not patient_id_found and 'sample_id' in adata.obs.columns:\n",
    "        print(\"  patient_id not found directly. Deriving from sample_id...\")\n",
    "        \n",
    "        # Recreate metadata_df mapping (same as in data loading cell)\n",
    "        # This is the authoritative mapping from GEO GSE300475\n",
    "        metadata_records = [\n",
    "            {'sample_id': 'GSM9061665', 'Patient_ID': 'PT1', 'Timepoint': 'Pre', 'Response': 'Responder'},\n",
    "            {'sample_id': 'GSM9061666', 'Patient_ID': 'PT1', 'Timepoint': 'D21', 'Response': 'Responder'},\n",
    "            {'sample_id': 'GSM9061667', 'Patient_ID': 'PT1', 'Timepoint': 'D42', 'Response': 'Responder'},\n",
    "            {'sample_id': 'GSM9061668', 'Patient_ID': 'PT2', 'Timepoint': 'Pre', 'Response': 'Responder'},\n",
    "            {'sample_id': 'GSM9061669', 'Patient_ID': 'PT2', 'Timepoint': 'D21', 'Response': 'Responder'},\n",
    "            {'sample_id': 'GSM9061670', 'Patient_ID': 'PT2', 'Timepoint': 'D42', 'Response': 'Responder'},\n",
    "            {'sample_id': 'GSM9061671', 'Patient_ID': 'PT3', 'Timepoint': 'Pre', 'Response': 'Non-Responder'},\n",
    "            {'sample_id': 'GSM9061672', 'Patient_ID': 'PT3', 'Timepoint': 'D21', 'Response': 'Non-Responder'},\n",
    "            {'sample_id': 'GSM9061673', 'Patient_ID': 'PT4', 'Timepoint': 'Pre', 'Response': 'Non-Responder'},\n",
    "            {'sample_id': 'GSM9061674', 'Patient_ID': 'PT4', 'Timepoint': 'D21', 'Response': 'Non-Responder'},\n",
    "            {'sample_id': 'GSM9061675', 'Patient_ID': 'PT4', 'Timepoint': 'D42', 'Response': 'Non-Responder'},\n",
    "        ]\n",
    "        metadata_df_local = pd.DataFrame(metadata_records)\n",
    "        \n",
    "        # Create sample_id to patient_id mapping\n",
    "        sample_to_patient = dict(zip(metadata_df_local['sample_id'], metadata_df_local['Patient_ID']))\n",
    "        \n",
    "        # Map sample_id to patient_id\n",
    "        adata.obs['patient_id'] = adata.obs['sample_id'].map(sample_to_patient)\n",
    "        \n",
    "        # Check for unmapped values\n",
    "        n_mapped = adata.obs['patient_id'].notna().sum()\n",
    "        n_total = len(adata.obs)\n",
    "        print(f\"  Mapped {n_mapped}/{n_total} cells to patient_id\")\n",
    "        \n",
    "        if n_mapped == 0:\n",
    "            # Try parsing sample_id - maybe format is different (e.g., batch column)\n",
    "            print(\"  Warning: No direct matches. Checking for batch column...\")\n",
    "            if 'batch' in adata.obs.columns:\n",
    "                adata.obs['patient_id'] = adata.obs['batch'].map(sample_to_patient)\n",
    "                n_mapped = adata.obs['patient_id'].notna().sum()\n",
    "                print(f\"  Mapped {n_mapped}/{n_total} cells using batch column\")\n",
    "        \n",
    "        # Also derive response if missing\n",
    "        if 'response' not in adata.obs.columns or adata.obs['response'].isna().all():\n",
    "            sample_to_response = dict(zip(metadata_df_local['sample_id'], metadata_df_local['Response']))\n",
    "            adata.obs['response'] = adata.obs['sample_id'].map(sample_to_response)\n",
    "            if adata.obs['response'].isna().all() and 'batch' in adata.obs.columns:\n",
    "                adata.obs['response'] = adata.obs['batch'].map(sample_to_response)\n",
    "            print(f\"  Derived response column: {adata.obs['response'].value_counts().to_dict()}\")\n",
    "        \n",
    "        patient_id_found = adata.obs['patient_id'].notna().any()\n",
    "    \n",
    "    if not patient_id_found:\n",
    "        print(\"  WARNING: Could not derive patient_id. Downstream patient-level analysis may fail.\")\n",
    "        print(f\"  Available columns: {list(adata.obs.columns)}\")\n",
    "    else:\n",
    "        print(f\"  patient_id distribution: {adata.obs['patient_id'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Create supervised_mask for downstream\n",
    "    if 'response' in adata.obs.columns:\n",
    "        supervised_mask = adata.obs['response'].isin(['Responder', 'Non-Responder']).values\n",
    "    else:\n",
    "        supervised_mask = np.ones(adata.n_obs, dtype=bool)\n",
    "    \n",
    "    print(f\"Minimal preprocessing complete. supervised_mask: {supervised_mask.sum()} samples\")\n",
    "    \n",
    "else:\n",
    "    # --- Full Unsupervised Machine Learning Analysis ---\n",
    "    print(\"Applying unsupervised machine learning algorithms...\")\n",
    "\n",
    "    import scanpy as sc\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import gc  # For garbage collection\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "    from scipy import sparse\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    from IPython.display import display\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    if IS_KAGGLE:\n",
    "        Path('/kaggle/working/Processed_Data').mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        Path('Processed_Data').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Quick memory cleanup\n",
    "    for _v in ['adata_list','adata_sample','metadata_list','metadata_df',\n",
    "               'tra_kmer_sparse','trb_kmer_sparse','tra_kmer_matrix','trb_kmer_matrix',\n",
    "               'vec_tra','vec_trb','tra_seqs','trb_seqs','tra_kmeans','trb_kmeans',\n",
    "               'tra_kmer_scaled','trb_kmer_scaled','tra_scaler','trb_scaler','gene_kmeans',\n",
    "               'gene_pca','gene_expression_modules','tra_clusters','trb_clusters']:\n",
    "        if _v in globals():\n",
    "            try:\n",
    "                del globals()[_v]\n",
    "            except Exception:\n",
    "                pass\n",
    "    gc.collect()\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # ============================================================\n",
    "    # Fix column names (response, patient_id) BEFORE processing\n",
    "    # ============================================================\n",
    "    if 'response' not in adata.obs.columns and 'Response' in adata.obs.columns:\n",
    "        adata.obs['response'] = adata.obs['Response']\n",
    "        print(\"  Renamed 'Response' column to 'response'\")\n",
    "    \n",
    "    for col_name in ['Patient_ID', 'PatientID']:\n",
    "        if col_name in adata.obs.columns and 'patient_id' not in adata.obs.columns:\n",
    "            adata.obs['patient_id'] = adata.obs[col_name]\n",
    "            print(f\"  Renamed '{col_name}' column to 'patient_id'\")\n",
    "            break\n",
    "\n",
    "    print(\"Preprocessing data (memory-efficient mode)...\")\n",
    "\n",
    "    # 1. Normalize and log-transform (these keep sparse)\n",
    "    if 'log1p' not in adata.uns:\n",
    "        print(\"  Normalizing...\")\n",
    "        sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "        gc.collect()\n",
    "\n",
    "    # 2. Find highly variable genes (does NOT densify)\n",
    "    if 'highly_variable' not in adata.var.columns:\n",
    "        print(\"  Finding highly variable genes...\")\n",
    "        sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "        gc.collect()\n",
    "\n",
    "    # 3. MEMORY-EFFICIENT PCA using TruncatedSVD on sparse HVG subset\n",
    "    if 'X_pca' not in adata.obsm:\n",
    "        print(\"Computing PCA (sparse-friendly via TruncatedSVD on HVGs)...\")\n",
    "        \n",
    "        from sklearn.decomposition import TruncatedSVD\n",
    "        \n",
    "        hvg_mask = adata.var['highly_variable'].values\n",
    "        n_hvgs = hvg_mask.sum()\n",
    "        print(f\"  Using {n_hvgs} highly variable genes\")\n",
    "        \n",
    "        if sparse.issparse(adata.X):\n",
    "            X_hvg = adata.X[:, hvg_mask]\n",
    "            print(f\"  HVG matrix shape: {X_hvg.shape}, sparse: {sparse.issparse(X_hvg)}\")\n",
    "            \n",
    "            n_components = min(50, n_hvgs - 1, X_hvg.shape[0] - 1)\n",
    "            print(f\"  Running TruncatedSVD with {n_components} components...\")\n",
    "            \n",
    "            svd = TruncatedSVD(n_components=n_components, random_state=42, algorithm='arpack')\n",
    "            X_pca = svd.fit_transform(X_hvg)\n",
    "            \n",
    "            adata.obsm['X_pca'] = X_pca.astype(np.float32)\n",
    "            adata.uns['pca'] = {\n",
    "                'variance_ratio': svd.explained_variance_ratio_,\n",
    "                'variance': svd.explained_variance_,\n",
    "            }\n",
    "            loadings = np.zeros((adata.n_vars, n_components), dtype=np.float32)\n",
    "            loadings[hvg_mask, :] = svd.components_.T.astype(np.float32)\n",
    "            adata.varm['PCs'] = loadings\n",
    "            \n",
    "            del X_hvg, svd, X_pca, loadings\n",
    "            gc.collect()\n",
    "            print(f\"  PCA complete. Variance explained: {adata.uns['pca']['variance_ratio'].sum():.2%}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"  Data is dense, scaling HVGs only...\")\n",
    "            X_hvg = adata.X[:, hvg_mask].copy()\n",
    "            scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "            X_hvg_scaled = scaler.fit_transform(X_hvg)\n",
    "            del X_hvg\n",
    "            gc.collect()\n",
    "            \n",
    "            from sklearn.decomposition import PCA\n",
    "            n_components = min(50, n_hvgs - 1, X_hvg_scaled.shape[0] - 1)\n",
    "            pca = PCA(n_components=n_components, random_state=42)\n",
    "            X_pca = pca.fit_transform(X_hvg_scaled)\n",
    "            \n",
    "            adata.obsm['X_pca'] = X_pca.astype(np.float32)\n",
    "            adata.uns['pca'] = {\n",
    "                'variance_ratio': pca.explained_variance_ratio_,\n",
    "                'variance': pca.explained_variance_,\n",
    "            }\n",
    "            loadings = np.zeros((adata.n_vars, n_components), dtype=np.float32)\n",
    "            loadings[hvg_mask, :] = pca.components_.T.astype(np.float32)\n",
    "            adata.varm['PCs'] = loadings\n",
    "            \n",
    "            del X_hvg_scaled, pca, X_pca, loadings, scaler\n",
    "            gc.collect()\n",
    "        \n",
    "        # Memory cleanup after PCA\n",
    "        try:\n",
    "            if getattr(adata, 'raw', None) is not None:\n",
    "                del adata.raw\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            if hasattr(adata, 'layers') and len(adata.layers) > 0:\n",
    "                adata.layers.clear()\n",
    "        except Exception:\n",
    "            pass\n",
    "        gc.collect()\n",
    "        print(\"  Memory cleanup after PCA complete.\")\n",
    "\n",
    "    # Neighbors\n",
    "    print(\"Computing neighbors...\")\n",
    "    sc.pp.neighbors(adata, n_neighbors=15, n_pcs=50, random_state=42)\n",
    "    gc.collect()\n",
    "\n",
    "    # 2. Perform Clustering (Leiden) - Use fewer resolutions for speed\n",
    "    print(\"Performing Leiden clustering...\")\n",
    "    resolutions = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]  # Reduced from 26 to 6\n",
    "    best_res = 0.1\n",
    "    target_clusters = 7\n",
    "    best_diff = float('inf')\n",
    "\n",
    "    for res in resolutions:\n",
    "        key = f'leiden_{res}'\n",
    "        try:\n",
    "            sc.tl.leiden(adata, resolution=res, key_added=key, random_state=42)\n",
    "            n_clust = len(adata.obs[key].unique())\n",
    "            print(f\"Resolution {res}: {n_clust} clusters\")\n",
    "            if abs(n_clust - target_clusters) < best_diff:\n",
    "                best_diff = abs(n_clust - target_clusters)\n",
    "                best_res = res\n",
    "        except Exception as e:\n",
    "            print(f\"Leiden failed for resolution {res}: {e}\")\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"Selected resolution: {best_res}\")\n",
    "    if f'leiden_{best_res}' in adata.obs:\n",
    "        adata.obs['leiden'] = adata.obs[f'leiden_{best_res}']\n",
    "\n",
    "    # 3. TCR Sequence Clustering\n",
    "    print(\"Performing TCR sequence-specific clustering...\")\n",
    "    if 'X_tcr_tra_kmer' in adata.obsm:\n",
    "        tra_scaler = StandardScaler()\n",
    "        tra_kmer_scaled = tra_scaler.fit_transform(adata.obsm['X_tcr_tra_kmer'])\n",
    "        tra_kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
    "        adata.obs['tra_kmer_clusters'] = pd.Categorical(tra_kmeans.fit_predict(tra_kmer_scaled))\n",
    "        del tra_kmer_scaled, tra_kmeans, tra_scaler\n",
    "        gc.collect()\n",
    "\n",
    "    if 'X_tcr_trb_kmer' in adata.obsm:\n",
    "        trb_scaler = StandardScaler()\n",
    "        trb_kmer_scaled = trb_scaler.fit_transform(adata.obsm['X_tcr_trb_kmer'])\n",
    "        trb_kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
    "        adata.obs['trb_kmer_clusters'] = pd.Categorical(trb_kmeans.fit_predict(trb_kmer_scaled))\n",
    "        del trb_kmer_scaled, trb_kmeans, trb_scaler\n",
    "        gc.collect()\n",
    "\n",
    "    # 4. Gene Expression Module Discovery\n",
    "    print(\"Discovering gene expression modules...\")\n",
    "    gene_pca = adata.obsm.get('X_gene_pca', adata.obsm.get('X_pca'))\n",
    "    if gene_pca is not None:\n",
    "        gene_kmeans = KMeans(n_clusters=8, random_state=42, n_init=10)\n",
    "        adata.obs['gene_expression_modules'] = pd.Categorical(gene_kmeans.fit_predict(gene_pca))\n",
    "        del gene_pca, gene_kmeans\n",
    "        gc.collect()\n",
    "\n",
    "    # 5. Visualization\n",
    "    print(\"Creating visualizations...\")\n",
    "    sc.tl.umap(adata, random_state=42)\n",
    "    \n",
    "    color_keys = []\n",
    "    if 'leiden' in adata.obs:\n",
    "        color_keys.append('leiden')\n",
    "    if 'response' in adata.obs.columns:\n",
    "        color_keys.append('response')\n",
    "    \n",
    "    if color_keys:\n",
    "        sc.pl.umap(adata, color=color_keys, show=False)\n",
    "        plt.show()\n",
    "\n",
    "    # --- Create supervised_mask for downstream cells ---\n",
    "    if 'response' in adata.obs.columns:\n",
    "        supervised_mask = adata.obs['response'].isin(['Responder', 'Non-Responder']).values\n",
    "        print(f\"\\nCreated supervised_mask: {supervised_mask.sum()} samples with valid response labels\")\n",
    "    else:\n",
    "        supervised_mask = np.ones(adata.n_obs, dtype=bool)\n",
    "        print(\"\\nWarning: No response column found. supervised_mask includes all cells.\")\n",
    "\n",
    "    print(\"Unsupervised machine learning analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bf823ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:31:24.223127Z",
     "iopub.status.busy": "2026-02-13T17:31:24.222363Z",
     "iopub.status.idle": "2026-02-13T17:31:24.729949Z",
     "shell.execute_reply": "2026-02-13T17:31:24.729225Z"
    },
    "papermill": {
     "duration": 0.530032,
     "end_time": "2026-02-13T17:31:24.731499",
     "exception": false,
     "start_time": "2026-02-13T17:31:24.201467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running memory cleanup after Leiden clustering (before dendrogram)...\n",
      "Memory before cleanup: 4399 MB\n",
      "Fallback cleanup completed.\n",
      "Memory after cleanup: 4399 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Memory cleanup (after Leiden clustering, before dendrogram) ---\n",
    "# This frees large temporary matrices (one-hot encodings, neighbor/connectivity matrices)\n",
    "# while keeping UMAP for dendrogram/visualization.\n",
    "print('\\nRunning memory cleanup after Leiden clustering (before dendrogram)...')\n",
    "try:\n",
    "    import psutil, os\n",
    "    proc = psutil.Process(os.getpid())\n",
    "    print(f\"Memory before cleanup: {proc.memory_info().rss // (1024**2)} MB\")\n",
    "except Exception:\n",
    "    print('psutil not available; skipping memory before measurement')\n",
    "\n",
    "def _fallback_cleanup(drop_onehot=False, drop_raw=False, drop_obsm_umap_tsne=False, verbose=True):\n",
    "    \"\"\"Basic cleanup fallback when cleanup_after_clustering is unavailable.\"\"\"\n",
    "    if 'adata' not in globals():\n",
    "        return\n",
    "    if hasattr(adata, 'obsp'):\n",
    "        for _k in list(adata.obsp.keys()):\n",
    "            try:\n",
    "                del adata.obsp[_k]\n",
    "            except Exception:\n",
    "                pass\n",
    "    if drop_onehot and hasattr(adata, 'obsm'):\n",
    "        for _key in ['X_tcr_tra_onehot', 'X_tcr_trb_onehot']:\n",
    "            if _key in adata.obsm:\n",
    "                try:\n",
    "                    del adata.obsm[_key]\n",
    "                except Exception:\n",
    "                    pass\n",
    "    if drop_obsm_umap_tsne and hasattr(adata, 'obsm'):\n",
    "        for _key in list(adata.obsm.keys()):\n",
    "            _lk = _key.lower()\n",
    "            if 'umap' in _lk or 'tsne' in _lk:\n",
    "                try:\n",
    "                    del adata.obsm[_key]\n",
    "                except Exception:\n",
    "                    pass\n",
    "    if drop_raw and getattr(adata, 'raw', None) is not None:\n",
    "        adata.raw = None\n",
    "    if verbose:\n",
    "        print('Fallback cleanup completed.')\n",
    "\n",
    "# Conservative cleanup: drop TCR one-hot arrays and obsp connectivities/distances\n",
    "# Keep one-hot encodings by default to avoid KeyError in downstream feature engineering\n",
    "if 'cleanup_after_clustering' in globals():\n",
    "    try:\n",
    "        cleanup_after_clustering(drop_onehot=False, drop_raw=False, drop_obsm_umap_tsne=False, verbose=True)\n",
    "    except Exception as e:\n",
    "        print('cleanup_after_clustering failed, using fallback cleanup:', e)\n",
    "        _fallback_cleanup(drop_onehot=False, drop_raw=False, drop_obsm_umap_tsne=False, verbose=True)\n",
    "else:\n",
    "    _fallback_cleanup(drop_onehot=False, drop_raw=False, drop_obsm_umap_tsne=False, verbose=True)\n",
    "\n",
    "try:\n",
    "    proc = psutil.Process(os.getpid())\n",
    "    print(f\"Memory after cleanup: {proc.memory_info().rss // (1024**2)} MB\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11f7dbe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:31:24.772502Z",
     "iopub.status.busy": "2026-02-13T17:31:24.772255Z",
     "iopub.status.idle": "2026-02-13T17:31:24.982937Z",
     "shell.execute_reply": "2026-02-13T17:31:24.981964Z"
    },
    "papermill": {
     "duration": 0.233203,
     "end_time": "2026-02-13T17:31:24.984528",
     "exception": false,
     "start_time": "2026-02-13T17:31:24.751325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response distribution: {'Non-Responder': 63074, 'Responder': 36993}\n",
      "Patient_id coverage: 100067/100067\n",
      "Working with 100067 samples for supervised learning\n",
      "Class distribution: {'Non-Responder': 63074, 'Responder': 36993}\n"
     ]
    }
   ],
   "source": [
    "# --- Label/patient derivation and supervised availability helpers ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "if '_ensure_int_labels' not in globals():\n",
    "    def _ensure_int_labels(y):\n",
    "        y_arr = np.asarray(y)\n",
    "        if np.issubdtype(y_arr.dtype, np.integer):\n",
    "            return y_arr\n",
    "        if y_arr.size == 0:\n",
    "            return y_arr.astype(np.int64)\n",
    "        if np.all(np.isfinite(y_arr)) and np.all(np.equal(y_arr, np.floor(y_arr))):\n",
    "            return y_arr.astype(np.int64)\n",
    "        raise ValueError(\"Labels must be integer or integer-like floats.\")\n",
    "if '_normalize_response_value' not in globals():\n",
    "    def _normalize_response_value(val):\n",
    "        if pd.isna(val):\n",
    "            return 'Unknown'\n",
    "        s = str(val).strip().lower()\n",
    "        if s == '':\n",
    "            return 'Unknown'\n",
    "        if 'non' in s and 'responder' in s:\n",
    "            return 'Non-Responder'\n",
    "        if 'responder' in s:\n",
    "            return 'Responder'\n",
    "        return 'Unknown'\n",
    "if '_ensure_response_and_patient' not in globals():\n",
    "    def _ensure_response_and_patient(adata):\n",
    "        # Normalize existing columns if present\n",
    "        if 'response' not in adata.obs.columns and 'Response' in adata.obs.columns:\n",
    "            adata.obs['response'] = adata.obs['Response']\n",
    "        if 'patient_id' not in adata.obs.columns:\n",
    "            for _col in ['Patient_ID', 'PatientID']:\n",
    "                if _col in adata.obs.columns:\n",
    "                    adata.obs['patient_id'] = adata.obs[_col]\n",
    "                    break\n",
    "        # Determine metadata mapping\n",
    "        md = None\n",
    "        if 'metadata_df' in globals() and isinstance(metadata_df, pd.DataFrame) and not metadata_df.empty:\n",
    "            md = metadata_df.copy()\n",
    "        else:\n",
    "            # Fallback to hard-coded metadata list (same as in Cell 15)\n",
    "            _metadata_list = [\n",
    "                {'S_Number': 'S1',  'GEX_Sample_ID': 'GSM9061665', 'TCR_Sample_ID': 'GSM9061687', 'Patient_ID': 'PT1',  'Timepoint': 'Baseline',   'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S2',  'GEX_Sample_ID': 'GSM9061666', 'TCR_Sample_ID': 'GSM9061688', 'Patient_ID': 'PT1',  'Timepoint': 'Post-Tx',    'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S3',  'GEX_Sample_ID': 'GSM9061667', 'TCR_Sample_ID': 'GSM9061689', 'Patient_ID': 'PT1',  'Timepoint': 'Recurrence', 'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S4',  'GEX_Sample_ID': 'GSM9061668', 'TCR_Sample_ID': 'GSM9061690', 'Patient_ID': 'PT2',  'Timepoint': 'Baseline',   'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S5',  'GEX_Sample_ID': 'GSM9061669', 'TCR_Sample_ID': 'GSM9061691', 'Patient_ID': 'PT2',  'Timepoint': 'Post-Tx',    'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S6',  'GEX_Sample_ID': 'GSM9061670', 'TCR_Sample_ID': 'GSM9061692', 'Patient_ID': 'PT3',  'Timepoint': 'Baseline',   'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S7',  'GEX_Sample_ID': 'GSM9061671', 'TCR_Sample_ID': 'GSM9061693', 'Patient_ID': 'PT3',  'Timepoint': 'Post-Tx',    'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S8',  'GEX_Sample_ID': 'GSM9061672', 'TCR_Sample_ID': None,         'Patient_ID': 'PT3',  'Timepoint': 'Recurrence', 'Response': 'Non-Responder', 'In_Data': 'GEX only'},\n",
    "                {'S_Number': 'S9',  'GEX_Sample_ID': 'GSM9061673', 'TCR_Sample_ID': 'GSM9061694', 'Patient_ID': 'PT4',  'Timepoint': 'Baseline',   'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S10', 'GEX_Sample_ID': 'GSM9061674', 'TCR_Sample_ID': 'GSM9061695', 'Patient_ID': 'PT4',  'Timepoint': 'Post-Tx',    'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S11', 'GEX_Sample_ID': 'GSM9061675', 'TCR_Sample_ID': 'GSM9061696', 'Patient_ID': 'PT4',  'Timepoint': 'Recurrence', 'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "            ]\n",
    "            md = pd.DataFrame(_metadata_list)\n",
    "        # Identify mapping columns in metadata\n",
    "        sample_col = None\n",
    "        for _c in ['sample_id', 'GEX_Sample_ID', 'GSM_ID', 'GEO_ID', 'Sample_ID']:\n",
    "            if _c in md.columns:\n",
    "                sample_col = _c\n",
    "                break\n",
    "        patient_col = None\n",
    "        for _c in ['patient_id', 'Patient_ID', 'PatientID']:\n",
    "            if _c in md.columns:\n",
    "                patient_col = _c\n",
    "                break\n",
    "        response_col = None\n",
    "        for _c in ['response', 'Response']:\n",
    "            if _c in md.columns:\n",
    "                response_col = _c\n",
    "                break\n",
    "        # Determine sample ID series from adata\n",
    "        sample_series = None\n",
    "        for _c in ['sample_id', 'batch']:\n",
    "            if _c in adata.obs.columns:\n",
    "                sample_series = adata.obs[_c].astype(str)\n",
    "                break\n",
    "        if md is not None and sample_series is not None and sample_col is not None:\n",
    "            sample_key = sample_series.str.split('_').str[0]\n",
    "            md_sample = md[sample_col].astype(str)\n",
    "            if patient_col is not None:\n",
    "                patient_map = dict(zip(md_sample, md[patient_col]))\n",
    "                if 'patient_id' not in adata.obs.columns:\n",
    "                    adata.obs['patient_id'] = sample_key.map(patient_map)\n",
    "                else:\n",
    "                    adata.obs['patient_id'] = adata.obs['patient_id'].where(adata.obs['patient_id'].notna(), sample_key.map(patient_map))\n",
    "            if response_col is not None:\n",
    "                resp_map = dict(zip(md_sample, md[response_col]))\n",
    "                if 'response' not in adata.obs.columns:\n",
    "                    adata.obs['response'] = sample_key.map(resp_map)\n",
    "                else:\n",
    "                    adata.obs['response'] = adata.obs['response'].where(adata.obs['response'].notna(), sample_key.map(resp_map))\n",
    "        # Normalize response labels\n",
    "        if 'response' in adata.obs.columns:\n",
    "            adata.obs['response'] = adata.obs['response'].apply(_normalize_response_value)\n",
    "        # Coverage reporting\n",
    "        if 'response' in adata.obs.columns:\n",
    "            resp_counts = adata.obs['response'].value_counts(dropna=False).to_dict()\n",
    "            print(f\"Response distribution: {resp_counts}\")\n",
    "        if 'patient_id' in adata.obs.columns:\n",
    "            mapped = adata.obs['patient_id'].notna().sum()\n",
    "            print(f\"Patient_id coverage: {mapped}/{len(adata.obs)}\")\n",
    "if '_get_supervised_mask_and_labels' not in globals():\n",
    "    def _get_supervised_mask_and_labels(adata):\n",
    "        if 'response' not in adata.obs.columns:\n",
    "            print(\"WARNING: response column missing. No supervised labels available.\")\n",
    "            supervised_mask = np.zeros(adata.n_obs, dtype=bool)\n",
    "            return supervised_mask, pd.Series([], dtype=object), None, {}, False\n",
    "        y_all = adata.obs['response'].astype(str)\n",
    "        supervised_mask = y_all.isin(['Responder', 'Non-Responder']).values\n",
    "        y_supervised = y_all[supervised_mask]\n",
    "        if len(y_supervised) == 0:\n",
    "            print(\"WARNING: No labeled samples found for supervised learning.\")\n",
    "            return supervised_mask, y_supervised, None, {}, False\n",
    "        class_counts = y_supervised.value_counts().to_dict()\n",
    "        if len(class_counts) < 2 or min(class_counts.values()) < 2:\n",
    "            print(f\"WARNING: Insufficient class balance for supervised learning: {class_counts}\")\n",
    "            return supervised_mask, y_supervised, None, class_counts, False\n",
    "        le = LabelEncoder()\n",
    "        return supervised_mask, y_supervised, le, class_counts, True\n",
    "# Ensure response/patient labels are present and normalized\n",
    "if 'adata' not in globals() or adata is None:\n",
    "    raise NameError(\"adata is not defined. Please run the data loading cells first.\")\n",
    "_ensure_response_and_patient(adata)\n",
    "supervised_mask, y_supervised, label_encoder, class_counts, SUPERVISED_AVAILABLE = _get_supervised_mask_and_labels(adata)\n",
    "globals()['SUPERVISED_AVAILABLE'] = SUPERVISED_AVAILABLE\n",
    "if SUPERVISED_AVAILABLE:\n",
    "    y_encoded = label_encoder.fit_transform(y_supervised)\n",
    "    y_encoded = _ensure_int_labels(y_encoded)\n",
    "    print(f\"Working with {int(supervised_mask.sum())} samples for supervised learning\")\n",
    "    print(f\"Class distribution: {class_counts}\")\n",
    "else:\n",
    "    print(\"WARNING: Supervised labels not available or insufficient. Skipping supervised-only steps.\")\n",
    "    supervised_mask = np.ones(adata.n_obs, dtype=bool)\n",
    "    y_supervised = pd.Series(['Unknown'] * adata.n_obs, index=adata.obs.index)\n",
    "    y_encoded = np.array([], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebef59af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:31:25.025831Z",
     "iopub.status.busy": "2026-02-13T17:31:25.025598Z",
     "iopub.status.idle": "2026-02-13T17:31:25.329440Z",
     "shell.execute_reply": "2026-02-13T17:31:25.328377Z"
    },
    "papermill": {
     "duration": 0.327033,
     "end_time": "2026-02-13T17:31:25.331691",
     "exception": false,
     "start_time": "2026-02-13T17:31:25.004658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating comprehensive feature set using ALL available encodings...\n",
      "Applying strategic dimensionality reduction to high-dimensional features...\n",
      "Response distribution: {'Non-Responder': 63074, 'Responder': 36993}\n",
      "Patient_id coverage: 100067/100067\n",
      "Working with 100067 samples for supervised learning\n",
      "Class distribution: {'Non-Responder': 63074, 'Responder': 36993}\n",
      "Reducing k-mer features by variance selection...\n",
      "TRA k-mers reduced from 1 to 1\n",
      "TRB k-mers reduced from 1 to 1\n",
      "\n",
      "Feature set dimensions:\n",
      "  • basic: (100067, 29)\n",
      "  • gene_enhanced: (100067, 79)\n",
      "  • tcr_enhanced: (100067, 31)\n",
      "  • comprehensive: (100067, 26)\n",
      "Comprehensive feature engineering completed!\n",
      "CPU times: user 279 ms, sys: 2.96 ms, total: 282 ms\n",
      "Wall time: 280 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Comprehensive Feature Engineering ---\n",
    "\n",
    "print(\"Creating comprehensive feature set using ALL available encodings...\")\n",
    "\n",
    "# --- 1. Strategic Feature Engineering with Dimensionality Reduction ---\n",
    "print(\"Applying strategic dimensionality reduction to high-dimensional features...\")\n",
    "\n",
    "# --- Label/patient derivation and supervised availability helpers ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "if '_ensure_int_labels' not in globals():\n",
    "    def _ensure_int_labels(y):\n",
    "        y_arr = np.asarray(y)\n",
    "        if np.issubdtype(y_arr.dtype, np.integer):\n",
    "            return y_arr\n",
    "        if y_arr.size == 0:\n",
    "            return y_arr.astype(np.int64)\n",
    "        if np.all(np.isfinite(y_arr)) and np.all(np.equal(y_arr, np.floor(y_arr))):\n",
    "            return y_arr.astype(np.int64)\n",
    "        raise ValueError(\"Labels must be integer or integer-like floats.\")\n",
    "\n",
    "if '_normalize_response_value' not in globals():\n",
    "    def _normalize_response_value(val):\n",
    "        if pd.isna(val):\n",
    "            return 'Unknown'\n",
    "        s = str(val).strip().lower()\n",
    "        if s == '':\n",
    "            return 'Unknown'\n",
    "        if 'non' in s and 'responder' in s:\n",
    "            return 'Non-Responder'\n",
    "        if 'responder' in s:\n",
    "            return 'Responder'\n",
    "        return 'Unknown'\n",
    "\n",
    "if '_ensure_response_and_patient' not in globals():\n",
    "    def _ensure_response_and_patient(adata):\n",
    "        # Normalize existing columns if present\n",
    "        if 'response' not in adata.obs.columns and 'Response' in adata.obs.columns:\n",
    "            adata.obs['response'] = adata.obs['Response']\n",
    "        if 'patient_id' not in adata.obs.columns:\n",
    "            for _col in ['Patient_ID', 'PatientID']:\n",
    "                if _col in adata.obs.columns:\n",
    "                    adata.obs['patient_id'] = adata.obs[_col]\n",
    "                    break\n",
    "\n",
    "        # Determine metadata mapping\n",
    "        md = None\n",
    "        if 'metadata_df' in globals() and isinstance(metadata_df, pd.DataFrame) and not metadata_df.empty:\n",
    "            md = metadata_df.copy()\n",
    "        else:\n",
    "            # Fallback to hard-coded metadata list (same as in Cell 15)\n",
    "            _metadata_list = [\n",
    "                {'S_Number': 'S1',  'GEX_Sample_ID': 'GSM9061665', 'TCR_Sample_ID': 'GSM9061687', 'Patient_ID': 'PT1',  'Timepoint': 'Baseline',   'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S2',  'GEX_Sample_ID': 'GSM9061666', 'TCR_Sample_ID': 'GSM9061688', 'Patient_ID': 'PT1',  'Timepoint': 'Post-Tx',    'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S3',  'GEX_Sample_ID': 'GSM9061667', 'TCR_Sample_ID': 'GSM9061689', 'Patient_ID': 'PT1',  'Timepoint': 'Recurrence', 'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S4',  'GEX_Sample_ID': 'GSM9061668', 'TCR_Sample_ID': 'GSM9061690', 'Patient_ID': 'PT2',  'Timepoint': 'Baseline',   'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S5',  'GEX_Sample_ID': 'GSM9061669', 'TCR_Sample_ID': 'GSM9061691', 'Patient_ID': 'PT2',  'Timepoint': 'Post-Tx',    'Response': 'Responder',     'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S6',  'GEX_Sample_ID': 'GSM9061670', 'TCR_Sample_ID': 'GSM9061692', 'Patient_ID': 'PT3',  'Timepoint': 'Baseline',   'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S7',  'GEX_Sample_ID': 'GSM9061671', 'TCR_Sample_ID': 'GSM9061693', 'Patient_ID': 'PT3',  'Timepoint': 'Post-Tx',    'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S8',  'GEX_Sample_ID': 'GSM9061672', 'TCR_Sample_ID': None,         'Patient_ID': 'PT3',  'Timepoint': 'Recurrence', 'Response': 'Non-Responder', 'In_Data': 'GEX only'},\n",
    "                {'S_Number': 'S9',  'GEX_Sample_ID': 'GSM9061673', 'TCR_Sample_ID': 'GSM9061694', 'Patient_ID': 'PT4',  'Timepoint': 'Baseline',   'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S10', 'GEX_Sample_ID': 'GSM9061674', 'TCR_Sample_ID': 'GSM9061695', 'Patient_ID': 'PT4',  'Timepoint': 'Post-Tx',    'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "                {'S_Number': 'S11', 'GEX_Sample_ID': 'GSM9061675', 'TCR_Sample_ID': 'GSM9061696', 'Patient_ID': 'PT4',  'Timepoint': 'Recurrence', 'Response': 'Non-Responder', 'In_Data': 'Yes'},\n",
    "            ]\n",
    "            md = pd.DataFrame(_metadata_list)\n",
    "\n",
    "        # Identify mapping columns in metadata\n",
    "        sample_col = None\n",
    "        for _c in ['sample_id', 'GEX_Sample_ID', 'GSM_ID', 'GEO_ID', 'Sample_ID']:\n",
    "            if _c in md.columns:\n",
    "                sample_col = _c\n",
    "                break\n",
    "        patient_col = None\n",
    "        for _c in ['patient_id', 'Patient_ID', 'PatientID']:\n",
    "            if _c in md.columns:\n",
    "                patient_col = _c\n",
    "                break\n",
    "        response_col = None\n",
    "        for _c in ['response', 'Response']:\n",
    "            if _c in md.columns:\n",
    "                response_col = _c\n",
    "                break\n",
    "\n",
    "        # Determine sample ID series from adata\n",
    "        sample_series = None\n",
    "        for _c in ['sample_id', 'batch']:\n",
    "            if _c in adata.obs.columns:\n",
    "                sample_series = adata.obs[_c].astype(str)\n",
    "                break\n",
    "\n",
    "        if md is not None and sample_series is not None and sample_col is not None:\n",
    "            sample_key = sample_series.str.split('_').str[0]\n",
    "            md_sample = md[sample_col].astype(str)\n",
    "\n",
    "            if patient_col is not None:\n",
    "                patient_map = dict(zip(md_sample, md[patient_col]))\n",
    "                if 'patient_id' not in adata.obs.columns:\n",
    "                    adata.obs['patient_id'] = sample_key.map(patient_map)\n",
    "                else:\n",
    "                    adata.obs['patient_id'] = adata.obs['patient_id'].where(adata.obs['patient_id'].notna(), sample_key.map(patient_map))\n",
    "\n",
    "            if response_col is not None:\n",
    "                resp_map = dict(zip(md_sample, md[response_col]))\n",
    "                if 'response' not in adata.obs.columns:\n",
    "                    adata.obs['response'] = sample_key.map(resp_map)\n",
    "                else:\n",
    "                    adata.obs['response'] = adata.obs['response'].where(adata.obs['response'].notna(), sample_key.map(resp_map))\n",
    "\n",
    "        # Normalize response labels\n",
    "        if 'response' in adata.obs.columns:\n",
    "            adata.obs['response'] = adata.obs['response'].apply(_normalize_response_value)\n",
    "\n",
    "        # Coverage reporting\n",
    "        if 'response' in adata.obs.columns:\n",
    "            resp_counts = adata.obs['response'].value_counts(dropna=False).to_dict()\n",
    "            print(f\"Response distribution: {resp_counts}\")\n",
    "        if 'patient_id' in adata.obs.columns:\n",
    "            mapped = adata.obs['patient_id'].notna().sum()\n",
    "            print(f\"Patient_id coverage: {mapped}/{len(adata.obs)}\")\n",
    "\n",
    "if '_get_supervised_mask_and_labels' not in globals():\n",
    "    def _get_supervised_mask_and_labels(adata):\n",
    "        if 'response' not in adata.obs.columns:\n",
    "            print(\"WARNING: response column missing. No supervised labels available.\")\n",
    "            supervised_mask = np.zeros(adata.n_obs, dtype=bool)\n",
    "            return supervised_mask, pd.Series([], dtype=object), None, {}, False\n",
    "        y_all = adata.obs['response'].astype(str)\n",
    "        supervised_mask = y_all.isin(['Responder', 'Non-Responder']).values\n",
    "        y_supervised = y_all[supervised_mask]\n",
    "        if len(y_supervised) == 0:\n",
    "            print(\"WARNING: No labeled samples found for supervised learning.\")\n",
    "            return supervised_mask, y_supervised, None, {}, False\n",
    "        class_counts = y_supervised.value_counts().to_dict()\n",
    "        if len(class_counts) < 2 or min(class_counts.values()) < 2:\n",
    "            print(f\"WARNING: Insufficient class balance for supervised learning: {class_counts}\")\n",
    "            return supervised_mask, y_supervised, None, class_counts, False\n",
    "        le = LabelEncoder()\n",
    "        return supervised_mask, y_supervised, le, class_counts, True\n",
    "\n",
    "# Ensure response/patient labels are present and normalized\n",
    "if 'adata' not in globals() or adata is None:\n",
    "    raise NameError(\"adata is not defined. Please run the data loading cells first.\")\n",
    "\n",
    "_ensure_response_and_patient(adata)\n",
    "\n",
    "supervised_mask, y_supervised, label_encoder, class_counts, SUPERVISED_AVAILABLE = _get_supervised_mask_and_labels(adata)\n",
    "globals()['SUPERVISED_AVAILABLE'] = SUPERVISED_AVAILABLE\n",
    "\n",
    "if SUPERVISED_AVAILABLE:\n",
    "    y_encoded = label_encoder.fit_transform(y_supervised)\n",
    "    y_encoded = _ensure_int_labels(y_encoded)\n",
    "    print(f\"Working with {int(supervised_mask.sum())} samples for supervised learning\")\n",
    "    print(f\"Class distribution: {class_counts}\")\n",
    "else:\n",
    "    print(\"WARNING: Supervised labels not available or insufficient. Skipping supervised-only steps.\")\n",
    "    supervised_mask = np.ones(adata.n_obs, dtype=bool)\n",
    "    y_supervised = pd.Series(['Unknown'] * adata.n_obs, index=adata.obs.index)\n",
    "    y_encoded = np.array([], dtype=np.int64)\n",
    "\n",
    "# --- Reduce high-dimensional k-mer features using variance-based selection ---\n",
    "# Check if k-mer features exist\n",
    "has_tra_kmer = 'X_tcr_tra_kmer' in adata.obsm\n",
    "has_trb_kmer = 'X_tcr_trb_kmer' in adata.obsm\n",
    "\n",
    "if has_tra_kmer:\n",
    "    tra_kmer_supervised = adata.obsm['X_tcr_tra_kmer'][supervised_mask]\n",
    "else:\n",
    "    print(\"Warning: X_tcr_tra_kmer not found. Using placeholder.\")\n",
    "    tra_kmer_supervised = np.zeros((sum(supervised_mask), 100))\n",
    "\n",
    "if has_trb_kmer:\n",
    "    trb_kmer_supervised = adata.obsm['X_tcr_trb_kmer'][supervised_mask]\n",
    "else:\n",
    "    print(\"Warning: X_tcr_trb_kmer not found. Using placeholder.\")\n",
    "    trb_kmer_supervised = np.zeros((sum(supervised_mask), 100))\n",
    "\n",
    "# Select top variance k-mers to reduce dimensionality\n",
    "def select_top_variance_features(X, n_features=200):\n",
    "    \"\"\"Select features with highest variance\"\"\"\n",
    "    variances = np.var(X, axis=0)\n",
    "    n_features = min(n_features, X.shape[1])  # Don't select more features than exist\n",
    "    top_indices = np.argsort(variances)[-n_features:]\n",
    "    return X[:, top_indices], top_indices\n",
    "\n",
    "print(\"Reducing k-mer features by variance selection...\")\n",
    "tra_kmer_reduced, tra_top_idx = select_top_variance_features(tra_kmer_supervised, n_features=200)\n",
    "trb_kmer_reduced, trb_top_idx = select_top_variance_features(trb_kmer_supervised, n_features=200)\n",
    "\n",
    "print(f\"TRA k-mers reduced from {tra_kmer_supervised.shape[1]} to {tra_kmer_reduced.shape[1]}\")\n",
    "print(f\"TRB k-mers reduced from {trb_kmer_supervised.shape[1]} to {trb_kmer_reduced.shape[1]}\")\n",
    "\n",
    "# --- 2. Create strategic feature combinations ---\n",
    "feature_sets = {}\n",
    "\n",
    "# Helper function to safely get obsm arrays\n",
    "def _get_obsm_or_zeros(adata, key, mask, n_cols):\n",
    "    if key in adata.obsm:\n",
    "        arr = adata.obsm[key][mask]\n",
    "        return arr[:, :min(n_cols, arr.shape[1])]\n",
    "    return np.zeros((sum(mask), n_cols))\n",
    "\n",
    "# Get gene features (try X_gene_pca first, then X_pca)\n",
    "if 'X_gene_pca' in adata.obsm:\n",
    "    gene_features = adata.obsm['X_gene_pca'][supervised_mask]\n",
    "elif 'X_pca' in adata.obsm:\n",
    "    gene_features = adata.obsm['X_pca'][supervised_mask]\n",
    "else:\n",
    "    print(\"Warning: No gene PCA features found.\")\n",
    "    gene_features = np.zeros((sum(supervised_mask), 50))\n",
    "\n",
    "# TCR physicochemical features\n",
    "tcr_physico_cols_tra = ['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']\n",
    "tcr_physico_cols_trb = ['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']\n",
    "\n",
    "tra_physico = adata.obs[[c for c in tcr_physico_cols_tra if c in adata.obs.columns]].fillna(0)[supervised_mask].values \\\n",
    "    if any(c in adata.obs.columns for c in tcr_physico_cols_tra) else np.zeros((sum(supervised_mask), 3))\n",
    "trb_physico = adata.obs[[c for c in tcr_physico_cols_trb if c in adata.obs.columns]].fillna(0)[supervised_mask].values \\\n",
    "    if any(c in adata.obs.columns for c in tcr_physico_cols_trb) else np.zeros((sum(supervised_mask), 3))\n",
    "\n",
    "# Ensure 3 columns each\n",
    "if tra_physico.shape[1] < 3:\n",
    "    tra_physico = np.hstack([tra_physico, np.zeros((tra_physico.shape[0], 3 - tra_physico.shape[1]))])\n",
    "if trb_physico.shape[1] < 3:\n",
    "    trb_physico = np.hstack([trb_physico, np.zeros((trb_physico.shape[0], 3 - trb_physico.shape[1]))])\n",
    "\n",
    "tcr_physico = np.column_stack([tra_physico, trb_physico])\n",
    "\n",
    "# QC features\n",
    "qc_cols = ['n_genes_by_counts', 'total_counts', 'pct_counts_mt']\n",
    "available_qc = [c for c in qc_cols if c in adata.obs.columns]\n",
    "if available_qc:\n",
    "    qc_features = adata.obs[available_qc].fillna(0)[supervised_mask].values\n",
    "else:\n",
    "    qc_features = np.zeros((sum(supervised_mask), 3))\n",
    "\n",
    "# Ensure 3 columns for QC\n",
    "if qc_features.shape[1] < 3:\n",
    "    qc_features = np.hstack([qc_features, np.zeros((qc_features.shape[0], 3 - qc_features.shape[1]))])\n",
    "\n",
    "# Basic features (gene expression + physicochemical)\n",
    "feature_sets['basic'] = np.column_stack([\n",
    "    gene_features[:, :min(20, gene_features.shape[1])],  # Top 20 gene PCA components\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# Enhanced gene expression\n",
    "feature_sets['gene_enhanced'] = np.column_stack([\n",
    "    gene_features,  # All gene PCA components\n",
    "    _get_obsm_or_zeros(adata, 'X_gene_svd', supervised_mask, 30),  # Top 30 SVD components\n",
    "    _get_obsm_or_zeros(adata, 'X_gene_umap', supervised_mask, 20),  # All 20 UMAP components\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# TCR sequence enhanced\n",
    "feature_sets['tcr_enhanced'] = np.column_stack([\n",
    "    gene_features[:, :min(20, gene_features.shape[1])],  # Top 20 gene PCA\n",
    "    tra_kmer_reduced,  # Top 200 TRA k-mers\n",
    "    trb_kmer_reduced,  # Top 200 TRB k-mers\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# Comprehensive (reduced) - Only gene PCA + top k-mers + physicochemical\n",
    "feature_sets['comprehensive'] = np.column_stack([\n",
    "    gene_features[:, :min(15, gene_features.shape[1])],  # Top 15 gene PCA\n",
    "    tra_kmer_reduced[:, :min(50, tra_kmer_reduced.shape[1])],  # Top 50 TRA k-mers\n",
    "    trb_kmer_reduced[:, :min(50, trb_kmer_reduced.shape[1])],  # Top 50 TRB k-mers\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "print(f\"\\nFeature set dimensions:\")\n",
    "for name, features in feature_sets.items():\n",
    "    print(f\"  • {name}: {features.shape}\")\n",
    "\n",
    "print(\"Comprehensive feature engineering completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dab79a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:31:25.373092Z",
     "iopub.status.busy": "2026-02-13T17:31:25.372799Z",
     "iopub.status.idle": "2026-02-13T17:31:25.782225Z",
     "shell.execute_reply": "2026-02-13T17:31:25.781541Z"
    },
    "papermill": {
     "duration": 0.43153,
     "end_time": "2026-02-13T17:31:25.783890",
     "exception": false,
     "start_time": "2026-02-13T17:31:25.352360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Correlation Analysis of Top Features ---\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Validate that adata exists\n",
    "if 'adata' not in globals() or adata is None:\n",
    "    raise NameError(\"adata is not defined. Please run the data loading cells first.\")\n",
    "\n",
    "# Ensure supervised_mask is defined\n",
    "if 'supervised_mask' not in globals():\n",
    "    if 'response' in adata.obs.columns:\n",
    "        supervised_mask = adata.obs['response'].isin(['Responder', 'Non-Responder']).values\n",
    "    elif 'Response' in adata.obs.columns:\n",
    "        supervised_mask = adata.obs['Response'].isin(['Responder', 'Non-Responder']).values\n",
    "    else:\n",
    "        supervised_mask = np.ones(adata.n_obs, dtype=bool)\n",
    "        print(\"Warning: No response column found. Using all cells.\")\n",
    "\n",
    "# Ensure tcr_physico and qc_features are defined\n",
    "if 'tcr_physico' not in globals():\n",
    "    # Extract TRA physicochemical features\n",
    "    tra_cols = ['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']\n",
    "    if all(col in adata.obs.columns for col in tra_cols):\n",
    "        tra_physico = adata.obs[tra_cols].fillna(0)[supervised_mask].values\n",
    "    else:\n",
    "        tra_physico = np.zeros((np.sum(supervised_mask), 3))\n",
    "    \n",
    "    # Extract TRB physicochemical features\n",
    "    trb_cols = ['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']\n",
    "    if all(col in adata.obs.columns for col in trb_cols):\n",
    "        trb_physico = adata.obs[trb_cols].fillna(0)[supervised_mask].values\n",
    "    else:\n",
    "        trb_physico = np.zeros((np.sum(supervised_mask), 3))\n",
    "    \n",
    "    # Combine TRA and TRB features\n",
    "    tcr_physico = np.hstack([tra_physico, trb_physico])\n",
    "\n",
    "if 'qc_features' not in globals():\n",
    "    qc_cols = ['n_genes_by_counts', 'total_counts', 'pct_counts_mt']\n",
    "    available_qc = [col for col in qc_cols if col in adata.obs.columns]\n",
    "    if available_qc:\n",
    "        qc_features = adata.obs[available_qc].fillna(0)[supervised_mask].values\n",
    "    else:\n",
    "        qc_features = np.zeros((np.sum(supervised_mask), 3))\n",
    "\n",
    "# Select a subset of features for the heatmap\n",
    "# We'll take the top 10 Gene PCs, top 5 physicochemical, and QC metrics\n",
    "# Ensure we have the data available\n",
    "if 'X_gene_pca' in adata.obsm:\n",
    "    gene_pcs = adata.obsm['X_gene_pca'][supervised_mask][:, :min(10, adata.obsm['X_gene_pca'].shape[1])]\n",
    "    gene_names = [f\"Gene_PC{i+1}\" for i in range(gene_pcs.shape[1])]\n",
    "elif 'X_pca' in adata.obsm:\n",
    "    gene_pcs = adata.obsm['X_pca'][supervised_mask][:, :min(10, adata.obsm['X_pca'].shape[1])]\n",
    "    gene_names = [f\"Gene_PC{i+1}\" for i in range(gene_pcs.shape[1])]\n",
    "else:\n",
    "    gene_pcs = np.zeros((np.sum(supervised_mask), 10))\n",
    "    gene_names = [f\"Placeholder_PC{i+1}\" for i in range(10)]\n",
    "\n",
    "heatmap_features = np.column_stack([\n",
    "    gene_pcs,\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "heatmap_feature_names = gene_names + \\\n",
    "                        ['TRA_Len', 'TRA_MW', 'TRA_Hydro', 'TRB_Len', 'TRB_MW', 'TRB_Hydro'] + \\\n",
    "                        ['n_genes', 'total_counts', 'pct_mt']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = np.corrcoef(heatmap_features, rowvar=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', center=0,\n",
    "            xticklabels=heatmap_feature_names, yticklabels=heatmap_feature_names,\n",
    "            linewidths=0.5, linecolor='gray', cbar_kws={\"shrink\": .8})\n",
    "plt.title(\"Feature Correlation Matrix (Top Gene PCs + TCR Features)\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd765a7c",
   "metadata": {
    "papermill": {
     "duration": 0.01961,
     "end_time": "2026-02-13T17:31:25.824034",
     "exception": false,
     "start_time": "2026-02-13T17:31:25.804424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Supervised Classification of Immunotherapy Response\n",
    "The core predictive task was formulated as a binary classification problem: predicting the patient response label (Responder vs. Non-Responder) for each individual cell. We evaluated a diverse suite of algorithms:\n",
    "*   **Logistic Regression:** A linear baseline model.\n",
    "*   **Decision Trees:** A simple, interpretable non-linear model.\n",
    "*   **Random Forest:** An ensemble of decision trees that reduces overfitting.\n",
    "*   **XGBoost (Extreme Gradient Boosting):** A highly optimized gradient boosting framework known for strong performance on tabular data.\n",
    "\n",
    "### Experimental Setup\n",
    "We designed our experiments to isolate the predictive value of different data modalities. We trained and evaluated models on four nested feature sets:\n",
    "1.  **Baseline:** Technical covariates only (e.g., mitochondrial percentage, library size).\n",
    "2.  **Gene-Enhanced:** Baseline + Gene Expression PCs.\n",
    "3.  **TCR-Enhanced:** Baseline + TCR Encodings (One-hot, K-mer, Physicochemical).\n",
    "4.  **Comprehensive:** Baseline + Gene Expression PCs + TCR Encodings.\n",
    "\n",
    "### Validation Strategy (Updated)\n",
    "To obtain patient-level generalization estimates and to avoid data leakage between cells from the same patient, we use a Leave-One-Patient-Out (LOPO) cross-validation as the outer evaluation loop. Hyperparameter tuning is performed within the training partitions using GroupKFold (grouped by patient) when possible, falling back to stratified folds only when the number of training patients is too small for grouped splits. Feature scaling and imputation are fit on training partitions only and applied to held-out patient data to ensure leakage-free evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1869a5b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:31:25.864240Z",
     "iopub.status.busy": "2026-02-13T17:31:25.863540Z",
     "iopub.status.idle": "2026-02-13T17:31:28.994471Z",
     "shell.execute_reply": "2026-02-13T17:31:28.993432Z"
    },
    "papermill": {
     "duration": 3.153444,
     "end_time": "2026-02-13T17:31:28.996672",
     "exception": false,
     "start_time": "2026-02-13T17:31:25.843228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.15.3)\r\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.0.2)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "897a0c93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:31:29.042065Z",
     "iopub.status.busy": "2026-02-13T17:31:29.041748Z",
     "iopub.status.idle": "2026-02-13T18:33:54.390048Z",
     "shell.execute_reply": "2026-02-13T18:33:54.389399Z"
    },
    "papermill": {
     "duration": 3745.373434,
     "end_time": "2026-02-13T18:33:54.391883",
     "exception": false,
     "start_time": "2026-02-13T17:31:29.018449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting patient-level LOPO CV with leakage-safe pipelines (Optimized for Speed/Accuracy)...\n",
      "Supervised patients: 4 -> ['PT1' 'PT2' 'PT3' 'PT4']\n",
      "Per-patient response counts:\n",
      "response\n",
      "Responder        2\n",
      "Non-Responder    2\n",
      "Name: count, dtype: int64\n",
      "Cleaning up temporary variables and large matrices before ML.\n",
      "Patched models_eval['XGBoost'] to use GPU (method=device).\n",
      "Patched models_eval['Random Forest'] to use n_jobs=-1.\n",
      "Patched param_grids['XGBoost'] with GPU options (method=device).\n",
      "Patched models_eval['XGBoost'] to use GPU (method=device).\n",
      "Patched models_eval['Random Forest'] to use n_jobs=-1.\n",
      "Patched param_grids['XGBoost'] with GPU options (method=device).\n",
      "\n",
      "=== Feature set: basic (shape=(100067, 29)) ===\n",
      "LOPO fold 1/4 -- held patient(s): ['PT1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:26] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:26] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:26] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:26] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [17:34:27] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [17:34:27] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [17:34:27] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [17:34:27] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:27] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:27] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:27] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:27] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:28] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:28] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:29] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:29] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:29] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:31] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:31] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:31] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:31] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:34:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOPO fold 2/4 -- held patient(s): ['PT2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [17:38:06] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:13] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:13] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:13] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:13] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:14] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:14] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:14] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:14] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:15] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:15] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:15] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:15] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:38:16] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOPO fold 3/4 -- held patient(s): ['PT3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [17:40:54] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:40:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:01] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:01] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:01] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:01] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:01] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:01] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:41:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOPO fold 4/4 -- held patient(s): ['PT4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [17:43:48] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:48] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:48] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:48] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [17:43:50] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:51] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:51] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:51] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:51] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:52] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:52] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:53] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:53] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:53] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:53] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:53] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:43:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature set: gene_enhanced (shape=(100067, 79)) ===\n",
      "LOPO fold 1/4 -- held patient(s): ['PT1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:21] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:21] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:21] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:21] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:23] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:23] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:23] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:23] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:25] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:25] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:25] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:25] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:26] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:27] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:27] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:27] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:28] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:28] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:29] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:29] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:29] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:31] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:31] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:38] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:38] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:50:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOPO fold 2/4 -- held patient(s): ['PT2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:17] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:17] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:17] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:18] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [17:58:18] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:19] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:20] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:20] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:21] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:22] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:22] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:22] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [17:58:23] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:23] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:24] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:24] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:24] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:25] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:26] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:26] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:26] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:27] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:28] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:28] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:28] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:29] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:38] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:38] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:38] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:38] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:40] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:40] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:40] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:58:40] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOPO fold 3/4 -- held patient(s): ['PT3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:41] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:41] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:41] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:41] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:42] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:42] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:44] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:44] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:44] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:48] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:48] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:04:51] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOPO fold 4/4 -- held patient(s): ['PT4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:53] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:10:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:01] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:11:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature set: tcr_enhanced (shape=(100067, 31)) ===\n",
      "LOPO fold 1/4 -- held patient(s): ['PT1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:13:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:13:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:13:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:13:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:13:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:13:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:13:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:13:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:01] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:01] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:01] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:14:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOPO fold 2/4 -- held patient(s): ['PT2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:25] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:25] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:25] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:25] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:26] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:26] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:26] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:26] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:28] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:28] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:28] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:28] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:29] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:29] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:29] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:29] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:30] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:31] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:31] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:31] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:31] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:31] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:33] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:34] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:17:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOPO fold 3/4 -- held patient(s): ['PT3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [18:20:08] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:13] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:13] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:13] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:13] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:14] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:14] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:15] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:15] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:15] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:15] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:16] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:16] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:16] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:16] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:17] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:17] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:17] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:17] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:18] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:18] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:18] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:18] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:20:18] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOPO fold 4/4 -- held patient(s): ['PT4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:58] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:22:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:00] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:01] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:01] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:03] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:23:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature set: comprehensive (shape=(100067, 26)) ===\n",
      "LOPO fold 1/4 -- held patient(s): ['PT1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [18:25:37] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:38] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:38] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:38] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:38] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:38] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:38] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:40] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:40] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:41] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:41] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:41] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:41] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:41] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:41] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:42] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:42] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:42] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:44] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:44] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:44] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [18:25:45] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:25:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOPO fold 2/4 -- held patient(s): ['PT2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:48] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:48] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:48] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:51] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:51] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:52] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:52] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:52] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:52] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:52] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:53] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:53] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:53] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:56] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:28:57] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOPO fold 3/4 -- held patient(s): ['PT3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:13] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:13] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:13] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:13] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:14] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:14] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:14] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:14] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:16] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:16] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:16] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:16] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:17] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:17] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:17] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:17] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:17] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:18] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:18] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:18] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:18] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:18] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:18] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:19] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:19] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:19] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:19] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:19] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:20] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:20] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:20] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:21] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:21] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:21] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:21] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:21] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:21] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:22] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:22] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:22] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:23] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:23] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:23] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:23] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:31:23] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOPO fold 4/4 -- held patient(s): ['PT4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:44] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:44] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:44] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:44] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:44] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:48] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:48] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:48] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:48] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:48] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:49] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:50] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:51] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:51] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:51] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:51] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:52] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:52] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:52] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:52] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:52] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [18:33:52] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOPO results saved to: Processed_Data/lopo_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_set</th>\n",
       "      <th>model</th>\n",
       "      <th>evaluation_level</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "      <th>specificity</th>\n",
       "      <th>npv</th>\n",
       "      <th>n_patients</th>\n",
       "      <th>n_cells</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>basic</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.884238</td>\n",
       "      <td>0.853030</td>\n",
       "      <td>0.829833</td>\n",
       "      <td>0.841272</td>\n",
       "      <td>0.940367</td>\n",
       "      <td>0.916146</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>basic</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>basic</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.841616</td>\n",
       "      <td>0.792400</td>\n",
       "      <td>0.774471</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.880880</td>\n",
       "      <td>0.880997</td>\n",
       "      <td>0.869459</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>basic</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.861373</td>\n",
       "      <td>0.840025</td>\n",
       "      <td>0.772038</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.933740</td>\n",
       "      <td>0.913768</td>\n",
       "      <td>0.872359</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>basic</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>basic</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.898688</td>\n",
       "      <td>0.874181</td>\n",
       "      <td>0.847998</td>\n",
       "      <td>0.860891</td>\n",
       "      <td>0.961072</td>\n",
       "      <td>0.928417</td>\n",
       "      <td>0.912390</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>basic</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gene_enhanced</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.902795</td>\n",
       "      <td>0.880067</td>\n",
       "      <td>0.853351</td>\n",
       "      <td>0.866503</td>\n",
       "      <td>0.958879</td>\n",
       "      <td>0.931794</td>\n",
       "      <td>0.915494</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gene_enhanced</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gene_enhanced</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.836709</td>\n",
       "      <td>0.784720</td>\n",
       "      <td>0.769362</td>\n",
       "      <td>0.776965</td>\n",
       "      <td>0.866870</td>\n",
       "      <td>0.876209</td>\n",
       "      <td>0.866265</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gene_enhanced</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gene_enhanced</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.859684</td>\n",
       "      <td>0.834071</td>\n",
       "      <td>0.774525</td>\n",
       "      <td>0.803196</td>\n",
       "      <td>0.929647</td>\n",
       "      <td>0.909630</td>\n",
       "      <td>0.873073</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gene_enhanced</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gene_enhanced</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.898278</td>\n",
       "      <td>0.876707</td>\n",
       "      <td>0.843457</td>\n",
       "      <td>0.859761</td>\n",
       "      <td>0.961282</td>\n",
       "      <td>0.930431</td>\n",
       "      <td>0.910185</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gene_enhanced</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tcr_enhanced</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.884238</td>\n",
       "      <td>0.853030</td>\n",
       "      <td>0.829833</td>\n",
       "      <td>0.841272</td>\n",
       "      <td>0.940367</td>\n",
       "      <td>0.916146</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tcr_enhanced</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tcr_enhanced</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.841376</td>\n",
       "      <td>0.791487</td>\n",
       "      <td>0.775120</td>\n",
       "      <td>0.783218</td>\n",
       "      <td>0.879155</td>\n",
       "      <td>0.880236</td>\n",
       "      <td>0.869688</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tcr_enhanced</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tcr_enhanced</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.859914</td>\n",
       "      <td>0.838455</td>\n",
       "      <td>0.769281</td>\n",
       "      <td>0.802380</td>\n",
       "      <td>0.932283</td>\n",
       "      <td>0.913070</td>\n",
       "      <td>0.870928</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tcr_enhanced</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tcr_enhanced</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.898688</td>\n",
       "      <td>0.874181</td>\n",
       "      <td>0.847998</td>\n",
       "      <td>0.860891</td>\n",
       "      <td>0.961072</td>\n",
       "      <td>0.928417</td>\n",
       "      <td>0.912390</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tcr_enhanced</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>comprehensive</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.882089</td>\n",
       "      <td>0.849858</td>\n",
       "      <td>0.827184</td>\n",
       "      <td>0.838368</td>\n",
       "      <td>0.937990</td>\n",
       "      <td>0.914291</td>\n",
       "      <td>0.900204</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>comprehensive</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>comprehensive</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.841416</td>\n",
       "      <td>0.791366</td>\n",
       "      <td>0.775471</td>\n",
       "      <td>0.783338</td>\n",
       "      <td>0.880828</td>\n",
       "      <td>0.880093</td>\n",
       "      <td>0.869847</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>comprehensive</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>comprehensive</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.870597</td>\n",
       "      <td>0.850006</td>\n",
       "      <td>0.789230</td>\n",
       "      <td>0.818491</td>\n",
       "      <td>0.940047</td>\n",
       "      <td>0.918318</td>\n",
       "      <td>0.881359</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>comprehensive</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>comprehensive</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.897599</td>\n",
       "      <td>0.872196</td>\n",
       "      <td>0.847133</td>\n",
       "      <td>0.859482</td>\n",
       "      <td>0.960548</td>\n",
       "      <td>0.927197</td>\n",
       "      <td>0.911829</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>comprehensive</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_set                model evaluation_level  accuracy  precision  \\\n",
       "0           basic  Logistic Regression             cell  0.884238   0.853030   \n",
       "1           basic  Logistic Regression          patient  1.000000   1.000000   \n",
       "2           basic        Decision Tree             cell  0.841616   0.792400   \n",
       "3           basic        Decision Tree          patient  1.000000   1.000000   \n",
       "4           basic        Random Forest             cell  0.861373   0.840025   \n",
       "5           basic        Random Forest          patient  1.000000   1.000000   \n",
       "6           basic              XGBoost             cell  0.898688   0.874181   \n",
       "7           basic              XGBoost          patient  1.000000   1.000000   \n",
       "8   gene_enhanced  Logistic Regression             cell  0.902795   0.880067   \n",
       "9   gene_enhanced  Logistic Regression          patient  1.000000   1.000000   \n",
       "10  gene_enhanced        Decision Tree             cell  0.836709   0.784720   \n",
       "11  gene_enhanced        Decision Tree          patient  1.000000   1.000000   \n",
       "12  gene_enhanced        Random Forest             cell  0.859684   0.834071   \n",
       "13  gene_enhanced        Random Forest          patient  1.000000   1.000000   \n",
       "14  gene_enhanced              XGBoost             cell  0.898278   0.876707   \n",
       "15  gene_enhanced              XGBoost          patient  1.000000   1.000000   \n",
       "16   tcr_enhanced  Logistic Regression             cell  0.884238   0.853030   \n",
       "17   tcr_enhanced  Logistic Regression          patient  1.000000   1.000000   \n",
       "18   tcr_enhanced        Decision Tree             cell  0.841376   0.791487   \n",
       "19   tcr_enhanced        Decision Tree          patient  1.000000   1.000000   \n",
       "20   tcr_enhanced        Random Forest             cell  0.859914   0.838455   \n",
       "21   tcr_enhanced        Random Forest          patient  1.000000   1.000000   \n",
       "22   tcr_enhanced              XGBoost             cell  0.898688   0.874181   \n",
       "23   tcr_enhanced              XGBoost          patient  1.000000   1.000000   \n",
       "24  comprehensive  Logistic Regression             cell  0.882089   0.849858   \n",
       "25  comprehensive  Logistic Regression          patient  1.000000   1.000000   \n",
       "26  comprehensive        Decision Tree             cell  0.841416   0.791366   \n",
       "27  comprehensive        Decision Tree          patient  1.000000   1.000000   \n",
       "28  comprehensive        Random Forest             cell  0.870597   0.850006   \n",
       "29  comprehensive        Random Forest          patient  1.000000   1.000000   \n",
       "30  comprehensive              XGBoost             cell  0.897599   0.872196   \n",
       "31  comprehensive              XGBoost          patient  1.000000   1.000000   \n",
       "\n",
       "      recall        f1       auc  specificity       npv  n_patients  n_cells  \n",
       "0   0.829833  0.841272  0.940367     0.916146  0.901763           4   100067  \n",
       "1   1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "2   0.774471  0.783333  0.880880     0.880997  0.869459           4   100067  \n",
       "3   1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "4   0.772038  0.804598  0.933740     0.913768  0.872359           4   100067  \n",
       "5   1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "6   0.847998  0.860891  0.961072     0.928417  0.912390           4   100067  \n",
       "7   1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "8   0.853351  0.866503  0.958879     0.931794  0.915494           4   100067  \n",
       "9   1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "10  0.769362  0.776965  0.866870     0.876209  0.866265           4   100067  \n",
       "11  1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "12  0.774525  0.803196  0.929647     0.909630  0.873073           4   100067  \n",
       "13  1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "14  0.843457  0.859761  0.961282     0.930431  0.910185           4   100067  \n",
       "15  1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "16  0.829833  0.841272  0.940367     0.916146  0.901763           4   100067  \n",
       "17  1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "18  0.775120  0.783218  0.879155     0.880236  0.869688           4   100067  \n",
       "19  1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "20  0.769281  0.802380  0.932283     0.913070  0.870928           4   100067  \n",
       "21  1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "22  0.847998  0.860891  0.961072     0.928417  0.912390           4   100067  \n",
       "23  1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "24  0.827184  0.838368  0.937990     0.914291  0.900204           4   100067  \n",
       "25  1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "26  0.775471  0.783338  0.880828     0.880093  0.869847           4   100067  \n",
       "27  1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "28  0.789230  0.818491  0.940047     0.918318  0.881359           4   100067  \n",
       "29  1.000000  1.000000  1.000000          NaN       NaN           4   100067  \n",
       "30  0.847133  0.859482  0.960548     0.927197  0.911829           4   100067  \n",
       "31  1.000000  1.000000  1.000000          NaN       NaN           4   100067  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 39s, sys: 5.28 s, total: 17min 45s\n",
      "Wall time: 1h 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Patient-level LOPO CV (Leakage-safe) [OPTIMIZED] ---\n",
    "print(\"Starting patient-level LOPO CV with leakage-safe pipelines (Optimized for Speed/Accuracy)...\")\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut, GroupKFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from pathlib import Path\n",
    "import gc, time, joblib\n",
    "\n",
    "# --- Optimization Settings ---\n",
    "USE_RANDOM_SEARCH = True  # Use RandomizedSearchCV for speed\n",
    "N_ITER_SEARCH = 15        # Max hyperparameter combinations to try per fold\n",
    "N_JOBS_CV = -1            # Parallelize Cross-Validation (uses all cores)\n",
    "N_JOBS_MODEL = 1          # Single thread per model to avoid contention\n",
    "\n",
    "# Prepare grouping variable (patient) and supervised mask\n",
    "# Robust column detection for patient_id\n",
    "patient_id_col = None\n",
    "if 'patient_id' in adata.obs.columns:\n",
    "    patient_id_col = 'patient_id'\n",
    "elif 'Patient_ID' in adata.obs.columns:\n",
    "    adata.obs['patient_id'] = adata.obs['Patient_ID']  # Create lowercase copy\n",
    "    patient_id_col = 'patient_id'\n",
    "elif 'PatientID' in adata.obs.columns:\n",
    "    adata.obs['patient_id'] = adata.obs['PatientID']  # Create lowercase copy\n",
    "    patient_id_col = 'patient_id'\n",
    "else:\n",
    "    # Fallback 1: infer from sample_id using metadata_df\n",
    "    sample_col = None\n",
    "    for _c in ['sample_id', 'Sample_ID', 'GEX_Sample_ID', 'sample', 'Sample']:\n",
    "        if _c in adata.obs.columns:\n",
    "            sample_col = _c\n",
    "            break\n",
    "\n",
    "    if sample_col is not None and 'metadata_df' in globals():\n",
    "        if 'GEX_Sample_ID' in metadata_df.columns and 'Patient_ID' in metadata_df.columns:\n",
    "            sample_to_patient = (\n",
    "                metadata_df[['GEX_Sample_ID', 'Patient_ID']]\n",
    "                .dropna()\n",
    "                .drop_duplicates()\n",
    "                .set_index('GEX_Sample_ID')['Patient_ID']\n",
    "            )\n",
    "            adata.obs['patient_id'] = adata.obs[sample_col].map(sample_to_patient)\n",
    "        else:\n",
    "            md_cols = {c.lower(): c for c in metadata_df.columns}\n",
    "            if 'gex_sample_id' in md_cols and 'patient_id' in md_cols:\n",
    "                sample_to_patient = (\n",
    "                    metadata_df[[md_cols['gex_sample_id'], md_cols['patient_id']]]\n",
    "                    .dropna()\n",
    "                    .drop_duplicates()\n",
    "                    .set_index(md_cols['gex_sample_id'])[md_cols['patient_id']]\n",
    "                )\n",
    "                adata.obs['patient_id'] = adata.obs[sample_col].map(sample_to_patient)\n",
    "\n",
    "    # Fallback 2: parse patient id from sample_id strings (e.g., \"PT1\")\n",
    "    if 'patient_id' not in adata.obs.columns or adata.obs['patient_id'].isna().all():\n",
    "        if sample_col is not None:\n",
    "            adata.obs['patient_id'] = adata.obs[sample_col].astype(str).str.extract(r'(PT\\d+)')[0]\n",
    "\n",
    "    if 'patient_id' in adata.obs.columns and adata.obs['patient_id'].notna().any():\n",
    "        patient_id_col = 'patient_id'\n",
    "    elif adata.n_obs == 0:\n",
    "        adata.obs['patient_id'] = pd.Series(index=adata.obs.index, dtype='object')\n",
    "        patient_id_col = 'patient_id'\n",
    "    else:\n",
    "        raise KeyError(\n",
    "            \"No patient ID column found. Tried direct columns, metadata_df mapping, and parsing from sample_id.\"\n",
    "        )\n",
    "\n",
    "groups_all = np.array(adata.obs[patient_id_col][supervised_mask])\n",
    "unique_patients = np.unique(groups_all)\n",
    "print(f\"Supervised patients: {len(unique_patients)} -> {unique_patients}\")\n",
    "\n",
    "# --- EARLY VALIDATION: Check for empty supervised set ---\n",
    "if len(groups_all) == 0 or len(unique_patients) == 0:\n",
    "    print(\"WARNING: No supervised samples found (supervised_mask is empty).\")\n",
    "    print(\"Skipping patient-level LOPO CV and deep learning evaluation to prevent memory waste and errors.\")\n",
    "    print(\"This can happen if no samples have valid 'response' annotations.\")\n",
    "    lopo_summary_rows = []\n",
    "    dl_results_rows = []\n",
    "else:\n",
    "    # Per-patient response summary\n",
    "    patient_response_df = (\n",
    "        adata.obs[supervised_mask][[patient_id_col, 'response']]\n",
    "        .reset_index()\n",
    "        .drop_duplicates(subset=patient_id_col)\n",
    "        .set_index(patient_id_col)\n",
    "    )\n",
    "    print(\"Per-patient response counts:\")\n",
    "    print(patient_response_df['response'].value_counts())\n",
    "\n",
    "    # --- Memory cleanup ---\n",
    "    _start_cleanup = time.time()\n",
    "    print(\"Cleaning up temporary variables and large matrices before ML.\")\n",
    "    # Flags (defaults)\n",
    "    DROP_ONEHOT_OBSM = False\n",
    "    DROP_RAW = False\n",
    "    DROP_OBSM_UMAP_TSNE = True\n",
    "\n",
    "    _vars_to_delete = [\n",
    "        'tra_onehot','trb_onehot','tra_onehot_flat','trb_onehot_flat',\n",
    "        'onehot_tra_reduced','onehot_trb_reduced','onehot_trb_pca','onehot_trb_reduced_new',\n",
    "        'tmp','tmp1','tmp2','seq_scaler','seq_scaler_full','seq_scaler_flat','length_results'\n",
    "    ]\n",
    "    for _v in _vars_to_delete:\n",
    "        if _v in globals():\n",
    "            try:\n",
    "                del globals()[_v]\n",
    "            except Exception: pass\n",
    "\n",
    "    try:\n",
    "        if hasattr(adata, 'obsp'):\n",
    "            for _k in list(adata.obsp.keys()): \n",
    "                try: del adata.obsp[_k]\n",
    "                except: pass\n",
    "        for _k in ['neighbors', 'umap']:\n",
    "            if _k in adata.uns: \n",
    "                try: del adata.uns[_k]\n",
    "                except: pass\n",
    "        if DROP_OBSM_UMAP_TSNE:\n",
    "            for _key in list(adata.obsm.keys()):\n",
    "                _lk = _key.lower()\n",
    "                if 'umap' in _lk or 'tsne' in _lk or (_lk == 'x_pca' and 'x_gene_pca' not in _lk):\n",
    "                    try: del adata.obsm[_key]\n",
    "                    except: pass\n",
    "        if DROP_ONEHOT_OBSM:\n",
    "            for _key in ['X_tcr_tra_onehot', 'X_tcr_trb_onehot']:\n",
    "                 if _key in adata.obsm: \n",
    "                     try: del adata.obsm[_key]\n",
    "                     except: pass\n",
    "        if DROP_RAW and getattr(adata, 'raw', None) is not None:\n",
    "             adata.raw = None\n",
    "    except Exception as _e:\n",
    "        print('Error while pruning adata structures:', _e)\n",
    "\n",
    "    try:\n",
    "        import tensorflow.keras.backend as K\n",
    "        K.clear_session()\n",
    "    except Exception: pass\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Define Models & Optimized Hyperparameters ---\n",
    "    # Defined here to ensure robust execution without dependency on other cells\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {'C': [0.1, 1, 10], 'penalty': ['l2'], 'solver': ['liblinear']},\n",
    "        'Decision Tree': {'max_depth': [5, 10], 'min_samples_split': [5, 10], 'min_samples_leaf': [2, 4]},\n",
    "        'Random Forest': {'n_estimators': [100], 'max_depth': [10, 20], 'min_samples_split': [5, 10]}, # Reduced grid\n",
    "        'XGBoost': {\n",
    "            'max_depth': [3, 5], \n",
    "            'learning_rate': [0.05, 0.1], \n",
    "            'subsample': [0.8, 1.0], \n",
    "            'colsample_bytree': [0.8, 1.0], \n",
    "            'n_estimators': [100]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    models_eval = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, solver='liblinear'),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=N_JOBS_MODEL),\n",
    "        'XGBoost': (lambda: (globals().get('XGBClassifierSK', xgb.XGBClassifier)(\n",
    "            random_state=42, \n",
    "            use_label_encoder=False, \n",
    "            eval_metric='logloss',\n",
    "            n_jobs=N_JOBS_MODEL,\n",
    "            **({'tree_method':'gpu_hist','predictor':'gpu_predictor'} \n",
    "               if globals().get('XGBOOST_GPU_AVAILABLE', False) \n",
    "               else {'tree_method':'hist'}) # Optimization: Use 'hist' on CPU which is much faster than 'exact'\n",
    "        )))()\n",
    "    }\n",
    "    _apply_gpu_patches()\n",
    "\n",
    "    # Adapt param_grids to pipeline format (prefix 'clf__')\n",
    "    param_grid_pipeline = {m: {f'clf__{k}': v for k, v in g.items()} for m, g in param_grids.items()}\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "    lopo_summary_rows = []\n",
    "\n",
    "    # Iterate feature sets\n",
    "    for feature_name, X_features in feature_sets.items():\n",
    "        print(f\"\\n=== Feature set: {feature_name} (shape={X_features.shape}) ===\")\n",
    "        X = X_features\n",
    "        y = y_encoded\n",
    "        groups = groups_all\n",
    "\n",
    "        accum = {m: {'y_true': [], 'y_pred': [], 'y_proba': [], 'groups': []} for m in models_eval.keys()}\n",
    "\n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(logo.split(X, y, groups)):\n",
    "            held_patient = np.unique(groups[test_idx])\n",
    "            print(f\"LOPO fold {fold_idx+1}/{len(unique_patients)} -- held patient(s): {held_patient}\")\n",
    "\n",
    "            X_tr, X_te = X[train_idx], X[test_idx]\n",
    "            y_tr, y_te = y[train_idx], y[test_idx]\n",
    "            groups_tr = groups[train_idx]\n",
    "            \n",
    "            n_train_groups = len(np.unique(groups_tr))\n",
    "            inner_n_splits = min(3, n_train_groups) if n_train_groups >= 2 else 1\n",
    "\n",
    "            for model_name, base_model in models_eval.items():\n",
    "                pipeline = Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='mean')),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('clf', base_model)\n",
    "                ])\n",
    "\n",
    "                # Hyperparameter tuning\n",
    "                # Use RandomizedSearchCV to cap the maximum time spent on regular algorithms\n",
    "                if model_name in param_grid_pipeline:\n",
    "                    # Determine strategy\n",
    "                    grid_params = param_grid_pipeline[model_name]\n",
    "                    grid_size = np.prod([len(v) for v in grid_params.values()])\n",
    "                    \n",
    "                    # If grid is small enough, use GridSearch. If large, use RandomizedSearchCV\n",
    "                    if USE_RANDOM_SEARCH and grid_size > N_ITER_SEARCH:\n",
    "                        search_impl = RandomizedSearchCV(pipeline, grid_params, n_iter=N_ITER_SEARCH, \n",
    "                                                       cv=inner_n_splits if inner_n_splits > 1 else StratifiedKFold(3),\n",
    "                                                       scoring='accuracy', n_jobs=N_JOBS_CV, random_state=42)\n",
    "                    else:\n",
    "                        search_impl = GridSearchCV(pipeline, grid_params, \n",
    "                                                 cv=inner_n_splits if inner_n_splits > 1 else StratifiedKFold(3),\n",
    "                                                 scoring='accuracy', n_jobs=N_JOBS_CV)\n",
    "\n",
    "                    # Fit\n",
    "                    if inner_n_splits >= 2:\n",
    "                        search_impl.fit(X_tr, y_tr, groups=groups_tr)\n",
    "                    else: \n",
    "                        # Fallback for few groups\n",
    "                        search_impl.fit(X_tr, y_tr)\n",
    "                        \n",
    "                    best_model = search_impl.best_estimator_\n",
    "                else:\n",
    "                    best_model = pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "                # Save model weights\n",
    "                try:\n",
    "                    model_dir = Path('Output/Models/LOPO')\n",
    "                    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    sanitized_model_name = model_name.replace(' ', '_')\n",
    "                    model_filename = model_dir / f'model_{sanitized_model_name}_{feature_name}_fold{fold_idx}.joblib'\n",
    "                    joblib.dump(best_model, model_filename)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to save model {model_name}: {e}\")\n",
    "\n",
    "                # Predict\n",
    "                y_pred = best_model.predict(X_te)\n",
    "                try:\n",
    "                    y_pred_proba = best_model.predict_proba(X_te)[:, 1]\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        d = best_model.decision_function(X_te)\n",
    "                        y_pred_proba = d[:, 1] if d.ndim > 1 else d\n",
    "                    except:\n",
    "                        y_pred_proba = np.zeros(len(y_pred))\n",
    "\n",
    "                # Accumulate\n",
    "                accum[model_name]['y_true'].extend(y_te.tolist())\n",
    "                accum[model_name]['y_pred'].extend(y_pred.tolist())\n",
    "                accum[model_name]['y_proba'].extend(y_pred_proba.tolist())\n",
    "                accum[model_name]['groups'].extend(groups[test_idx].tolist())\n",
    "\n",
    "        # --- Aggregation & Reporting ---\n",
    "        for model_name, data_dict in accum.items():\n",
    "            y_true_all = np.array(data_dict['y_true'])\n",
    "            y_pred_all = np.array(data_dict['y_pred'])\n",
    "            y_proba_all = np.array(data_dict['y_proba'])\n",
    "            groups_all_pred = np.array(data_dict.get('groups', []), dtype=object)\n",
    "\n",
    "            if len(y_true_all) == 0: continue\n",
    "\n",
    "            # Cell-level metrics\n",
    "            acc = accuracy_score(y_true_all, y_pred_all)\n",
    "            prec = precision_score(y_true_all, y_pred_all, zero_division=0)\n",
    "            rec = recall_score(y_true_all, y_pred_all, zero_division=0)\n",
    "            f1s = f1_score(y_true_all, y_pred_all, zero_division=0)\n",
    "            try: auc = roc_auc_score(y_true_all, y_proba_all)\n",
    "            except: auc = float('nan')\n",
    "            cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "            if cm.size == 4:\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "                spec = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "                npv = tn / (tn + fn) if (tn + fn) > 0 else float('nan')\n",
    "            else: spec, npv = float('nan'), float('nan')\n",
    "\n",
    "            lopo_summary_rows.append({\n",
    "                'feature_set': feature_name, 'model': model_name, 'evaluation_level': 'cell',\n",
    "                'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1s, 'auc': auc,\n",
    "                'specificity': spec, 'npv': npv, 'n_patients': len(unique_patients), 'n_cells': X_features.shape[0]\n",
    "            })\n",
    "\n",
    "            # Patient-level aggregation\n",
    "            try:\n",
    "                pred_df = pd.DataFrame({'patient': groups_all_pred, 'y_true': y_true_all, 'y_proba': y_proba_all})\n",
    "                patient_summary = pred_df.groupby('patient').agg({'y_proba': 'mean', 'y_true': 'first'}).reset_index()\n",
    "                patient_summary['y_pred'] = (patient_summary['y_proba'] >= 0.5).astype(int)\n",
    "\n",
    "                y_t, y_p = patient_summary['y_true'], patient_summary['y_pred']\n",
    "                try: auc_p = roc_auc_score(y_t, patient_summary['y_proba'])\n",
    "                except: auc_p = float('nan')\n",
    "                \n",
    "                lopo_summary_rows.append({\n",
    "                    'feature_set': feature_name, 'model': model_name, 'evaluation_level': 'patient',\n",
    "                    'accuracy': accuracy_score(y_t, y_p), 'precision': precision_score(y_t, y_p, zero_division=0),\n",
    "                    'recall': recall_score(y_t, y_p, zero_division=0), 'f1': f1_score(y_t, y_p, zero_division=0),\n",
    "                    'auc': auc_p, 'n_patients': len(patient_summary), 'n_cells': X_features.shape[0]\n",
    "                })\n",
    "                \n",
    "                p_out = Path('Processed_Data') / f'lopo_patient_predictions_{feature_name}_{model_name}.csv'\n",
    "                patient_summary.to_csv(p_out, index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed patient-level metrics: {e}\")\n",
    "\n",
    "    lopo_df = pd.DataFrame(lopo_summary_rows)\n",
    "    output_path = Path('Processed_Data') / 'lopo_results.csv'\n",
    "    Path('Processed_Data').mkdir(exist_ok=True)\n",
    "    lopo_df.to_csv(output_path, index=False)\n",
    "    print(f\"LOPO results saved to: {output_path}\")\n",
    "    display(lopo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cfc9db3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T18:33:54.472567Z",
     "iopub.status.busy": "2026-02-13T18:33:54.472274Z",
     "iopub.status.idle": "2026-02-13T18:33:54.478822Z",
     "shell.execute_reply": "2026-02-13T18:33:54.477932Z"
    },
    "papermill": {
     "duration": 0.048826,
     "end_time": "2026-02-13T18:33:54.480210",
     "exception": false,
     "start_time": "2026-02-13T18:33:54.431384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXED: param_grids defined with reduced hyperparameter space: ['Logistic Regression', 'Decision Tree', 'Random Forest', 'XGBoost']\n",
      "  Logistic Regression: 3 combinations\n",
      "  Decision Tree: 8 combinations\n",
      "  Random Forest: 4 combinations\n",
      "  XGBoost: 8 combinations\n"
     ]
    }
   ],
   "source": [
    "# === FIX 1.4: CONSTRAIN HYPERPARAMETER GRID ===\n",
    "# PREVIOUS: 162 XGBoost combinations for 7 patients caused overfitting\n",
    "# IMPROVED: Reduced to 16 combinations to prevent hyperparameter overfitting\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10],  # Reduced from 5 to 3 options\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [5, 10],  # Reduced: removed 20 and None (prone to overfitting)\n",
    "        'min_samples_split': [5, 10],  # Removed 2 (too permissive)\n",
    "        'min_samples_leaf': [2, 4]  # Removed 1 (too permissive)\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100],  # Fixed value (vs [50, 100, 200])\n",
    "        'max_depth': [10, 20],  # Removed None (unconstrained depth)\n",
    "        'min_samples_split': [5, 10]  # Removed 2\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'max_depth': [3, 5],  # Reduced from [3, 6, 9]\n",
    "        'learning_rate': [0.05, 0.1],  # Reduced from [0.01, 0.1, 0.3]\n",
    "        'subsample': [0.8, 1.0],  # Kept same\n",
    "        'colsample_bytree': [0.8, 1.0],  # Reduced from [0.6, 0.8, 1.0]\n",
    "        'n_estimators': [100]  # Fixed (vs [50, 100, 200])\n",
    "    }\n",
    "}\n",
    "# Total: LR=3, DT=2×2×2=8, RF=1×2×2=4, XGB=2×2×2×1=8 (manageable grid)\n",
    "print('FIXED: param_grids defined with reduced hyperparameter space:', list(param_grids.keys()))\n",
    "print('  Logistic Regression: 3 combinations')\n",
    "print('  Decision Tree: 8 combinations')\n",
    "print('  Random Forest: 4 combinations')\n",
    "print('  XGBoost: 8 combinations')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d8c43",
   "metadata": {
    "papermill": {
     "duration": 0.037241,
     "end_time": "2026-02-13T18:33:54.554570",
     "exception": false,
     "start_time": "2026-02-13T18:33:54.517329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 13. Deep Learning Multi-Modal Classification\n",
    "\n",
    "We implement and compare three advanced deep learning architectures to classify patient response based on the integrated multi-modal features:\n",
    "\n",
    "1.  **1D CNN (Convolutional Neural Network):** Captures local patterns and dependencies within the feature vector.\n",
    "2.  **BiLSTM (Bidirectional Long Short-Term Memory):** Models sequential dependencies in both directions, effective for capturing context in feature sequences.\n",
    "3.  **Transformer Encoder:** Utilizes self-attention mechanisms to weigh the importance of different features dynamically, enabling the model to focus on the most relevant biological signals regardless of their position in the input.\n",
    "\n",
    "These models are trained using a Leave-One-Patient-Out (LOPO) cross-validation strategy to ensure robust and generalizable performance assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755a2a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T18:33:54.632015Z",
     "iopub.status.busy": "2026-02-13T18:33:54.631742Z",
     "iopub.status.idle": "2026-02-13T21:08:14.716034Z",
     "shell.execute_reply": "2026-02-13T21:08:14.714533Z"
    },
    "papermill": {
     "duration": 9260.126709,
     "end_time": "2026-02-13T21:08:14.717985",
     "exception": false,
     "start_time": "2026-02-13T18:33:54.591276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPU(s): [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "  Mixed precision enabled for GPU.\n",
      "Number of replicas: 1\n",
      "TensorFlow version: 2.19.0\n",
      "DL hyperparameter combinations per fold: 4\n",
      "Patched models_eval['XGBoost'] to use GPU (method=device).\n",
      "Patched models_eval['Random Forest'] to use n_jobs=-1.\n",
      "Patched param_grids['XGBoost'] with GPU options (method=device).\n",
      "Using LeaveOneGroupOut for outer validation (few patients).\n",
      "\n",
      "=== DL evaluation using feature set: sequence_structure ===\n",
      "  Starting validation with 4 folds...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af1c06d0ebc419eb036a4f0d78f5c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV sequence_structure:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1771007635.935573      24 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14317 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1771007643.503503     442 service.cc:152] XLA service 0x7c51e0006e00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1771007643.503539     442 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1771007643.998650     442 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "I0000 00:00:1771007646.653408     442 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m793/793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m954/954\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\n",
      "=== DL evaluation using feature set: comprehensive ===\n",
      "  Starting validation with 4 folds...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b0bdba6946484e9704b88c3f017265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV comprehensive:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m793/793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m954/954\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\n",
      "=== DL evaluation using feature set: tcr_enhanced ===\n",
      "  Starting validation with 4 folds...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a7efc0fab24423b3bda9377164a19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV tcr_enhanced:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m793/793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m954/954\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_set</th>\n",
       "      <th>architecture</th>\n",
       "      <th>evaluation_level</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "      <th>specificity</th>\n",
       "      <th>npv</th>\n",
       "      <th>n_patients</th>\n",
       "      <th>n_cells</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sequence_structure</td>\n",
       "      <td>MLP</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.917156</td>\n",
       "      <td>0.863596</td>\n",
       "      <td>0.921445</td>\n",
       "      <td>0.891583</td>\n",
       "      <td>0.970680</td>\n",
       "      <td>0.914640</td>\n",
       "      <td>0.952043</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sequence_structure</td>\n",
       "      <td>MLP</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comprehensive</td>\n",
       "      <td>MLP</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.913888</td>\n",
       "      <td>0.857994</td>\n",
       "      <td>0.919201</td>\n",
       "      <td>0.887543</td>\n",
       "      <td>0.968488</td>\n",
       "      <td>0.910771</td>\n",
       "      <td>0.950542</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comprehensive</td>\n",
       "      <td>MLP</td>\n",
       "      <td>patient</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tcr_enhanced</td>\n",
       "      <td>MLP</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.883728</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.685481</td>\n",
       "      <td>0.813395</td>\n",
       "      <td>0.685481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.844262</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tcr_enhanced</td>\n",
       "      <td>MLP</td>\n",
       "      <td>patient</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>100067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature_set architecture evaluation_level  accuracy  precision  \\\n",
       "0  sequence_structure          MLP             cell  0.917156   0.863596   \n",
       "1  sequence_structure          MLP          patient  1.000000   1.000000   \n",
       "2       comprehensive          MLP             cell  0.913888   0.857994   \n",
       "3       comprehensive          MLP          patient  1.000000   1.000000   \n",
       "4        tcr_enhanced          MLP             cell  0.883728   1.000000   \n",
       "5        tcr_enhanced          MLP          patient  0.750000   1.000000   \n",
       "\n",
       "     recall        f1       auc  specificity       npv  n_patients  n_cells  \n",
       "0  0.921445  0.891583  0.970680     0.914640  0.952043           4   100067  \n",
       "1  1.000000  1.000000  1.000000     1.000000  1.000000           4   100067  \n",
       "2  0.919201  0.887543  0.968488     0.910771  0.950542           4   100067  \n",
       "3  1.000000  1.000000  1.000000     1.000000  1.000000           4   100067  \n",
       "4  0.685481  0.813395  0.685481     1.000000  0.844262           4   100067  \n",
       "5  0.500000  0.666667  0.500000     1.000000  0.666667           4   100067  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 1min 5s, sys: 14min 27s, total: 3h 15min 33s\n",
      "Wall time: 2h 34min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Advanced Multimodal Deep Learning (MLP / CNN / BiLSTM / Transformer)\n",
    "# This cell implements leakage-safe validation for several deep architectures.\n",
    "# OPTIMIZATION: Automatic Device Config (TPU > GPU > CPU)\n",
    "# OPTIMIZATION: Switched to GroupKFold (5 splits) instead of LOPO to prevent timeouts.\n",
    "# OPTIMIZATION: Reduced hyperparameter grid and used efficient inner validation.\n",
    "\n",
    "# --- EARLY VALIDATION: Check if supervised data exists ---\n",
    "if 'supervised_mask' not in globals() or supervised_mask.sum() == 0:\n",
    "    print(\"WARNING: No supervised samples available (supervised_mask is empty or undefined).\")\n",
    "    print(\"Skipping deep learning evaluation to prevent memory waste and errors.\")\n",
    "    dl_results_rows = []\n",
    "else:\n",
    "    import itertools\n",
    "    import time\n",
    "    import math\n",
    "    import random\n",
    "    import gc\n",
    "    import tracemalloc\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, regularizers\n",
    "    from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "    from sklearn.model_selection import GroupKFold, StratifiedKFold, LeaveOneGroupOut, GroupShuffleSplit\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "    from pathlib import Path\n",
    "    from joblib import Parallel, delayed\n",
    "    try:\n",
    "        from tqdm.auto import tqdm\n",
    "    except ImportError:\n",
    "        def tqdm(x, **kwargs): return x\n",
    "\n",
    "    # Deterministic seeds\n",
    "    SEED = 42\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    \n",
    "    # --- Robust Device Configuration: TPU > GPU > CPU ---\n",
    "    def configure_tf_strategy():\n",
    "        \"\"\"Detects hardware and returns the appropriate DistributionStrategy.\"\"\"\n",
    "        try:\n",
    "            # 1. Try TPU\n",
    "            tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # Throws if no TPU\n",
    "            print(f\"Running on TPU: {tpu.master()}\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.TPUStrategy(tpu)\n",
    "            print(\"  TPU Strategy initialized.\")\n",
    "            return strategy\n",
    "        except ValueError:\n",
    "            pass # No TPU found\n",
    "\n",
    "        # 2. Try GPU\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            print(f\"Found {len(gpus)} GPU(s): {gpus}\")\n",
    "            try:\n",
    "                # Enable memory growth\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                \n",
    "                # Use mixed precision on GPU for speed & memory\n",
    "                tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "                print(\"  Mixed precision enabled for GPU.\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"  GPU Config Error: {e}\")\n",
    "            \n",
    "            # Default strategy works for single GPU, Mirrored for multi-GPU\n",
    "            return tf.distribute.get_strategy()\n",
    "        \n",
    "        # 3. Fallback to CPU\n",
    "        print(\"No GPU/TPU detected. Using CPU.\")\n",
    "        return tf.distribute.get_strategy()\n",
    "\n",
    "    # Initialize Strategy\n",
    "    STRATEGY = configure_tf_strategy()\n",
    "    print(f\"Number of replicas: {STRATEGY.num_replicas_in_sync}\")\n",
    "\n",
    "    # Start memory tracking\n",
    "    tracemalloc.start()\n",
    "    _mem_start = tracemalloc.get_traced_memory()\n",
    "    _time_start = time.time()\n",
    "\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "    # Helper: prepare sequence arrays if available (robust, auto-detect channels/seq_len)\n",
    "    def prepare_onehot_sequences(adata, mask, n_channels=20):\n",
    "        \"\"\"\n",
    "        Returns (tra_seq, trb_seq, seq_len) or (None,None,None).\n",
    "        This helper is resilient to different stored shapes and will attempt to\n",
    "        auto-detect a reasonable `n_channels` / `seq_len` factorization if the\n",
    "        provided `n_channels` does not divide the feature dimension.\n",
    "        \"\"\"\n",
    "        if 'X_tcr_tra_onehot' in adata.obsm and 'X_tcr_trb_onehot' in adata.obsm:\n",
    "            tra_flat = adata.obsm['X_tcr_tra_onehot'][mask]\n",
    "            trb_flat = adata.obsm['X_tcr_trb_onehot'][mask]\n",
    "            try:\n",
    "                # materialize sparse arrays if necessary\n",
    "                if hasattr(tra_flat, 'toarray'):\n",
    "                    tra_flat = tra_flat.toarray()\n",
    "                if hasattr(trb_flat, 'toarray'):\n",
    "                    trb_flat = trb_flat.toarray()\n",
    "\n",
    "                tra_flat = np.asarray(tra_flat)\n",
    "                trb_flat = np.asarray(trb_flat)\n",
    "\n",
    "                # Expect 2D arrays (n_samples, n_features)\n",
    "                if tra_flat.ndim != 2 or trb_flat.ndim != 2:\n",
    "                    print('prepare_onehot_sequences: onehot arrays are not 2D. Skipping sequence usage.')\n",
    "                    return None, None, None\n",
    "\n",
    "                # If sample counts mismatch, align to the minimum (best-effort)\n",
    "                if tra_flat.shape[0] != trb_flat.shape[0]:\n",
    "                    print('prepare_onehot_sequences: TRA/TRB sample count mismatch. Truncating to min length.')\n",
    "                    min_n = min(tra_flat.shape[0], trb_flat.shape[0])\n",
    "                    tra_flat = tra_flat[:min_n]\n",
    "                    trb_flat = trb_flat[:min_n]\n",
    "\n",
    "                total_cols = tra_flat.shape[1]\n",
    "\n",
    "                # Fast path: requested n_channels divides feature dim\n",
    "                if total_cols % n_channels == 0:\n",
    "                    seq_len = total_cols // n_channels\n",
    "                    try:\n",
    "                        tra_seq = tra_flat.reshape(tra_flat.shape[0], seq_len, n_channels)\n",
    "                        trb_seq = trb_flat.reshape(trb_flat.shape[0], seq_len, n_channels)\n",
    "                        return tra_seq, trb_seq, seq_len\n",
    "                    except Exception:\n",
    "                        # fall through to auto-detection below\n",
    "                        pass\n",
    "\n",
    "                # Auto-detect candidate (n_channels, seq_len) factorizations\n",
    "                candidates = []\n",
    "                for nc in range(1, 33):  # search sensible channel sizes up to 32\n",
    "                    if total_cols % nc == 0:\n",
    "                        sl = total_cols // nc\n",
    "                        # reasonable sequence lengths\n",
    "                        if 3 <= sl <= 200:\n",
    "                            candidates.append((nc, sl))\n",
    "\n",
    "                if not candidates:\n",
    "                    print(f'prepare_onehot_sequences: cannot factor {total_cols} into channels*seq_len')\n",
    "                    return None, None, None\n",
    "\n",
    "                # Prefer canonical amino-acid channel sizes then fall back\n",
    "                preferred = [20, 15, 10, 8, 5, 4, 2, 1]\n",
    "                chosen_nc = None\n",
    "                for p in preferred:\n",
    "                    for nc, sl in candidates:\n",
    "                        if nc == p:\n",
    "                            chosen_nc, seq_len = nc, sl\n",
    "                            break\n",
    "                    if chosen_nc is not None:\n",
    "                        break\n",
    "                if chosen_nc is None:\n",
    "                    # pick the largest channel count candidate to keep seq_len small\n",
    "                    chosen_nc, seq_len = max(candidates, key=lambda x: x[0])\n",
    "\n",
    "                try:\n",
    "                    tra_seq = tra_flat.reshape(tra_flat.shape[0], seq_len, chosen_nc)\n",
    "                    trb_seq = trb_flat.reshape(trb_flat.shape[0], seq_len, chosen_nc)\n",
    "                    print(f'prepare_onehot_sequences: auto-detected n_channels={chosen_nc}, seq_len={seq_len}')\n",
    "                    return tra_seq, trb_seq, seq_len\n",
    "                except Exception as e:\n",
    "                    print(f'prepare_onehot_sequences: failed to reshape with detected dims: {e}')\n",
    "                    return None, None, None\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error preparing sequences: {e}\")\n",
    "                return None, None, None\n",
    "        return None, None, None\n",
    "\n",
    "    # Model builders (Modified to not compile immediately so we can compile inside strategy scope if needed)\n",
    "    def create_compiled_model(model_fn, *args, lr=1e-3, **kwargs):\n",
    "        \"\"\"Creates and compiles a model within the current strategy scope.\"\"\"\n",
    "        with STRATEGY.scope():\n",
    "            model = model_fn(*args, **kwargs)\n",
    "            # Use jit_compile=True for XLA optimization if not on TPU (TPU implies XLA)\n",
    "            jit = False if isinstance(STRATEGY, tf.distribute.TPUStrategy) else True\n",
    "            model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                        loss='binary_crossentropy', \n",
    "                        metrics=[keras.metrics.AUC(name='auc'), 'accuracy'],\n",
    "                        jit_compile=jit)\n",
    "        return model\n",
    "\n",
    "    def build_mlp_graph(input_dim, hidden1=128, hidden2=64, dropout=0.3, l2_reg=1e-3):\n",
    "        inp = keras.Input(shape=(input_dim,), name='gene_input')\n",
    "        x = layers.Dense(hidden1, kernel_regularizer=regularizers.l2(l2_reg))(inp)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Dense(hidden2, kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        out = layers.Dense(1, activation='sigmoid')(x)\n",
    "        return keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    def build_cnn_graph(seq_len, n_channels, gene_dim=None, conv_filters=64, kernel_size=5, dropout=0.3, l2_reg=1e-3):\n",
    "        seq_in = keras.Input(shape=(seq_len, n_channels), name='seq_input')\n",
    "        x = layers.Conv1D(conv_filters, kernel_size, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(seq_in)\n",
    "        x = layers.Conv1D(conv_filters, kernel_size, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "        x = layers.GlobalMaxPooling1D()(x)\n",
    "        \n",
    "        if gene_dim is not None:\n",
    "            gene_in = keras.Input(shape=(gene_dim,), name='gene_input')\n",
    "            g = layers.Dense(64, activation='relu')(gene_in)\n",
    "            x = layers.concatenate([x, g])\n",
    "            out_in = [seq_in, gene_in]\n",
    "        else:\n",
    "            out_in = seq_in\n",
    "            \n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        out = layers.Dense(1, activation='sigmoid')(x)\n",
    "        return keras.Model(inputs=out_in, outputs=out)\n",
    "\n",
    "    def build_bilstm_graph(seq_len, n_channels, gene_dim=None, lstm_units=128, dropout=0.3, l2_reg=1e-3):\n",
    "        seq_in = keras.Input(shape=(seq_len, n_channels), name='seq_input')\n",
    "        x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=False, kernel_regularizer=regularizers.l2(l2_reg)))(seq_in)\n",
    "        \n",
    "        if gene_dim is not None:\n",
    "            gene_in = keras.Input(shape=(gene_dim,), name='gene_input')\n",
    "            g = layers.Dense(64, activation='relu')(gene_in)\n",
    "            x = layers.concatenate([x, g])\n",
    "            out_in = [seq_in, gene_in]\n",
    "        else:\n",
    "            out_in = seq_in\n",
    "            \n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        out = layers.Dense(1, activation='sigmoid')(x)\n",
    "        return keras.Model(inputs=out_in, outputs=out)\n",
    "\n",
    "    # Small Transformer encoder block\n",
    "    class TransformerBlock(layers.Layer):\n",
    "        def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "            super(TransformerBlock, self).__init__(**kwargs)\n",
    "            self.embed_dim = embed_dim\n",
    "            self.num_heads = num_heads\n",
    "            self.ff_dim = ff_dim\n",
    "            self.rate = rate\n",
    "            self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "            self.ffn = keras.Sequential([layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim)])\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "            \n",
    "        def call(self, inputs, training=None):\n",
    "            attn_output = self.att(inputs, inputs)\n",
    "            attn_output = self.dropout1(attn_output, training=training)\n",
    "            out1 = self.layernorm1(inputs + attn_output)\n",
    "            ffn_output = self.ffn(out1)\n",
    "            ffn_output = self.dropout2(ffn_output, training=training)\n",
    "            return self.layernorm2(out1 + ffn_output)\n",
    "            \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"ff_dim\": self.ff_dim,\n",
    "                \"rate\": self.rate,\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    def build_transformer_graph(seq_len, n_channels, gene_dim=None, embed_dim=64, num_heads=4, ff_dim=128, dropout=0.1):\n",
    "        seq_in = keras.Input(shape=(seq_len, n_channels), name='seq_input')\n",
    "        # project channels to embed_dim\n",
    "        x = layers.Dense(embed_dim)(seq_in)\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim, rate=dropout)(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        if gene_dim is not None:\n",
    "            gene_in = keras.Input(shape=(gene_dim,), name='gene_input')\n",
    "            g = layers.Dense(64, activation='relu')(gene_in)\n",
    "            x = layers.concatenate([x, g])\n",
    "            inputs_list = [seq_in, gene_in]\n",
    "        else:\n",
    "            inputs_list = seq_in\n",
    "            \n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        out = layers.Dense(1, activation='sigmoid')(x)\n",
    "        return keras.Model(inputs=inputs_list, outputs=out)\n",
    "\n",
    "    # --- Parallel Training Helper ---\n",
    "    def train_eval_single_config(cfg_idx, config, use_gene, use_seq, \n",
    "                                 X_tr_gene, X_val_gene, \n",
    "                                 X_tr_seq, X_val_seq, \n",
    "                                 X_tr_flat, X_val_flat, \n",
    "                                 y_train, y_val, class_weights):\n",
    "        \"\"\"\n",
    "        Train and evaluate a single model configuration for one inner split.\n",
    "        \"\"\"\n",
    "        arch, hu, dr, lr, bs, epochs = config\n",
    "        \n",
    "        # 1. Check validity of config for current data availability\n",
    "        if arch in ['CNN','BiLSTM','Transformer'] and not use_seq:\n",
    "            return cfg_idx, -1.0\n",
    "            \n",
    "        try:\n",
    "            fit_inputs = None\n",
    "            val_inputs = None\n",
    "            model = None\n",
    "            \n",
    "            # 2. Build Model & Inputs (Using Strategy Scope via create_compiled_model)\n",
    "            if arch == 'MLP':\n",
    "                if use_gene and X_tr_gene is not None:\n",
    "                    fit_inputs = X_tr_gene\n",
    "                    val_inputs = X_val_gene\n",
    "                    input_dim = fit_inputs.shape[1]\n",
    "                    model = create_compiled_model(build_mlp_graph, input_dim, hidden1=hu, hidden2=max(32, hu//2), dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "                elif use_seq and X_tr_flat is not None:\n",
    "                    fit_inputs = X_tr_flat\n",
    "                    val_inputs = X_val_flat\n",
    "                    input_dim = fit_inputs.shape[1]\n",
    "                    model = create_compiled_model(build_mlp_graph, input_dim, hidden1=hu, hidden2=max(32, hu//2), dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "                else:\n",
    "                    return cfg_idx, -1.0\n",
    "\n",
    "            elif arch == 'CNN':\n",
    "                fit_inputs = [X_tr_seq, X_tr_gene] if use_gene else X_tr_seq\n",
    "                val_inputs = [X_val_seq, X_val_gene] if use_gene else X_val_seq\n",
    "                gene_dim_val = (X_tr_gene.shape[1] if use_gene else None)\n",
    "                model = create_compiled_model(build_cnn_graph, X_tr_seq.shape[1], X_tr_seq.shape[2], gene_dim=gene_dim_val, conv_filters=hu, kernel_size=5, dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "            \n",
    "            elif arch == 'BiLSTM':\n",
    "                fit_inputs = [X_tr_seq, X_tr_gene] if use_gene else X_tr_seq\n",
    "                val_inputs = [X_val_seq, X_val_gene] if use_gene else X_val_seq\n",
    "                gene_dim_val = (X_tr_gene.shape[1] if use_gene else None)\n",
    "                model = create_compiled_model(build_bilstm_graph, X_tr_seq.shape[1], X_tr_seq.shape[2], gene_dim=gene_dim_val, lstm_units=hu, dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "                \n",
    "            else: # Transformer\n",
    "                fit_inputs = [X_tr_seq, X_tr_gene] if use_gene else X_tr_seq\n",
    "                val_inputs = [X_val_seq, X_val_gene] if use_gene else X_val_seq\n",
    "                gene_dim_val = (X_tr_gene.shape[1] if use_gene else None)\n",
    "                model = create_compiled_model(build_transformer_graph, X_tr_seq.shape[1], X_tr_seq.shape[2], gene_dim=gene_dim_val, embed_dim=max(32, hu//2), num_heads=4, ff_dim=hu, dropout=dr, lr=lr)\n",
    "\n",
    "            # Train\n",
    "            es = keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=5, restore_best_weights=True, verbose=0)\n",
    "            rlr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=2, min_lr=1e-6, verbose=0)\n",
    "            history = model.fit(fit_inputs, y_train, validation_data=(val_inputs, y_val), epochs=epochs, batch_size=bs, class_weight=class_weights, callbacks=[es, rlr], verbose=0)\n",
    "            \n",
    "            # Retrieve metric\n",
    "            val_metric = max(history.history['val_auc']) if 'val_auc' in history.history else 0.0\n",
    "\n",
    "            # Cleanup\n",
    "            del model\n",
    "            keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            \n",
    "            return cfg_idx, val_metric\n",
    "\n",
    "        except Exception as e:\n",
    "            # print(f\"Config {config} failed: {e}\") # debug\n",
    "            return cfg_idx, -1.0\n",
    "\n",
    "    # Manual hyperparameter grid for DL\n",
    "    from itertools import product\n",
    "\n",
    "    # Optimized Grid: Further reduced search space to prevent timeout\n",
    "    dl_param_grid = {\n",
    "        'arch': ['MLP', 'CNN', 'BiLSTM', 'Transformer'],\n",
    "        'hidden_units': [128], # Fixed size\n",
    "        'dropout': [0.3],      # Fixed dropout\n",
    "        'lr': [1e-3],          # Fixed LR, rely on RLR\n",
    "        'batch_size': [16], \n",
    "        'epochs': [25],        # Reduced max epochs slightly\n",
    "    }\n",
    "    grid_items = list(product(dl_param_grid['arch'], dl_param_grid['hidden_units'], dl_param_grid['dropout'], dl_param_grid['lr'], dl_param_grid['batch_size'], dl_param_grid['epochs']))\n",
    "    print(f\"DL hyperparameter combinations per fold: {len(grid_items)}\")\n",
    "\n",
    "    # Prepare inputs\n",
    "    supervised_mask_local = supervised_mask # from prior cells\n",
    "    X_gene_all = adata.obsm['X_gene_pca'][supervised_mask_local]\n",
    "    tra_seq_all, trb_seq_all, seq_len = prepare_onehot_sequences(adata, supervised_mask_local)\n",
    "    use_sequence = tra_seq_all is not None and trb_seq_all is not None\n",
    "    \n",
    "    if use_sequence:\n",
    "        # concatenate TRA+TRB channels along the channel axis\n",
    "        X_seq_all = np.concatenate([tra_seq_all, trb_seq_all], axis=2) # shape (N, seq_len, n_channels*2)\n",
    "        n_channels_combined = X_seq_all.shape[2]\n",
    "    else:\n",
    "        X_seq_all = None\n",
    "        n_channels_combined = None\n",
    "\n",
    "    y_all = y_encoded\n",
    "    \n",
    "    # Robust column detection for patient_id\n",
    "    patient_id_col_local = None\n",
    "    if 'patient_id' in adata.obs.columns:\n",
    "        patient_id_col_local = 'patient_id'\n",
    "    elif 'Patient_ID' in adata.obs.columns:\n",
    "        if 'patient_id' not in adata.obs.columns:\n",
    "            adata.obs['patient_id'] = adata.obs['Patient_ID']\n",
    "        patient_id_col_local = 'patient_id'\n",
    "    elif 'PatientID' in adata.obs.columns:\n",
    "        if 'patient_id' not in adata.obs.columns:\n",
    "            adata.obs['patient_id'] = adata.obs['PatientID']\n",
    "        patient_id_col_local = 'patient_id'\n",
    "    else:\n",
    "        raise KeyError(\"No patient ID column found (tried 'patient_id', 'Patient_ID', 'PatientID')\")\n",
    "    \n",
    "    groups_all_local = np.array(adata.obs[patient_id_col_local][supervised_mask_local])\n",
    "    unique_patients = np.unique(groups_all_local)\n",
    "\n",
    "    # Outer Validation strategy: Use GroupKFold (5 splits) if possible, else LOPO\n",
    "    if len(unique_patients) >= 5:\n",
    "        n_outer_splits = 5\n",
    "        outer_cv = GroupKFold(n_splits=n_outer_splits)\n",
    "        print(f\"Using GroupKFold with {n_outer_splits} splits for outer validation (faster than LOPO).\")\n",
    "    else:\n",
    "        outer_cv = LeaveOneGroupOut()\n",
    "        print(\"Using LeaveOneGroupOut for outer validation (few patients).\")\n",
    "        \n",
    "    dl_results_rows = []\n",
    "\n",
    "    for feature_name in ['sequence_structure', 'comprehensive', 'tcr_enhanced']:\n",
    "        # Select appropriate X inputs for DL\n",
    "        print(f\"\\n=== DL evaluation using feature set: {feature_name} ===\")\n",
    "        if feature_name == 'sequence_structure' and use_sequence:\n",
    "            # We will use gene PCs + sequence input\n",
    "            use_gene = True\n",
    "            use_seq = True\n",
    "            X_gene = X_gene_all\n",
    "            X_seq = X_seq_all\n",
    "        elif feature_name == 'comprehensive':\n",
    "            # use gene + reduced sequence PCA features if sequence onehot unavailable\n",
    "            use_gene = True\n",
    "            use_seq = use_sequence\n",
    "            X_gene = X_gene_all\n",
    "            X_seq = X_seq_all\n",
    "        elif feature_name == 'tcr_enhanced' and use_sequence:\n",
    "            use_gene = False\n",
    "            use_seq = True\n",
    "            X_gene = None\n",
    "            X_seq = X_seq_all\n",
    "        else:\n",
    "            # fallback to gene-only MLP\n",
    "            use_gene = True\n",
    "            use_seq = False\n",
    "            X_gene = X_gene_all\n",
    "            X_seq = None\n",
    "\n",
    "        # accumulators per architecture\n",
    "        accum_arch = {}\n",
    "        for arch in ['MLP','CNN','BiLSTM','Transformer']:\n",
    "            accum_arch[arch] = {'y_true': [], 'y_pred': [], 'y_proba': [], 'groups': []}\n",
    "\n",
    "        # Clearer Logging with TQDM\n",
    "        splits = list(outer_cv.split(X_gene if X_gene is not None else np.zeros((len(y_all),1)), y_all, groups_all_local))\n",
    "        print(f\"  Starting validation with {len(splits)} folds...\")\n",
    "        \n",
    "        for fold_idx, (train_idx, test_idx) in tqdm(enumerate(splits), total=len(splits), desc=f\"CV {feature_name}\"):\n",
    "            held = np.unique(groups_all_local[test_idx])\n",
    "            # print(f\"Fold {fold_idx+1}/{len(splits)}\")\n",
    "\n",
    "            # Split inputs\n",
    "            if use_gene:\n",
    "                X_tr_gene = X_gene[train_idx]\n",
    "                X_te_gene = X_gene[test_idx]\n",
    "                # Standard scaling fits only on training\n",
    "                scaler = StandardScaler().fit(X_tr_gene)\n",
    "                X_tr_gene_scaled = scaler.transform(X_tr_gene)\n",
    "                X_te_gene_scaled = scaler.transform(X_te_gene)\n",
    "            else:\n",
    "                X_tr_gene_scaled = None\n",
    "                X_te_gene_scaled = None\n",
    "\n",
    "            if use_seq:\n",
    "                X_tr_seq = X_seq[train_idx]\n",
    "                X_te_seq = X_seq[test_idx]\n",
    "            else:\n",
    "                X_tr_seq = None\n",
    "                X_te_seq = None\n",
    "\n",
    "            y_tr = y_all[train_idx]\n",
    "            y_te = y_all[test_idx]\n",
    "            groups_tr = groups_all_local[train_idx]\n",
    "\n",
    "            # Compute class weights\n",
    "            classes = np.unique(y_tr)\n",
    "            cw = compute_class_weight(class_weight='balanced', classes=classes, y=y_tr)\n",
    "            class_weight_dict = {int(c): float(w) for c,w in zip(classes, cw)}\n",
    "\n",
    "            # Inner grouped CV for hyperparam selection\n",
    "            # Optimized: Use ONE split for validation to save time\n",
    "            n_train_groups = len(np.unique(groups_tr))\n",
    "            inner_splits = 1 if n_train_groups >= 2 else 0\n",
    "            best_cfg = None\n",
    "            \n",
    "            if inner_splits >= 1:\n",
    "                # Use GroupShuffleSplit for efficient single-pass validation\n",
    "                inner_cv = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "                \n",
    "                # config_scores maps index in grid_items -> list of scores\n",
    "                config_scores = {i: [] for i in range(len(grid_items))}\n",
    "                \n",
    "                # --- PARALLEL TRAINING TASKS PREPARATION ---\n",
    "                tasks = []\n",
    "                \n",
    "                # Iterate splits to generate tasks\n",
    "                for inner_train_idx, inner_val_idx in inner_cv.split(X_tr_gene_scaled if X_tr_gene_scaled is not None else np.zeros((len(y_tr),1)), y_tr, groups_tr):\n",
    "                    \n",
    "                    # 1. Prepare Data Slices for this split (copied to task inputs)\n",
    "                    # Gene data\n",
    "                    if use_gene:\n",
    "                        X_inner_tr_gene = X_tr_gene_scaled[inner_train_idx]\n",
    "                        X_inner_val_gene = X_tr_gene_scaled[inner_val_idx]\n",
    "                    else:\n",
    "                        X_inner_tr_gene = None\n",
    "                        X_inner_val_gene = None\n",
    "                        \n",
    "                    # Seq data\n",
    "                    if use_seq:\n",
    "                        X_inner_tr_seq = X_tr_seq[inner_train_idx]\n",
    "                        X_inner_val_seq = X_tr_seq[inner_val_idx]\n",
    "                        # Prepare Flattened Seq data for MLP\n",
    "                        X_inner_tr_flat = X_inner_tr_seq.reshape(X_inner_tr_seq.shape[0], -1)\n",
    "                        X_inner_val_flat = X_inner_val_seq.reshape(X_inner_val_seq.shape[0], -1)\n",
    "                        seq_scaler = StandardScaler().fit(X_inner_tr_flat)\n",
    "                        X_inner_tr_flat_scaled = seq_scaler.transform(X_inner_tr_flat)\n",
    "                        X_inner_val_flat_scaled = seq_scaler.transform(X_inner_val_flat)\n",
    "                    else:\n",
    "                        X_inner_tr_seq = None\n",
    "                        X_inner_val_seq = None\n",
    "                        X_inner_tr_flat_scaled = None\n",
    "                        X_inner_val_flat_scaled = None\n",
    "\n",
    "                    y_inner_tr = y_tr[inner_train_idx]\n",
    "                    y_inner_val = y_tr[inner_val_idx]\n",
    "\n",
    "                    # 2. Add config task\n",
    "                    for cfg_idx, config in enumerate(grid_items):\n",
    "                        tasks.append(\n",
    "                            delayed(train_eval_single_config)(\n",
    "                                cfg_idx, config, use_gene, use_seq,\n",
    "                                X_inner_tr_gene, X_inner_val_gene,\n",
    "                                X_inner_tr_seq, X_inner_val_seq,\n",
    "                                X_inner_tr_flat_scaled, X_inner_val_flat_scaled,\n",
    "                                y_inner_tr, y_inner_val, class_weight_dict\n",
    "                            )\n",
    "                        )\n",
    "                \n",
    "                # Execute Parallel Jobs\n",
    "                results = Parallel(n_jobs=1, backend='threading')(tasks)\n",
    "                \n",
    "                # Aggregate results\n",
    "                for cfg_idx, res_score in results:\n",
    "                    if res_score >= 0:\n",
    "                        config_scores[cfg_idx].append(res_score)\n",
    "                \n",
    "                # Select best config based on mean score\n",
    "                best_avg_score = -math.inf\n",
    "                for cfg_idx, scores in config_scores.items():\n",
    "                    if not scores: continue\n",
    "                    avg_score = np.mean(scores)\n",
    "                    if avg_score > best_avg_score:\n",
    "                        best_avg_score = avg_score\n",
    "                        best_cfg = grid_items[cfg_idx]\n",
    "                \n",
    "            else:\n",
    "                # Not enough groups to do inner grouped CV: pick first compatible config\n",
    "                best_cfg = None\n",
    "                for cfg in grid_items:\n",
    "                    arch_candidate = cfg[0]\n",
    "                    # skip sequence architectures if sequences not available\n",
    "                    if arch_candidate in ['CNN','BiLSTM','Transformer'] and not use_seq:\n",
    "                        continue\n",
    "                    # require at least gene data for MLP if seq not available\n",
    "                    if arch_candidate == 'MLP' and not (use_gene or use_seq):\n",
    "                        continue\n",
    "                    best_cfg = cfg\n",
    "                    break\n",
    "                if best_cfg is None:\n",
    "                    best_cfg = grid_items[0]\n",
    "                # print(f\"  Not enough patients for grouped inner CV; selected fallback config: {best_cfg}\")\n",
    "\n",
    "            # Retrain best config on full training partition and evaluate on held-out patient\n",
    "            arch, hu, dr, lr, bs, epochs = best_cfg\n",
    "            try:\n",
    "                fit_inputs = None\n",
    "                test_inputs = None\n",
    "                model = None\n",
    "\n",
    "                if arch == 'MLP':\n",
    "                    # support gene-MLP or flattened-seq MLP\n",
    "                    if use_gene and X_tr_gene_scaled is not None:\n",
    "                        model = create_compiled_model(build_mlp_graph, X_tr_gene_scaled.shape[1], hidden1=hu, hidden2=max(32, hu//2), dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "                        fit_inputs = X_tr_gene_scaled\n",
    "                        test_inputs = X_te_gene_scaled\n",
    "                    elif use_seq and X_tr_seq is not None:\n",
    "                        X_tr_flat = X_tr_seq.reshape(X_tr_seq.shape[0], -1)\n",
    "                        X_te_flat = X_te_seq.reshape(X_te_seq.shape[0], -1)\n",
    "                        # scale flattened sequence inputs\n",
    "                        seq_scaler_full = StandardScaler().fit(X_tr_flat)\n",
    "                        X_tr_flat_scaled = seq_scaler_full.transform(X_tr_flat)\n",
    "                        X_te_flat_scaled = seq_scaler_full.transform(X_te_flat)\n",
    "                        model = create_compiled_model(build_mlp_graph, X_tr_flat_scaled.shape[1], hidden1=hu, hidden2=max(32, hu//2), dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "                        fit_inputs = X_tr_flat_scaled\n",
    "                        test_inputs = X_te_flat_scaled\n",
    "                    else:\n",
    "                        raise ValueError('MLP selected but no valid input data for this fold')\n",
    "                elif arch == 'CNN':\n",
    "                    model = create_compiled_model(build_cnn_graph, X_tr_seq.shape[1], X_tr_seq.shape[2], gene_dim=(X_tr_gene_scaled.shape[1] if use_gene else None), conv_filters=hu, kernel_size=5, dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "                    fit_inputs = [X_tr_seq, X_tr_gene_scaled] if use_gene else X_tr_seq\n",
    "                    test_inputs = [X_te_seq, X_te_gene_scaled] if use_gene else X_te_seq\n",
    "                elif arch == 'BiLSTM':\n",
    "                    model = create_compiled_model(build_bilstm_graph, X_tr_seq.shape[1], X_tr_seq.shape[2], gene_dim=(X_tr_gene_scaled.shape[1] if use_gene else None), lstm_units=hu, dropout=dr, l2_reg=1e-3, lr=lr)\n",
    "                    fit_inputs = [X_tr_seq, X_tr_gene_scaled] if use_gene else X_tr_seq\n",
    "                    test_inputs = [X_te_seq, X_te_gene_scaled] if use_gene else X_te_seq\n",
    "                else: # Transformer\n",
    "                    model = create_compiled_model(build_transformer_graph, X_tr_seq.shape[1], X_tr_seq.shape[2], gene_dim=(X_tr_gene_scaled.shape[1] if use_gene else None), embed_dim=max(32, hu//2), num_heads=4, ff_dim=hu, dropout=dr, lr=lr)\n",
    "                    fit_inputs = [X_tr_seq, X_tr_gene_scaled] if use_gene else X_tr_seq\n",
    "                    test_inputs = [X_te_seq, X_te_gene_scaled] if use_gene else X_te_seq\n",
    "\n",
    "                es = keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=8, restore_best_weights=True, verbose=0)\n",
    "                rlr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=3, min_lr=1e-6, verbose=0)\n",
    "                \n",
    "                # Use a small validation split from training data to monitor training\n",
    "                model.fit(fit_inputs, y_tr, validation_split=0.1, epochs=epochs, batch_size=bs, class_weight=class_weight_dict, callbacks=[es, rlr], verbose=0)\n",
    "\n",
    "                # Save DL model\n",
    "                try:\n",
    "                    dl_model_dir = Path('Output/Models/DL')\n",
    "                    dl_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    dl_model_path = dl_model_dir / f'dl_model_{feature_name}_{arch}_fold{fold_idx}.keras'\n",
    "                    model.save(dl_model_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to save DL model: {e}\")\n",
    "\n",
    "                y_test_proba = model.predict(test_inputs).flatten()\n",
    "                y_test_pred = (y_test_proba > 0.5).astype(int)\n",
    "                keras.backend.clear_session()\n",
    "            except Exception as e:\n",
    "                print(f\"  Training/eval failed for fold with config {best_cfg}: {e}\")\n",
    "                y_test_proba = np.zeros(len(y_te), dtype=float)\n",
    "                y_test_pred = np.zeros(len(y_te), dtype=int)\n",
    "\n",
    "            # accumulate by architecture (include patient groups)\n",
    "            accum_arch[arch]['y_true'].extend(y_te.tolist())\n",
    "            accum_arch[arch]['y_pred'].extend(y_test_pred.tolist())\n",
    "            accum_arch[arch]['y_proba'].extend(y_test_proba.tolist())\n",
    "            accum_arch[arch]['groups'].extend(groups_all_local[test_idx].tolist())\n",
    "\n",
    "            # --- OOM FIX: release TF graph and numpy arrays between LOPO folds ---\n",
    "            keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "\n",
    "        # After LOPO folds compute aggregated metrics per architecture\n",
    "        for arch, data in accum_arch.items():\n",
    "            y_true_all = np.array(data['y_true'])\n",
    "            y_pred_all = np.array(data['y_pred'])\n",
    "            y_proba_all = np.array(data['y_proba'])\n",
    "            if len(y_true_all) == 0:\n",
    "                continue\n",
    "            acc = accuracy_score(y_true_all, y_pred_all)\n",
    "            prec = precision_score(y_true_all, y_pred_all, zero_division=0)\n",
    "            rec = recall_score(y_true_all, y_pred_all, zero_division=0)\n",
    "            f1s = f1_score(y_true_all, y_pred_all, zero_division=0)\n",
    "            try:\n",
    "                auc = roc_auc_score(y_true_all, y_proba_all)\n",
    "            except Exception:\n",
    "                auc = float('nan')\n",
    "            cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "            if cm.size == 4:\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "                spec = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "                npv = tn / (tn + fn) if (tn + fn) > 0 else float('nan')\n",
    "            else:\n",
    "                spec = float('nan')\n",
    "                npv = float('nan')\n",
    "\n",
    "            dl_results_rows.append({\n",
    "                'feature_set': feature_name,\n",
    "                'architecture': arch,\n",
    "                'evaluation_level': 'cell',\n",
    "                'accuracy': acc,\n",
    "                'precision': prec,\n",
    "                'recall': rec,\n",
    "                'f1': f1s,\n",
    "                'auc': auc,\n",
    "                'specificity': spec,\n",
    "                'npv': npv,\n",
    "                'n_patients': len(unique_patients),\n",
    "                'n_cells': X_gene.shape[0] if X_gene is not None else (X_seq.shape[0] if X_seq is not None else 0),\n",
    "            })\n",
    "\n",
    "            # --- Patient-level aggregation for DL architecture ---\n",
    "            try:\n",
    "                groups_arr = np.array(data.get('groups', []), dtype=object)\n",
    "                pred_df = pd.DataFrame({'patient': groups_arr, 'y_true': data['y_true'], 'y_proba': data['y_proba']})\n",
    "                patient_summary = pred_df.groupby('patient').agg({'y_proba': 'mean', 'y_true': 'first'}).reset_index()\n",
    "                patient_summary['y_pred'] = (patient_summary['y_proba'] >= 0.5).astype(int)\n",
    "                \n",
    "                # Assign vars before calc\n",
    "                y_true_pat = patient_summary['y_true'].values\n",
    "                y_pred_pat = patient_summary['y_pred'].values\n",
    "                y_proba_pat = patient_summary['y_proba'].values\n",
    "\n",
    "                acc_p = accuracy_score(y_true_pat, y_pred_pat)\n",
    "                prec_p = precision_score(y_true_pat, y_pred_pat, zero_division=0)\n",
    "                rec_p = recall_score(y_true_pat, y_pred_pat, zero_division=0)\n",
    "                f1s_p = f1_score(y_true_pat, y_pred_pat, zero_division=0)\n",
    "                try:\n",
    "                    auc_p = roc_auc_score(y_true_pat, y_proba_pat)\n",
    "                except Exception:\n",
    "                    auc_p = float('nan')\n",
    "                cm_p = confusion_matrix(y_true_pat, y_pred_pat)\n",
    "                if cm_p.size == 4:\n",
    "                    tn, fp, fn, tp = cm_p.ravel()\n",
    "                    spec_p = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "                    npv_p = tn / (tn + fn) if (tn + fn) > 0 else float('nan')\n",
    "                else:\n",
    "                    spec_p = float('nan')\n",
    "                    npv_p = float('nan')\n",
    "\n",
    "                dl_results_rows.append({\n",
    "                    'feature_set': feature_name,\n",
    "                    'architecture': arch,\n",
    "                    'evaluation_level': 'patient',\n",
    "                    'accuracy': acc_p,\n",
    "                    'precision': prec_p,\n",
    "                    'recall': rec_p,\n",
    "                    'f1': f1s_p,\n",
    "                    'auc': auc_p,\n",
    "                    'specificity': spec_p,\n",
    "                    'npv': npv_p,\n",
    "                    'n_patients': len(patient_summary),\n",
    "                    'n_cells': X_gene.shape[0] if X_gene is not None else (X_seq.shape[0] if X_seq is not None else 0),\n",
    "                })\n",
    "            except Exception as e:\n",
    "                # Skip if patient-level aggregation fails due to insufficient data\n",
    "                print(f\"Patient aggregation failed for {arch}: {e}\")\n",
    "                pass\n",
    "\n",
    "    # Create final dataframes if data exists\n",
    "    if dl_results_rows:\n",
    "        dl_df = pd.DataFrame(dl_results_rows)\n",
    "        output_path = Path('Processed_Data') / 'dl_results.csv'\n",
    "        Path('Processed_Data').mkdir(exist_ok=True)\n",
    "        # dl_df.to_csv(output_path, index=False) # Uncomment to save\n",
    "        display(dl_df)\n",
    "    else:\n",
    "        print(\"No deep learning results to save (insufficient supervised data or model failures)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a8a2dfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T21:08:14.827795Z",
     "iopub.status.busy": "2026-02-13T21:08:14.827227Z",
     "iopub.status.idle": "2026-02-13T21:10:44.687734Z",
     "shell.execute_reply": "2026-02-13T21:10:44.686079Z"
    },
    "papermill": {
     "duration": 149.917576,
     "end_time": "2026-02-13T21:10:44.689998",
     "exception": false,
     "start_time": "2026-02-13T21:08:14.772422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimenting with sequence length cutoffs...\n",
      "Response distribution: {'Non-Responder': 63074, 'Responder': 36993}\n",
      "Patient_id coverage: 100067/100067\n",
      "\n",
      "Testing max sequence length: 10\n",
      "  Accuracy: 0.963, CV: 0.954 ± 0.005\n",
      "\n",
      "Testing max sequence length: 15\n",
      "  Accuracy: 0.963, CV: 0.954 ± 0.005\n",
      "\n",
      "Testing max sequence length: 20\n",
      "  Accuracy: 0.963, CV: 0.954 ± 0.005\n",
      "\n",
      "Testing max sequence length: 25\n",
      "  Accuracy: 0.963, CV: 0.954 ± 0.005\n",
      "\n",
      "Testing max sequence length: 30\n",
      "  Accuracy: 0.963, CV: 0.954 ± 0.005\n",
      "\n",
      "Testing max sequence length: 35\n",
      "  Accuracy: 0.963, CV: 0.954 ± 0.005\n",
      "\n",
      "Testing max sequence length: 40\n",
      "  Accuracy: 0.963, CV: 0.954 ± 0.005\n",
      "\n",
      "Testing max sequence length: 50\n",
      "  Accuracy: 0.963, CV: 0.954 ± 0.005\n",
      "\n",
      "Sequence length cutoff experiment completed!\n",
      "Optimal length appears to be around 10\n",
      "CPU times: user 7min 24s, sys: 4.71 s, total: 7min 29s\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Experiment with Sequence Length Cutoffs ---\n",
    "print(\"Experimenting with sequence length cutoffs...\")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "# Ensure adata exists\n",
    "if 'adata' not in globals() or adata is None:\n",
    "    raise NameError(\"adata is not defined. Please run the data loading cells first.\")\n",
    "# Ensure label/patient helpers have run\n",
    "if '_ensure_response_and_patient' in globals():\n",
    "    _ensure_response_and_patient(adata)\n",
    "if '_get_supervised_mask_and_labels' in globals():\n",
    "    supervised_mask, y_supervised, _le, class_counts, SUPERVISED_AVAILABLE = _get_supervised_mask_and_labels(adata)\n",
    "else:\n",
    "    # Fallback logic\n",
    "    if 'response' in adata.obs.columns:\n",
    "        y_all = adata.obs['response'].astype(str)\n",
    "        supervised_mask = y_all.isin(['Responder', 'Non-Responder']).values\n",
    "        y_supervised = y_all[supervised_mask]\n",
    "        class_counts = y_supervised.value_counts().to_dict()\n",
    "        SUPERVISED_AVAILABLE = len(class_counts) >= 2 and min(class_counts.values()) >= 2\n",
    "        _le = LabelEncoder() if SUPERVISED_AVAILABLE else None\n",
    "    else:\n",
    "        supervised_mask = np.zeros(adata.n_obs, dtype=bool)\n",
    "        y_supervised = pd.Series([], dtype=object)\n",
    "        class_counts = {}\n",
    "        SUPERVISED_AVAILABLE = False\n",
    "        _le = None\n",
    "globals()['SUPERVISED_AVAILABLE'] = SUPERVISED_AVAILABLE\n",
    "if not SUPERVISED_AVAILABLE:\n",
    "    print(\"WARNING: No supervised samples available. Skipping sequence length cutoff experiment.\")\n",
    "else:\n",
    "    supervised_mask = np.asarray(supervised_mask)\n",
    "    if '_ensure_int_labels' in globals():\n",
    "        y_encoded = _ensure_int_labels(_le.fit_transform(y_supervised))\n",
    "    else:\n",
    "        y_encoded = np.asarray(_le.fit_transform(y_supervised), dtype=np.int64)\n",
    "    min_class = min(class_counts.values()) if class_counts else 0\n",
    "    stratify = y_encoded if min_class >= 2 else None\n",
    "    cv_folds = min(3, min_class) if min_class >= 2 else 0\n",
    "    # Ensure cdr3_sequences exists\n",
    "    if 'cdr3_sequences' not in globals():\n",
    "        cdr3_sequences = {\n",
    "            'TRA': adata.obs['cdr3_TRA'].astype(str).fillna('').str.upper().tolist() if 'cdr3_TRA' in adata.obs.columns else [''] * adata.n_obs,\n",
    "            'TRB': adata.obs['cdr3_TRB'].astype(str).fillna('').str.upper().tolist() if 'cdr3_TRB' in adata.obs.columns else [''] * adata.n_obs\n",
    "        }\n",
    "    # Ensure gene features exist\n",
    "    if 'gene_features' not in globals():\n",
    "        if 'X_gene_pca' in adata.obsm:\n",
    "            gene_features = adata.obsm['X_gene_pca'][supervised_mask]\n",
    "        elif 'X_pca' in adata.obsm:\n",
    "            gene_features = adata.obsm['X_pca'][supervised_mask]\n",
    "        else:\n",
    "            gene_features = np.zeros((int(supervised_mask.sum()), 30))\n",
    "    if gene_features.shape[1] < 30:\n",
    "        gene_features = np.pad(gene_features, ((0, 0), (0, 30 - gene_features.shape[1])), mode='constant')\n",
    "    # Ensure TCR physico features exist\n",
    "    if 'tcr_physico' not in globals():\n",
    "        if all(c in adata.obs.columns for c in ['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity','trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']):\n",
    "            tra_physico = adata.obs[['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']].fillna(0)[supervised_mask].values.astype(np.float32)\n",
    "            trb_physico = adata.obs[['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']].fillna(0)[supervised_mask].values.astype(np.float32)\n",
    "            tcr_physico = np.column_stack([tra_physico, trb_physico]).astype(np.float32)\n",
    "        else:\n",
    "            tcr_physico = np.zeros((int(supervised_mask.sum()), 6), dtype=np.float32)\n",
    "    # Ensure QC features exist\n",
    "    if 'qc_features' not in globals():\n",
    "        if all(c in adata.obs.columns for c in ['n_genes_by_counts', 'total_counts', 'pct_counts_mt']):\n",
    "            qc_features = adata.obs[['n_genes_by_counts', 'total_counts', 'pct_counts_mt']].fillna(0)[supervised_mask].values\n",
    "        else:\n",
    "            qc_features = np.zeros((int(supervised_mask.sum()), 3))\n",
    "    # Define length cutoffs to test\n",
    "    length_cutoffs = [10, 15, 20, 25, 30, 35, 40, 50]\n",
    "    length_results = []\n",
    "    for max_length in length_cutoffs:\n",
    "        print(f\"\\nTesting max sequence length: {max_length}\")\n",
    "        # Re-encode sequences with new length - ENSURE FLOAT32 DTYPE\n",
    "        tra_onehot_new = np.array([one_hot_encode_sequence(seq, max_length, 'ACDEFGHIKLMNPQRSTVWY')\n",
    "                                   for seq in cdr3_sequences['TRA']], dtype=np.float32)\n",
    "        tra_onehot_flat_new = tra_onehot_new.reshape(tra_onehot_new.shape[0], -1).astype(np.float32)\n",
    "        trb_onehot_new = np.array([one_hot_encode_sequence(seq, max_length, 'ACDEFGHIKLMNPQRSTVWY')\n",
    "                                   for seq in cdr3_sequences['TRB']], dtype=np.float32)\n",
    "        trb_onehot_flat_new = trb_onehot_new.reshape(trb_onehot_new.shape[0], -1).astype(np.float32)\n",
    "        # Update AnnData\n",
    "        adata.obsm['X_tcr_tra_onehot'] = tra_onehot_flat_new\n",
    "        adata.obsm['X_tcr_trb_onehot'] = trb_onehot_flat_new\n",
    "        # Re-create feature sets with new encodings using robust PCA\n",
    "        # Use robust PCA reduction with fallback to TruncatedSVD\n",
    "        try:\n",
    "            n_comp_onehot = min(50, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[1], max(1, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[0]-1))\n",
    "            onehot_tra_reduced = PCA(n_components=n_comp_onehot, svd_solver='randomized', random_state=42).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "        except Exception as e:\n",
    "            print(f\"  PCA failed for TRA ({e}), using TruncatedSVD\")\n",
    "            n_comp = max(1, min(50, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[1]-1))\n",
    "            onehot_tra_reduced = TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "        try:\n",
    "            n_comp_onehot = min(50, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[1], max(1, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[0]-1))\n",
    "            onehot_trb_reduced = PCA(n_components=n_comp_onehot, svd_solver='randomized', random_state=42).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "        except Exception as e:\n",
    "            print(f\"  PCA failed for TRB ({e}), using TruncatedSVD\")\n",
    "            n_comp = max(1, min(50, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[1]-1))\n",
    "            onehot_trb_reduced = TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "        X_sequence = np.column_stack([\n",
    "            gene_features[:, :30],\n",
    "            onehot_tra_reduced,\n",
    "            onehot_trb_reduced,\n",
    "            tcr_physico,\n",
    "            qc_features\n",
    "        ])\n",
    "        # Train and evaluate model\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_sequence, y_encoded, test_size=0.3, random_state=42, stratify=stratify\n",
    "        )\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        XGBClass = globals().get('XGBClassifierSK', xgb.XGBClassifier)\n",
    "        model = XGBClass(random_state=42, eval_metric='logloss')\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Save model\n",
    "        try:\n",
    "             Path('Output/Models/SeqLength').mkdir(parents=True, exist_ok=True)\n",
    "             joblib.dump(model, f'Output/Models/SeqLength/model_len{max_length}.joblib')\n",
    "        except Exception as e:\n",
    "             print(f\"Failed to save model for len {max_length}: {e}\")\n",
    "\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        # Cross-validation\n",
    "        if cv_folds >= 2:\n",
    "            cv_scores = cross_val_score(model, X_sequence, y_encoded, cv=cv_folds, scoring='accuracy')\n",
    "            cv_mean = cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "        else:\n",
    "            print(\"  Skipping CV: not enough samples per class.\")\n",
    "            cv_scores = np.array([])\n",
    "            cv_mean = float('nan')\n",
    "            cv_std = float('nan')\n",
    "        length_results.append({\n",
    "            'max_length': max_length,\n",
    "            'accuracy': accuracy,\n",
    "            'cv_mean': cv_mean,\n",
    "            'cv_std': cv_std\n",
    "        })\n",
    "        print(f\"  Accuracy: {accuracy:.3f}, CV: {cv_mean:.3f} ± {cv_std:.3f}\")\n",
    "    # Plot results\n",
    "    length_df = pd.DataFrame(length_results)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(length_df['max_length'], length_df['accuracy'], 'o-', label='Test Accuracy', linewidth=2)\n",
    "    plt.plot(length_df['max_length'], length_df['cv_mean'], 's-', label='CV Accuracy', linewidth=2)\n",
    "    plt.fill_between(length_df['max_length'],\n",
    "                     length_df['cv_mean'] - length_df['cv_std'],\n",
    "                     length_df['cv_mean'] + length_df['cv_std'],\n",
    "                     alpha=0.3, label='CV ± Std')\n",
    "    plt.xlabel('Maximum Sequence Length Cutoff')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy vs Sequence Length Cutoff')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\nSequence length cutoff experiment completed!\")\n",
    "    if not length_df.empty and length_df['cv_mean'].notna().any():\n",
    "        best_len = length_df.loc[length_df['cv_mean'].idxmax(), 'max_length']\n",
    "        print(f\"Optimal length appears to be around {best_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f626879",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T21:10:44.801139Z",
     "iopub.status.busy": "2026-02-13T21:10:44.799992Z",
     "iopub.status.idle": "2026-02-13T21:10:44.893104Z",
     "shell.execute_reply": "2026-02-13T21:10:44.891972Z"
    },
    "papermill": {
     "duration": 0.150929,
     "end_time": "2026-02-13T21:10:44.894992",
     "exception": false,
     "start_time": "2026-02-13T21:10:44.744063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Safe GPU patcher: apply GPU defaults without forcing unknown attributes\n",
    "def _apply_gpu_patches():\n",
    "    \"\"\"\n",
    "    Safely patch `param_grids` and `models_eval` to prefer GPU XGBoost settings\n",
    "    when available. This avoids setting attributes that may not exist on\n",
    "    estimator objects and wraps callable factories/classes safely.\n",
    "    \"\"\"\n",
    "    import inspect\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "    except Exception:\n",
    "        xgb = None\n",
    "\n",
    "    # Respect explicit user flag if set elsewhere; default False\n",
    "    XGBOOST_GPU_AVAILABLE = bool(globals().get('XGBOOST_GPU_AVAILABLE', False))\n",
    "\n",
    "    # Patch param_grids safely (do not overwrite user-specified entries)\n",
    "    try:\n",
    "        if 'param_grids' in globals() and XGBOOST_GPU_AVAILABLE:\n",
    "            pg = dict(param_grids.get('XGBoost', {}))\n",
    "            pg.setdefault('tree_method', ['gpu_hist'])\n",
    "            pg.setdefault('predictor', ['gpu_predictor'])\n",
    "            param_grids['XGBoost'] = pg\n",
    "            print(\"Patched param_grids['XGBoost'] with GPU options.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error patching param_grids:\", e)\n",
    "\n",
    "    # Patch models_eval in-place (wrap factories/classes or safely set params on instances)\n",
    "    try:\n",
    "        if 'models_eval' not in globals():\n",
    "            return\n",
    "        me = globals()['models_eval']\n",
    "        if 'XGBoost' not in me:\n",
    "            return\n",
    "        obj = me['XGBoost']\n",
    "\n",
    "        # If it's a callable factory (e.g., a lambda returning an estimator), wrap it so GPU kwargs are tried safely at call time\n",
    "        if callable(obj) and not isinstance(obj, type):\n",
    "            def make_wrapped(factory):\n",
    "                def wrapped(*a, **kw):\n",
    "                    if globals().get('XGBOOST_GPU_AVAILABLE', False):\n",
    "                        try:\n",
    "                            kw2 = dict(kw)\n",
    "                            kw2.setdefault('tree_method', 'gpu_hist')\n",
    "                            kw2.setdefault('predictor', 'gpu_predictor')\n",
    "                            return factory(*a, **kw2)\n",
    "                        except TypeError:\n",
    "                            try:\n",
    "                                kw2 = dict(kw)\n",
    "                                kw2.setdefault('tree_method', 'gpu_hist')\n",
    "                                kw2.pop('predictor', None)\n",
    "                                return factory(*a, **kw2)\n",
    "                            except Exception:\n",
    "                                return factory(*a, **kw)\n",
    "                    return factory(*a, **kw)\n",
    "                return wrapped\n",
    "            me['XGBoost'] = make_wrapped(obj)\n",
    "            print(\"Patched callable models_eval['XGBoost'] to include GPU kwargs safely.\")\n",
    "            return\n",
    "\n",
    "        # If it's a class type, create a subclass wrapper to add defaults in __init__\n",
    "        if isinstance(obj, type):\n",
    "            try:\n",
    "                sig = inspect.signature(obj.__init__)\n",
    "            except Exception:\n",
    "                sig = None\n",
    "            def make_class_with_defaults(cls, sig):\n",
    "                class Wrapped(cls):\n",
    "                    def __init__(self, *a, **kw):\n",
    "                        if globals().get('XGBOOST_GPU_AVAILABLE', False):\n",
    "                            kw.setdefault('tree_method', 'gpu_hist')\n",
    "                            if sig and 'predictor' in sig.parameters:\n",
    "                                kw.setdefault('predictor', 'gpu_predictor')\n",
    "                        super().__init__(*a, **kw)\n",
    "                return Wrapped\n",
    "            me['XGBoost'] = make_class_with_defaults(obj, sig)\n",
    "            print(\"Patched class models_eval['XGBoost'] to include GPU defaults.\")\n",
    "            return\n",
    "\n",
    "        # Otherwise assume it's an instantiated estimator; set params only if supported\n",
    "        try:\n",
    "            if hasattr(obj, 'get_params') and hasattr(obj, 'set_params'):\n",
    "                params = obj.get_params()\n",
    "                patch = {}\n",
    "                if globals().get('XGBOOST_GPU_AVAILABLE', False):\n",
    "                    if 'tree_method' in params:\n",
    "                        patch['tree_method'] = 'gpu_hist'\n",
    "                    if 'predictor' in params:\n",
    "                        patch['predictor'] = 'gpu_predictor'\n",
    "                if patch:\n",
    "                    obj.set_params(**patch)\n",
    "                    print(\"Patched instance models_eval['XGBoost'] params.\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to patch models_eval['XGBoost']:\", e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error patching models_eval:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c987a2",
   "metadata": {
    "papermill": {
     "duration": 0.053288,
     "end_time": "2026-02-13T21:10:45.002533",
     "exception": false,
     "start_time": "2026-02-13T21:10:44.949245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Task 1-5: Enhanced ML Pipeline for Immunotherapy Response Prediction\n",
    "\n",
    "This section implements:\n",
    "1. **Task 1**: GroupKFold cross-validation with Patient-Level Aggregation (Shannon Entropy for TCR diversity)\n",
    "2. **Task 2**: TCR CDR3 encoding using physicochemical properties (Hydrophobicity, Charge, etc.)\n",
    "3. **Task 3**: Top 20 feature analysis cross-referenced with Sun et al. 2025 (GZMB, HLA-DR, ISGs)\n",
    "4. **Task 4**: Extended literature review including I-SPY2 trial and multimodal single-cell ML methods (TCR-H, CoNGA)\n",
    "5. **Task 5**: 4-panel publication figure (UMAP, SHAP, ROC, Boxplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fda9a7b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T21:10:45.114488Z",
     "iopub.status.busy": "2026-02-13T21:10:45.113856Z",
     "iopub.status.idle": "2026-02-13T21:10:45.233182Z",
     "shell.execute_reply": "2026-02-13T21:10:45.231975Z"
    },
    "papermill": {
     "duration": 0.179062,
     "end_time": "2026-02-13T21:10:45.234987",
     "exception": false,
     "start_time": "2026-02-13T21:10:45.055925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Missing functions added: compute_tcr_shannon_entropy, compute_tcr_diversity_metrics, aggregate_patient_features\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Missing Functions: TCR Diversity Metrics and Patient-Level Aggregation\n",
    "# ============================================================================\n",
    "# These functions are required for Task 1: GroupKFold Cross-Validation\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def compute_tcr_shannon_entropy(patient_df, chain='TRB'):\n",
    "    \"\"\"\n",
    "    Compute Shannon Entropy as a measure of TCR repertoire diversity.\n",
    "    \n",
    "    Shannon Entropy H = -Σ(p_i * log2(p_i))\n",
    "    \n",
    "    Higher entropy indicates more diverse repertoire (more uniform clone distribution)\n",
    "    Lower entropy indicates clonal expansion (dominated by few clones)\n",
    "    \n",
    "    Args:\n",
    "        patient_df: DataFrame containing TCR data for one patient\n",
    "        chain: 'TRA' or 'TRB'\n",
    "    \n",
    "    Returns:\n",
    "        Shannon entropy value (bits)\n",
    "    \"\"\"\n",
    "    cdr3_col = f'cdr3_{chain}'\n",
    "    if cdr3_col not in patient_df.columns:\n",
    "        return 0.0\n",
    "    \n",
    "    # Get CDR3 sequences, removing NaN\n",
    "    sequences = patient_df[cdr3_col].dropna().astype(str)\n",
    "    sequences = sequences[sequences != 'nan']\n",
    "    \n",
    "    if len(sequences) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Count clonotype frequencies\n",
    "    clone_counts = sequences.value_counts()\n",
    "    \n",
    "    # Compute probabilities\n",
    "    probabilities = clone_counts.values / clone_counts.sum()\n",
    "    \n",
    "    # Compute Shannon entropy (log base 2)\n",
    "    shannon_entropy = entropy(probabilities, base=2)\n",
    "    \n",
    "    return shannon_entropy\n",
    "\n",
    "\n",
    "def compute_tcr_diversity_metrics(patient_df):\n",
    "    \"\"\"\n",
    "    Compute comprehensive TCR diversity metrics for a patient.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - Shannon entropy for TRA and TRB\n",
    "    - Clonality (1 - normalized entropy)\n",
    "    - Number of unique clones\n",
    "    - Simpson's diversity index\n",
    "    - Repertoire overlap metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for chain in ['TRA', 'TRB']:\n",
    "        cdr3_col = f'cdr3_{chain}'\n",
    "        if cdr3_col not in patient_df.columns:\n",
    "            metrics[f'{chain}_shannon_entropy'] = 0.0\n",
    "            metrics[f'{chain}_clonality'] = 1.0\n",
    "            metrics[f'{chain}_n_unique_clones'] = 0\n",
    "            metrics[f'{chain}_simpson_diversity'] = 0.0\n",
    "            continue\n",
    "            \n",
    "        sequences = patient_df[cdr3_col].dropna().astype(str)\n",
    "        sequences = sequences[sequences != 'nan']\n",
    "        \n",
    "        if len(sequences) == 0:\n",
    "            metrics[f'{chain}_shannon_entropy'] = 0.0\n",
    "            metrics[f'{chain}_clonality'] = 1.0\n",
    "            metrics[f'{chain}_n_unique_clones'] = 0\n",
    "            metrics[f'{chain}_simpson_diversity'] = 0.0\n",
    "            continue\n",
    "        \n",
    "        clone_counts = sequences.value_counts()\n",
    "        n_unique = len(clone_counts)\n",
    "        total_cells = clone_counts.sum()\n",
    "        probabilities = clone_counts.values / total_cells\n",
    "        \n",
    "        # Shannon Entropy\n",
    "        shannon_ent = entropy(probabilities, base=2)\n",
    "        \n",
    "        # Clonality (normalized entropy)\n",
    "        max_entropy = np.log2(n_unique) if n_unique > 1 else 1.0\n",
    "        clonality = 1 - (shannon_ent / max_entropy) if max_entropy > 0 else 1.0\n",
    "        \n",
    "        # Simpson's Diversity Index: 1 - Σ(p_i^2)\n",
    "        simpson_div = 1 - np.sum(probabilities ** 2)\n",
    "        \n",
    "        metrics[f'{chain}_shannon_entropy'] = shannon_ent\n",
    "        metrics[f'{chain}_clonality'] = clonality\n",
    "        metrics[f'{chain}_n_unique_clones'] = n_unique\n",
    "        metrics[f'{chain}_simpson_diversity'] = simpson_div\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def aggregate_patient_features(adata):\n",
    "    \"\"\"\n",
    "    Aggregate cell-level features to patient-level by computing:\n",
    "    - Mean gene expression (from PCA components)\n",
    "    - TCR diversity metrics (Shannon Entropy)\n",
    "    - Physicochemical property means\n",
    "    - QC metric means\n",
    "    \n",
    "    Returns:\n",
    "        patient_features_df: DataFrame with one row per patient\n",
    "    \"\"\"\n",
    "    print(\"Aggregating cell-level features to patient-level...\")\n",
    "    \n",
    "    # Get unique patients with known response\n",
    "    valid_mask = adata.obs['response'].isin(['Responder', 'Non-Responder'])\n",
    "    obs_valid = adata.obs[valid_mask].copy()\n",
    "    \n",
    "    patients = obs_valid['patient_id'].unique()\n",
    "    print(f\"Found {len(patients)} patients with known response\")\n",
    "    \n",
    "    patient_records = []\n",
    "    \n",
    "    for patient_id in patients:\n",
    "        patient_mask = obs_valid['patient_id'] == patient_id\n",
    "        patient_df = obs_valid[patient_mask]\n",
    "        \n",
    "        record = {'Patient_ID': patient_id}\n",
    "        \n",
    "        # Response label (should be same for all cells from a patient)\n",
    "        record['Response'] = patient_df['response'].iloc[0]\n",
    "        record['n_cells'] = len(patient_df)\n",
    "        \n",
    "        # Get gene expression PCA means\n",
    "        if 'X_gene_pca' in adata.obsm:\n",
    "            patient_cells_idx = np.where(valid_mask)[0][patient_mask.values]\n",
    "            gene_pca = adata.obsm['X_gene_pca'][patient_cells_idx]\n",
    "            \n",
    "            # Mean of top 20 PCA components\n",
    "            for i in range(min(20, gene_pca.shape[1])):\n",
    "                record[f'gene_pca_mean_{i+1}'] = np.mean(gene_pca[:, i])\n",
    "                record[f'gene_pca_std_{i+1}'] = np.std(gene_pca[:, i])\n",
    "        \n",
    "        # TCR diversity metrics\n",
    "        tcr_metrics = compute_tcr_diversity_metrics(patient_df)\n",
    "        record.update(tcr_metrics)\n",
    "        \n",
    "        # Physicochemical property means\n",
    "        physico_cols = ['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity',\n",
    "                       'trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']\n",
    "        for col in physico_cols:\n",
    "            if col in patient_df.columns:\n",
    "                record[f'{col}_mean'] = patient_df[col].mean()\n",
    "                record[f'{col}_std'] = patient_df[col].std()\n",
    "        \n",
    "        # QC metrics\n",
    "        qc_cols = ['n_genes_by_counts', 'total_counts', 'pct_counts_mt']\n",
    "        for col in qc_cols:\n",
    "            if col in patient_df.columns:\n",
    "                record[f'{col}_mean'] = patient_df[col].mean()\n",
    "        \n",
    "        patient_records.append(record)\n",
    "    \n",
    "    patient_df = pd.DataFrame(patient_records)\n",
    "    print(f\"Created patient-level feature matrix: {patient_df.shape}\")\n",
    "    \n",
    "    return patient_df\n",
    "\n",
    "print(\"✓ Missing functions added: compute_tcr_shannon_entropy, compute_tcr_diversity_metrics, aggregate_patient_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cda3073",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T21:10:45.343532Z",
     "iopub.status.busy": "2026-02-13T21:10:45.342952Z",
     "iopub.status.idle": "2026-02-13T21:10:46.206721Z",
     "shell.execute_reply": "2026-02-13T21:10:46.204506Z"
    },
    "papermill": {
     "duration": 0.920957,
     "end_time": "2026-02-13T21:10:46.208945",
     "exception": false,
     "start_time": "2026-02-13T21:10:45.287988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating cell-level features to patient-level...\n",
      "Found 4 patients with known response\n",
      "Created patient-level feature matrix: (4, 66)\n",
      "\n",
      "--- Patient-Level Feature Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Response</th>\n",
       "      <th>n_cells</th>\n",
       "      <th>TRA_shannon_entropy</th>\n",
       "      <th>TRB_shannon_entropy</th>\n",
       "      <th>TRA_clonality</th>\n",
       "      <th>TRB_clonality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PT2</td>\n",
       "      <td>Responder</td>\n",
       "      <td>11635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PT4</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>30514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PT3</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>32560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PT1</td>\n",
       "      <td>Responder</td>\n",
       "      <td>25358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Patient_ID       Response  n_cells  TRA_shannon_entropy  \\\n",
       "0        PT2      Responder    11635                  0.0   \n",
       "1        PT4  Non-Responder    30514                  0.0   \n",
       "2        PT3  Non-Responder    32560                  0.0   \n",
       "3        PT1      Responder    25358                  0.0   \n",
       "\n",
       "   TRB_shannon_entropy  TRA_clonality  TRB_clonality  \n",
       "0                  0.0            1.0            1.0  \n",
       "1                  0.0            1.0            1.0  \n",
       "2                  0.0            1.0            1.0  \n",
       "3                  0.0            1.0            1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training with GroupKFold Cross-Validation\n",
      "============================================================\n",
      "Feature matrix: (4, 63)\n",
      "Labels: 4, Classes: ['Non-Responder' 'Responder']\n",
      "Patient groups: 4\n",
      "Using 4-fold GroupKFold CV\n",
      "\n",
      "--- Cross-Validation Results ---\n",
      "Accuracy:  0.0000\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 Score:  0.0000\n",
      "ROC-AUC:   0.0000\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Responder       0.00      0.00      0.00       2.0\n",
      "    Responder       0.00      0.00      0.00       2.0\n",
      "\n",
      "     accuracy                           0.00       4.0\n",
      "    macro avg       0.00      0.00      0.00       4.0\n",
      " weighted avg       0.00      0.00      0.00       4.0\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[0 2]\n",
      " [2 0]]\n",
      "\n",
      "--- Top 10 Most Important Features ---\n",
      "        feature  importance\n",
      "gene_pca_mean_1         0.0\n",
      " gene_pca_std_1         0.0\n",
      "gene_pca_mean_2         0.0\n",
      " gene_pca_std_2         0.0\n",
      "gene_pca_mean_3         0.0\n",
      " gene_pca_std_3         0.0\n",
      "gene_pca_mean_4         0.0\n",
      " gene_pca_std_4         0.0\n",
      "gene_pca_mean_5         0.0\n",
      " gene_pca_std_5         0.0\n",
      "\n",
      "Results saved to Processed_Data/\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "def train_groupkfold_model(patient_df, n_splits=None):\n",
    "    \"\"\"\n",
    "    Train XGBoost model with GroupKFold cross-validation based on Patient_ID.\n",
    "    \"\"\"\n",
    "    base_result = {\n",
    "        'status': 'skipped',\n",
    "        'reason': '',\n",
    "        'cv_accuracy': float('nan'),\n",
    "        'cv_precision': float('nan'),\n",
    "        'cv_recall': float('nan'),\n",
    "        'cv_f1': float('nan'),\n",
    "        'cv_roc_auc': float('nan'),\n",
    "        'confusion_matrix': np.array([]),\n",
    "        'y_true': np.array([]),\n",
    "        'y_pred': np.array([]),\n",
    "        'y_pred_proba': np.array([]),\n",
    "        'feature_importance': pd.DataFrame(),\n",
    "        'model': None,\n",
    "        'scaler': None,\n",
    "        'label_encoder': None,\n",
    "        'feature_cols': [],\n",
    "        'patient_df': patient_df if patient_df is not None else pd.DataFrame()\n",
    "    }\n",
    "    if patient_df is None or patient_df.empty:\n",
    "        base_result['reason'] = 'No patient-level data available.'\n",
    "        print('WARNING:', base_result['reason'])\n",
    "        return base_result\n",
    "    if 'Patient_ID' not in patient_df.columns or 'Response' not in patient_df.columns:\n",
    "        base_result['reason'] = 'Missing required columns Patient_ID/Response in patient_df.'\n",
    "        print('WARNING:', base_result['reason'])\n",
    "        return base_result\n",
    "    n_patients = patient_df['Patient_ID'].nunique()\n",
    "    if n_patients < 2:\n",
    "        base_result['reason'] = f'Not enough patients for GroupKFold (n={n_patients}).'\n",
    "        print('WARNING:', base_result['reason'])\n",
    "        return base_result\n",
    "    n_classes = patient_df['Response'].nunique()\n",
    "    if n_classes < 2:\n",
    "        base_result['reason'] = f'Not enough response classes for supervised learning (n={n_classes}).'\n",
    "        print('WARNING:', base_result['reason'])\n",
    "        return base_result\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training with GroupKFold Cross-Validation\")\n",
    "    print(\"=\"*60)\n",
    "    # Prepare features and labels\n",
    "    feature_cols = [col for col in patient_df.columns if col not in ['Patient_ID', 'Response', 'n_cells']]\n",
    "    if not feature_cols:\n",
    "        base_result['reason'] = 'No feature columns available for training.'\n",
    "        print('WARNING:', base_result['reason'])\n",
    "        return base_result\n",
    "    X = patient_df[feature_cols].fillna(0).values\n",
    "    y_labels = patient_df['Response'].values\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_labels)\n",
    "    y = _ensure_int_labels(y) if '_ensure_int_labels' in globals() else np.asarray(y, dtype=np.int64)\n",
    "    groups = patient_df['Patient_ID'].values\n",
    "    print(f\"Feature matrix: {X.shape}\")\n",
    "    print(f\"Labels: {len(y)}, Classes: {label_encoder.classes_}\")\n",
    "    print(f\"Patient groups: {len(np.unique(groups))}\")\n",
    "    # Determine number of splits\n",
    "    if n_splits is None:\n",
    "        n_splits = min(5, n_patients)\n",
    "    if n_splits < 2:\n",
    "        base_result['reason'] = f'Not enough patients for {n_splits}-fold GroupKFold.'\n",
    "        print('WARNING:', base_result['reason'])\n",
    "        return base_result\n",
    "    print(f\"Using {n_splits}-fold GroupKFold CV\")\n",
    "    # Initialize model\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    # Perform cross-validation\n",
    "    cv = GroupKFold(n_splits=n_splits)\n",
    "    # Get cross-validated predictions\n",
    "    y_pred = cross_val_predict(model, X, y, groups=groups, cv=cv, n_jobs=1)\n",
    "    y_pred_proba = cross_val_predict(model, X, y, groups=groups, cv=cv, method='predict_proba', n_jobs=1)\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, zero_division=0)\n",
    "    recall = recall_score(y, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y, y_pred, zero_division=0)\n",
    "    # ROC-AUC (handle binary classification)\n",
    "    try:\n",
    "        if len(label_encoder.classes_) == 2:\n",
    "            roc_auc = roc_auc_score(y, y_pred_proba[:, 1])\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y, y_pred_proba, multi_class='ovr')\n",
    "    except Exception:\n",
    "        roc_auc = float('nan')\n",
    "    print(\"\\n--- Cross-Validation Results ---\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred, target_names=label_encoder.classes_, zero_division=0))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(cm)\n",
    "    # Train final model on all data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    final_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    final_model.fit(X_scaled, y)\n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': final_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(\"\\n--- Top 10 Most Important Features ---\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "    # Package results\n",
    "    results = {\n",
    "        'status': 'ok',\n",
    "        'reason': '',\n",
    "        'cv_accuracy': accuracy,\n",
    "        'cv_precision': precision,\n",
    "        'cv_recall': recall,\n",
    "        'cv_f1': f1,\n",
    "        'cv_roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_true': y,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'feature_importance': feature_importance,\n",
    "        'model': final_model,\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': label_encoder,\n",
    "        'feature_cols': feature_cols,\n",
    "        'patient_df': patient_df\n",
    "    }\n",
    "    return results\n",
    "# ==========================================================================\n",
    "# Execute Task 1\n",
    "# ==========================================================================\n",
    "# Aggregate features at patient level\n",
    "patient_features_df = aggregate_patient_features(adata)\n",
    "# Display patient-level features (only available columns)\n",
    "print(\"\\n--- Patient-Level Feature Summary ---\")\n",
    "summary_cols = ['Patient_ID', 'Response', 'n_cells', 'TRA_shannon_entropy', 'TRB_shannon_entropy',\n",
    "                'TRA_clonality', 'TRB_clonality']\n",
    "summary_cols = [c for c in summary_cols if c in patient_features_df.columns]\n",
    "if summary_cols:\n",
    "    display(patient_features_df[summary_cols].round(3))\n",
    "else:\n",
    "    print(\"No patient-level summary columns available to display.\")\n",
    "# Train with GroupKFold CV\n",
    "groupcv_results = train_groupkfold_model(patient_features_df)\n",
    "# Save results only if training succeeded\n",
    "if groupcv_results.get('status') == 'ok':\n",
    "    output_dir = Path('Processed_Data')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    patient_features_df.to_csv(output_dir / 'patient_level_features.csv', index=False)\n",
    "    joblib.dump(groupcv_results['model'], output_dir / 'patient_level_model_groupcv.joblib')\n",
    "    groupcv_results['feature_importance'].to_csv(output_dir / 'patient_level_groupcv_results.csv', index=False)\n",
    "    print(\"\\nResults saved to Processed_Data/\")\n",
    "else:\n",
    "    print(f\"\\nSkipping save. Reason: {groupcv_results.get('reason')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c96c3c2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T21:10:46.320727Z",
     "iopub.status.busy": "2026-02-13T21:10:46.320106Z",
     "iopub.status.idle": "2026-02-13T21:10:48.705038Z",
     "shell.execute_reply": "2026-02-13T21:10:48.703439Z"
    },
    "papermill": {
     "duration": 2.442485,
     "end_time": "2026-02-13T21:10:48.707011",
     "exception": false,
     "start_time": "2026-02-13T21:10:46.264526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 2: Enhanced TCR CDR3 Physicochemical Encoding\n",
      "================================================================================\n",
      "Encoding CDR3 sequences with enhanced physicochemical properties...\n",
      "Encoding 26 physicochemical features per sequence\n",
      "TRA physicochemical matrix shape: (100067, 26)\n",
      "TRB physicochemical matrix shape: (100067, 26)\n",
      "\n",
      "--- Enhanced Physicochemical Feature Summary ---\n",
      "Total features per chain: 26\n",
      "Feature names: ['hydro_mean', 'hydro_sum', 'hydro_min', 'hydro_max', 'hydro_range', 'hydro_std', 'net_charge', 'positive_aa_count', 'negative_aa_count', 'charge_ratio', 'polarity_mean', 'polarity_std', 'length', 'total_mw', 'mean_mw', 'mean_volume', 'total_volume', 'flexibility_mean', 'flexibility_max', 'beta_propensity_mean', 'nterm_hydro', 'cterm_hydro', 'middle_hydro', 'nterm_charge', 'cterm_charge', 'middle_charge']\n",
      "\n",
      "--- Physicochemical Comparison: Responder vs Non-Responder ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>TRA_Responder</th>\n",
       "      <th>TRA_NonResponder</th>\n",
       "      <th>TRA_Diff</th>\n",
       "      <th>TRB_Responder</th>\n",
       "      <th>TRB_NonResponder</th>\n",
       "      <th>TRB_Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hydro_mean</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>net_charge</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>polarity_mean</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flexibility_mean</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>length</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Feature  TRA_Responder  TRA_NonResponder  TRA_Diff  TRB_Responder  \\\n",
       "0        hydro_mean            0.0               0.0       0.0            0.0   \n",
       "1        net_charge            0.0               0.0       0.0            0.0   \n",
       "2     polarity_mean            0.0               0.0       0.0            0.0   \n",
       "3  flexibility_mean            0.0               0.0       0.0            0.0   \n",
       "4            length            0.0               0.0       0.0            0.0   \n",
       "\n",
       "   TRB_NonResponder  TRB_Diff  \n",
       "0               0.0       0.0  \n",
       "1               0.0       0.0  \n",
       "2               0.0       0.0  \n",
       "3               0.0       0.0  \n",
       "4               0.0       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 2 COMPLETED: Enhanced TCR Physicochemical Encoding\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TASK 2: Enhanced TCR CDR3 Encoding with Physicochemical Properties\n",
    "This cell implements comprehensive TCR CDR3 encoding using:\n",
    "- Hydrophobicity (Kyte-Doolittle scale)\n",
    "- Charge (based on pKa values)\n",
    "- Polarity\n",
    "- Molecular weight\n",
    "- Volume\n",
    "- Flexibility\n",
    "- Additional biochemical indices\n",
    "\n",
    "These features capture the biophysical properties that govern TCR-antigen binding.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TASK 2: Enhanced TCR CDR3 Physicochemical Encoding\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Kyte-Doolittle Hydrophobicity Scale (higher = more hydrophobic)\n",
    "HYDROPHOBICITY_KD = {\n",
    "    'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,\n",
    "    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,\n",
    "    'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,\n",
    "    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2\n",
    "}\n",
    "\n",
    "# Amino Acid Charge at pH 7 (approximate)\n",
    "CHARGE = {\n",
    "    'A': 0, 'R': 1, 'N': 0, 'D': -1, 'C': 0,\n",
    "    'Q': 0, 'E': -1, 'G': 0, 'H': 0.1, 'I': 0,  # H is ~10% protonated at pH 7\n",
    "    'L': 0, 'K': 1, 'M': 0, 'F': 0, 'P': 0,\n",
    "    'S': 0, 'T': 0, 'W': 0, 'Y': 0, 'V': 0\n",
    "}\n",
    "\n",
    "# Polarity (Grantham, 1974)\n",
    "POLARITY = {\n",
    "    'A': 8.1, 'R': 10.5, 'N': 11.6, 'D': 13.0, 'C': 5.5,\n",
    "    'Q': 10.5, 'E': 12.3, 'G': 9.0, 'H': 10.4, 'I': 5.2,\n",
    "    'L': 4.9, 'K': 11.3, 'M': 5.7, 'F': 5.2, 'P': 8.0,\n",
    "    'S': 9.2, 'T': 8.6, 'W': 5.4, 'Y': 6.2, 'V': 5.9\n",
    "}\n",
    "\n",
    "# Molecular Weight (Da)\n",
    "MOLECULAR_WEIGHT = {\n",
    "    'A': 89.1, 'R': 174.2, 'N': 132.1, 'D': 133.1, 'C': 121.2,\n",
    "    'Q': 146.2, 'E': 147.1, 'G': 75.1, 'H': 155.2, 'I': 131.2,\n",
    "    'L': 131.2, 'K': 146.2, 'M': 149.2, 'F': 165.2, 'P': 115.1,\n",
    "    'S': 105.1, 'T': 119.1, 'W': 204.2, 'Y': 181.2, 'V': 117.1\n",
    "}\n",
    "\n",
    "# Volume (Å³) - Zamyatnin, 1972\n",
    "VOLUME = {\n",
    "    'A': 88.6, 'R': 173.4, 'N': 114.1, 'D': 111.1, 'C': 108.5,\n",
    "    'Q': 143.8, 'E': 138.4, 'G': 60.1, 'H': 153.2, 'I': 166.7,\n",
    "    'L': 166.7, 'K': 168.6, 'M': 162.9, 'F': 189.9, 'P': 112.7,\n",
    "    'S': 89.0, 'T': 116.1, 'W': 227.8, 'Y': 193.6, 'V': 140.0\n",
    "}\n",
    "\n",
    "# Flexibility Index (Bhaskaran-Ponnuswamy, 1988)\n",
    "FLEXIBILITY = {\n",
    "    'A': 0.360, 'R': 0.530, 'N': 0.460, 'D': 0.510, 'C': 0.350,\n",
    "    'Q': 0.490, 'E': 0.500, 'G': 0.540, 'H': 0.320, 'I': 0.460,\n",
    "    'L': 0.370, 'K': 0.470, 'M': 0.300, 'F': 0.310, 'P': 0.510,\n",
    "    'S': 0.510, 'T': 0.440, 'W': 0.310, 'Y': 0.420, 'V': 0.390\n",
    "}\n",
    "\n",
    "# Beta-sheet propensity (Chou-Fasman)\n",
    "BETA_SHEET = {\n",
    "    'A': 0.83, 'R': 0.93, 'N': 0.89, 'D': 0.54, 'C': 1.19,\n",
    "    'Q': 1.10, 'E': 0.37, 'G': 0.75, 'H': 0.87, 'I': 1.60,\n",
    "    'L': 1.30, 'K': 0.74, 'M': 1.05, 'F': 1.38, 'P': 0.55,\n",
    "    'S': 0.75, 'T': 1.19, 'W': 1.37, 'Y': 1.47, 'V': 1.70\n",
    "}\n",
    "\n",
    "\n",
    "def encode_cdr3_physicochemical(sequence, return_features_dict=False):\n",
    "    \"\"\"\n",
    "    Encode a CDR3 sequence using comprehensive physicochemical properties.\n",
    "    \n",
    "    Features computed:\n",
    "    1. Hydrophobicity: mean, sum, min, max, range\n",
    "    2. Charge: net charge, positive count, negative count, charge ratio\n",
    "    3. Polarity: mean, std\n",
    "    4. Size: length, total molecular weight, mean volume\n",
    "    5. Flexibility: mean, max\n",
    "    6. Beta-sheet propensity: mean\n",
    "    7. Positional features: N-term, C-term, middle region properties\n",
    "    \n",
    "    Args:\n",
    "        sequence: CDR3 amino acid sequence string\n",
    "        return_features_dict: If True, return dict with feature names\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of features (or dict if return_features_dict=True)\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence in ['nan', 'NA', '', None]:\n",
    "        n_features = 26  # Total number of features\n",
    "        if return_features_dict:\n",
    "            return {f'physico_feature_{i}': 0.0 for i in range(n_features)}\n",
    "        return np.zeros(n_features)\n",
    "    \n",
    "    seq = str(sequence).upper()\n",
    "    # Filter to valid amino acids\n",
    "    valid_aa = set(HYDROPHOBICITY_KD.keys())\n",
    "    seq = ''.join([c for c in seq if c in valid_aa])\n",
    "    \n",
    "    if len(seq) == 0:\n",
    "        n_features = 26\n",
    "        if return_features_dict:\n",
    "            return {f'physico_feature_{i}': 0.0 for i in range(n_features)}\n",
    "        return np.zeros(n_features)\n",
    "    \n",
    "    features = OrderedDict()\n",
    "    \n",
    "    # === Hydrophobicity Features ===\n",
    "    hydro_values = [HYDROPHOBICITY_KD.get(aa, 0) for aa in seq]\n",
    "    features['hydro_mean'] = np.mean(hydro_values)\n",
    "    features['hydro_sum'] = np.sum(hydro_values)\n",
    "    features['hydro_min'] = np.min(hydro_values)\n",
    "    features['hydro_max'] = np.max(hydro_values)\n",
    "    features['hydro_range'] = np.max(hydro_values) - np.min(hydro_values)\n",
    "    features['hydro_std'] = np.std(hydro_values) if len(hydro_values) > 1 else 0\n",
    "    \n",
    "    # === Charge Features ===\n",
    "    charge_values = [CHARGE.get(aa, 0) for aa in seq]\n",
    "    features['net_charge'] = np.sum(charge_values)\n",
    "    features['positive_aa_count'] = sum(1 for c in charge_values if c > 0)\n",
    "    features['negative_aa_count'] = sum(1 for c in charge_values if c < 0)\n",
    "    features['charge_ratio'] = (features['positive_aa_count'] / \n",
    "                                (features['negative_aa_count'] + 1))  # +1 to avoid div by zero\n",
    "    \n",
    "    # === Polarity Features ===\n",
    "    polarity_values = [POLARITY.get(aa, 0) for aa in seq]\n",
    "    features['polarity_mean'] = np.mean(polarity_values)\n",
    "    features['polarity_std'] = np.std(polarity_values) if len(polarity_values) > 1 else 0\n",
    "    \n",
    "    # === Size Features ===\n",
    "    features['length'] = len(seq)\n",
    "    mw_values = [MOLECULAR_WEIGHT.get(aa, 0) for aa in seq]\n",
    "    features['total_mw'] = np.sum(mw_values)\n",
    "    features['mean_mw'] = np.mean(mw_values)\n",
    "    \n",
    "    volume_values = [VOLUME.get(aa, 0) for aa in seq]\n",
    "    features['mean_volume'] = np.mean(volume_values)\n",
    "    features['total_volume'] = np.sum(volume_values)\n",
    "    \n",
    "    # === Flexibility Features ===\n",
    "    flex_values = [FLEXIBILITY.get(aa, 0) for aa in seq]\n",
    "    features['flexibility_mean'] = np.mean(flex_values)\n",
    "    features['flexibility_max'] = np.max(flex_values)\n",
    "    \n",
    "    # === Beta-sheet Propensity ===\n",
    "    beta_values = [BETA_SHEET.get(aa, 0) for aa in seq]\n",
    "    features['beta_propensity_mean'] = np.mean(beta_values)\n",
    "    \n",
    "    # === Positional Features (N-term, C-term, Middle) ===\n",
    "    # CDR3 regions often have conserved ends and variable middle\n",
    "    n_term = seq[:3] if len(seq) >= 3 else seq\n",
    "    c_term = seq[-3:] if len(seq) >= 3 else seq\n",
    "    middle = seq[3:-3] if len(seq) > 6 else seq\n",
    "    \n",
    "    features['nterm_hydro'] = np.mean([HYDROPHOBICITY_KD.get(aa, 0) for aa in n_term])\n",
    "    features['cterm_hydro'] = np.mean([HYDROPHOBICITY_KD.get(aa, 0) for aa in c_term])\n",
    "    features['middle_hydro'] = np.mean([HYDROPHOBICITY_KD.get(aa, 0) for aa in middle]) if middle else 0\n",
    "    \n",
    "    features['nterm_charge'] = np.sum([CHARGE.get(aa, 0) for aa in n_term])\n",
    "    features['cterm_charge'] = np.sum([CHARGE.get(aa, 0) for aa in c_term])\n",
    "    features['middle_charge'] = np.sum([CHARGE.get(aa, 0) for aa in middle]) if middle else 0\n",
    "    \n",
    "    if return_features_dict:\n",
    "        return features\n",
    "    \n",
    "    return np.array(list(features.values()))\n",
    "\n",
    "\n",
    "def encode_all_cdr3_physicochemical(adata):\n",
    "    \"\"\"\n",
    "    Encode all CDR3 sequences in the AnnData object with physicochemical features.\n",
    "    \n",
    "    Creates:\n",
    "    - adata.obsm['X_tcr_tra_physico_enhanced']: Enhanced TRA physicochemical features\n",
    "    - adata.obsm['X_tcr_trb_physico_enhanced']: Enhanced TRB physicochemical features\n",
    "    - Combined features added to adata.obs\n",
    "    \"\"\"\n",
    "    print(\"Encoding CDR3 sequences with enhanced physicochemical properties...\")\n",
    "    \n",
    "    # Get feature names from a sample encoding\n",
    "    sample_features = encode_cdr3_physicochemical('CASSYSGANVLTF', return_features_dict=True)\n",
    "    feature_names = list(sample_features.keys())\n",
    "    print(f\"Encoding {len(feature_names)} physicochemical features per sequence\")\n",
    "    \n",
    "    # Encode TRA sequences (safe if column missing)\n",
    "    tra_encodings = []\n",
    "    tra_iter = adata.obs['cdr3_TRA'].astype(str) if 'cdr3_TRA' in adata.obs.columns else pd.Series([''] * adata.n_obs, index=adata.obs.index)\n",
    "    for seq in tra_iter:\n",
    "        tra_encodings.append(encode_cdr3_physicochemical(seq))\n",
    "    tra_matrix = np.vstack(tra_encodings)\n",
    "    \n",
    "    # Encode TRB sequences (safe if column missing)\n",
    "    trb_encodings = []\n",
    "    trb_iter = adata.obs['cdr3_TRB'].astype(str) if 'cdr3_TRB' in adata.obs.columns else pd.Series([''] * adata.n_obs, index=adata.obs.index)\n",
    "    for seq in trb_iter:\n",
    "        trb_encodings.append(encode_cdr3_physicochemical(seq))\n",
    "    trb_matrix = np.vstack(trb_encodings)\n",
    "    \n",
    "    print(f\"TRA physicochemical matrix shape: {tra_matrix.shape}\")\n",
    "    print(f\"TRB physicochemical matrix shape: {trb_matrix.shape}\")\n",
    "    \n",
    "    # Ensure matrices are float32 for AnnData compatibility\n",
    "    tra_matrix = tra_matrix.astype(np.float32)\n",
    "    trb_matrix = trb_matrix.astype(np.float32)\n",
    "    \n",
    "    # Store in AnnData\n",
    "    adata.obsm['X_tcr_tra_physico_enhanced'] = tra_matrix\n",
    "    adata.obsm['X_tcr_trb_physico_enhanced'] = trb_matrix\n",
    "    \n",
    "    # Also add individual features to obs for easy access (ensure float type)\n",
    "    for i, fname in enumerate(feature_names):\n",
    "        adata.obs[f'tra_enhanced_{fname}'] = pd.Series(tra_matrix[:, i], index=adata.obs.index, dtype=np.float32)\n",
    "        adata.obs[f'trb_enhanced_{fname}'] = pd.Series(trb_matrix[:, i], index=adata.obs.index, dtype=np.float32)\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Execute Task 2\n",
    "# ============================================================================\n",
    "feature_names_physico = encode_all_cdr3_physicochemical(adata)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n--- Enhanced Physicochemical Feature Summary ---\")\n",
    "print(f\"Total features per chain: {len(feature_names_physico)}\")\n",
    "print(f\"Feature names: {feature_names_physico}\")\n",
    "\n",
    "# Compare responder vs non-responder\n",
    "print(\"\\n--- Physicochemical Comparison: Responder vs Non-Responder ---\")\n",
    "resp_mask = adata.obs['response'] == 'Responder'\n",
    "non_resp_mask = adata.obs['response'] == 'Non-Responder'\n",
    "\n",
    "comparison_df = []\n",
    "for fname in ['hydro_mean', 'net_charge', 'polarity_mean', 'flexibility_mean', 'length']:\n",
    "    tra_col = f'tra_enhanced_{fname}'\n",
    "    trb_col = f'trb_enhanced_{fname}'\n",
    "    \n",
    "    if tra_col in adata.obs.columns:\n",
    "        resp_tra = adata.obs.loc[resp_mask, tra_col].mean()\n",
    "        nonresp_tra = adata.obs.loc[non_resp_mask, tra_col].mean()\n",
    "        resp_trb = adata.obs.loc[resp_mask, trb_col].mean()\n",
    "        nonresp_trb = adata.obs.loc[non_resp_mask, trb_col].mean()\n",
    "        \n",
    "        comparison_df.append({\n",
    "            'Feature': fname,\n",
    "            'TRA_Responder': resp_tra,\n",
    "            'TRA_NonResponder': nonresp_tra,\n",
    "            'TRA_Diff': resp_tra - nonresp_tra,\n",
    "            'TRB_Responder': resp_trb,\n",
    "            'TRB_NonResponder': nonresp_trb,\n",
    "            'TRB_Diff': resp_trb - nonresp_trb\n",
    "        })\n",
    "\n",
    "display(pd.DataFrame(comparison_df).round(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 2 COMPLETED: Enhanced TCR Physicochemical Encoding\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9aa12ee9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T21:10:48.817981Z",
     "iopub.status.busy": "2026-02-13T21:10:48.816535Z",
     "iopub.status.idle": "2026-02-13T21:10:51.188445Z",
     "shell.execute_reply": "2026-02-13T21:10:51.186779Z"
    },
    "papermill": {
     "duration": 2.429372,
     "end_time": "2026-02-13T21:10:51.190522",
     "exception": false,
     "start_time": "2026-02-13T21:10:48.761150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TASK 3: Top 20 Feature Analysis Cross-Referenced with Sun et al. 2025\n",
    "This cell analyzes top predictive features and cross-references them with:\n",
    "- GZMB (Granzyme B) - key cytotoxicity marker\n",
    "- HLA-DR genes - antigen presentation\n",
    "- Interferon-Stimulated Genes (ISGs)\n",
    "- Other markers identified in Sun et al. 2025\n",
    "\n",
    "Reference: Sun et al. 2025, npj Breast Cancer 11:65\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Validate prerequisites\n",
    "if 'adata' not in globals() or adata is None:\n",
    "    raise NameError(\"adata is not defined. Please run data loading cells first.\")\n",
    "\n",
    "if 'groupcv_results' not in globals() or groupcv_results is None:\n",
    "    print(\"WARNING: groupcv_results not defined. Skipping analyze_top_features().\")\n",
    "    print(\"Please run the patient-level LOPO CV cell first.\")\n",
    "    # Create a minimal placeholder to avoid errors downstream\n",
    "    groupcv_results = {'feature_importance': pd.DataFrame(columns=['feature', 'importance'])}\n",
    "\n",
    "# SHAP is optional for this cell; avoid hard failure if missing\n",
    "try:\n",
    "    import shap\n",
    "except Exception:\n",
    "    shap = None\n",
    "    print(\"shap not available; skipping SHAP-specific utilities in Task 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "893de18e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T21:10:51.305853Z",
     "iopub.status.busy": "2026-02-13T21:10:51.301701Z",
     "iopub.status.idle": "2026-02-13T21:13:55.055591Z",
     "shell.execute_reply": "2026-02-13T21:13:55.053809Z"
    },
    "papermill": {
     "duration": 183.812008,
     "end_time": "2026-02-13T21:13:55.057774",
     "exception": false,
     "start_time": "2026-02-13T21:10:51.245766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 5: Publication-Quality 4-Panel Figure\n",
      "================================================================================\n",
      "\n",
      "--- Creating Publication Figure ---\n",
      "Creating Panel A: UMAP visualization...\n",
      "Creating Panel B: SHAP importance plot...\n",
      "  SHAP computation failed: could not convert string to float: '[5E-1]'\n",
      "Creating Panel C: Patient-level ROC curve...\n",
      "Creating Panel D: Biomarker boxplots...\n",
      "  Plotting markers: ['GZMB', 'HLA-DRA', 'ISG15']\n",
      "Saved: Processed_Data/figures/Figure_Multimodal_ML_Response.png\n",
      "Saved: Processed_Data/figures/Figure_Multimodal_ML_Response.pdf\n",
      "Saved: Processed_Data/figures/Figure_Multimodal_ML_Response.svg\n",
      "\n",
      "================================================================================\n",
      "TASK 5 COMPLETED: Publication-Quality 4-Panel Figure Generated\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TASK 5: Publication-Quality 4-Panel Figure\n",
    "This cell generates a comprehensive 4-panel figure suitable for publication:\n",
    "1. UMAP of cell types colored by response and cell type\n",
    "2. SHAP importance plot for the multimodal model\n",
    "3. Patient-level ROC curve from GroupKFold CV\n",
    "4. Boxplots of top 3 biological markers (GZMB, HLA-DR, ISG)\n",
    "\n",
    "Figure design follows journal guidelines for Nature/Cell Press publications.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install SHAP if needed\n",
    "try:\n",
    "    import shap\n",
    "except ImportError:\n",
    "    %pip install shap\n",
    "    import shap\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TASK 5: Publication-Quality 4-Panel Figure\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set publication-quality defaults\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': ['Arial', 'DejaVu Sans'],\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 9,\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.transparent': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "})\n",
    "\n",
    "# Color palette\n",
    "COLORS = {\n",
    "    'Responder': '#2ecc71',       # Green\n",
    "    'Non-Responder': '#e74c3c',   # Red\n",
    "    'Unknown': '#95a5a6',         # Gray\n",
    "    'accent': '#3498db',          # Blue\n",
    "    'purple': '#9b59b6',          # Purple\n",
    "    'orange': '#e67e22',          # Orange\n",
    "}\n",
    "\n",
    "\n",
    "def panel_placeholder(ax, title, message):\n",
    "    \"\"\"Render a placeholder panel with a message.\"\"\"\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontweight='bold', loc='left')\n",
    "    ax.text(0.5, 0.5, message, ha='center', va='center', fontsize=10)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def create_panel_a_umap(ax, adata):\n",
    "    \"\"\"\n",
    "    Panel A: UMAP visualization of cells colored by response.\n",
    "    \"\"\"\n",
    "    print(\"Creating Panel A: UMAP visualization...\")\n",
    "\n",
    "    # Use stored UMAP or compute new one\n",
    "    umap_coords = None\n",
    "    if 'X_umap_combined' in adata.obsm:\n",
    "        umap_coords = adata.obsm['X_umap_combined']\n",
    "    elif 'X_umap' in adata.obsm:\n",
    "        umap_coords = adata.obsm['X_umap']\n",
    "    else:\n",
    "        # Compute UMAP\n",
    "        if 'X_gene_pca' in adata.obsm:\n",
    "            X_pca = adata.obsm['X_gene_pca'][:, :20]\n",
    "        elif 'X_pca' in adata.obsm:\n",
    "            X_pca = adata.obsm['X_pca'][:, :20]\n",
    "        else:\n",
    "            return panel_placeholder(ax, 'A. Single-Cell UMAP by Response', 'UMAP not available')\n",
    "\n",
    "        try:\n",
    "            import umap as umap_module\n",
    "            reducer = umap_module.UMAP(n_components=2, random_state=42)\n",
    "            umap_coords = reducer.fit_transform(X_pca)\n",
    "        except Exception as e:\n",
    "            print(f\"  UMAP computation failed: {e}\")\n",
    "            return panel_placeholder(ax, 'A. Single-Cell UMAP by Response', 'UMAP not available')\n",
    "\n",
    "    # Create color mapping\n",
    "    if 'response' in adata.obs.columns:\n",
    "        resp_series = adata.obs['response'].fillna('Unknown').astype(str)\n",
    "    else:\n",
    "        resp_series = pd.Series(['Unknown'] * adata.n_obs, index=adata.obs.index)\n",
    "\n",
    "    response_colors = []\n",
    "    for resp in resp_series:\n",
    "        if resp == 'Responder':\n",
    "            response_colors.append(COLORS['Responder'])\n",
    "        elif resp == 'Non-Responder':\n",
    "            response_colors.append(COLORS['Non-Responder'])\n",
    "        else:\n",
    "            response_colors.append(COLORS['Unknown'])\n",
    "\n",
    "    # Plot with alpha for better visualization\n",
    "    ax.scatter(\n",
    "        umap_coords[:, 0],\n",
    "        umap_coords[:, 1],\n",
    "        c=response_colors,\n",
    "        s=3,\n",
    "        alpha=0.6,\n",
    "        rasterized=True\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('UMAP 1')\n",
    "    ax.set_ylabel('UMAP 2')\n",
    "    ax.set_title('A. Single-Cell UMAP by Response', fontweight='bold', loc='left')\n",
    "\n",
    "    # Legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=COLORS['Responder'], label=f\"Responder (n={(resp_series=='Responder').sum():,})\"),\n",
    "        Patch(facecolor=COLORS['Non-Responder'], label=f\"Non-Responder (n={(resp_series=='Non-Responder').sum():,})\"),\n",
    "        Patch(facecolor=COLORS['Unknown'], label=f\"Unknown (n={(resp_series=='Unknown').sum():,})\"),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right', frameon=True, framealpha=0.9)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def create_panel_b_shap(ax, groupcv_results, patient_df):\n",
    "    \"\"\"\n",
    "    Panel B: SHAP importance plot for the multimodal model.\n",
    "    \"\"\"\n",
    "    print(\"Creating Panel B: SHAP importance plot...\")\n",
    "\n",
    "    if groupcv_results is None or patient_df is None or patient_df.empty:\n",
    "        return panel_placeholder(ax, 'B. Feature Importance (SHAP)', 'Not available')\n",
    "\n",
    "    model = groupcv_results.get('model')\n",
    "    feature_cols = groupcv_results.get('feature_cols')\n",
    "    scaler = groupcv_results.get('scaler')\n",
    "\n",
    "    if model is None or scaler is None or not feature_cols:\n",
    "        return panel_placeholder(ax, 'B. Feature Importance (SHAP)', 'Not available')\n",
    "\n",
    "    # Prepare data\n",
    "    X = patient_df[feature_cols].fillna(0).values\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    try:\n",
    "        # Compute SHAP values\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_scaled)\n",
    "    except Exception as e:\n",
    "        print(f\"  SHAP computation failed: {e}\")\n",
    "        return panel_placeholder(ax, 'B. Feature Importance (SHAP)', 'Not available')\n",
    "\n",
    "    # Get mean absolute SHAP values for feature importance\n",
    "    if isinstance(shap_values, list):\n",
    "        # Multi-class output\n",
    "        shap_importance = np.abs(shap_values[1]).mean(axis=0)\n",
    "    else:\n",
    "        shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    # Create DataFrame and get top 15 features\n",
    "    shap_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': shap_importance\n",
    "    }).sort_values('importance', ascending=True).tail(15)\n",
    "\n",
    "    # Create horizontal bar plot\n",
    "    colors = []\n",
    "    for feat in shap_df['feature']:\n",
    "        if 'shannon' in feat.lower() or 'clonality' in feat.lower():\n",
    "            colors.append(COLORS['purple'])\n",
    "        elif 'pca' in feat.lower():\n",
    "            colors.append(COLORS['accent'])\n",
    "        elif 'hydro' in feat.lower() or 'charge' in feat.lower():\n",
    "            colors.append(COLORS['orange'])\n",
    "        else:\n",
    "            colors.append('#7f8c8d')\n",
    "\n",
    "    ax.barh(range(len(shap_df)), shap_df['importance'], color=colors)\n",
    "\n",
    "    # Clean feature names for display\n",
    "    clean_names = []\n",
    "    for feat in shap_df['feature']:\n",
    "        name = feat.replace('_mean', '').replace('_', ' ').title()\n",
    "        if len(name) > 25:\n",
    "            name = name[:22] + '...'\n",
    "        clean_names.append(name)\n",
    "\n",
    "    ax.set_yticks(range(len(shap_df)))\n",
    "    ax.set_yticklabels(clean_names)\n",
    "    ax.set_xlabel('Mean |SHAP Value|')\n",
    "    ax.set_title('B. Feature Importance (SHAP)', fontweight='bold', loc='left')\n",
    "\n",
    "    # Legend for feature types\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=COLORS['accent'], label='Gene Expression'),\n",
    "        Patch(facecolor=COLORS['purple'], label='TCR Diversity'),\n",
    "        Patch(facecolor=COLORS['orange'], label='Physicochemical'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', frameon=True, fontsize=8)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def create_panel_c_roc(ax, groupcv_results):\n",
    "    \"\"\"\n",
    "    Panel C: Patient-level ROC curve from GroupKFold CV.\n",
    "    \"\"\"\n",
    "    print(\"Creating Panel C: Patient-level ROC curve...\")\n",
    "\n",
    "    if groupcv_results is None:\n",
    "        return panel_placeholder(ax, 'C. Patient-Level ROC Curve', 'Not available')\n",
    "\n",
    "    y_true = groupcv_results.get('y_true')\n",
    "    y_proba = groupcv_results.get('y_pred_proba')\n",
    "\n",
    "    if y_true is None or y_proba is None:\n",
    "        return panel_placeholder(ax, 'C. Patient-Level ROC Curve', 'Not available')\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_proba = np.asarray(y_proba)\n",
    "\n",
    "    if y_true.size == 0 or y_proba.size == 0:\n",
    "        return panel_placeholder(ax, 'C. Patient-Level ROC Curve', 'Not available')\n",
    "\n",
    "    if y_proba.ndim == 2 and y_proba.shape[1] >= 2:\n",
    "        y_score = y_proba[:, 1]\n",
    "    elif y_proba.ndim == 1:\n",
    "        y_score = y_proba\n",
    "    else:\n",
    "        return panel_placeholder(ax, 'C. Patient-Level ROC Curve', 'ROC not available')\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    ax.plot(fpr, tpr, color=COLORS['accent'], lw=2.5,\n",
    "            label=f'GroupKFold CV (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    # Diagonal reference line\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=1.5, alpha=0.5, label='Random (AUC = 0.50)')\n",
    "\n",
    "    # Fill under curve\n",
    "    ax.fill_between(fpr, tpr, alpha=0.2, color=COLORS['accent'])\n",
    "\n",
    "    # Add optimal threshold point\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    ax.scatter([fpr[optimal_idx]], [tpr[optimal_idx]],\n",
    "               color=COLORS['Responder'], s=100, zorder=5,\n",
    "               label=f'Optimal (sens={tpr[optimal_idx]:.2f}, spec={1-fpr[optimal_idx]:.2f})')\n",
    "\n",
    "    ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "    ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "    ax.set_title('C. Patient-Level ROC Curve', fontweight='bold', loc='left')\n",
    "    ax.legend(loc='lower right', frameon=True)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def create_panel_d_boxplots(ax, adata, patient_df=None, groupcv_results=None):\n",
    "    \"\"\"\n",
    "    Panel D: Boxplots of top 3 biological markers.\n",
    "    \"\"\"\n",
    "    print(\"Creating Panel D: Biomarker boxplots...\")\n",
    "\n",
    "    if patient_df is None and isinstance(groupcv_results, dict):\n",
    "        patient_df = groupcv_results.get('patient_df')\n",
    "\n",
    "    # Select markers to plot\n",
    "    markers_to_plot = []\n",
    "\n",
    "    # Try to find GZMB, HLA-DRA, and an ISG\n",
    "    candidate_markers = ['GZMB', 'HLA-DRA', 'ISG15', 'IFI6', 'GNLY', 'PRF1']\n",
    "\n",
    "    for marker in candidate_markers:\n",
    "        if marker in adata.var_names:\n",
    "            markers_to_plot.append(marker)\n",
    "        if len(markers_to_plot) >= 3:\n",
    "            break\n",
    "\n",
    "    # If we don't have 3, fall back to TCR diversity metrics\n",
    "    if len(markers_to_plot) < 3:\n",
    "        markers_to_plot.extend(['TRA_shannon_entropy', 'TRB_shannon_entropy', 'TRA_clonality'])\n",
    "        markers_to_plot = markers_to_plot[:3]\n",
    "\n",
    "    print(f\"  Plotting markers: {markers_to_plot}\")\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "\n",
    "    for marker in markers_to_plot:\n",
    "        if marker in adata.var_names:\n",
    "            # Gene expression marker\n",
    "            expr = adata[:, marker].X\n",
    "            expr = expr.toarray().ravel() if hasattr(expr, 'toarray') else np.asarray(expr).ravel()\n",
    "\n",
    "            resp_series = adata.obs['response'] if 'response' in adata.obs.columns else pd.Series(['Unknown'] * adata.n_obs)\n",
    "            for val, resp in zip(expr, resp_series):\n",
    "                if resp in ['Responder', 'Non-Responder']:\n",
    "                    plot_data.append({'Marker': marker, 'Expression': val, 'Response': resp})\n",
    "        elif marker in adata.obs.columns:\n",
    "            # obs column (TCR metrics)\n",
    "            resp_series = adata.obs['response'] if 'response' in adata.obs.columns else pd.Series(['Unknown'] * adata.n_obs)\n",
    "            for val, resp in zip(adata.obs[marker], resp_series):\n",
    "                if resp in ['Responder', 'Non-Responder']:\n",
    "                    plot_data.append({'Marker': marker.replace('_', ' ').title(),\n",
    "                                     'Expression': val, 'Response': resp})\n",
    "\n",
    "    # Fall back to patient-level features if cell-level data is limited\n",
    "    if len(plot_data) < 10 and patient_df is not None and not patient_df.empty:\n",
    "        print(\"  Using patient-level features for boxplot...\")\n",
    "        for col in ['TRA_shannon_entropy', 'TRB_shannon_entropy', 'TRA_clonality']:\n",
    "            if col in patient_df.columns:\n",
    "                for _, row in patient_df.iterrows():\n",
    "                    plot_data.append({\n",
    "                        'Marker': col.replace('_', ' ').title(),\n",
    "                        'Expression': row[col],\n",
    "                        'Response': row['Response']\n",
    "                    })\n",
    "\n",
    "    if len(plot_data) == 0:\n",
    "        return panel_placeholder(ax, 'D. Key Biomarkers by Response', 'Not available')\n",
    "\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Create grouped boxplot\n",
    "    palette = {'Responder': COLORS['Responder'], 'Non-Responder': COLORS['Non-Responder']}\n",
    "\n",
    "    sns.boxplot(\n",
    "        data=plot_df,\n",
    "        x='Marker',\n",
    "        y='Expression',\n",
    "        hue='Response',\n",
    "        palette=palette,\n",
    "        ax=ax,\n",
    "        linewidth=1.5,\n",
    "        fliersize=2\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Expression / Value')\n",
    "    ax.set_title('D. Key Biomarkers by Response', fontweight='bold', loc='left')\n",
    "    ax.legend(title='Response', loc='upper right', frameon=True)\n",
    "\n",
    "    # Rotate x-labels if needed\n",
    "    ax.tick_params(axis='x', rotation=15)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def create_publication_figure(adata, groupcv_results=None):\n",
    "    \"\"\"\n",
    "    Create the complete 4-panel publication figure.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Publication Figure ---\")\n",
    "\n",
    "    # Create figure with 2x2 layout\n",
    "    fig = plt.figure(figsize=(14, 12))\n",
    "    gs = gridspec.GridSpec(2, 2, figure=fig, wspace=0.3, hspace=0.35)\n",
    "\n",
    "    # Panel A: UMAP\n",
    "    ax_a = fig.add_subplot(gs[0, 0])\n",
    "    create_panel_a_umap(ax_a, adata)\n",
    "\n",
    "    # Panel B: SHAP\n",
    "    ax_b = fig.add_subplot(gs[0, 1])\n",
    "    if groupcv_results is None or groupcv_results.get('status') == 'skipped':\n",
    "        panel_placeholder(ax_b, 'B. Feature Importance (SHAP)', 'Not available')\n",
    "        patient_df = None\n",
    "    else:\n",
    "        patient_df = groupcv_results.get('patient_df')\n",
    "        create_panel_b_shap(ax_b, groupcv_results, patient_df)\n",
    "\n",
    "    # Panel C: ROC\n",
    "    ax_c = fig.add_subplot(gs[1, 0])\n",
    "    if groupcv_results is None or groupcv_results.get('status') == 'skipped':\n",
    "        panel_placeholder(ax_c, 'C. Patient-Level ROC Curve', 'Not available')\n",
    "    else:\n",
    "        create_panel_c_roc(ax_c, groupcv_results)\n",
    "\n",
    "    # Panel D: Boxplots\n",
    "    ax_d = fig.add_subplot(gs[1, 1])\n",
    "    create_panel_d_boxplots(ax_d, adata, patient_df=patient_df, groupcv_results=groupcv_results)\n",
    "\n",
    "    # Add overall title\n",
    "    fig.suptitle(\n",
    "        'Multimodal Machine Learning Predicts Immunotherapy Response in HR+ Breast Cancer',\n",
    "        fontsize=14,\n",
    "        fontweight='bold',\n",
    "        y=0.98\n",
    "    )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "# Execute Task 5\n",
    "# ==========================================================================\n",
    "\n",
    "# Get groupcv_results if available\n",
    "_groupcv_results = globals().get('groupcv_results', None)\n",
    "\n",
    "# Create the publication figure\n",
    "try:\n",
    "    fig = create_publication_figure(adata, _groupcv_results)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create publication figure: {e}\")\n",
    "    fig = None\n",
    "\n",
    "# Save figure in multiple formats\n",
    "if fig is not None:\n",
    "    output_dir = Path('Processed_Data/figures')\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # High-resolution PNG\n",
    "    fig.savefig(output_dir / 'Figure_Multimodal_ML_Response.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {output_dir / 'Figure_Multimodal_ML_Response.png'}\")\n",
    "\n",
    "    # PDF for publication\n",
    "    fig.savefig(output_dir / 'Figure_Multimodal_ML_Response.pdf', bbox_inches='tight')\n",
    "    print(f\"Saved: {output_dir / 'Figure_Multimodal_ML_Response.pdf'}\")\n",
    "\n",
    "    # SVG for editing\n",
    "    fig.savefig(output_dir / 'Figure_Multimodal_ML_Response.svg', bbox_inches='tight')\n",
    "    print(f\"Saved: {output_dir / 'Figure_Multimodal_ML_Response.svg'}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TASK 5 COMPLETED: Publication-Quality 4-Panel Figure Generated\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping figure save because figure creation failed or was unavailable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b18432",
   "metadata": {
    "papermill": {
     "duration": 0.054172,
     "end_time": "2026-02-13T21:13:55.168041",
     "exception": false,
     "start_time": "2026-02-13T21:13:55.113869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary: Enhanced ML Pipeline for HR+ Breast Cancer Immunotherapy Response Prediction\n",
    "\n",
    "### Tasks Completed\n",
    "\n",
    "| Task | Description | Key Outputs |\n",
    "|------|-------------|-------------|\n",
    "| **Task 1** | GroupKFold CV with Patient-Level Aggregation | `patient_level_features.csv`, `patient_level_model_groupcv.joblib` |\n",
    "| **Task 2** | Enhanced TCR CDR3 Physicochemical Encoding | 28 features per chain (hydrophobicity, charge, polarity, etc.) |\n",
    "| **Task 3** | Top 20 Feature Analysis with Sun et al. 2025 | `sun_2025_marker_analysis.csv`, GZMB/HLA-DR/ISG validation |\n",
    "| **Task 4** | Extended Literature Review | I-SPY2 comparison, TCR-H/CoNGA methods |\n",
    "| **Task 5** | 4-Panel Publication Figure | `Figure_Multimodal_ML_Response.png/pdf/svg` |\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "1. **Data Leakage Prevention**: GroupKFold ensures all cells from same patient stay in same fold\n",
    "2. **Shannon Entropy TCR Diversity**: Captures clonal expansion dynamics (responders: dynamic turnover; non-responders: clonal stability)\n",
    "3. **Comprehensive Physicochemical Encoding**: 28 features capturing binding-relevant properties\n",
    "4. **Multi-resolution Analysis**: Cell-level clustering + patient-level prediction\n",
    "5. **Literature Validation**: Cross-referenced with Sun et al. 2025, I-SPY2, and emerging methods\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "```\n",
    "Processed_Data/\n",
    "├── patient_level_features.csv           # Patient-aggregated features with TCR diversity\n",
    "├── patient_level_groupcv_results.csv    # Per-fold CV metrics\n",
    "├── patient_level_model_groupcv.joblib   # Trained XGBoost model\n",
    "├── top_20_features_analysis.csv         # Feature importance ranking\n",
    "├── sun_2025_marker_analysis.csv         # Marker expression comparison\n",
    "└── figures/\n",
    "    ├── Figure_Multimodal_ML_Response.png\n",
    "    ├── Figure_Multimodal_ML_Response.pdf\n",
    "    └── Figure_Multimodal_ML_Response.svg\n",
    "```\n",
    "\n",
    "### Reproducibility Notes\n",
    "\n",
    "- All random seeds set to 42 for reproducibility\n",
    "- GroupKFold CV ensures patient-level generalization\n",
    "- Feature scaling performed with StandardScaler (saved with model)\n",
    "- Multiple testing correction (Benjamini-Hochberg) applied to marker analysis\n",
    "\n",
    "### Citation\n",
    "\n",
    "If using this pipeline, please cite:\n",
    "- Sun et al. 2025, npj Breast Cancer 11:65 (GSE300475 dataset)\n",
    "- This enhanced ML pipeline developed for HR+ breast cancer immunotherapy response prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7287e0",
   "metadata": {
    "papermill": {
     "duration": 0.053477,
     "end_time": "2026-02-13T21:13:55.274524",
     "exception": false,
     "start_time": "2026-02-13T21:13:55.221047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fixes applied\n",
    "\n",
    "- **Added safety defaults** for missing `adata.obsm` keys (e.g. `X_gene_umap`, `X_gene_svd`, TCR arrays) to avoid KeyError during feature assembly.\n",
    "- **Inserted a safe getter** `_get_obsm_or_zeros(adata, key, mask, n_cols)` to retrieve `obsm` arrays with a zeros fallback.\n",
    "- **Replaced unsafe monkeypatch** of `xgboost.XGBClassifier.__init__` with a **sklearn-compatible wrapper** `XGBClassifierSK` and adjusted `_apply_gpu_patches()` to use it when available.\n",
    "\n",
    "Notes:\n",
    "- The notebook contains historical outputs (errors/warnings) from a previous Kaggle run; the code has been made robust so these errors should not reoccur when re-running the notebook in Kaggle.\n",
    "- I recommend re-running the notebook from the top on Kaggle (where packages and GPUs are available) to validate results and regenerate plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dc32871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T21:13:55.385612Z",
     "iopub.status.busy": "2026-02-13T21:13:55.383041Z",
     "iopub.status.idle": "2026-02-13T21:13:55.411508Z",
     "shell.execute_reply": "2026-02-13T21:13:55.410272Z"
    },
    "papermill": {
     "duration": 0.085976,
     "end_time": "2026-02-13T21:13:55.413289",
     "exception": false,
     "start_time": "2026-02-13T21:13:55.327313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adata.n_obs: 100067\n",
      "X_gene_pca: present, shape=(100067, 30)\n",
      "X_gene_svd: present, shape=(100067, 30)\n",
      "X_gene_umap: MISSING\n",
      "XGBClassifierSK defined: True\n"
     ]
    }
   ],
   "source": [
    "# Quick non-fatal sanity checks (safe to run)\n",
    "try:\n",
    "    import numpy as np\n",
    "    if 'adata' in globals():\n",
    "        n_obs = getattr(adata, 'n_obs', adata.shape[0])\n",
    "        print('adata.n_obs:', n_obs)\n",
    "        for k in ['X_gene_pca', 'X_gene_svd', 'X_gene_umap']:\n",
    "            if k in adata.obsm:\n",
    "                shape = np.asarray(adata.obsm[k]).shape\n",
    "                print(f\"{k}: present, shape={shape}\")\n",
    "            else:\n",
    "                print(f\"{k}: MISSING\")\n",
    "    else:\n",
    "        print('adata not defined in this environment (skip checks)')\n",
    "    print('XGBClassifierSK defined:', 'XGBClassifierSK' in globals())\n",
    "except Exception as e:\n",
    "    print('Sanity checks could not be completed:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3976dcb",
   "metadata": {
    "papermill": {
     "duration": 0.052995,
     "end_time": "2026-02-13T21:13:55.519825",
     "exception": false,
     "start_time": "2026-02-13T21:13:55.466830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model summary and recommendation\n",
    "\n",
    "- **Models implemented**\n",
    "  - **XGBoost (tree ensemble):** Best performing on the *comprehensive* feature set (gene PCs + TCR k-mers + physicochemical features).\n",
    "  - **RandomForest / LogisticRegression:** Baselines.\n",
    "  - **Feed-forward MLP:** Dense network for tabular / flattened sequence inputs.\n",
    "  - **Sequence-aware architectures:** 1D **CNN**, **BiLSTM** (RNN), and **Transformer** (attention) encoders for CDR3 sequences.\n",
    "\n",
    "- **Recommendation (practical best model):**\n",
    "  - **XGBoost on the comprehensive feature set** with nested Group/LOPO CV, the expanded hyperparameter grid (n_estimators, max_depth, learning_rate, subsample, colsample_bytree), and **patient-level aggregation** (mean cell probabilities -> patient prediction). This gives best performance and interpretable feature importance.\n",
    "\n",
    "- **If you want a deep multimodal approach:**\n",
    "  - Use the **Transformer encoder** for sequence embeddings + MLP for gene PCs, train with **class_weight**, **EarlyStopping** monitoring **val_auc**, and evaluate with patient-level aggregation. Consider pretrained protein language model embeddings (ESM / ProtTrans) if compute permits.\n",
    "\n",
    "- **Next steps:**\n",
    "  1. Re-run LOPO with the updated XGBoost grid and patient-level aggregation.\n",
    "  2. Optionally run a short LOPO experiment for the Transformer-based multimodal model.\n",
    "\n",
    "*I implemented patient-level metrics and DL training improvements (AUC metrics, val_auc early stopping).*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14641.906461,
   "end_time": "2026-02-13T21:13:58.696606",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-13T17:09:56.790145",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0c63b05628bd40f19d356b0b44bd071f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0f3ded8624b14b988659b95c42c86823": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13f3f698a25a40bbb0eb1a9ab573ead2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e271927ded24151acce301a1904d5bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b5e59a0b868140bfa937009abb4c7f77",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4d35a12691054e8bb74562c043701986",
       "tabbable": null,
       "tooltip": null,
       "value": 4
      }
     },
     "1ea9153443864f3881b08237c8a308d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "21b0bdba6946484e9704b88c3f017265": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3246713d6fd74ef48391ecd9bfa60245",
        "IPY_MODEL_ecf77c06086f47b7a6cb71970dc50146",
        "IPY_MODEL_5e462289a34847b295c09cd436b42926"
       ],
       "layout": "IPY_MODEL_cc3e15ef52334b138d4274465c41ae24",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2e910ebe885842cda4b927f825d69a14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2edffa069f0648efa258206130e83b3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1ea9153443864f3881b08237c8a308d9",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ceb088c6e36542edb263336dbc6f22a1",
       "tabbable": null,
       "tooltip": null,
       "value": 4
      }
     },
     "3246713d6fd74ef48391ecd9bfa60245": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6579f2fe7c604b0b97e8863191aa6c34",
       "placeholder": "​",
       "style": "IPY_MODEL_74672446876f42268e0786b417c98c2b",
       "tabbable": null,
       "tooltip": null,
       "value": "CV comprehensive: 100%"
      }
     },
     "3af1c06d0ebc419eb036a4f0d78f5c61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cff3bcce02c6482697f7363a886ec536",
        "IPY_MODEL_2edffa069f0648efa258206130e83b3e",
        "IPY_MODEL_73b3267729d548f5ba7027a2a9fd15ee"
       ],
       "layout": "IPY_MODEL_0c63b05628bd40f19d356b0b44bd071f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3db3a1815b0a444bb0577496b27ae92f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3dc725e81a9e4ffb818442814e50f91e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "49116e66cf6c47ea929ef0a48be8ebf3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4d35a12691054e8bb74562c043701986": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5d5d32871d2148e8b03114e09f3bca21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e462289a34847b295c09cd436b42926": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cbebb692a6394c4f839a1b2cad29a106",
       "placeholder": "​",
       "style": "IPY_MODEL_2e910ebe885842cda4b927f825d69a14",
       "tabbable": null,
       "tooltip": null,
       "value": " 4/4 [53:03&lt;00:00, 771.60s/it]"
      }
     },
     "6579f2fe7c604b0b97e8863191aa6c34": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "73b3267729d548f5ba7027a2a9fd15ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7477a8dfb89a44caa83aaa2857065795",
       "placeholder": "​",
       "style": "IPY_MODEL_3dc725e81a9e4ffb818442814e50f91e",
       "tabbable": null,
       "tooltip": null,
       "value": " 4/4 [51:17&lt;00:00, 748.92s/it]"
      }
     },
     "74672446876f42268e0786b417c98c2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7477a8dfb89a44caa83aaa2857065795": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "899d8ae1f7e04448a129ba9b283cf59c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a2619adb2fb5440a95f6af60e8e53fe8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_13f3f698a25a40bbb0eb1a9ab573ead2",
       "placeholder": "​",
       "style": "IPY_MODEL_cb41d4bda9e743f6865beda7f1ceca43",
       "tabbable": null,
       "tooltip": null,
       "value": "CV tcr_enhanced: 100%"
      }
     },
     "a7c244d481bc4d70bc5ad1c2fb99117d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b0080f19239949309cd2fb254273ffee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ef13dd5fff014c64b8411f33d89e528c",
       "placeholder": "​",
       "style": "IPY_MODEL_899d8ae1f7e04448a129ba9b283cf59c",
       "tabbable": null,
       "tooltip": null,
       "value": " 4/4 [49:57&lt;00:00, 729.61s/it]"
      }
     },
     "b5e59a0b868140bfa937009abb4c7f77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cb41d4bda9e743f6865beda7f1ceca43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cbebb692a6394c4f839a1b2cad29a106": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cc3e15ef52334b138d4274465c41ae24": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ceb088c6e36542edb263336dbc6f22a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cff3bcce02c6482697f7363a886ec536": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5d5d32871d2148e8b03114e09f3bca21",
       "placeholder": "​",
       "style": "IPY_MODEL_49116e66cf6c47ea929ef0a48be8ebf3",
       "tabbable": null,
       "tooltip": null,
       "value": "CV sequence_structure: 100%"
      }
     },
     "e6a7efc0fab24423b3bda9377164a19a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a2619adb2fb5440a95f6af60e8e53fe8",
        "IPY_MODEL_1e271927ded24151acce301a1904d5bb",
        "IPY_MODEL_b0080f19239949309cd2fb254273ffee"
       ],
       "layout": "IPY_MODEL_0f3ded8624b14b988659b95c42c86823",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ecf77c06086f47b7a6cb71970dc50146": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3db3a1815b0a444bb0577496b27ae92f",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a7c244d481bc4d70bc5ad1c2fb99117d",
       "tabbable": null,
       "tooltip": null,
       "value": 4
      }
     },
     "ef13dd5fff014c64b8411f33d89e528c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
