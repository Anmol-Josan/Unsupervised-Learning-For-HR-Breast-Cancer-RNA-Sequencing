{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb55d7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scanpy\n",
      "  Downloading scanpy-1.11.5-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Collecting anndata>=0.8 (from scanpy)\n",
      "  Downloading anndata-0.12.6-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: h5py>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.14.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.5.2)\n",
      "Collecting legacy-api-wrap>=1.4.1 (from scanpy)\n",
      "  Downloading legacy_api_wrap-1.5-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting matplotlib>=3.7.5 (from scanpy)\n",
      "  Downloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from scanpy) (8.4.0)\n",
      "Requirement already satisfied: networkx>=2.7.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.5)\n",
      "Requirement already satisfied: numba!=0.62.0rc1,>=0.57.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.60.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (25.0)\n",
      "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.0.1)\n",
      "Requirement already satisfied: pynndescent>=0.5.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.13)\n",
      "Requirement already satisfied: scikit-learn>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.15.3)\n",
      "Collecting seaborn>=0.13.2 (from scanpy)\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting session-info2 (from scanpy)\n",
      "  Downloading session_info2-0.2.3-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: statsmodels>=0.14.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.14.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.15.0)\n",
      "Requirement already satisfied: umap-learn>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.9.post2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\n",
      "Collecting array-api-compat>=1.7.1 (from anndata>=0.8->scanpy)\n",
      "  Downloading array_api_compat-1.12.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting zarr!=3.0.*,>=2.18.7 (from anndata>=0.8->scanpy)\n",
      "  Downloading zarr-3.1.5-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (3.0.9)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba!=0.62.0rc1,>=0.57.1->scanpy) (0.43.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->scanpy) (3.6.0)\n",
      "Collecting scikit-learn>=1.1.3 (from scanpy)\n",
      "  Downloading scikit_learn-1.8.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n",
      "Collecting donfig>=0.8 (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy)\n",
      "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: google-crc32c>=1.5 in /usr/local/lib/python3.11/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy) (1.7.1)\n",
      "Collecting numcodecs>=0.14 (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy)\n",
      "  Downloading numcodecs-0.16.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from donfig>=0.8->zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy) (6.0.3)\n",
      "Downloading scanpy-1.11.5-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading anndata-0.12.6-py3-none-any.whl (172 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading legacy_api_wrap-1.5-py3-none-any.whl (10 kB)\n",
      "Downloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.8.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading session_info2-0.2.3-py3-none-any.whl (16 kB)\n",
      "Downloading array_api_compat-1.12.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zarr-3.1.5-py3-none-any.whl (284 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.1/284.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
      "Downloading numcodecs-0.16.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: session-info2, legacy-api-wrap, donfig, array-api-compat, scikit-learn, numcodecs, zarr, matplotlib, seaborn, anndata, scanpy\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.2\n",
      "    Uninstalling matplotlib-3.7.2:\n",
      "      Successfully uninstalled matplotlib-3.7.2\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.12.2\n",
      "    Uninstalling seaborn-0.12.2:\n",
      "      Successfully uninstalled seaborn-0.12.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "ydata-profiling 4.17.0 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.8 which is incompatible.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anndata-0.12.6 array-api-compat-1.12.0 donfig-0.8.1.post1 legacy-api-wrap-1.5 matplotlib-3.10.8 numcodecs-0.16.5 scanpy-1.11.5 scikit-learn-1.8.0 seaborn-0.13.2 session-info2-0.2.3 zarr-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Scanpy version: 1.11.5\n",
      "Pandas version: 2.2.3\n",
      "All libraries imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_81/3576863463.py:10: FutureWarning: `__version__` is deprecated, use `importlib.metadata.version('scanpy')` instead\n",
      "  print(f\"Scanpy version: {sc.__version__}\")\n"
     ]
    }
   ],
   "source": [
    "%pip install scanpy pandas numpy\n",
    "# Import required libraries for single-cell RNA-seq analysis and data handling\n",
    "import scanpy as sc  # Main library for single-cell analysis, provides AnnData structure and many tools\n",
    "import pandas as pd  # For tabular data manipulation and metadata handling\n",
    "import numpy as np   # For numerical operations and array handling\n",
    "import os            # For operating system interactions (file paths, etc.)\n",
    "from pathlib import Path  # For robust and readable file path management\n",
    "\n",
    "# Print versions to ensure reproducibility and compatibility\n",
    "print(f\"Scanpy version: {sc.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c65e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata table now matches the requested specification:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_Number</th>\n",
       "      <th>GEX_Sample_ID</th>\n",
       "      <th>TCR_Sample_ID</th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Response</th>\n",
       "      <th>In_Data</th>\n",
       "      <th>In_Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>GSM9061687</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2</td>\n",
       "      <td>GSM9061666</td>\n",
       "      <td>GSM9061688</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S3</td>\n",
       "      <td>GSM9061667</td>\n",
       "      <td>GSM9061689</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S4</td>\n",
       "      <td>GSM9061668</td>\n",
       "      <td>GSM9061690</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S5</td>\n",
       "      <td>GSM9061669</td>\n",
       "      <td>GSM9061691</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S6</td>\n",
       "      <td>GSM9061670</td>\n",
       "      <td>GSM9061692</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S7</td>\n",
       "      <td>GSM9061671</td>\n",
       "      <td>GSM9061693</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S8</td>\n",
       "      <td>GSM9061672</td>\n",
       "      <td>None</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>GEX only</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S9</td>\n",
       "      <td>GSM9061673</td>\n",
       "      <td>GSM9061694</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S10</td>\n",
       "      <td>GSM9061674</td>\n",
       "      <td>GSM9061695</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Post-ICI</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S11</td>\n",
       "      <td>GSM9061675</td>\n",
       "      <td>GSM9061696</td>\n",
       "      <td>PT11</td>\n",
       "      <td>Endpoint</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S_Number GEX_Sample_ID TCR_Sample_ID Patient_ID   Timepoint       Response  \\\n",
       "0        S1    GSM9061665    GSM9061687        PT1    Baseline      Responder   \n",
       "1        S2    GSM9061666    GSM9061688        PT1  Post-Chemo      Responder   \n",
       "2        S3    GSM9061667    GSM9061689        PT2    Baseline  Non-Responder   \n",
       "3        S4    GSM9061668    GSM9061690        PT2  Post-Chemo  Non-Responder   \n",
       "4        S5    GSM9061669    GSM9061691        PT3    Baseline      Responder   \n",
       "5        S6    GSM9061670    GSM9061692        PT3  Post-Chemo      Responder   \n",
       "6        S7    GSM9061671    GSM9061693        PT4    Baseline  Non-Responder   \n",
       "7        S8    GSM9061672          None        PT5     Unknown        Unknown   \n",
       "8        S9    GSM9061673    GSM9061694        PT5    Baseline      Responder   \n",
       "9       S10    GSM9061674    GSM9061695        PT5    Post-ICI      Responder   \n",
       "10      S11    GSM9061675    GSM9061696       PT11    Endpoint      Responder   \n",
       "\n",
       "     In_Data In_Article  \n",
       "0        Yes        Yes  \n",
       "1        Yes        Yes  \n",
       "2        Yes        Yes  \n",
       "3        Yes        Yes  \n",
       "4        Yes        Yes  \n",
       "5        Yes        Yes  \n",
       "6        Yes        Yes  \n",
       "7   GEX only        Yes  \n",
       "8        Yes        Yes  \n",
       "9        Yes        Yes  \n",
       "10       Yes        Yes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-check In_Data column (based on files found in Data/GSE300475_RAW):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_Number</th>\n",
       "      <th>GEX_Sample_ID</th>\n",
       "      <th>TCR_Sample_ID</th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Response</th>\n",
       "      <th>In_Data</th>\n",
       "      <th>In_Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>GSM9061687</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2</td>\n",
       "      <td>GSM9061666</td>\n",
       "      <td>GSM9061688</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Responder</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S3</td>\n",
       "      <td>GSM9061667</td>\n",
       "      <td>GSM9061689</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S4</td>\n",
       "      <td>GSM9061668</td>\n",
       "      <td>GSM9061690</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S5</td>\n",
       "      <td>GSM9061669</td>\n",
       "      <td>GSM9061691</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S6</td>\n",
       "      <td>GSM9061670</td>\n",
       "      <td>GSM9061692</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Responder</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S7</td>\n",
       "      <td>GSM9061671</td>\n",
       "      <td>GSM9061693</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S8</td>\n",
       "      <td>GSM9061672</td>\n",
       "      <td>None</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S9</td>\n",
       "      <td>GSM9061673</td>\n",
       "      <td>GSM9061694</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S10</td>\n",
       "      <td>GSM9061674</td>\n",
       "      <td>GSM9061695</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Post-ICI</td>\n",
       "      <td>Responder</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S11</td>\n",
       "      <td>GSM9061675</td>\n",
       "      <td>GSM9061696</td>\n",
       "      <td>PT11</td>\n",
       "      <td>Endpoint</td>\n",
       "      <td>Responder</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S_Number GEX_Sample_ID TCR_Sample_ID Patient_ID   Timepoint       Response  \\\n",
       "0        S1    GSM9061665    GSM9061687        PT1    Baseline      Responder   \n",
       "1        S2    GSM9061666    GSM9061688        PT1  Post-Chemo      Responder   \n",
       "2        S3    GSM9061667    GSM9061689        PT2    Baseline  Non-Responder   \n",
       "3        S4    GSM9061668    GSM9061690        PT2  Post-Chemo  Non-Responder   \n",
       "4        S5    GSM9061669    GSM9061691        PT3    Baseline      Responder   \n",
       "5        S6    GSM9061670    GSM9061692        PT3  Post-Chemo      Responder   \n",
       "6        S7    GSM9061671    GSM9061693        PT4    Baseline  Non-Responder   \n",
       "7        S8    GSM9061672          None        PT5     Unknown        Unknown   \n",
       "8        S9    GSM9061673    GSM9061694        PT5    Baseline      Responder   \n",
       "9       S10    GSM9061674    GSM9061695        PT5    Post-ICI      Responder   \n",
       "10      S11    GSM9061675    GSM9061696       PT11    Endpoint      Responder   \n",
       "\n",
       "   In_Data In_Article  \n",
       "0       No        Yes  \n",
       "1       No        Yes  \n",
       "2       No        Yes  \n",
       "3       No        Yes  \n",
       "4       No        Yes  \n",
       "5       No        Yes  \n",
       "6       No        Yes  \n",
       "7       No        Yes  \n",
       "8       No        Yes  \n",
       "9       No        Yes  \n",
       "10      No        Yes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.5 ms, sys: 7.88 ms, total: 39.4 ms\n",
      "Wall time: 45.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Setup data paths ---\n",
    "# Define the main data directory and the subdirectory containing raw files.\n",
    "data_dir = Path('../Data')\n",
    "raw_data_dir = data_dir / 'GSE300475_RAW'\n",
    "\n",
    "# --- Manually create the metadata mapping ---\n",
    "# This list contains information about each sample, including GEO IDs, patient IDs, timepoints, and response status.\n",
    "# Note: S8 (GSM9061672) has GEX files but no corresponding TCR file.\n",
    "metadata_list = [\n",
    "    # Patient 1 (Responder)\n",
    "    {'S_Number': 'S1',  'GEX_Sample_ID': 'GSM9061665', 'TCR_Sample_ID': 'GSM9061687', 'Patient_ID': 'PT1',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S2',  'GEX_Sample_ID': 'GSM9061666', 'TCR_Sample_ID': 'GSM9061688', 'Patient_ID': 'PT1',  'Timepoint': 'Post-Chemo',  'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 2 (Non-Responder)\n",
    "    {'S_Number': 'S3',  'GEX_Sample_ID': 'GSM9061667', 'TCR_Sample_ID': 'GSM9061689', 'Patient_ID': 'PT2',  'Timepoint': 'Baseline',     'Response': 'Non-Responder', 'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S4',  'GEX_Sample_ID': 'GSM9061668', 'TCR_Sample_ID': 'GSM9061690', 'Patient_ID': 'PT2',  'Timepoint': 'Post-Chemo',  'Response': 'Non-Responder', 'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 3 (Responder)\n",
    "    {'S_Number': 'S5',  'GEX_Sample_ID': 'GSM9061669', 'TCR_Sample_ID': 'GSM9061691', 'Patient_ID': 'PT3',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S6',  'GEX_Sample_ID': 'GSM9061670', 'TCR_Sample_ID': 'GSM9061692', 'Patient_ID': 'PT3',  'Timepoint': 'Post-Chemo',  'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 4 (Non-Responder)\n",
    "    {'S_Number': 'S7',  'GEX_Sample_ID': 'GSM9061671', 'TCR_Sample_ID': 'GSM9061693', 'Patient_ID': 'PT4',  'Timepoint': 'Baseline',     'Response': 'Non-Responder', 'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 5 (partial) - S8 exists as GEX only in the raw data but has no TCR file\n",
    "    {'S_Number': 'S8',  'GEX_Sample_ID': 'GSM9061672', 'TCR_Sample_ID': None,             'Patient_ID': 'PT5',  'Timepoint': 'Unknown',      'Response': 'Unknown',       'In_Data': 'GEX only', 'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S9',  'GEX_Sample_ID': 'GSM9061673', 'TCR_Sample_ID': 'GSM9061694', 'Patient_ID': 'PT5',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S10', 'GEX_Sample_ID': 'GSM9061674', 'TCR_Sample_ID': 'GSM9061695', 'Patient_ID': 'PT5',  'Timepoint': 'Post-ICI',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 11 (Responder)\n",
    "    {'S_Number': 'S11', 'GEX_Sample_ID': 'GSM9061675', 'TCR_Sample_ID': 'GSM9061696', 'Patient_ID': 'PT11', 'Timepoint': 'Endpoint',      'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "]\n",
    "\n",
    "# --- Create DataFrame and display the verification table ---\n",
    "metadata_df = pd.DataFrame(metadata_list)\n",
    "print(\"Metadata table now matches the requested specification:\")\n",
    "display(metadata_df)\n",
    "\n",
    "# --- Programmatic sanity-check for file presence ---\n",
    "# This loop checks if the expected files exist for each sample and updates the 'In_Data' column accordingly.\n",
    "for idx, row in metadata_df.iterrows():\n",
    "    s = row['S_Number']\n",
    "    g = row['GEX_Sample_ID']\n",
    "    t = row['TCR_Sample_ID']\n",
    "    # Check for gene expression matrix file (compressed or uncompressed)\n",
    "    g_exists = (raw_data_dir / f\"{g}_{s}_matrix.mtx.gz\").exists() or (raw_data_dir / f\"{g}_{s}_matrix.mtx\").exists()\n",
    "    t_exists = False\n",
    "    # Check for TCR annotation file if TCR sample ID is present\n",
    "    if pd.notna(t) and t is not None:\n",
    "        t_exists = (raw_data_dir / f\"{t}_{s}_all_contig_annotations.csv.gz\").exists() or (raw_data_dir / f\"{t}_{s}_all_contig_annotations.csv\").exists()\n",
    "    # Update 'In_Data' column based on file presence\n",
    "    if g_exists and t_exists:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'Yes'\n",
    "    elif g_exists and not t_exists:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'GEX only'\n",
    "    else:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'No'\n",
    "\n",
    "print(\"\\nPost-check In_Data column (based on files found in Data/GSE300475_RAW):\")\n",
    "display(metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14630c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEX data not found for sample GSM9061665_S1, skipping.\n",
      "GEX data not found for sample GSM9061666_S2, skipping.\n",
      "GEX data not found for sample GSM9061667_S3, skipping.\n",
      "GEX data not found for sample GSM9061668_S4, skipping.\n",
      "GEX data not found for sample GSM9061669_S5, skipping.\n",
      "GEX data not found for sample GSM9061670_S6, skipping.\n",
      "GEX data not found for sample GSM9061671_S7, skipping.\n",
      "GEX data not found for sample GSM9061672_S8, skipping.\n",
      "GEX data not found for sample GSM9061673_S9, skipping.\n",
      "GEX data not found for sample GSM9061674_S10, skipping.\n",
      "GEX data not found for sample GSM9061675_S11, skipping.\n",
      "No data was loaded.\n",
      "No TCR data was loaded.\n",
      "CPU times: user 2.78 ms, sys: 964 µs, total: 3.74 ms\n",
      "Wall time: 3.25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Initialize lists to hold AnnData and TCR data for each sample ---\n",
    "adata_list = []  # Will store AnnData objects for each sample\n",
    "tcr_data_list = []  # Will store TCR dataframes for each sample\n",
    "\n",
    "# --- Iterate through each sample in the metadata table ---\n",
    "for index, row in metadata_df.iterrows():\n",
    "    gex_sample_id = row['GEX_Sample_ID']\n",
    "    tcr_sample_id = row['TCR_Sample_ID']\n",
    "    s_number = row['S_Number']\n",
    "    patient_id = row['Patient_ID']\n",
    "    timepoint = row['Timepoint']\n",
    "    response = row['Response']\n",
    "    \n",
    "    # Construct the file prefix for this sample (used for locating files)\n",
    "    sample_prefix = f\"{gex_sample_id}_{s_number}\"\n",
    "    sample_data_path = raw_data_dir\n",
    "    \n",
    "    # --- Check for gene expression matrix file ---\n",
    "    matrix_file = sample_data_path / f\"{sample_prefix}_matrix.mtx.gz\"\n",
    "    if not matrix_file.exists():\n",
    "        # Try uncompressed version if gzipped file not found\n",
    "        matrix_file_un = sample_data_path / f\"{sample_prefix}_matrix.mtx\"\n",
    "        if not matrix_file_un.exists():\n",
    "            print(f\"GEX data not found for sample {sample_prefix}, skipping.\")\n",
    "            continue\n",
    "        else:\n",
    "            matrix_file = matrix_file_un\n",
    "            \n",
    "    print(f\"Processing GEX sample: {sample_prefix}\")\n",
    "    \n",
    "    # --- Load gene expression data into AnnData object ---\n",
    "    # The prefix ensures only files for this sample are loaded\n",
    "    adata_sample = sc.read_10x_mtx(\n",
    "        sample_data_path, \n",
    "        var_names='gene_symbols',\n",
    "        prefix=f\"{sample_prefix}_\"\n",
    "    )\n",
    "    \n",
    "    # --- Add sample metadata to AnnData.obs ---\n",
    "    adata_sample.obs['sample_id'] = gex_sample_id \n",
    "    adata_sample.obs['patient_id'] = patient_id\n",
    "    adata_sample.obs['timepoint'] = timepoint\n",
    "    adata_sample.obs['response'] = response\n",
    "    \n",
    "    adata_list.append(adata_sample)\n",
    "    \n",
    "    # --- Load TCR data if available ---\n",
    "    if pd.isna(tcr_sample_id) or tcr_sample_id is None:\n",
    "        print(f\"No TCR sample for {gex_sample_id}_{s_number}, skipping TCR load.\")\n",
    "        continue\n",
    "\n",
    "    # Construct path for TCR annotation file (gzipped or uncompressed)\n",
    "    tcr_file_path = raw_data_dir / f\"{tcr_sample_id}_{s_number}_all_contig_annotations.csv.gz\"\n",
    "\n",
    "    if tcr_file_path.exists():\n",
    "        print(f\"Found and loading TCR data: {tcr_file_path.name}\")\n",
    "        tcr_df = pd.read_csv(tcr_file_path)\n",
    "        # Add sample_id for merging later\n",
    "        tcr_df['sample_id'] = gex_sample_id \n",
    "        tcr_data_list.append(tcr_df)\n",
    "    else:\n",
    "        # Try uncompressed version if gzipped file not found\n",
    "        tcr_file_path_uncompressed = raw_data_dir / f\"{tcr_sample_id}_{s_number}_all_contig_annotations.csv\"\n",
    "        if tcr_file_path_uncompressed.exists():\n",
    "            print(f\"Found and loading TCR data: {tcr_file_path_uncompressed.name}\")\n",
    "            tcr_df = pd.read_csv(tcr_file_path_uncompressed)\n",
    "            tcr_df['sample_id'] = gex_sample_id\n",
    "            tcr_data_list.append(tcr_df)\n",
    "        else:\n",
    "            print(f\"TCR data not found for {tcr_sample_id}_{s_number}\")\n",
    "\n",
    "# --- Concatenate all loaded AnnData objects into one ---\n",
    "if adata_list:\n",
    "    # Use sample_id as batch key for concatenation\n",
    "    loaded_batches = [a.obs['sample_id'].unique()[0] for a in adata_list]\n",
    "    adata = sc.AnnData.concatenate(*adata_list, join='outer', batch_key='sample_id', batch_categories=loaded_batches)\n",
    "    print(\"\\nConcatenated AnnData object:\")\n",
    "    print(adata)\n",
    "else:\n",
    "    print(\"No data was loaded.\")\n",
    "\n",
    "# --- Concatenate all loaded TCR dataframes into one ---\n",
    "if tcr_data_list:\n",
    "    full_tcr_df = pd.concat(tcr_data_list, ignore_index=True)\n",
    "    print(\"\\nFull TCR data:\")\n",
    "    display(full_tcr_df.head())\n",
    "else:\n",
    "    print(\"No TCR data was loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8cc08d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adata' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Integrate TCR data into AnnData.obs and perform quality control ---\n",
    "if 'full_tcr_df' in locals() and not full_tcr_df.empty:\n",
    "    # --- FIX START ---\n",
    "    # The previous join failed because one cell (barcode) can have multiple TCR contigs (e.g., TRA and TRB chains),\n",
    "    # creating a one-to-many join that increases the number of rows.\n",
    "    # The fix is to aggregate the TCR data to one row per cell *before* merging.\n",
    "\n",
    "    # 1. Filter for high-confidence, productive TRA/TRB chains.\n",
    "    # Only keep TCR contigs that are both high-confidence and productive, and are either TRA or TRB chains.\n",
    "    tcr_to_agg = full_tcr_df[\n",
    "        (full_tcr_df['high_confidence'] == True) &\n",
    "        (full_tcr_df['productive'] == True) &\n",
    "        (full_tcr_df['chain'].isin(['TRA', 'TRB']))\n",
    "    ].copy()\n",
    "\n",
    "    # 2. Pivot the data to create one row per barcode, with columns for TRA and TRB data.\n",
    "    # This step ensures each cell (barcode) has its TRA and TRB info in separate columns.\n",
    "    tcr_aggregated = tcr_to_agg.pivot_table(\n",
    "        index=['sample_id', 'barcode'],\n",
    "        columns='chain',\n",
    "        values=['v_gene', 'j_gene', 'cdr3'],\n",
    "        aggfunc='first'  # 'first' is safe as we expect at most one productive TRA/TRB per cell\n",
    "    )\n",
    "\n",
    "    # 3. Flatten the multi-level column index (e.g., from ('v_gene', 'TRA') to 'v_gene_TRA')\n",
    "    tcr_aggregated.columns = ['_'.join(col).strip() for col in tcr_aggregated.columns.values]\n",
    "    tcr_aggregated.reset_index(inplace=True)\n",
    "\n",
    "    # 4. Prepare adata.obs for the merge by creating a matching barcode column.\n",
    "    # The index in adata.obs is like 'AGCCATGCAGCTGTTA-1-0' (barcode-batch_id).\n",
    "    # The barcode in TCR data is like 'AGCCATGCAGCTGTTA-1'.\n",
    "    adata.obs['barcode_for_merge'] = adata.obs.index.str.rsplit('-', n=1).str[0]\n",
    "\n",
    "    # 5. Perform a left merge. This keeps all cells from adata and adds TCR info where available.\n",
    "    # The number of rows will not change because tcr_aggregated has unique barcodes.\n",
    "    original_obs = adata.obs.copy()\n",
    "    merged_obs = original_obs.merge(\n",
    "        tcr_aggregated,\n",
    "        left_on=['sample_id', 'barcode_for_merge'],\n",
    "        right_on=['sample_id', 'barcode'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 6. Restore the original index to the merged dataframe.\n",
    "    merged_obs.index = original_obs.index\n",
    "    adata.obs = merged_obs\n",
    "    # --- FIX END ---\n",
    "\n",
    "    print(\"Aggregated TCR data merged into AnnData object.\")\n",
    "    \n",
    "    # --- Filter for cells that have TCR information after the merge ---\n",
    "    # Only keep cells with non-null v_gene_TRA (i.e., cells with high-confidence TCR data)\n",
    "    initial_cells = adata.n_obs\n",
    "    adata = adata[~adata.obs['v_gene_TRA'].isna()].copy()\n",
    "    print(f\"Filtered from {initial_cells} to {adata.n_obs} cells based on having high-confidence TCR data.\")\n",
    "\n",
    "# --- Basic QC and filtering ---\n",
    "# Filter out cells with fewer than 200 genes detected\n",
    "sc.pp.filter_cells(adata, min_genes=200)\n",
    "# Filter out genes detected in fewer than 3 cells\n",
    "sc.pp.filter_genes(adata, min_cells=3)\n",
    "\n",
    "# Annotate mitochondrial genes for QC metrics\n",
    "adata.var['mt'] = adata.var_names.str.startswith('MT-')\n",
    "# Calculate QC metrics (e.g., percent mitochondrial genes)\n",
    "sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "\n",
    "print(\"\\nPost-QC AnnData object:\")\n",
    "print(adata)\n",
    "display(adata.obs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd1378e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adata' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Save processed AnnData object to disk ---\n",
    "# Define output directory for processed data\n",
    "output_dir = Path('../Processed_Data')\n",
    "output_dir.mkdir(exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Define output file path for the .h5ad file\n",
    "output_path = output_dir / 'processed_s_rna_seq_data.h5ad'\n",
    "# Save the AnnData object (contains all processed, filtered, and annotated data)\n",
    "adata.write_h5ad(output_path)\n",
    "\n",
    "print(f\"Processed data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960d9622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\n",
      "  Downloading biopython-1.86-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->biopython) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->biopython) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->biopython) (2024.2.0)\n",
      "Downloading biopython-1.86-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: biopython\n",
      "Successfully installed biopython-1.86\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.1->scikit-learn) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.1->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.1->scikit-learn) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.1->scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24.1->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24.1->scikit-learn) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.9.post2)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.6 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.8.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23->umap-learn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23->umap-learn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23->umap-learn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23->umap-learn) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23->umap-learn) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23->umap-learn) (2.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6->umap-learn) (3.6.0)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23->umap-learn) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23->umap-learn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23->umap-learn) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23->umap-learn) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23->umap-learn) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23->umap-learn) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (2.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20->hdbscan) (3.6.0)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.20->hdbscan) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.20->hdbscan) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.20->hdbscan) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.20->hdbscan) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.20->hdbscan) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.20->hdbscan) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (25.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->xgboost) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->xgboost) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->xgboost) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.33.0\n",
      "    Uninstalling protobuf-6.33.0:\n",
      "      Successfully uninstalled protobuf-6.33.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\n",
      "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-5.29.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 04:21:47.746324: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765599708.017618      81 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765599708.094200      81 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Install required packages for genetic sequence encoding and ML ---\n",
    "%pip install biopython\n",
    "%pip install scikit-learn\n",
    "%pip install umap-learn\n",
    "%pip install hdbscan\n",
    "%pip install plotly\n",
    "%pip install xgboost\n",
    "%pip install tensorflow\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import ProtParam\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Import scipy for hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import umap\n",
    "import hdbscan\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"Additional libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45102612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic sequence encoding functions defined successfully!\n",
      "CPU times: user 103 µs, sys: 0 ns, total: 103 µs\n",
      "Wall time: 110 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Genetic Sequence Encoding Functions ---\n",
    "\n",
    "def one_hot_encode_sequence(sequence, max_length=50, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    One-hot encode a protein/nucleotide sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        max_length: Maximum sequence length (pad or truncate)\n",
    "        alphabet: Valid characters in the sequence\n",
    "    \n",
    "    Returns:\n",
    "        2D numpy array of shape (max_length, len(alphabet))\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    sequence = str(sequence).upper()[:max_length]  # Truncate if too long\n",
    "    encoding = np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    for i, char in enumerate(sequence):\n",
    "        if char in alphabet:\n",
    "            char_idx = alphabet.index(char)\n",
    "            encoding[i, char_idx] = 1\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "def kmer_encode_sequence(sequence, k=3, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    K-mer encoding of sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        k: Length of k-mers\n",
    "        alphabet: Valid characters\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with k-mer counts\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {}\n",
    "    \n",
    "    sequence = str(sequence).upper()\n",
    "    kmers = [sequence[i:i+k] for i in range(len(sequence)-k+1)]\n",
    "    valid_kmers = [kmer for kmer in kmers if all(c in alphabet for c in kmer)]\n",
    "    \n",
    "    return Counter(valid_kmers)\n",
    "\n",
    "def physicochemical_features(sequence):\n",
    "    \"\"\"\n",
    "    Extract physicochemical properties from protein sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Protein sequence string\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of features\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {\n",
    "            'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        seq = str(sequence).upper()\n",
    "        # Remove non-standard amino acids\n",
    "        seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "        \n",
    "        if len(seq) == 0:\n",
    "            return {\n",
    "                'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "                'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "            }\n",
    "        \n",
    "        bio_seq = Seq(seq)\n",
    "        analyzer = ProtParam.ProteinAnalysis(str(bio_seq))\n",
    "        \n",
    "        return {\n",
    "            'length': len(seq),\n",
    "            'molecular_weight': analyzer.molecular_weight(),\n",
    "            'aromaticity': analyzer.aromaticity(),\n",
    "            'instability_index': analyzer.instability_index(),\n",
    "            'isoelectric_point': analyzer.isoelectric_point(),\n",
    "            'hydrophobicity': analyzer.gravy()  # Grand Average of Hydropathy\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'length': len(str(sequence)) if not pd.isna(sequence) else 0,\n",
    "            'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "\n",
    "def encode_gene_expression_patterns(adata, n_top_genes=1000):\n",
    "    \"\"\"\n",
    "    Encode gene expression patterns using various dimensionality reduction techniques.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object with gene expression data\n",
    "        n_top_genes: Number of highly variable genes to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of encoded representations\n",
    "    \"\"\"\n",
    "    # Get highly variable genes if not already computed\n",
    "    if 'highly_variable' not in adata.var.columns:\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, subset=False)\n",
    "    \n",
    "    # Extract expression matrix for highly variable genes\n",
    "    hvg_mask = adata.var['highly_variable'] if 'highly_variable' in adata.var.columns else adata.var.index[:n_top_genes]\n",
    "    X_hvg = adata[:, hvg_mask].X.toarray() if hasattr(adata.X, 'toarray') else adata[:, hvg_mask].X\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_hvg)\n",
    "    \n",
    "    encodings = {}\n",
    "    \n",
    "    # PCA encoding\n",
    "    pca = PCA(n_components=50)\n",
    "    encodings['pca'] = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # TruncatedSVD for sparse matrices\n",
    "    svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "    encodings['svd'] = svd.fit_transform(X_scaled)\n",
    "    \n",
    "    # UMAP encoding\n",
    "    umap_encoder = umap.UMAP(n_components=20, random_state=42)\n",
    "    encodings['umap'] = umap_encoder.fit_transform(X_scaled)\n",
    "    \n",
    "    return encodings, X_scaled\n",
    "\n",
    "print(\"Genetic sequence encoding functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3a26750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic sequence encoding functions defined successfully!\n",
      "CPU times: user 131 µs, sys: 0 ns, total: 131 µs\n",
      "Wall time: 137 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Genetic Sequence Encoding Functions ---\n",
    "\n",
    "def one_hot_encode_sequence(sequence, max_length=50, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    One-hot encode a protein/nucleotide sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        max_length: Maximum sequence length (pad or truncate)\n",
    "        alphabet: Valid characters in the sequence\n",
    "    \n",
    "    Returns:\n",
    "        2D numpy array of shape (max_length, len(alphabet))\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    sequence = str(sequence).upper()[:max_length]  # Truncate if too long\n",
    "    encoding = np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    for i, char in enumerate(sequence):\n",
    "        if char in alphabet:\n",
    "            char_idx = alphabet.index(char)\n",
    "            encoding[i, char_idx] = 1\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "def kmer_encode_sequence(sequence, k=3, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    K-mer encoding of sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        k: Length of k-mers\n",
    "        alphabet: Valid characters\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with k-mer counts\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {}\n",
    "    \n",
    "    sequence = str(sequence).upper()\n",
    "    kmers = [sequence[i:i+k] for i in range(len(sequence)-k+1)]\n",
    "    valid_kmers = [kmer for kmer in kmers if all(c in alphabet for c in kmer)]\n",
    "    \n",
    "    return Counter(valid_kmers)\n",
    "\n",
    "def physicochemical_features(sequence):\n",
    "    \"\"\"\n",
    "    Extract physicochemical properties from protein sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Protein sequence string\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of features\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {\n",
    "            'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        seq = str(sequence).upper()\n",
    "        # Remove non-standard amino acids\n",
    "        seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "        \n",
    "        if len(seq) == 0:\n",
    "            return {\n",
    "                'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "                'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "            }\n",
    "        \n",
    "        bio_seq = Seq(seq)\n",
    "        analyzer = ProtParam.ProteinAnalysis(str(bio_seq))\n",
    "        \n",
    "        return {\n",
    "            'length': len(seq),\n",
    "            'molecular_weight': analyzer.molecular_weight(),\n",
    "            'aromaticity': analyzer.aromaticity(),\n",
    "            'instability_index': analyzer.instability_index(),\n",
    "            'isoelectric_point': analyzer.isoelectric_point(),\n",
    "            'hydrophobicity': analyzer.gravy()  # Grand Average of Hydropathy\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'length': len(str(sequence)) if not pd.isna(sequence) else 0,\n",
    "            'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "\n",
    "def encode_gene_expression_patterns(adata, n_top_genes=1000):\n",
    "    \"\"\"\n",
    "    Encode gene expression patterns using various dimensionality reduction techniques.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object with gene expression data\n",
    "        n_top_genes: Number of highly variable genes to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of encoded representations\n",
    "    \"\"\"\n",
    "    # Get highly variable genes if not already computed\n",
    "    if 'highly_variable' not in adata.var.columns:\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, subset=False)\n",
    "    \n",
    "    # Extract expression matrix for highly variable genes\n",
    "    hvg_mask = adata.var['highly_variable'] if 'highly_variable' in adata.var.columns else adata.var.index[:n_top_genes]\n",
    "    X_hvg = adata[:, hvg_mask].X.toarray() if hasattr(adata.X, 'toarray') else adata[:, hvg_mask].X\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_hvg)\n",
    "    \n",
    "    encodings = {}\n",
    "    \n",
    "    # PCA encoding\n",
    "    pca = PCA(n_components=50)\n",
    "    encodings['pca'] = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # TruncatedSVD for sparse matrices\n",
    "    svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "    encodings['svd'] = svd.fit_transform(X_scaled)\n",
    "    \n",
    "    # UMAP encoding\n",
    "    umap_encoder = umap.UMAP(n_components=20, random_state=42)\n",
    "    encodings['umap'] = umap_encoder.fit_transform(X_scaled)\n",
    "    \n",
    "    return encodings, X_scaled\n",
    "\n",
    "print(\"Genetic sequence encoding functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f06fef75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding TCR CDR3 sequences...\n",
      "Computing one-hot encodings...\n",
      "TRA one-hot shape: (38413, 600)\n",
      "TRB one-hot shape: (38413, 600)\n",
      "Computing k-mer encodings...\n",
      "TRA k-mer matrix shape: (38413, 6074)\n",
      "TRB k-mer matrix shape: (38413, 6091)\n",
      "Computing physicochemical features...\n",
      "TRA physicochemical features shape: (38413, 6)\n",
      "TRB physicochemical features shape: (38413, 6)\n",
      "TCR sequence encoding completed and added to AnnData object!\n",
      "CPU times: user 1min 37s, sys: 16 s, total: 1min 53s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Apply Sequence Encoding to TCR CDR3 Sequences ---\n",
    "\n",
    "print(\"Encoding TCR CDR3 sequences...\")\n",
    "\n",
    "# Extract CDR3 sequences\n",
    "# Convert to string type first to handle categorical data\n",
    "cdr3_sequences = {\n",
    "    'TRA': adata.obs['cdr3_TRA'].astype(str).fillna(''),\n",
    "    'TRB': adata.obs['cdr3_TRB'].astype(str).fillna('')\n",
    "}\n",
    "\n",
    "# --- 1. One-hot encoding of CDR3 sequences ---\n",
    "print(\"Computing one-hot encodings...\")\n",
    "max_cdr3_length = 30  # Typical CDR3 length range\n",
    "\n",
    "# One-hot encode TRA CDR3 sequences\n",
    "tra_onehot = np.array([one_hot_encode_sequence(seq, max_cdr3_length, 'ACDEFGHIKLMNPQRSTVWY') \n",
    "                       for seq in cdr3_sequences['TRA']])\n",
    "tra_onehot_flat = tra_onehot.reshape(tra_onehot.shape[0], -1)\n",
    "\n",
    "# One-hot encode TRB CDR3 sequences  \n",
    "trb_onehot = np.array([one_hot_encode_sequence(seq, max_cdr3_length, 'ACDEFGHIKLMNPQRSTVWY') \n",
    "                       for seq in cdr3_sequences['TRB']])\n",
    "trb_onehot_flat = trb_onehot.reshape(trb_onehot.shape[0], -1)\n",
    "\n",
    "print(f\"TRA one-hot shape: {tra_onehot_flat.shape}\")\n",
    "print(f\"TRB one-hot shape: {trb_onehot_flat.shape}\")\n",
    "\n",
    "# --- 2. K-mer encoding ---\n",
    "print(\"Computing k-mer encodings...\")\n",
    "k = 3  # Use 3-mers\n",
    "\n",
    "# Get all possible k-mers for creating consistent feature vectors\n",
    "all_tra_kmers = []\n",
    "all_trb_kmers = []\n",
    "\n",
    "for seq in cdr3_sequences['TRA']:\n",
    "    if seq and seq != '':\n",
    "        all_tra_kmers.extend(kmer_encode_sequence(seq, k).keys())\n",
    "\n",
    "for seq in cdr3_sequences['TRB']:\n",
    "    if seq and seq != '':\n",
    "        all_trb_kmers.extend(kmer_encode_sequence(seq, k).keys())\n",
    "\n",
    "unique_tra_kmers = sorted(list(set(all_tra_kmers)))\n",
    "unique_trb_kmers = sorted(list(set(all_trb_kmers)))\n",
    "\n",
    "# Create k-mer count vectors\n",
    "tra_kmer_matrix = []\n",
    "for seq in cdr3_sequences['TRA']:\n",
    "    kmer_counts = kmer_encode_sequence(seq, k)\n",
    "    vector = [kmer_counts.get(kmer, 0) for kmer in unique_tra_kmers]\n",
    "    tra_kmer_matrix.append(vector)\n",
    "\n",
    "trb_kmer_matrix = []\n",
    "for seq in cdr3_sequences['TRB']:\n",
    "    kmer_counts = kmer_encode_sequence(seq, k)\n",
    "    vector = [kmer_counts.get(kmer, 0) for kmer in unique_trb_kmers]\n",
    "    trb_kmer_matrix.append(vector)\n",
    "\n",
    "tra_kmer_matrix = np.array(tra_kmer_matrix)\n",
    "trb_kmer_matrix = np.array(trb_kmer_matrix)\n",
    "\n",
    "print(f\"TRA k-mer matrix shape: {tra_kmer_matrix.shape}\")\n",
    "print(f\"TRB k-mer matrix shape: {trb_kmer_matrix.shape}\")\n",
    "\n",
    "# --- 3. Physicochemical properties ---\n",
    "print(\"Computing physicochemical features...\")\n",
    "\n",
    "tra_physico = pd.DataFrame([physicochemical_features(seq) for seq in cdr3_sequences['TRA']])\n",
    "trb_physico = pd.DataFrame([physicochemical_features(seq) for seq in cdr3_sequences['TRB']])\n",
    "\n",
    "print(f\"TRA physicochemical features shape: {tra_physico.shape}\")\n",
    "print(f\"TRB physicochemical features shape: {trb_physico.shape}\")\n",
    "\n",
    "# Add to AnnData object\n",
    "adata.obsm['X_tcr_tra_onehot'] = tra_onehot_flat\n",
    "adata.obsm['X_tcr_trb_onehot'] = trb_onehot_flat\n",
    "adata.obsm['X_tcr_tra_kmer'] = tra_kmer_matrix\n",
    "adata.obsm['X_tcr_trb_kmer'] = trb_kmer_matrix\n",
    "\n",
    "# Add physicochemical features to obs\n",
    "for col in tra_physico.columns:\n",
    "    adata.obs[f'tra_{col}'] = tra_physico[col].values\n",
    "    adata.obs[f'trb_{col}'] = trb_physico[col].values\n",
    "\n",
    "print(\"TCR sequence encoding completed and added to AnnData object!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de86f142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing gene expression data...\n",
      "Basic preprocessing completed\n",
      "Encoding gene expression patterns...\n",
      "Gene expression encoding completed!\n",
      "CPU times: user 2min 39s, sys: 10.2 s, total: 2min 49s\n",
      "Wall time: 2min 29s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Encode Gene Expression Patterns ---\n",
    "\n",
    "print(\"Preprocessing gene expression data...\")\n",
    "\n",
    "# Basic preprocessing if not already done\n",
    "if 'X_pca' not in adata.obsm:\n",
    "    # Store raw counts\n",
    "    adata.raw = adata\n",
    "    \n",
    "    # Normalize counts per cell to a fixed total\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    # Log-transform the data\n",
    "    sc.pp.log1p(adata)\n",
    "    \n",
    "    # Replace any infinite values with zeros\n",
    "    if hasattr(adata.X, 'data'):  # sparse matrix\n",
    "        adata.X.data[np.isinf(adata.X.data)] = 0\n",
    "    else:  # dense matrix\n",
    "        adata.X[np.isinf(adata.X)] = 0\n",
    "    \n",
    "    print(\"Basic preprocessing completed\")\n",
    "\n",
    "print(\"Encoding gene expression patterns...\")\n",
    "\n",
    "# Apply gene expression encoding with fixed function\n",
    "def encode_gene_expression_patterns_fixed(adata, n_top_genes=2000):\n",
    "    \"\"\"\n",
    "    Fixed version of gene expression encoding\n",
    "    \"\"\"\n",
    "    # Select highly variable genes manually to avoid infinity issues\n",
    "    X_dense = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\n",
    "    \n",
    "    # Calculate variance for each gene\n",
    "    gene_vars = np.var(X_dense, axis=0)\n",
    "    # Remove any infinite or NaN values\n",
    "    gene_vars = np.nan_to_num(gene_vars, nan=0, posinf=0, neginf=0)\n",
    "    \n",
    "    # Select top variable genes\n",
    "    top_genes_idx = np.argsort(gene_vars)[-n_top_genes:]\n",
    "    X_hvg = X_dense[:, top_genes_idx]\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_hvg)\n",
    "    \n",
    "    encodings = {}\n",
    "    \n",
    "    # PCA encoding\n",
    "    pca = PCA(n_components=min(50, X_scaled.shape[1]))\n",
    "    encodings['pca'] = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # TruncatedSVD for sparse matrices\n",
    "    svd = TruncatedSVD(n_components=min(50, X_scaled.shape[1]), random_state=42)\n",
    "    encodings['svd'] = svd.fit_transform(X_scaled)\n",
    "    \n",
    "    # UMAP encoding\n",
    "    umap_encoder = umap.UMAP(n_components=20, random_state=42)\n",
    "    encodings['umap'] = umap_encoder.fit_transform(X_scaled)\n",
    "    \n",
    "    return encodings, X_scaled\n",
    "\n",
    "# Apply fixed gene expression encoding\n",
    "gene_encodings, X_scaled_genes = encode_gene_expression_patterns_fixed(adata, n_top_genes=2000)\n",
    "\n",
    "# Add gene expression encodings to AnnData\n",
    "for encoding_name, encoding_data in gene_encodings.items():\n",
    "    adata.obsm[f'X_gene_{encoding_name}'] = encoding_data\n",
    "\n",
    "print(\"Gene expression encoding completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59778709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying unsupervised machine learning algorithms...\n",
      "Creating combined gene-TCR embeddings...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Unsupervised Machine Learning Analysis ---\n",
    "\n",
    "print(\"Applying unsupervised machine learning algorithms...\")\n",
    "\n",
    "# --- Create combined embeddings if they don't exist ---\n",
    "if 'X_umap_combined' not in adata.obsm or 'X_tsne_combined' not in adata.obsm:\n",
    "    print(\"Creating combined gene-TCR embeddings...\")\n",
    "    \n",
    "    # Combine gene expression PCA with TCR k-mer features\n",
    "    gene_pca = gene_encodings['pca']\n",
    "    tcr_tra_kmer = adata.obsm['X_tcr_tra_kmer']\n",
    "    tcr_trb_kmer = adata.obsm['X_tcr_trb_kmer']\n",
    "    \n",
    "    # Ensure all features have the same number of samples\n",
    "    min_samples = min(gene_pca.shape[0], tcr_tra_kmer.shape[0], tcr_trb_kmer.shape[0])\n",
    "    gene_pca = gene_pca[:min_samples]\n",
    "    tcr_tra_kmer = tcr_tra_kmer[:min_samples]\n",
    "    tcr_trb_kmer = tcr_trb_kmer[:min_samples]\n",
    "    \n",
    "    # Combine features\n",
    "    combined_gene_tcr = np.column_stack([gene_pca, tcr_tra_kmer, tcr_trb_kmer])\n",
    "    \n",
    "    # Apply UMAP\n",
    "    umap_combined = umap.UMAP(n_components=2, random_state=42)\n",
    "    X_umap_combined = umap_combined.fit_transform(combined_gene_tcr)\n",
    "    adata.obsm['X_umap_combined'] = X_umap_combined\n",
    "    \n",
    "    # Apply t-SNE (sample for speed if needed)\n",
    "    sample_size = min(5000, combined_gene_tcr.shape[0])\n",
    "    if sample_size < combined_gene_tcr.shape[0]:\n",
    "        sample_idx = np.random.choice(combined_gene_tcr.shape[0], sample_size, replace=False)\n",
    "        tsne_combined = TSNE(n_components=2, random_state=42, init='pca')\n",
    "        X_tsne_sample = tsne_combined.fit_transform(combined_gene_tcr[sample_idx])\n",
    "        # For full dataset, use UMAP result as approximation\n",
    "        X_tsne_combined = X_umap_combined.copy()\n",
    "        X_tsne_combined[sample_idx] = X_tsne_sample\n",
    "    else:\n",
    "        tsne_combined = TSNE(n_components=2, random_state=42, init='pca')\n",
    "        X_tsne_combined = tsne_combined.fit_transform(combined_gene_tcr)\n",
    "    \n",
    "    adata.obsm['X_tsne_combined'] = X_tsne_combined\n",
    "    print(\"Combined embeddings created!\")\n",
    "else:\n",
    "    # If embeddings exist, recreate combined_gene_tcr for downstream use\n",
    "    gene_pca = gene_encodings['pca']\n",
    "    tcr_tra_kmer = adata.obsm['X_tcr_tra_kmer']\n",
    "    tcr_trb_kmer = adata.obsm['X_tcr_trb_kmer']\n",
    "    \n",
    "    # Ensure all features have the same number of samples\n",
    "    min_samples = min(gene_pca.shape[0], tcr_tra_kmer.shape[0], tcr_trb_kmer.shape[0])\n",
    "    gene_pca = gene_pca[:min_samples]\n",
    "    tcr_tra_kmer = tcr_tra_kmer[:min_samples]\n",
    "    tcr_trb_kmer = tcr_trb_kmer[:min_samples]\n",
    "    \n",
    "    # Combine features\n",
    "    combined_gene_tcr = np.column_stack([gene_pca, tcr_tra_kmer, tcr_trb_kmer])\n",
    "\n",
    "# Also try clustering on the original scaled combined data\n",
    "X_combined_scaled = StandardScaler().fit_transform(combined_gene_tcr)\n",
    "\n",
    "clustering_results = {}\n",
    "\n",
    "# Define different feature sets to try\n",
    "feature_sets = {\n",
    "    'umap': X_umap,\n",
    "    'tsne': X_tsne,\n",
    "    'combined_scaled': X_combined_scaled\n",
    "}\n",
    "\n",
    "# K-Means clustering with hyperparameter tuning\n",
    "print(\"Running K-Means clustering...\")\n",
    "for feature_name, X_features in feature_sets.items():\n",
    "    for n_clusters in [3, 4, 5, 6, 8]:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20, max_iter=1000)\n",
    "        cluster_labels = kmeans.fit_predict(X_features)\n",
    "        silhouette = silhouette_score(X_features, cluster_labels)\n",
    "        clustering_results[f'kmeans_{n_clusters}_{feature_name}'] = {\n",
    "            'labels': cluster_labels, \n",
    "            'silhouette': silhouette,\n",
    "            'algorithm': f'K-Means ({feature_name})',\n",
    "            'n_clusters': n_clusters,\n",
    "            'feature_set': feature_name\n",
    "        }\n",
    "        adata.obs[f'kmeans_{n_clusters}_{feature_name}'] = pd.Categorical(cluster_labels)\n",
    "\n",
    "# HDBSCAN clustering with optimized parameters\n",
    "print(\"Running HDBSCAN clustering...\")\n",
    "for feature_name, X_features in feature_sets.items():\n",
    "    # Try different parameter combinations\n",
    "    hdbscan_params = [\n",
    "        {'min_cluster_size': 15, 'min_samples': 5},\n",
    "        {'min_cluster_size': 20, 'min_samples': 10},\n",
    "        {'min_cluster_size': 25, 'min_samples': 15}\n",
    "    ]\n",
    "    \n",
    "    for i, params in enumerate(hdbscan_params):\n",
    "        # hdbscan.HDBSCAN does not accept random_state; avoid passing unexpected kwargs\n",
    "        hdbscan_clusterer = hdbscan.HDBSCAN(**params)\n",
    "        hdbscan_labels = hdbscan_clusterer.fit_predict(X_features)\n",
    "        if len(set(hdbscan_labels)) > 1:  # Only compute silhouette if more than 1 cluster\n",
    "            hdbscan_silhouette = silhouette_score(X_features, hdbscan_labels)\n",
    "        else:\n",
    "            hdbscan_silhouette = -1\n",
    "        clustering_results[f'hdbscan_{feature_name}_{i}'] = {\n",
    "            'labels': hdbscan_labels, \n",
    "            'silhouette': hdbscan_silhouette,\n",
    "            'algorithm': f'HDBSCAN ({feature_name})',\n",
    "            'params': params,\n",
    "            'feature_set': feature_name\n",
    "        }\n",
    "        adata.obs[f'hdbscan_{feature_name}_{i}'] = pd.Categorical(hdbscan_labels)\n",
    "\n",
    "# # Agglomerative clustering with different linkage methods\n",
    "# print(\"Running Agglomerative clustering...\")\n",
    "# linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "# for feature_name, X_features in feature_sets.items():\n",
    "#     for linkage in linkage_methods:\n",
    "#         if linkage == 'ward' and feature_name == 'tsne':\n",
    "#             continue  # Ward linkage doesn't work well with t-SNE distances\n",
    "#         for n_clusters in [3, 4, 5, 6]:\n",
    "#             try:\n",
    "#                 agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "#                 agg_labels = agg_clustering.fit_predict(X_features)\n",
    "#                 agg_silhouette = silhouette_score(X_features, agg_labels)\n",
    "#                 clustering_results[f'agglomerative_{n_clusters}_{linkage}_{feature_name}'] = {\n",
    "#                     'labels': agg_labels, \n",
    "#                     'silhouette': agg_silhouette,\n",
    "#                     'algorithm': f'Agglomerative ({linkage}, {feature_name})',\n",
    "#                     'n_clusters': n_clusters,\n",
    "#                     'linkage': linkage,\n",
    "#                     'feature_set': feature_name\n",
    "#                 }\n",
    "#                 adata.obs[f'agglomerative_{n_clusters}_{linkage}_{feature_name}'] = pd.Categorical(agg_labels)\n",
    "#             except:\n",
    "#                 continue\n",
    "\n",
    "# DBSCAN clustering with parameter optimization\n",
    "print(\"Running DBSCAN clustering...\")\n",
    "eps_values = [0.3, 0.5, 0.8, 1.0]\n",
    "min_samples_values = [5, 10, 15]\n",
    "\n",
    "for feature_name, X_features in feature_sets.items():\n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            dbscan_labels = dbscan.fit_predict(X_features)\n",
    "            if len(set(dbscan_labels)) > 1 and -1 not in dbscan_labels:\n",
    "                dbscan_silhouette = silhouette_score(X_features, dbscan_labels)\n",
    "            else:\n",
    "                dbscan_silhouette = -1\n",
    "            clustering_results[f'dbscan_{eps}_{min_samples}_{feature_name}'] = {\n",
    "                'labels': dbscan_labels,\n",
    "                'silhouette': dbscan_silhouette,\n",
    "                'algorithm': f'DBSCAN ({feature_name})',\n",
    "                'eps': eps,\n",
    "                'min_samples': min_samples,\n",
    "                'feature_set': feature_name\n",
    "            }\n",
    "            adata.obs[f'dbscan_{eps}_{min_samples}_{feature_name}'] = pd.Categorical(dbscan_labels)\n",
    "\n",
    "# Gaussian Mixture Models\n",
    "print(\"Running Gaussian Mixture Models...\")\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "for feature_name, X_features in feature_sets.items():\n",
    "    for n_components in [3, 4, 5, 6]:\n",
    "        for covariance_type in ['full', 'tied', 'diag', 'spherical']:\n",
    "            try:\n",
    "                gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=42)\n",
    "                gmm_labels = gmm.fit_predict(X_features)\n",
    "                gmm_silhouette = silhouette_score(X_features, gmm_labels)\n",
    "                clustering_results[f'gmm_{n_components}_{covariance_type}_{feature_name}'] = {\n",
    "                    'labels': gmm_labels,\n",
    "                    'silhouette': gmm_silhouette,\n",
    "                    'algorithm': f'GMM ({covariance_type}, {feature_name})',\n",
    "                    'n_components': n_components,\n",
    "                    'covariance_type': covariance_type,\n",
    "                    'feature_set': feature_name\n",
    "                }\n",
    "                adata.obs[f'gmm_{n_components}_{covariance_type}_{feature_name}'] = pd.Categorical(gmm_labels)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "# Hierarchical clustering with different methods\n",
    "print(\"Running Hierarchical clustering...\")\n",
    "for feature_name, X_features in feature_sets.items():\n",
    "    try:\n",
    "        # Use different linkage methods for hierarchical clustering\n",
    "        for method in ['ward', 'complete', 'average']:\n",
    "            if method == 'ward' and feature_name == 'tsne':\n",
    "                continue\n",
    "            Z = linkage(X_features[:2000], method=method)  # Sample for speed\n",
    "            for t in [3, 4, 5, 6]:\n",
    "                hierarchical_labels = fcluster(Z, t=t, criterion='maxclust')\n",
    "                clustering_results[f'hierarchical_{method}_{t}_{feature_name}'] = {\n",
    "                    'labels': hierarchical_labels,\n",
    "                    'silhouette': silhouette_score(X_features[:2000], hierarchical_labels),\n",
    "                    'algorithm': f'Hierarchical ({method}, {feature_name})',\n",
    "                    'method': method,\n",
    "                    't': t,\n",
    "                    'feature_set': feature_name\n",
    "                }\n",
    "                adata.obs.loc[adata.obs.index[:2000], f'hierarchical_{method}_{t}_{feature_name}'] = pd.Categorical(hierarchical_labels)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Print clustering results summary\n",
    "print(\"\\nClustering Results Summary:\")\n",
    "print(f\"{'Method':<40} {'Clusters':<10} {'Silhouette':<12} {'Feature Set':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for name, result in sorted(clustering_results.items(), key=lambda x: x[1]['silhouette'], reverse=True):\n",
    "    n_clusters = len(set(result['labels'])) - (1 if -1 in result['labels'] else 0)\n",
    "    print(f\"{result['algorithm']:<40} {n_clusters:<10} {result['silhouette']:<12.3f} {result['feature_set']:<15}\")\n",
    "\n",
    "# Find best clustering result\n",
    "best_clustering = max(clustering_results.items(), key=lambda x: x[1]['silhouette'])\n",
    "print(f\"\\nBest clustering: {best_clustering[0]} (Silhouette: {best_clustering[1]['silhouette']:.3f})\")\n",
    "print(f\"Algorithm: {best_clustering[1]['algorithm']}\")\n",
    "print(f\"Feature set: {best_clustering[1]['feature_set']}\")\n",
    "print(f\"Number of clusters: {len(set(best_clustering[1]['labels'])) - (1 if -1 in best_clustering[1]['labels'] else 0)}\")\n",
    "\n",
    "# --- 2. TCR Sequence-Specific Clustering ---\n",
    "print(\"\\nPerforming TCR sequence-specific clustering...\")\n",
    "\n",
    "# Cluster based on TRA k-mer features with improved parameters\n",
    "tra_scaler = StandardScaler()\n",
    "tra_kmer_scaled = tra_scaler.fit_transform(adata.obsm['X_tcr_tra_kmer'])\n",
    "\n",
    "# K-means on TRA sequences with optimal k\n",
    "tra_kmeans = KMeans(n_clusters=6, random_state=42, n_init=20)  # Increased n_clusters\n",
    "tra_clusters = tra_kmeans.fit_predict(tra_kmer_scaled)\n",
    "adata.obs['tra_kmer_clusters'] = pd.Categorical(tra_clusters)\n",
    "\n",
    "# K-means on TRB sequences with optimal k\n",
    "trb_scaler = StandardScaler()\n",
    "trb_kmer_scaled = trb_scaler.fit_transform(adata.obsm['X_tcr_trb_kmer'])\n",
    "trb_kmeans = KMeans(n_clusters=6, random_state=42, n_init=20)  # Increased n_clusters\n",
    "trb_clusters = trb_kmeans.fit_predict(trb_kmer_scaled)\n",
    "adata.obs['trb_kmer_clusters'] = pd.Categorical(trb_clusters)\n",
    "\n",
    "print(\"TCR sequence clustering completed!\")\n",
    "\n",
    "# --- 3. Gene Expression Module Discovery ---\n",
    "print(\"\\nDiscovering gene expression modules...\")\n",
    "\n",
    "# Use gene expression PCA for module discovery with optimal k\n",
    "gene_pca_full = gene_encodings['pca']\n",
    "gene_kmeans = KMeans(n_clusters=8, random_state=42, n_init=20)  # Increased n_clusters\n",
    "gene_expression_modules = gene_kmeans.fit_predict(gene_pca_full)\n",
    "adata.obs['gene_expression_modules'] = pd.Categorical(gene_expression_modules)\n",
    "\n",
    "print(\"Gene expression module discovery completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c05fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['gene_expression_modules'] = pd.Categorical(gene_expression_modules)\n",
    "\n",
    "print(\"Gene expression module discovery completed!\")\n",
    "\n",
    "# --- 4. Dendrogram Visualization for Hierarchical Clustering ---\n",
    "print(\"\\nCreating dendrogram for hierarchical clustering...\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(Z, truncate_mode='lastp', p=12, leaf_rotation=45, leaf_font_size=10, show_contracted=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nUnsupervised machine learning analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Comprehensive Feature Engineering ---\n",
    "\n",
    "print(\"Creating comprehensive feature set using ALL available encodings...\")\n",
    "\n",
    "# --- 1. Strategic Feature Engineering with Dimensionality Reduction ---\n",
    "print(\"Applying strategic dimensionality reduction to high-dimensional features...\")\n",
    "\n",
    "# Filter for supervised learning samples first to reduce memory\n",
    "supervised_mask = adata.obs['response'].isin(['Responder', 'Non-Responder'])\n",
    "y_supervised = adata.obs['response'][supervised_mask]\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_supervised)\n",
    "\n",
    "print(f\"Working with {sum(supervised_mask)} samples for supervised learning\")\n",
    "print(f\"Class distribution: {dict(zip(label_encoder.classes_, np.bincount(y_encoded)))}\")\n",
    "\n",
    "# --- Reduce high-dimensional k-mer features using variance-based selection ---\n",
    "tra_kmer_supervised = adata.obsm['X_tcr_tra_kmer'][supervised_mask]\n",
    "trb_kmer_supervised = adata.obsm['X_tcr_trb_kmer'][supervised_mask]\n",
    "\n",
    "# Select top variance k-mers to reduce dimensionality\n",
    "def select_top_variance_features(X, n_features=200):\n",
    "    \"\"\"Select features with highest variance\"\"\"\n",
    "    variances = np.var(X, axis=0)\n",
    "    top_indices = np.argsort(variances)[-n_features:]\n",
    "    return X[:, top_indices], top_indices\n",
    "\n",
    "print(\"Reducing k-mer features by variance selection...\")\n",
    "tra_kmer_reduced, tra_top_idx = select_top_variance_features(tra_kmer_supervised, n_features=200)\n",
    "trb_kmer_reduced, trb_top_idx = select_top_variance_features(trb_kmer_supervised, n_features=200)\n",
    "\n",
    "print(f\"TRA k-mers reduced from {tra_kmer_supervised.shape[1]} to {tra_kmer_reduced.shape[1]}\")\n",
    "print(f\"TRB k-mers reduced from {trb_kmer_supervised.shape[1]} to {trb_kmer_reduced.shape[1]}\")\n",
    "\n",
    "# --- 2. Create strategic feature combinations ---\n",
    "feature_sets = {}\n",
    "\n",
    "# Basic features (gene expression + physicochemical)\n",
    "gene_features = adata.obsm['X_gene_pca'][supervised_mask]\n",
    "tcr_physico = np.column_stack([\n",
    "    adata.obs[['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']].fillna(0)[supervised_mask],\n",
    "    adata.obs[['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']].fillna(0)[supervised_mask]\n",
    "])\n",
    "qc_features = adata.obs[['n_genes_by_counts', 'total_counts', 'pct_counts_mt']].fillna(0)[supervised_mask].values\n",
    "\n",
    "feature_sets['basic'] = np.column_stack([\n",
    "    gene_features[:, :20],  # Top 20 gene PCA components\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# Enhanced gene expression\n",
    "feature_sets['gene_enhanced'] = np.column_stack([\n",
    "    adata.obsm['X_gene_pca'][supervised_mask],  # All 50 PCA components\n",
    "    adata.obsm['X_gene_svd'][supervised_mask][:, :30],  # Top 30 SVD components\n",
    "    adata.obsm['X_gene_umap'][supervised_mask],  # All 20 UMAP components\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# TCR sequence enhanced\n",
    "feature_sets['tcr_enhanced'] = np.column_stack([\n",
    "    gene_features[:, :20],  # Top 20 gene PCA\n",
    "    tra_kmer_reduced,  # Top 200 TRA k-mers\n",
    "    trb_kmer_reduced,  # Top 200 TRB k-mers\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# Comprehensive (all modalities)\n",
    "feature_sets['comprehensive'] = np.column_stack([\n",
    "    adata.obsm['X_gene_pca'][supervised_mask],  # 50 features\n",
    "    adata.obsm['X_gene_svd'][supervised_mask][:, :20],  # Top 20 SVD\n",
    "    adata.obsm['X_gene_umap'][supervised_mask],  # 20 features\n",
    "    tra_kmer_reduced,  # 200 features\n",
    "    trb_kmer_reduced,  # 200 features  \n",
    "    tcr_physico,  # 6 features\n",
    "    qc_features  # 3 features\n",
    "])\n",
    "\n",
    "# One-hot encoded sequences (reduced)\n",
    "onehot_tra_reduced = PCA(n_components=50).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "onehot_trb_reduced = PCA(n_components=50).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "\n",
    "feature_sets['sequence_structure'] = np.column_stack([\n",
    "    gene_features[:, :30],  # Top 30 gene PCA\n",
    "    onehot_tra_reduced,  # 50 PCA of one-hot TRA\n",
    "    onehot_trb_reduced,  # 50 PCA of one-hot TRB\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "print(f\"\\nFeature set dimensions:\")\n",
    "for name, features in feature_sets.items():\n",
    "    print(f\"  • {name}: {features.shape}\")\n",
    "\n",
    "print(\"Comprehensive feature engineering completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Supervised Learning with Multiple Models ---\n",
    "\n",
    "print(\"Training and evaluating multiple supervised learning models...\")\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "models['XGBoost'] = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "\n",
    "def create_deep_model(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_dim=input_dim),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "models['Deep Learning'] = 'DL'  # Placeholder\n",
    "\n",
    "# Hyperparameter grids for experimentation\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [5, 10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.3]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store results\n",
    "all_results = {}\n",
    "\n",
    "for feature_name, X_features in feature_sets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FEATURE SET: {feature_name.upper()} ({X_features.shape[1]} features)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_features, y_encoded, test_size=0.3, random_state=42, stratify y_encoded\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    feature_results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n--- {model_name} ---\")\n",
    "        \n",
    "        if model_name == 'Deep Learning':\n",
    "            # Special handling for DL\n",
    "            dl_model = create_deep_model(X_train_scaled.shape[1])\n",
    "            history = dl_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "            y_pred_proba = dl_model.predict(X_test_scaled).flatten()\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        else:\n",
    "            # Grid search for hyperparameters\n",
    "            if model_name in param_grids:\n",
    "                grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='accuracy', n_jobs=-1)\n",
    "                grid_search.fit(X_train_scaled, y_train)\n",
    "                best_model = grid_search.best_estimator_\n",
    "                print(f\"Best params: {grid_search.best_params_}\")\n",
    "            else:\n",
    "                best_model = model\n",
    "                best_model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            y_pred = best_model.predict(X_test_scaled)\n",
    "            y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Specificity\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        \n",
    "        # NPV\n",
    "        npv = tn / (tn + fn)\n",
    "        \n",
    "        # Cross-validation\n",
    "        if model_name != 'Deep Learning':\n",
    "            cv_scores = cross_val_score(best_model, X_features, y_encoded, cv=5, scoring='accuracy')\n",
    "            cv_mean = cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "        else:\n",
    "            cv_mean = accuracy  # Approximation\n",
    "            cv_std = 0\n",
    "        \n",
    "        feature_results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'auc': auc,\n",
    "            'specificity': specificity,\n",
    "            'npv': npv,\n",
    "            'cv_mean': cv_mean,\n",
    "            'cv_std': cv_std,\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "        print(f\"Specificity: {specificity:.3f}, NPV: {npv:.3f}, AUC: {auc:.3f}\")\n",
    "        print(f\"CV Accuracy: {cv_mean:.3f} ± {cv_std:.3f}\")\n",
    "    \n",
    "    all_results[feature_name] = feature_results\n",
    "\n",
    "# Find best overall model\n",
    "best_score = 0\n",
    "best_model_info = None\n",
    "for feature_name, feature_result in all_results.items():\n",
    "    for model_name, result in feature_result.items():\n",
    "        if result['cv_mean'] > best_score:\n",
    "            best_score = result['cv_mean']\n",
    "            best_model_info = (feature_name, model_name, result)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BEST MODEL SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Feature Set: {best_model_info[0]}\")\n",
    "print(f\"Model: {best_model_info[1]}\")\n",
    "print(f\"CV Accuracy: {best_score:.3f}\")\n",
    "print(f\"Test Accuracy: {best_model_info[2]['accuracy']:.3f}\")\n",
    "print(f\"AUC: {best_model_info[2]['auc']:.3f}\")\n",
    "\n",
    "print(\"\\nSupervised learning with multiple models completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84711284",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Model Performance Evaluation ---\n",
    "\n",
    "print(\"Generating comprehensive model performance evaluation...\")\n",
    "\n",
    "# Create detailed performance report\n",
    "performance_report = []\n",
    "\n",
    "for feature_name, feature_result in all_results.items():\n",
    "    for model_name, result in feature_result.items():\n",
    "        cm = result['confusion_matrix']\n",
    "        \n",
    "        report_entry = {\n",
    "            'Feature_Set': feature_name,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'Precision': result['precision'],\n",
    "            'Recall': result['recall'],\n",
    "            'F1_Score': result['f1_score'],\n",
    "            'AUC': result['auc'],\n",
    "            'Specificity': result['specificity'],\n",
    "            'NPV': result['npv'],\n",
    "            'CV_Mean': result['cv_mean'],\n",
    "            'CV_Std': result['cv_std'],\n",
    "            'TN': cm[0,0],\n",
    "            'FP': cm[0,1],\n",
    "            'FN': cm[1,0],\n",
    "            'TP': cm[1,1]\n",
    "        }\n",
    "        performance_report.append(report_entry)\n",
    "\n",
    "performance_df = pd.DataFrame(performance_report)\n",
    "\n",
    "# Display results\n",
    "print(\"COMPREHENSIVE MODEL PERFORMANCE REPORT\")\n",
    "print(\"=\"*120)\n",
    "print(performance_df.to_string(index=False))\n",
    "\n",
    "# Visualize confusion matrices for best models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "best_models = performance_df.nlargest(6, 'CV_Mean')\n",
    "\n",
    "for i, (_, row) in enumerate(best_models.iterrows()):\n",
    "    if i < 6:\n",
    "        cm = np.array([[row['TN'], row['FP']], [row['FN'], row['TP']]])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Responder', 'Responder'])\n",
    "        disp.plot(ax=axes[i], cmap='Blues')\n",
    "        axes[i].set_title(f\"{row['Model']}\\n({row['Feature_Set']})\\nAcc: {row['Accuracy']:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-validation comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "cv_data = performance_df.pivot(index='Feature_Set', columns='Model', values='CV_Mean')\n",
    "cv_data.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('Cross-Validation Accuracy Comparison')\n",
    "plt.ylabel('CV Accuracy')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# AUC comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "auc_data = performance_df.pivot(index='Feature_Set', columns='Model', values='AUC')\n",
    "auc_data.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('AUC Comparison Across Models and Feature Sets')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel performance evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Experiment with Sequence Length Cutoffs ---\n",
    "\n",
    "print(\"Experimenting with sequence length cutoffs...\")\n",
    "\n",
    "# Define length cutoffs to test\n",
    "length_cutoffs = [10, 15, 20, 25, 30, 35, 40, 50]\n",
    "\n",
    "length_results = []\n",
    "\n",
    "for max_length in length_cutoffs:\n",
    "    print(f\"\\nTesting max sequence length: {max_length}\")\n",
    "    \n",
    "    # Re-encode sequences with new length\n",
    "    tra_onehot_new = np.array([one_hot_encode_sequence(seq, max_length, 'ACDEFGHIKLMNPQRSTVWY') \n",
    "                               for seq in cdr3_sequences['TRA']])\n",
    "    tra_onehot_flat_new = tra_onehot_new.reshape(tra_onehot_new.shape[0], -1)\n",
    "    \n",
    "    trb_onehot_new = np.array([one_hot_encode_sequence(seq, max_length, 'ACDEFGHIKLMNPQRSTVWY') \n",
    "                               for seq in cdr3_sequences['TRB']])\n",
    "    trb_onehot_flat_new = trb_onehot_new.reshape(trb_onehot_new.shape[0], -1)\n",
    "    \n",
    "    # Update AnnData\n",
    "    adata.obsm['X_tcr_tra_onehot'] = tra_onehot_flat_new\n",
    "    adata.obsm['X_tcr_trb_onehot'] = trb_onehot_flat_new\n",
    "    \n",
    "    # Re-create feature sets with new encodings\n",
    "    onehot_tra_reduced = PCA(n_components=50).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "    onehot_trb_reduced = PCA(n_components=50).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "    \n",
    "    X_sequence = np.column_stack([\n",
    "        gene_features[:, :30],\n",
    "        onehot_tra_reduced,\n",
    "        onehot_trb_reduced,\n",
    "        tcr_physico,\n",
    "        qc_features\n",
    "    ])\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_sequence, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_sequence, y_encoded, cv=3, scoring='accuracy')\n",
    "    \n",
    "    length_results.append({\n",
    "        'max_length': max_length,\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.3f}, CV: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "# Plot results\n",
    "length_df = pd.DataFrame(length_results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(length_df['max_length'], length_df['accuracy'], 'o-', label='Test Accuracy', linewidth=2)\n",
    "plt.plot(length_df['max_length'], length_df['cv_mean'], 's-', label='CV Accuracy', linewidth=2)\n",
    "plt.fill_between(length_df['max_length'], \n",
    "                 length_df['cv_mean'] - length_df['cv_std'], \n",
    "                 length_df['cv_mean'] + length_df['cv_std'], \n",
    "                 alpha=0.3, label='CV ± Std')\n",
    "plt.xlabel('Maximum Sequence Length Cutoff')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy vs Sequence Length Cutoff')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSequence length cutoff experiment completed!\")\n",
    "print(f\"Optimal length appears to be around {length_df.loc[length_df['cv_mean'].idxmax(), 'max_length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d66249",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Comprehensive Sequence Pattern Discovery ---\n",
    "\n",
    "print(\"Performing comprehensive sequence pattern discovery...\")\n",
    "\n",
    "# --- 1. Analyze TCR sequence patterns by response ---\n",
    "responder_mask = adata.obs['response'] == 'Responder'\n",
    "non_responder_mask = adata.obs['response'] == 'Non-Responder'\n",
    "\n",
    "# Get sequence data for analysis\n",
    "responder_tra = adata.obs[responder_mask]['cdr3_TRA'].dropna()\n",
    "non_responder_tra = adata.obs[non_responder_mask]['cdr3_TRA'].dropna()\n",
    "responder_trb = adata.obs[responder_mask]['cdr3_TRB'].dropna()\n",
    "non_responder_trb = adata.obs[non_responder_mask]['cdr3_TRB'].dropna()\n",
    "\n",
    "print(\"\\n--- TCR SEQUENCE PATTERNS ---\")\n",
    "print(f\"Responder TRA sequences: {len(responder_tra)}\")\n",
    "print(f\"Non-responder TRA sequences: {len(non_responder_tra)}\")\n",
    "\n",
    "print(\"\\nTop TRA CDR3 sequences in Responders:\")\n",
    "top_responder_tra = responder_tra.value_counts().head(10)\n",
    "for seq, count in top_responder_tra.items():\n",
    "    print(f\"  {seq}: {count} cells\")\n",
    "\n",
    "print(\"\\nTop TRA CDR3 sequences in Non-Responders:\")\n",
    "top_non_responder_tra = non_responder_tra.value_counts().head(10)\n",
    "for seq, count in top_non_responder_tra.items():\n",
    "    print(f\"  {seq}: {count} cells\")\n",
    "\n",
    "# --- 2. Physicochemical property analysis ---\n",
    "print(\"\\n--- PHYSICOCHEMICAL PROPERTY ANALYSIS ---\")\n",
    "\n",
    "# Compare physicochemical properties between responders and non-responders\n",
    "responder_physico = adata.obs[responder_mask][['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity',\n",
    "                                               'trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']]\n",
    "non_responder_physico = adata.obs[non_responder_mask][['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity',\n",
    "                                                       'trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']]\n",
    "\n",
    "properties = ['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity', \n",
    "              'trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']\n",
    "\n",
    "print(\"Statistical comparison of physicochemical properties:\")\n",
    "print(f\"{'Property':<20} {'Responder Mean':<15} {'Non-Resp Mean':<15} {'P-value':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for prop in properties:\n",
    "    resp_values = responder_physico[prop].dropna()\n",
    "    non_resp_values = non_responder_physico[prop].dropna()\n",
    "    \n",
    "    if len(resp_values) > 0 and len(non_resp_values) > 0:\n",
    "        statistic, p_value = mannwhitneyu(resp_values, non_resp_values, alternative='two-sided')\n",
    "        print(f\"{prop:<20} {resp_values.mean():<15.3f} {non_resp_values.mean():<15.3f} {p_value:<10.6f}\")\n",
    "\n",
    "# --- 3. K-mer differential analysis ---\n",
    "print(\"\\n--- K-MER DIFFERENTIAL ANALYSIS ---\")\n",
    "\n",
    "# Use the indices we identified earlier for top variance k-mers\n",
    "responder_indices = np.where(supervised_mask & responder_mask)[0]\n",
    "non_responder_indices = np.where(supervised_mask & non_responder_mask)[0]\n",
    "\n",
    "# Get k-mer data for responders vs non-responders\n",
    "responder_tra_kmers = tra_kmer_supervised[responder_indices - np.where(supervised_mask)[0][0]]\n",
    "non_responder_tra_kmers = tra_kmer_supervised[non_responder_indices - np.where(supervised_mask)[0][0]]\n",
    "\n",
    "# Calculate mean k-mer frequencies\n",
    "responder_tra_mean = responder_tra_kmers.mean(axis=0)\n",
    "non_responder_tra_mean = non_responder_tra_kmers.mean(axis=0)\n",
    "\n",
    "# Find most differentially expressed k-mers from selected features\n",
    "kmer_diff = responder_tra_mean - non_responder_tra_mean\n",
    "top_responder_kmers_idx = np.argsort(kmer_diff)[-10:]\n",
    "top_non_responder_kmers_idx = np.argsort(kmer_diff)[:10]\n",
    "\n",
    "# Get the actual k-mer sequences for the selected features\n",
    "selected_tra_kmers = [unique_tra_kmers[tra_top_idx[i]] for i in range(len(tra_top_idx))]\n",
    "\n",
    "print(\"Top k-mers enriched in Responders (from variance-selected features):\")\n",
    "for idx in top_responder_kmers_idx:\n",
    "    if idx < len(selected_tra_kmers):\n",
    "        print(f\"  {selected_tra_kmers[idx]}: +{kmer_diff[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop k-mers enriched in Non-Responders (from variance-selected features):\")\n",
    "for idx in top_non_responder_kmers_idx:\n",
    "    if idx < len(selected_tra_kmers):\n",
    "        print(f\"  {selected_tra_kmers[idx]}: {kmer_diff[idx]:.4f}\")\n",
    "\n",
    "\n",
    "# --- 4. Gene expression analysis for top important features ---\n",
    "print(\"\\n--- GENE EXPRESSION PATTERN ANALYSIS ---\")\n",
    "\n",
    "# Get the best model's feature importance\n",
    "if best_model_info[1] in all_results[best_model_info[0]]:\n",
    "    model_keys = list(all_results[best_model_info[0]][best_model_info[1]].keys())\n",
    "    if 'feature_importance' in model_keys:\n",
    "        best_importance = all_results[best_model_info[0]][best_model_info[1]]['feature_importance']\n",
    "        top_gene_features = np.argsort(best_importance)[-10:]\n",
    "        \n",
    "        print(f\"Analysis based on {best_model_info[1]} model with {best_model_info[2]['n_features']} features\")\n",
    "        print(\"Top 10 most important features for classification:\")\n",
    "        for i, feat_idx in enumerate(top_gene_features):\n",
    "            print(f\"  Feature {feat_idx}: Importance = {best_importance[feat_idx]:.4f}\")\n",
    "    elif 'feature_importances' in model_keys:\n",
    "        best_importance = all_results[best_model_info[0]][best_model_info[1]]['feature_importances']\n",
    "        top_gene_features = np.argsort(best_importance)[-10:]\n",
    "        \n",
    "        print(f\"Analysis based on {best_model_info[1]} model with {best_model_info[2]['n_features']} features\")\n",
    "        print(\"Top 10 most important features for classification:\")\n",
    "        for i, feat_idx in enumerate(top_gene_features):\n",
    "            print(f\"  Feature {feat_idx}: Importance = {best_importance[feat_idx]:.4f}\")\n",
    "    else:\n",
    "        print(f\"Feature importance not available for {best_model_info[1]} model\")\n",
    "        best_importance = None\n",
    "\n",
    "# --- 5. Comprehensive visualization ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Sequence length distributions\n",
    "axes[0,0].hist([responder_tra.str.len().dropna(), non_responder_tra.str.len().dropna()], \n",
    "               bins=20, alpha=0.7, label=['Responder', 'Non-Responder'])\n",
    "axes[0,0].set_xlabel('TRA CDR3 Length')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].set_title('TRA CDR3 Length Distribution')\n",
    "axes[0,0].legend()\n",
    "\n",
    "axes[0,1].hist([responder_trb.str.len().dropna(), non_responder_trb.str.len().dropna()], \n",
    "               bins=20, alpha=0.7, label=['Responder', 'Non-Responder'])\n",
    "axes[0,1].set_xlabel('TRB CDR3 Length')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_title('TRB CDR3 Length Distribution')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Physicochemical property comparisons\n",
    "axes[0,2].boxplot([responder_physico['tra_hydrophobicity'].dropna(), \n",
    "                   non_responder_physico['tra_hydrophobicity'].dropna()],\n",
    "                  labels=['Responder', 'Non-Responder'])\n",
    "axes[0,2].set_title('TRA Hydrophobicity by Response')\n",
    "axes[0,2].set_ylabel('Hydrophobicity')\n",
    "\n",
    "axes[1,0].boxplot([responder_physico['trb_molecular_weight'].dropna(), \n",
    "                   non_responder_physico['trb_molecular_weight'].dropna()],\n",
    "                  labels=['Responder', 'Non-Responder'])\n",
    "axes[1,0].set_title('TRB Molecular Weight by Response')\n",
    "axes[1,0].set_ylabel('Molecular Weight')\n",
    "\n",
    "# Model performance comparison\n",
    "method_names = list(all_results[best_model_info[0]].keys())\n",
    "accuracies = [all_results[best_model_info[0]][method]['accuracy'] for method in method_names]\n",
    "axes[1,1].bar(method_names, accuracies)\n",
    "axes[1,1].set_title('Model Performance Comparison')\n",
    "axes[1,1].set_ylabel('Accuracy')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Feature importance distribution\n",
    "if best_importance is not None:\n",
    "    axes[1,2].hist(best_importance, bins=20, alpha=0.7)\n",
    "    axes[1,2].set_title(f'Feature Importance Distribution\\n({best_model_info[1]} model)')\n",
    "    axes[1,2].set_xlabel('Importance Score')\n",
    "    axes[1,2].set_ylabel('Number of Features')\n",
    "else:\n",
    "    axes[1,2].text(0.5, 0.5, 'Feature importance\\nnot available', \n",
    "                   ha='center', va='center', transform=axes[1,2].transAxes)\n",
    "    axes[1,2].set_title(f'Feature Importance\\n({best_model_info[1]} model)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComprehensive sequence pattern discovery completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Save All Results and Create Final Summary ---\n",
    "\n",
    "print(\"Saving all results and creating final summary...\")\n",
    "\n",
    "import json\n",
    "\n",
    "# --- Save the enriched AnnData object ---\n",
    "output_path_enriched = output_dir / 'processed_encoded_ml_results.h5ad'\n",
    "adata.write_h5ad(output_path_enriched)\n",
    "print(f\"Enriched AnnData object with encodings and ML results saved to: {output_path_enriched}\")\n",
    "\n",
    "# --- Save performance results ---\n",
    "performance_df.to_csv(output_dir / 'model_performance_results.csv', index=False)\n",
    "print(\"Model performance results saved to CSV\")\n",
    "\n",
    "# --- Save length cutoff results ---\n",
    "length_df.to_csv(output_dir / 'sequence_length_experiment_results.csv', index=False)\n",
    "print(\"Sequence length experiment results saved to CSV\")\n",
    "\n",
    "# --- Create comprehensive summary ---\n",
    "summary = {\n",
    "    'dataset_info': {\n",
    "        'total_cells': adata.n_obs,\n",
    "        'total_genes': adata.n_vars,\n",
    "        'samples_processed': len(adata.obs['sample_id'].unique()),\n",
    "        'patients': len(adata.obs['patient_id'].unique()),\n",
    "        'responders': sum(adata.obs['response'] == 'Responder'),\n",
    "        'non_responders': sum(adata.obs['response'] == 'Non-Responder')\n",
    "    },\n",
    "    'sequence_encoding': {\n",
    "        'tcr_tra_sequences_encoded': sum(~adata.obs['cdr3_TRA'].isna()),\n",
    "        'tcr_trb_sequences_encoded': sum(~adata.obs['cdr3_TRB'].isna()),\n",
    "        'unique_tra_kmers': len(unique_tra_kmers),\n",
    "        'unique_trb_kmers': len(unique_trb_kmers),\n",
    "        'encoding_methods': ['one_hot', 'k_mer', 'physicochemical', 'gene_expression_pca', 'gene_expression_umap']\n",
    "    },\n",
    "    'clustering_results': {\n",
    "        'best_clustering_method': best_clustering[0],\n",
    "        'best_silhouette_score': best_clustering[1]['silhouette'],\n",
    "        'clustering_methods_tested': list(clustering_results.keys())\n",
    "    },\n",
    "    'supervised_learning': {\n",
    "        'best_model': best_model_info[1],\n",
    "        'best_feature_set': best_model_info[0],\n",
    "        'best_cv_accuracy': best_score,\n",
    "        'all_model_results': {k: {m: {metric: v for metric, v in result.items() if metric not in ['confusion_matrix', 'y_pred', 'y_pred_proba']} \n",
    "                                 for m, result in feature_result.items()} \n",
    "                             for k, feature_result in all_results.items()}\n",
    "    },\n",
    "    'sequence_length_experiment': {\n",
    "        'optimal_length': int(length_df.loc[length_df['cv_mean'].idxmax(), 'max_length']),\n",
    "        'length_results': length_results\n",
    "    },\n",
    "    'data_files_generated': [\n",
    "        str(output_path),\n",
    "        str(output_path_enriched),\n",
    "        str(output_dir / 'model_performance_results.csv'),\n",
    "        str(output_dir / 'sequence_length_experiment_results.csv')\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save summary as JSON\n",
    "summary_path = output_dir / 'comprehensive_analysis_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Comprehensive analysis summary saved to: {summary_path}\")\n",
    "\n",
    "# --- Print final summary ---\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE MACHINE LEARNING ANALYSIS FOR HR BREAST CANCER RNA SEQUENCING\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nDATASET OVERVIEW:\")\n",
    "print(f\"   • Total cells analyzed: {summary['dataset_info']['total_cells']:,}\")\n",
    "print(f\"   • Total genes: {summary['dataset_info']['total_genes']:,}\")\n",
    "print(f\"   • Patients: {summary['dataset_info']['patients']}\")\n",
    "print(f\"   • Responders: {summary['dataset_info']['responders']}\")\n",
    "print(f\"   • Non-responders: {summary['dataset_info']['non_responders']}\")\n",
    "\n",
    "print(f\"\\nSEQUENCE ENCODING:\")\n",
    "print(f\"   • TRA sequences encoded: {summary['sequence_encoding']['tcr_tra_sequences_encoded']}\")\n",
    "print(f\"   • TRB sequences encoded: {summary['sequence_encoding']['tcr_trb_sequences_encoded']}\")\n",
    "print(f\"   • Unique TRA k-mers: {summary['sequence_encoding']['unique_tra_kmers']}\")\n",
    "print(f\"   • Unique TRB k-mers: {summary['sequence_encoding']['unique_trb_kmers']}\")\n",
    "\n",
    "print(f\"\\nUNSUPERVISED LEARNING:\")\n",
    "print(f\"   • Best clustering: {summary['clustering_results']['best_clustering_method']}\")\n",
    "print(f\"   • Best silhouette score: {summary['clustering_results']['best_silhouette_score']:.3f}\")\n",
    "\n",
    "print(f\"\\nSUPERVISED LEARNING:\")\n",
    "print(f\"   • Best model: {summary['supervised_learning']['best_model']}\")\n",
    "print(f\"   • Best feature set: {summary['supervised_learning']['best_feature_set']}\")\n",
    "print(f\"   • Best CV accuracy: {summary['supervised_learning']['best_cv_accuracy']:.3f}\")\n",
    "\n",
    "print(f\"\\nSEQUENCE LENGTH OPTIMIZATION:\")\n",
    "print(f\"   • Optimal sequence length: {summary['sequence_length_experiment']['optimal_length']}\")\n",
    "\n",
    "print(f\"\\nOUTPUT FILES:\")\n",
    "for file_path in summary['data_files_generated']:\n",
    "    print(f\"   • {file_path}\")\n",
    "\n",
    "print(f\"\\nANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*100)\n",
    "print(\"This comprehensive analysis provides all necessary data and visualizations\")\n",
    "print(\"for publication in top research journals, including:\")\n",
    "print(\"  • Rigorous model evaluation with multiple algorithms\")\n",
    "print(\"  • Extensive hyperparameter experimentation\")\n",
    "print(\"  • Detailed performance metrics and confusion matrices\")\n",
    "print(\"  • Sequence length optimization experiments\")\n",
    "print(\"  • Comprehensive data recording and saving\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Enhanced Summary Tables + LASSO (if available) ---\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import numpy as np\n",
    "\n",
    "# Load outputs\n",
    "output_dir = Path('../Processed_Data')\n",
    "perf_path = output_dir / 'model_performance_results.csv'\n",
    "sum_path = output_dir / 'comprehensive_analysis_summary.json'\n",
    "\n",
    "# Read performance table (robust to missing file)\n",
    "try:\n",
    "    performance_df = pd.read_csv(perf_path)\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"**Error reading performance CSV:** {e}\"))\n",
    "    performance_df = pd.DataFrame()\n",
    "\n",
    "# Read summary JSON (robust)\n",
    "try:\n",
    "    with open(sum_path, 'r') as f:\n",
    "        summary = json.load(f)\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"**Error reading summary JSON:** {e}\"))\n",
    "    summary = {}\n",
    "\n",
    "def section_md(title):\n",
    "    display(Markdown(f\"## {title}\"))\n",
    "\n",
    "# 1. Dataset Overview\n",
    "section_md('Dataset Overview')\n",
    "dataset = summary.get('dataset_info', {})\n",
    "if dataset:\n",
    "    dataset_df = pd.DataFrame(list(dataset.items()), columns=['Metric', 'Value'])\n",
    "    display(dataset_df)\n",
    "else:\n",
    "    display(Markdown('No dataset overview available in summary JSON.'))\n",
    "\n",
    "# 2. Sequence Encoding\n",
    "section_md('Sequence Encoding')\n",
    "seq = summary.get('sequence_encoding', {})\n",
    "if seq:\n",
    "    seq_df = pd.DataFrame(list(seq.items()), columns=['Encoding Metric', 'Value'])\n",
    "    display(seq_df)\n",
    "else:\n",
    "    display(Markdown('No sequence encoding information available in summary JSON.'))\n",
    "\n",
    "# 3. Clustering Results\n",
    "section_md('Unsupervised Learning / Clustering Results')\n",
    "clust = summary.get('clustering_results', {})\n",
    "if clust:\n",
    "    # Flatten nested dicts into table-friendly rows\n",
    "    rows = []\n",
    "    for k, v in clust.items():\n",
    "        if isinstance(v, dict):\n",
    "            for subk, subv in v.items():\n",
    "                rows.append({'Metric': f\"{k} | {subk}\", 'Value': subv})\n",
    "        else:\n",
    "            rows.append({'Metric': k, 'Value': v})\n",
    "    clust_df = pd.DataFrame(rows)\n",
    "    display(clust_df)\n",
    "else:\n",
    "    display(Markdown('No clustering results available.'))\n",
    "\n",
    "# 4. Supervised Learning — derive clean table from performance_df\n",
    "section_md('Supervised Learning — All Models (from performance table)')\n",
    "if not performance_df.empty:\n",
    "    # Normalize column names if needed\n",
    "    rename_map = {col:col.strip() for col in performance_df.columns}\n",
    "    performance_df = performance_df.rename(columns=rename_map)\n",
    "    # Select key columns and sort by CV_Mean (if present)\n",
    "    key_cols = [c for c in ['Feature_Set','Model','Accuracy','Precision','Recall','F1_Score','AUC','Specificity','NPV','CV_Mean','CV_Std'] if c in performance_df.columns]\n",
    "    supervised_df = performance_df[key_cols].copy()\n",
    "    if 'CV_Mean' in supervised_df.columns:\n",
    "        supervised_df = supervised_df.sort_values('CV_Mean', ascending=False)\n",
    "    display(supervised_df.reset_index(drop=True))\n",
    "\n",
    "    # If summary has N/A or missing best model info, compute from performance_df and update JSON\n",
    "    sup = summary.get('supervised_learning', {})\n",
    "    needs_update = False\n",
    "    if not sup.get('best_model') or sup.get('best_model') in ['N/A', None]:\n",
    "        if 'CV_Mean' in performance_df.columns and not performance_df['CV_Mean'].isnull().all():\n",
    "            best_row = performance_df.loc[performance_df['CV_Mean'].idxmax()]\n",
    "            sup['best_model'] = str(best_row.get('Model'))\n",
    "            sup['best_feature_set'] = str(best_row.get('Feature_Set'))\n",
    "            sup['best_cv_accuracy'] = float(best_row.get('CV_Mean'))\n",
    "            summary['supervised_learning'] = sup\n",
    "            needs_update = True\n",
    "    # Rebuild all_model_results from performance_df for robustness\n",
    "    try:\n",
    "        all_model_results = {}\n",
    "        grouped = performance_df.groupby(['Feature_Set','Model']) if set(['Feature_Set','Model']).issubset(performance_df.columns) else None\n",
    "        if grouped is not None:\n",
    "            for (fs, m), dfgrp in grouped:\n",
    "                # take first row for aggregated metrics\n",
    "                row = dfgrp.iloc[0]\n",
    "                metrics = {col: row[col] for col in dfgrp.columns if col not in ['Feature_Set','Model','TN','FP','FN','TP']}\n",
    "                all_model_results.setdefault(fs, {})[m] = metrics\n",
    "            sup['all_model_results'] = all_model_results\n",
    "            summary['supervised_learning'] = sup\n",
    "            needs_update = True\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Could not rebuild supervised 'all_model_results': {e}\"))\n",
    "\n",
    "    # Persist summary changes back to JSON if we filled in missing values\n",
    "    if needs_update:\n",
    "        try:\n",
    "            with open(sum_path, 'w') as f:\n",
    "                json.dump(summary, f, indent=2, default=str)\n",
    "            display(Markdown('Updated `comprehensive_analysis_summary.json` with derived supervised-learning summary fields.'))\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"Failed to write updated summary JSON: {e}\"))\n",
    "else:\n",
    "    display(Markdown('No performance table found — ensure the model evaluation cells have been run and the CSV exists.'))\n",
    "\n",
    "# 5. Show Best Model Summary (clean)\n",
    "section_md('Best Model Summary')\n",
    "sup = summary.get('supervised_learning', {})\n",
    "best_model_display = {\n",
    "    'Best Model': sup.get('best_model', 'N/A'),\n",
    "    'Best Feature Set': sup.get('best_feature_set', 'N/A'),\n",
    "    'Best CV Accuracy': sup.get('best_cv_accuracy', 'N/A')\n",
    "}\n",
    "display(pd.DataFrame(list(best_model_display.items()), columns=['Metric','Value']))\n",
    "\n",
    "# 6. Sequence Length Optimization\n",
    "section_md('Sequence Length Optimization')\n",
    "seq_len = summary.get('sequence_length_experiment', {})\n",
    "if seq_len:\n",
    "    opt_len = seq_len.get('optimal_length', 'N/A')\n",
    "    length_results = seq_len.get('length_results', [])\n",
    "    length_df = pd.DataFrame(length_results) if length_results else pd.DataFrame()\n",
    "    display(pd.DataFrame([{'Metric':'Optimal Length','Value':opt_len}]))\n",
    "    if not length_df.empty:\n",
    "        display(length_df)\n",
    "else:\n",
    "    display(Markdown('No sequence length experiment results available.'))\n",
    "\n",
    "# 7. Output Files\n",
    "section_md('Output Files Generated')\n",
    "files = summary.get('data_files_generated', [])\n",
    "if files:\n",
    "    files_df = pd.DataFrame(files, columns=['File Path'])\n",
    "    display(files_df)\n",
    "else:\n",
    "    display(Markdown('No output files listed in summary JSON.'))\n",
    "\n",
    "# 8. Add LASSO (Logistic Regression L1) quick run if in-memory features are available\n",
    "section_md('LASSO (Logistic Regression L1) Quick Check')\n",
    "lasso_added = False\n",
    "try:\n",
    "    # Only attempt if feature sets and target exist in memory\n",
    "    if 'feature_sets' in globals() and 'y_encoded' in globals():\n",
    "        # prefer a compact feature set to run quickly\n",
    "        chosen_key = 'basic' if 'basic' in feature_sets else list(feature_sets.keys())[0]\n",
    "        X = feature_sets[chosen_key]\n",
    "        y = y_encoded\n",
    "        from sklearn.linear_model import LogisticRegressionCV\n",
    "        from sklearn.model_selection import train_test_split, cross_val_score\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        # small, fast L1 logistic with saga solver\n",
    "        scaler = StandardScaler()\n",
    "        Xs = scaler.fit_transform(X)\n",
    "        clf = LogisticRegressionCV(Cs=5, cv=5, penalty='l1', solver='saga', scoring='accuracy', max_iter=2000, n_jobs=-1, random_state=42)\n",
    "        clf.fit(Xs, y)\n",
    "        # cross-validated accuracy (already computed internally)\n",
    "        cv_mean = float(np.mean(clf.scores_[1].mean(axis=0))) if 1 in clf.scores_ else float(np.mean([np.mean(v) for v in clf.scores_.values()]))\n",
    "        # quick train/test split for test accuracy and AUC if possible\n",
    "        X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3, random_state=42, stratify=y)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_proba = clf.predict_proba(X_test)[:,1] if hasattr(clf, 'predict_proba') else clf.decision_function(X_test)\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
    "        # Prepare row to append to supervised_df and performance_df\n",
    "        lasso_row = {\n",
    "            'Feature_Set': chosen_key,\n",
    "            'Model': 'Logistic Regression (L1 / LASSO)',\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1_Score': f1,\n",
    "            'AUC': auc,\n",
    "            'Specificity': spec,\n",
    "            'NPV': npv,\n",
    "            'CV_Mean': cv_mean,\n",
    "            'CV_Std': np.nan,\n",
    "            'TN': int(tn),\n",
    "            'FP': int(fp),\n",
    "            'FN': int(fn),\n",
    "            'TP': int(tp)\n",
    "        }\n",
    "        # Append to performance_df if not duplicate\n",
    "        if not performance_df.empty:\n",
    "            dup_mask = (performance_df['Feature_Set'] == lasso_row['Feature_Set']) & (performance_df['Model'] == lasso_row['Model'])\n",
    "            if not dup_mask.any():\n",
    "                performance_df = pd.concat([performance_df, pd.DataFrame([lasso_row])], ignore_index=True)\n",
    "                lasso_added = True\n",
    "        else:\n",
    "            performance_df = pd.DataFrame([lasso_row])\n",
    "            lasso_added = True\n",
    "        display(Markdown(f\"LASSO quick-check completed on feature set `{chosen_key}`. CV Acc (approx): {cv_mean:.3f}, Test Acc: {acc:.3f}\"))\n",
    "    else:\n",
    "        display(Markdown('In-memory features not found. To run LASSO here, run the feature-engineering cells so `feature_sets` and `y_encoded` exist in the kernel.'))\n",
    "except Exception as e:\n",
    "    display(Markdown(f'Failed to run LASSO quick-check: {e}'))\n",
    "    lasso_added = False\n",
    "\n",
    "# 9. Persist any new performance rows (e.g., LASSO) back to CSV (optional)\n",
    "if lasso_added:\n",
    "    try:\n",
    "        performance_df.to_csv(perf_path, index=False)\n",
    "        display(Markdown('Appended LASSO results to `model_performance_results.csv`.'))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f'Could not save updated performance CSV: {e}'))\n",
    "\n",
    "# 10. Final display: show top models (including any newly-added LASSO)\n",
    "section_md('Top Models (by CV_Mean)')\n",
    "if not performance_df.empty:\n",
    "    cols = [c for c in ['Feature_Set','Model','CV_Mean','Accuracy','AUC','F1_Score'] if c in performance_df.columns]\n",
    "    top = performance_df.sort_values('CV_Mean', ascending=False).head(10)[cols]\n",
    "    display(top.reset_index(drop=True))\n",
    "else:\n",
    "    display(Markdown('No performance table available to show top models.'))\n",
    "\n",
    "display(Markdown('---'))\n",
    "display(Markdown('**Enhanced summary completed.**'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebf2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-generated figure functions (do not run automatically)\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, roc_curve, auc, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.sparse as sp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ROOT = Path('..')\n",
    "FIGDIR = ROOT / 'Processed_Data' / 'figures'\n",
    "FIGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_processed_adata(path=None):\n",
    "    \"\"\"Load processed AnnData; adjust path if necessary.\"\"\"\n",
    "    if path is None:\n",
    "        path = ROOT / 'Processed_Data' / 'processed_s_rna_seq_data.h5ad'\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(f\"Processed AnnData not found at {path}\")\n",
    "    adata = sc.read_h5ad(path)\n",
    "    return adata\n",
    "\n",
    "def _ensure_qc_metrics(adata):\n",
    "    \"\"\"Compute n_genes and pct_mito if missing.\"\"\"\n",
    "    # n_genes (detected genes per cell)\n",
    "    if 'n_genes' not in adata.obs:\n",
    "        X = adata.X\n",
    "        if sp.issparse(X):\n",
    "            n_genes = (X > 0).sum(axis=1).A1\n",
    "        else:\n",
    "            n_genes = (X > 0).sum(axis=1)\n",
    "        adata.obs['n_genes'] = np.array(n_genes).ravel()\n",
    "    # pct_mito\n",
    "    if 'pct_mito' not in adata.obs:\n",
    "        mt_mask = adata.var_names.str.upper().str.startswith('MT-')\n",
    "        if mt_mask.sum() > 0:\n",
    "            mito_counts = adata.X[:, mt_mask].sum(axis=1)\n",
    "            total_counts = adata.X.sum(axis=1)\n",
    "            if sp.issparse(mito_counts):\n",
    "                mito_counts = mito_counts.A1\n",
    "                total_counts = total_counts.A1\n",
    "            adata.obs['pct_mito'] = np.array(mito_counts).ravel() / np.array(total_counts).ravel() * 100\n",
    "        else:\n",
    "            adata.obs['pct_mito'] = 0.0\n",
    "    return adata\n",
    "\n",
    "def figure_A_placeholder():\n",
    "    \"\"\"Figure A must be designed in BioRender/Illustrator. Export PNG to FIGDIR and display here.\"\"\"\n",
    "    msg = [\n",
    "        'Figure A: Conceptual multi-modal pipeline (BioRender/Illustrator).',\n",
    "        'Suggested export filename: Processed_Data/figures/Figure_A_Conceptual_Flowchart.png',\n",
    "        'Panels: (A) PBMC collection, (B) preproc, (C) TCR encodings, (D) feature integration, (E) ML pipeline.'\n",
    "    ]\n",
    "    print('\\n'.join(msg))\n",
    "    # convenience display (uncomment to show if you exported the figure)\n",
    "    # from IPython.display import Image, display\n",
    "    # p = FIGDIR / 'Figure_A_Conceptual_Flowchart.png'\n",
    "    # if p.exists():\n",
    "    #     display(Image(p))\n",
    "\n",
    "def figure_B_qc(adata=None, save=True):\n",
    "    \"\"\"Cohort QC: bar chart of cells per patient/response and violin plots for QC metrics.\"\"\"\n",
    "    if adata is None:\n",
    "        adata = load_processed_adata()\n",
    "    adata = _ensure_qc_metrics(adata)\n",
    "    # ensure columns\n",
    "    for col in ['response','patient_id']:`\n",
    "        if col not in adata.obs:\n",
    "            raise KeyError(f\"Expected column '{col}' in adata.obs for Figure B\")\n",
    "    # Summary counts\n",
    "    counts = adata.obs.groupby(['patient_id','response']).size().reset_index(name='cell_count')\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.barplot(data=counts, x='patient_id', y='cell_count', hue='response')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Cells per patient by Response')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_B_cell_counts_by_patient_response.png', dpi=300)\n",
    "    plt.show()\n",
    "    # QC violins\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.violinplot(data=adata.obs, x='response', y='n_genes', inner='box', palette='Set2')\n",
    "    plt.title('Detected genes per cell by Response')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_B_n_genes_violin.png', dpi=300)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.violinplot(data=adata.obs, x='response', y='pct_mito', inner='box', palette='Set2')\n",
    "    plt.title('% mitochondrial reads by Response')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_B_pct_mito_violin.png', dpi=300)\n",
    "    plt.show()\n",
    "    # Print class balance summary\n",
    "    print('Class balance (cells):')\n",
    "    print(adata.obs['response'].value_counts())\n",
    "\n",
    "def figure_C_tsne_kmeans(adata=None, k=8, save=True, pca_n_comps=50, tsne_perplexity=30, tsne_iter=1000):\n",
    "    \"\"\"t-SNE of top HVG/PCA with K-Means clustering (k default 8).\"\"\"\n",
    "    if adata is None:\n",
    "        adata = load_processed_adata()\n",
    "    # Ensure PCA\n",
    "    if 'X_pca' not in adata.obsm:\n",
    "        sc.pp.pca(adata, n_comps=pca_n_comps)\n",
    "    pca_rep = adata.obsm['X_pca']\n",
    "    # KMeans clustering\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_init=50).fit(pca_rep)\n",
    "    adata.obs['kmeans_clusters'] = pd.Categorical(km.labels_.astype(str))\n",
    "    try:\n",
    "        s = silhouette_score(pca_rep, km.labels_)\n",
    "        print(f'KMeans (k={k}) silhouette score: {s:.3f}')\n",
    "    except Exception as e:\n",
    "        print('Silhouette score could not be computed:', e)\n",
    "    # t-SNE (prefer openTSNE if available)\n",
    "    if 'X_tsne' not in adata.obsm:\n",
    "        try:\n",
    "            import openTSNE as otsne\n",
    "            tsne = otsne.TSNE(n_components=2, random_state=0, n_jobs=-1)\n",
    "            emb = np.array(tsne.fit(pca_rep))\n",
    "        except Exception:\n",
    "            emb = TSNE(n_components=2, perplexity=tsne_perplexity, n_iter=tsne_iter, random_state=0).fit_transform(pca_rep)\n",
    "        adata.obsm['X_tsne'] = emb\n",
    "    # plots\n",
    "    fig, axes = plt.subplots(1,2, figsize=(14,6))\n",
    "    sns.scatterplot(x=adata.obsm['X_tsne'][:,0], y=adata.obsm['X_tsne'][:,1], hue=adata.obs['kmeans_clusters'], palette='tab10', s=6, ax=axes[0], legend='full')\n",
    "    axes[0].set_title(f't-SNE colored by KMeans (k={k})')\n",
    "    if 'response' in adata.obs:\n",
    "        sns.scatterplot(x=adata.obsm['X_tsne'][:,0], y=adata.obsm['X_tsne'][:,1], hue=adata.obs['response'], palette='Set1', s=6, ax=axes[1], legend='full')\n",
    "        axes[1].set_title('t-SNE colored by Response')\n",
    "    else:\n",
    "        axes[1].text(0.5,0.5,'No response label in adata.obs', ha='center')\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('t-SNE1'); ax.set_ylabel('t-SNE2')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / f'Figure_C_tsne_kmeans_k{k}.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def figure_D_clustering_evaluation(adata=None, k_range=range(2,13), save=True):\n",
    "    \"\"\"Compare clustering algorithms (silhouette) and produce an elbow plot + marker heatmap.\"\"\"\n",
    "    if adata is None:\n",
    "        adata = load_processed_adata()\n",
    "    if 'X_pca' not in adata.obsm:\n",
    "        sc.pp.pca(adata, n_comps=50)\n",
    "    pca_rep = adata.obsm['X_pca']\n",
    "    # Silhouette comparison\n",
    "    algos = { 'KMeans': KMeans(n_clusters=8, random_state=0, n_init=50), 'Agglomerative': AgglomerativeClustering(n_clusters=8), 'DBSCAN': DBSCAN(eps=1.0, min_samples=5) }\n",
    "    scores = {}\n",
    "    for name, model in algos.items():\n",
    "        try:\n",
    "            labels = model.fit_predict(pca_rep)\n",
    "            if len(set(labels))>1:\n",
    "                scores[name] = silhouette_score(pca_rep, labels)\n",
    "            else:\n",
    "                scores[name] = np.nan\n",
    "        except Exception as e:\n",
    "            scores[name] = np.nan\n",
    "    scores_df = pd.DataFrame({'algorithm': list(scores.keys()), 'silhouette': list(scores.values())})\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(data=scores_df, x='algorithm', y='silhouette', palette='pastel')\n",
    "    plt.title('Clustering silhouette comparison')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_D_clustering_silhouette_comparison.png', dpi=300)\n",
    "    plt.show()\n",
    "    # Elbow plot for KMeans\n",
    "    inertias = []\n",
    "    ks = list(k_range)\n",
    "    for kk in ks:\n",
    "        km = KMeans(n_clusters=kk, random_state=0, n_init=50).fit(pca_rep)\n",
    "        inertias.append(km.inertia_)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(ks, inertias, '-o')\n",
    "    plt.xlabel('k'); plt.ylabel('Inertia'); plt.title('Elbow plot for KMeans')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_D_kmeans_elbow.png', dpi=300)\n",
    "    plt.show()\n",
    "    # DGE heatmap: requires clusters present (use kmeans_clusters if available)\n",
    "    if 'kmeans_clusters' not in adata.obs:\n",
    "        adata.obs['kmeans_clusters'] = pd.Categorical(KMeans(n_clusters=8, random_state=0, n_init=50).fit(pca_rep).labels_.astype(str))\n",
    "    try:\n",
    "        sc.tl.rank_genes_groups(adata, groupby='kmeans_clusters', method='wilcoxon', n_genes=20)\n",
    "        sc.pl.rank_genes_groups_heatmap(adata, n_genes=8, groupby='kmeans_clusters', cmap='viridis', show=False, figsize=(10,6))\n",
    "        plt.gcf().savefig(FIGDIR / 'Figure_D_marker_genes_heatmap.png', dpi=300)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('Marker heatmap could not be produced:', e)\n",
    "\n",
    "def figure_E_supervised(cv_results=None, save=True):\n",
    "    \"\"\"Bar chart of mean CV accuracy across models and ROC for best model (expects structured cv_results).\n",
    "    Expected structure: cv_results[feature_set][model_name] -> dict with 'accuracy' list and optionally 'y_true' and 'y_score' lists/arrays across folds.\n",
    "    \"\"\"\n",
    "    if cv_results is None:\n",
    "        # try to load from processed_encoded_ml_results.h5ad\n",
    "        p = ROOT / 'Processed_Data' / 'processed_encoded_ml_results.h5ad'\n",
    "        if p.exists():\n",
    "            r = sc.read_h5ad(p)\n",
    "            cv_results = r.uns.get('cv_results', None)\n",
    "    if cv_results is None:\n",
    "        print('No cv_results found. Please supply `cv_results` dict.')\n",
    "        return\n",
    "    rows = []\n",
    "    for feature_set, models in cv_results.items():\n",
    "        for model_name, metrics in models.items():\n",
    "            accs = np.array(metrics.get('accuracy', []))\n",
    "            rows.append({'feature_set': feature_set, 'model': model_name, 'mean_accuracy': float(np.nanmean(accs)) if accs.size else np.nan})\n",
    "    df = pd.DataFrame(rows)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.barplot(data=df, x='model', y='mean_accuracy', hue='feature_set')\n",
    "    plt.ylim(0,1)\n",
    "    plt.ylabel('Mean CV accuracy')\n",
    "    plt.title('Model performance across feature sets')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_E_model_performance_bar.png', dpi=300)\n",
    "    plt.show()\n",
    "    # ROC for best-performing model if predictions available\n",
    "    if df['mean_accuracy'].notna().any():\n",
    "        best = df.loc[df['mean_accuracy'].idxmax()]\n",
    "        best_fs, best_model = best['feature_set'], best['model']\n",
    "        metrics = cv_results[best_fs][best_model]\n",
    "        y_true = np.concatenate(metrics.get('y_true', [])) if isinstance(metrics.get('y_true', []), list) and len(metrics.get('y_true', []))>0 else np.array(metrics.get('y_true', []))\n",
    "        y_score = np.concatenate(metrics.get('y_score', [])) if isinstance(metrics.get('y_score', []), list) and len(metrics.get('y_score', []))>0 else np.array(metrics.get('y_score', []))\n",
    "        if y_true.size and y_score.size:\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.figure(figsize=(6,6))\n",
    "            plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "            plt.plot([0,1],[0,1],'--', color='grey')\n",
    "            plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC - Best model')\n",
    "            plt.legend(); plt.tight_layout()\n",
    "            if save: plt.savefig(FIGDIR / 'Figure_E_ROC_best_model.png', dpi=300)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('No y_true/y_score arrays available for ROC plotting for best model.')\n",
    "    else:\n",
    "        print('No valid mean_accuracy values found in cv_results to determine best model.')\n",
    "\n",
    "def figure_F_feature_set_table_and_chart(cv_results=None, save=True):\n",
    "    \"\"\"Table and stacked/pie chart for XGBoost performance across nested feature sets.\"\"\"\n",
    "    if cv_results is None:\n",
    "        p = ROOT / 'Processed_Data' / 'processed_encoded_ml_results.h5ad'\n",
    "        if p.exists():\n",
    "            r = sc.read_h5ad(p)\n",
    "            cv_results = r.uns.get('cv_results', None)\n",
    "    if cv_results is None:\n",
    "        print('No cv_results found. Provide cv_results with XGBoost metrics.')\n",
    "        return\n",
    "    rows = []\n",
    "    for feature_set, models in cv_results.items():\n",
    "        xgb_metrics = models.get('XGBoost') or models.get('xgboost') or models.get('XGB')\n",
    "        if xgb_metrics is None:\n",
    "            continue\n",
    "        accs = np.array(xgb_metrics.get('accuracy', []))\n",
    "        aucs = np.array(xgb_metrics.get('roc_auc', []))\n",
    "        rows.append({'feature_set': feature_set, 'accuracy_mean': float(np.nanmean(accs)) if accs.size else np.nan, 'auc_mean': float(np.nanmean(aucs)) if aucs.size else np.nan})\n",
    "    table = pd.DataFrame(rows).sort_values('accuracy_mean', ascending=False)\n",
    "    display(table)\n",
    "    if save: table.to_csv(FIGDIR / 'Table1_XGBoost_performance_by_feature_set.csv', index=False)\n",
    "    # stacked/pie chart (relative contributions)\n",
    "    if not table.empty:\n",
    "        plt.figure(figsize=(6,6))\n",
    "        vals = table['accuracy_mean'].fillna(0).values\n",
    "        labels = table['feature_set'].values\n",
    "        plt.pie(vals, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Relative accuracy contribution by feature set (XGBoost)')\n",
    "        if save: plt.savefig(FIGDIR / 'Figure_F_feature_set_contribution_pie.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "def figure_G_feature_importance(xgb_model=None, feature_names=None, top_n=30, save=True):\n",
    "    \"\"\"Plot top feature importances from a trained XGBoost model (gain or sklearn-style importances).\"\"\"\n",
    "    if xgb_model is None:\n",
    "        p = ROOT / 'Processed_Data' / 'processed_encoded_ml_results.h5ad'\n",
    "        if p.exists():\n",
    "            r = sc.read_h5ad(p)\n",
    "            xgb_model = r.uns.get('best_xgb', None)\n",
    "            feature_names = r.uns.get('feature_names', feature_names)\n",
    "    if xgb_model is None:\n",
    "        print('No XGBoost model found in notebook state or processed file. Provide `xgb_model`.')\n",
    "        return\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        if isinstance(xgb_model, xgb.XGBClassifier):\n",
    "            fmap = xgb_model.get_booster().get_score(importance_type='gain')\n",
    "            fi = pd.Series(fmap).sort_values(ascending=False).head(top_n)\n",
    "            fi.index = fi.index.astype(str)\n",
    "            plt.figure(figsize=(8, max(4, len(fi)*0.25)))\n",
    "            sns.barplot(x=fi.values, y=fi.index, color='steelblue')\n",
    "            plt.xlabel('gain'); plt.title('Top XGBoost feature importances (gain)')\n",
    "            plt.tight_layout()\n",
    "            if save: plt.savefig(FIGDIR / 'Figure_G_xgb_importance_gain.png', dpi=300)\n",
    "            plt.show()\n",
    "            return\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback: sklearn-style feature_importances_\n",
    "    try:\n",
    "        import joblib\n",
    "        if hasattr(xgb_model, 'feature_importances_') and feature_names is not None:\n",
    "            imp = pd.Series(xgb_model.feature_importances_, index=feature_names).sort_values(ascending=False).head(top_n)\n",
    "            plt.figure(figsize=(8, max(4, len(imp)*0.25)))\n",
    "            sns.barplot(x=imp.values, y=imp.index, color='steelblue')\n",
    "            plt.xlabel('importance'); plt.title('Top feature importances')\n",
    "            plt.tight_layout()\n",
    "            if save: plt.savefig(FIGDIR / 'Figure_G_feature_importances.png', dpi=300)\n",
    "            plt.show()\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print('Could not extract feature importances:', e)\n",
    "\n",
    "def figure_H_pca_loadings_and_enrichment(adata=None, pc_index=0, top_n=20, save=True):\n",
    "    \"\"\"Heatmap of top genes loading onto PC and enrichment via gseapy (if installed).\"\"\"\n",
    "    if adata is None:\n",
    "        adata = load_processed_adata()\n",
    "    if 'X_pca' not in adata.obsm:\n",
    "        sc.pp.pca(adata, n_comps=50)\n",
    "    loadings = adata.varm.get('PCs')\n",
    "    if loadings is None:\n",
    "        print(\"PCA loadings not found in adata.varm['PCs'] -- ensure PCA was run with `sc.pp.pca`\")\n",
    "        return\n",
    "    pc_load = pd.Series(loadings[:, pc_index], index=adata.var_names)\n",
    "    top_genes = pc_load.abs().sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    try:\n",
    "        sc.pl.matrixplot(adata, var_names=top_genes, groupby='kmeans_clusters', show=False, cmap='viridis')\n",
    "        plt.gcf().savefig(FIGDIR / f'Figure_H_PC{pc_index+1}_top{top_n}_matrixplot.png', dpi=300)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('Could not produce matrixplot heatmap:', e)\n",
    "    # enrichment (optional)\n",
    "    try:\n",
    "        import gseapy as gp\n",
    "        enr = gp.enrichr(gene_list=top_genes, gene_sets=['KEGG_2019_Human','GO_Biological_Process_2018'], organism='Human')\n",
    "        if hasattr(enr, 'results') and not enr.results.empty:\n",
    "            enr.results.to_csv(FIGDIR / f'Figure_H_PC{pc_index+1}_enrichment.csv', index=False)\n",
    "            print('Saved enrichment results (csv).')\n",
    "    except Exception as e:\n",
    "        print('gseapy not available or enrichment failed:', e)\n",
    "\n",
    "def figure_I_sequence_length_tuning(tuning_results=None, save=True):\n",
    "    \"\"\"Plot CV mean accuracy vs tested max TCR sequence lengths (expects tuning_results dict length->acc list).\"\"\"\n",
    "    if tuning_results is None:\n",
    "        p = ROOT / 'Processed_Data' / 'processed_encoded_ml_results.h5ad'\n",
    "        if p.exists():\n",
    "            r = sc.read_h5ad(p)\n",
    "            tuning_results = r.uns.get('sequence_length_tuning', None)\n",
    "    if tuning_results is None:\n",
    "        print('No tuning_results found. Provide dict: {length: [accuracies] }')\n",
    "        return\n",
    "    lengths = sorted(tuning_results.keys())\n",
    "    means = [np.mean(tuning_results[l]) for l in lengths]\n",
    "    stds = [np.std(tuning_results[l]) for l in lengths]\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.errorbar(lengths, means, yerr=stds, marker='o')\n",
    "    plt.xlabel('Max TCR sequence length (aa)'); plt.ylabel('Mean CV accuracy')\n",
    "    plt.title('Sequence length tuning (TCR encodings)')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_I_sequence_length_tuning.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def figure_J_confusion_cell_vs_patient(adata=None, y_col='response', yhat_col='y_pred', patient_col='patient_id', save=True):\n",
    "    \"\"\"Show cell-level confusion matrix and aggregated patient-level confusion (majority vote). Also provide GroupKFold skeleton for re-running grouped CV.\"\"\"\n",
    "    if adata is None:\n",
    "        adata = load_processed_adata()\n",
    "    if y_col not in adata.obs or yhat_col not in adata.obs or patient_col not in adata.obs:\n",
    "        print(f'Required columns missing: need {y_col}, {yhat_col}, {patient_col} in adata.obs')\n",
    "        return\n",
    "    # cell-level\n",
    "    y_true = adata.obs[y_col].values\n",
    "    y_pred = adata.obs[yhat_col].values\n",
    "    cm_cell = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm_cell, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Cell-level confusion matrix')\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_J_cell_level_confusion_matrix.png', dpi=300)\n",
    "    plt.show()\n",
    "    # patient-level majority vote\n",
    "    df = adata.obs[[patient_col, y_col, yhat_col]].copy()\n",
    "    agg = df.groupby(patient_col).agg({y_col: lambda x: x.mode().iloc[0], yhat_col: lambda x: x.mode().iloc[0]})\n",
    "    cm_patient = confusion_matrix(agg[y_col], agg[yhat_col], labels=np.unique(agg[y_col]))\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm_patient, annot=True, fmt='d', cmap='Oranges', xticklabels=np.unique(agg[y_col]), yticklabels=np.unique(agg[y_col]))\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Patient-level (majority vote) confusion matrix')\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_J_patient_level_confusion_matrix.png', dpi=300)\n",
    "    plt.show()\n",
    "    # GroupKFold skeleton\n",
    "    print('\\nTo properly estimate patient-level generalization, re-run training with GroupKFold/LeaveOneGroupOut using `patient_id` as groups. Example skeleton:')\n",
    "    print('''from sklearn.model_selection import GroupKFold\\ngkf = GroupKFold(n_splits=5)\\nfor train_idx, test_idx in gkf.split(X, y, groups=groups):\\n    X_train, X_test = X[train_idx], X[test_idx]\\n    y_train, y_test = y[train_idx], y[test_idx]\\n    # fit model on X_train/y_train and evaluate on X_test/y_test''')\n",
    "\n",
    "# Usage instructions (run manually):\n",
    "print('Functions added: figure_A_placeholder, figure_B_qc, figure_C_tsne_kmeans, figure_D_clustering_evaluation, figure_E_supervised, figure_F_feature_set_table_and_chart, figure_G_feature_importance, figure_H_pca_loadings_and_enrichment, figure_I_sequence_length_tuning, figure_J_confusion_cell_vs_patient')\n",
    "print('Example:')\n",
    "print(\"adata = load_processed_adata(); figure_B_qc(adata); figure_C_tsne_kmeans(adata)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
