{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce85886b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved patient-level features to ..\\Processed_Data\\patient_level_features.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>timepoint</th>\n",
       "      <th>n_cells</th>\n",
       "      <th>response</th>\n",
       "      <th>n_unique_clones</th>\n",
       "      <th>shannon_diversity</th>\n",
       "      <th>simpson_index</th>\n",
       "      <th>gini_clonality</th>\n",
       "      <th>top_clonotype_1_seq</th>\n",
       "      <th>top_clonotype_1_count</th>\n",
       "      <th>...</th>\n",
       "      <th>frac_leiden_integrated_14</th>\n",
       "      <th>frac_leiden_integrated_16</th>\n",
       "      <th>frac_leiden_integrated_19</th>\n",
       "      <th>frac_leiden_integrated_18</th>\n",
       "      <th>frac_leiden_integrated_21</th>\n",
       "      <th>frac_leiden_integrated_24</th>\n",
       "      <th>frac_leiden_integrated_23</th>\n",
       "      <th>frac_leiden_integrated_22</th>\n",
       "      <th>frac_leiden_integrated_20</th>\n",
       "      <th>leiden_integrated_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>4008</td>\n",
       "      <td>Responder</td>\n",
       "      <td>3524</td>\n",
       "      <td>8.066620</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.083556</td>\n",
       "      <td>CATSREGISGANVLTF</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013473</td>\n",
       "      <td>0.011727</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>0.005739</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.600359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PT1</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>3855</td>\n",
       "      <td>Responder</td>\n",
       "      <td>3133</td>\n",
       "      <td>7.869510</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.144393</td>\n",
       "      <td>CASSEEGRATDTQYF</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014527</td>\n",
       "      <td>0.019196</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.014267</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.012192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.644746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PT2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>3127</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>2748</td>\n",
       "      <td>7.853918</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.071568</td>\n",
       "      <td>CASSGGNQPQHF</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015350</td>\n",
       "      <td>0.019508</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.512519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PT2</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>2471</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>1975</td>\n",
       "      <td>7.354026</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.153540</td>\n",
       "      <td>CASSLGHYGYTF</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011736</td>\n",
       "      <td>0.023472</td>\n",
       "      <td>0.010522</td>\n",
       "      <td>0.006475</td>\n",
       "      <td>0.002428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.648230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PT3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>915</td>\n",
       "      <td>Responder</td>\n",
       "      <td>844</td>\n",
       "      <td>6.707935</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.043208</td>\n",
       "      <td>CSARDRTGNGYTF</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>0.015301</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.429956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  patient_id   timepoint  n_cells       response  n_unique_clones  \\\n",
       "0        PT1    Baseline     4008      Responder             3524   \n",
       "1        PT1  Post-Chemo     3855      Responder             3133   \n",
       "2        PT2    Baseline     3127  Non-Responder             2748   \n",
       "3        PT2  Post-Chemo     2471  Non-Responder             1975   \n",
       "4        PT3    Baseline      915      Responder              844   \n",
       "\n",
       "   shannon_diversity  simpson_index  gini_clonality top_clonotype_1_seq  \\\n",
       "0           8.066620       0.000550        0.083556    CATSREGISGANVLTF   \n",
       "1           7.869510       0.000833        0.144393     CASSEEGRATDTQYF   \n",
       "2           7.853918       0.000528        0.071568        CASSGGNQPQHF   \n",
       "3           7.354026       0.001772        0.153540        CASSLGHYGYTF   \n",
       "4           6.707935       0.001325        0.043208       CSARDRTGNGYTF   \n",
       "\n",
       "   top_clonotype_1_count  ...  frac_leiden_integrated_14  \\\n",
       "0                     50  ...                   0.013473   \n",
       "1                     51  ...                   0.014527   \n",
       "2                     32  ...                   0.015350   \n",
       "3                     71  ...                   0.011736   \n",
       "4                      9  ...                   0.009836   \n",
       "\n",
       "  frac_leiden_integrated_16  frac_leiden_integrated_19  \\\n",
       "0                  0.011727                   0.007236   \n",
       "1                  0.019196                   0.008301   \n",
       "2                  0.019508                   0.004157   \n",
       "3                  0.023472                   0.010522   \n",
       "4                  0.015301                   0.006557   \n",
       "\n",
       "   frac_leiden_integrated_18 frac_leiden_integrated_21  \\\n",
       "0                   0.005739                  0.003244   \n",
       "1                   0.014267                  0.001297   \n",
       "2                   0.008954                  0.002558   \n",
       "3                   0.006475                  0.002428   \n",
       "4                   0.005464                  0.001093   \n",
       "\n",
       "   frac_leiden_integrated_24  frac_leiden_integrated_23  \\\n",
       "0                   0.000749                   0.000749   \n",
       "1                   0.000778                   0.000259   \n",
       "2                   0.000640                   0.001919   \n",
       "3                   0.000000                   0.003238   \n",
       "4                   0.000000                   0.001093   \n",
       "\n",
       "  frac_leiden_integrated_22  frac_leiden_integrated_20  \\\n",
       "0                  0.000499                        0.0   \n",
       "1                  0.012192                        0.0   \n",
       "2                  0.000640                        0.0   \n",
       "3                  0.000000                        0.0   \n",
       "4                  0.000000                        0.0   \n",
       "\n",
       "   leiden_integrated_entropy  \n",
       "0                   2.600359  \n",
       "1                   2.644746  \n",
       "2                   2.512519  \n",
       "3                   2.648230  \n",
       "4                   2.429956  \n",
       "\n",
       "[5 rows x 186 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Patient-level aggregation: compute per-patient / per-timepoint features and save CSV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from pathlib import Path\n",
    "import scanpy as sc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "output_dir = Path('../Processed_Data')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure `adata` is available (load processed file if not in memory)\n",
    "if 'adata' not in globals():\n",
    "    processed_path = Path('../Processed_Data/processed_s_rna_seq_data.h5ad')\n",
    "    if processed_path.exists():\n",
    "        print(f'Loading processed AnnData from {processed_path}')\n",
    "        adata = sc.read_h5ad(processed_path)\n",
    "    else:\n",
    "        raise FileNotFoundError('`adata` not in memory and processed file not found.')\n",
    "\n",
    "# Identify patient and timepoint columns\n",
    "patient_col = 'patient_id' if 'patient_id' in adata.obs.columns else ('patient' if 'patient' in adata.obs.columns else None)\n",
    "timepoint_col = 'timepoint' if 'timepoint' in adata.obs.columns else ('sample_id' if 'sample_id' in adata.obs.columns else None)\n",
    "if patient_col is None:\n",
    "    raise RuntimeError('No patient identifier column found in adata.obs (expected `patient_id` or `patient`).')\n",
    "\n",
    "# Prefer TRB clonotypes, fallback to TRA or generic clone id\n",
    "clone_col = None\n",
    "for c in ['cdr3_TRB','cdr3_TRA','clone_id','clonotype']:\n",
    "    if c in adata.obs.columns:\n",
    "        clone_col = c\n",
    "        break\n",
    "\n",
    "# Helper diversity functions\n",
    "def simpson_index(counts):\n",
    "    counts = np.asarray(counts)\n",
    "    if counts.sum() == 0:\n",
    "        return np.nan\n",
    "    p = counts / counts.sum()\n",
    "    return float((p ** 2).sum())\n",
    "\n",
    "def gini_coefficient(counts):\n",
    "    x = np.asarray(counts, dtype=float)\n",
    "    if x.size == 0 or x.sum() == 0:\n",
    "        return np.nan\n",
    "    x = np.sort(x)\n",
    "    n = x.size\n",
    "    index = np.arange(1, n + 1)\n",
    "    return float((2.0 * np.sum(index * x)) / (n * x.sum()) - (n + 1) / n)\n",
    "\n",
    "# Heuristic cluster columns to produce cluster fractions\n",
    "cluster_columns = [c for c in adata.obs.columns if ('cluster' in c.lower() or c.endswith('_clusters') or c == 'gene_expression_modules')]\n",
    "if not cluster_columns:\n",
    "    cluster_columns = [c for c in adata.obs.columns if pd.api.types.is_categorical_dtype(adata.obs[c]) and 2 <= adata.obs[c].nunique() <= 50]\n",
    "cluster_columns = list(dict.fromkeys(cluster_columns))\n",
    "\n",
    "group_cols = [patient_col] + ([timepoint_col] if timepoint_col is not None else [])\n",
    "grp = adata.obs.groupby(group_cols)\n",
    "\n",
    "records = []\n",
    "for name, df in grp:\n",
    "    rec = {}\n",
    "    if isinstance(name, tuple):\n",
    "        rec['patient_id'] = name[0]\n",
    "        if timepoint_col is not None:\n",
    "            rec['timepoint'] = name[1]\n",
    "    else:\n",
    "        rec['patient_id'] = name\n",
    "        if timepoint_col is not None:\n",
    "            rec['timepoint'] = ''\n",
    "    rec['n_cells'] = int(df.shape[0])\n",
    "    # sample/patient-level response if present\n",
    "    if 'response' in df.columns:\n",
    "        try:\n",
    "            rec['response'] = str(df['response'].mode().iloc[0])\n",
    "        except Exception:\n",
    "            rec['response'] = ''\n",
    "    else:\n",
    "        rec['response'] = ''\n",
    "\n",
    "    # clonotype metrics\n",
    "    if clone_col is not None:\n",
    "        clones = df[clone_col].dropna().astype(str)\n",
    "        if len(clones) > 0:\n",
    "            vc = clones.value_counts()\n",
    "            counts = vc.values\n",
    "            rec['n_unique_clones'] = int(vc.size)\n",
    "            # Shannon (entropy), Simpson, Gini (clonality metrics)\n",
    "            rec['shannon_diversity'] = float(entropy(counts / counts.sum()))\n",
    "            rec['simpson_index'] = float(simpson_index(counts))\n",
    "            rec['gini_clonality'] = float(gini_coefficient(counts))\n",
    "            # top clonotypes\n",
    "            top = vc.head(5)\n",
    "            total = counts.sum() if counts.sum() > 0 else 1\n",
    "            for i, (cl, v) in enumerate(top.items(), start=1):\n",
    "                rec[f'top_clonotype_{i}_seq'] = cl\n",
    "                rec[f'top_clonotype_{i}_count'] = int(v)\n",
    "                rec[f'top_clonotype_{i}_frac'] = float(v / total)\n",
    "            rec['frac_top1'] = float(top.iloc[0] / total) if len(top) > 0 else 0.0\n",
    "            rec['frac_top5'] = float(top.sum() / total) if len(top) > 0 else 0.0\n",
    "        else:\n",
    "            rec['n_unique_clones'] = 0\n",
    "            rec['shannon_diversity'] = np.nan\n",
    "            rec['simpson_index'] = np.nan\n",
    "            rec['gini_clonality'] = np.nan\n",
    "            rec['frac_top1'] = np.nan\n",
    "            rec['frac_top5'] = np.nan\n",
    "    else:\n",
    "        rec['n_unique_clones'] = np.nan\n",
    "        rec['shannon_diversity'] = np.nan\n",
    "        rec['simpson_index'] = np.nan\n",
    "        rec['gini_clonality'] = np.nan\n",
    "        rec['frac_top1'] = np.nan\n",
    "        rec['frac_top5'] = np.nan\n",
    "\n",
    "    # mean physicochemical properties (if encoded)\n",
    "    phys_cols = [c for c in adata.obs.columns if c.startswith('tra_') or c.startswith('trb_')]\n",
    "    for pc in phys_cols:\n",
    "        rec[f'mean_{pc}'] = float(df[pc].dropna().mean()) if pc in df.columns else np.nan\n",
    "\n",
    "    # cluster fractions + cluster entropy\n",
    "    for ccol in cluster_columns:\n",
    "        vc = df[ccol].value_counts(normalize=True)\n",
    "        for label, frac in vc.items():\n",
    "            rec[f'frac_{ccol}_{label}'] = float(frac)\n",
    "        rec[f'{ccol}_entropy'] = float(entropy(vc.values)) if len(vc) > 0 else np.nan\n",
    "\n",
    "    records.append(rec)\n",
    "\n",
    "patient_df = pd.DataFrame(records).fillna(np.nan)\n",
    "out_path = output_dir / 'patient_level_features.csv'\n",
    "patient_df.to_csv(out_path, index=False)\n",
    "print(f'Saved patient-level features to {out_path}')\n",
    "patient_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f06b0223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GroupKFold results to Processed_Data/patient_level_groupcv_results.csv\n",
      "Saved trained model to Processed_Data/patient_level_model_groupcv.joblib\n",
      "Not enough positive/negative samples across folds to compute ROC curve\n"
     ]
    }
   ],
   "source": [
    "# 2) Grouped CV supervised pipeline (patient-level folds using GroupKFold)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "pf = Path('../Processed_Data/patient_level_features.csv')\n",
    "if not pf.exists():\n",
    "    raise FileNotFoundError('Run the patient-level aggregation cell first (patient_level_features.csv not found).')\n",
    "patient_df = pd.read_csv(pf)\n",
    "# require a response column with values like 'Responder' / 'Non-Responder'\n",
    "if 'response' not in patient_df.columns or patient_df['response'].isnull().all():\n",
    "    raise RuntimeError('`response` column not present in patient-level table. Ensure patient-level aggregation included response.')\n",
    "# normalize response strings\n",
    "patient_df['response_norm'] = patient_df['response'].astype(str).str.lower().map(lambda s: 'Responder' if 'respon' in s else ('Non-Responder' if 'non' in s else s))\n",
    "if patient_df['response_norm'].isnull().all():\n",
    "    raise RuntimeError('No recognizable response labels after normalization.')\n",
    "# binary label (1: Responder, 0: Non-Responder/Other)\n",
    "patient_df['label'] = (patient_df['response_norm'] == 'Responder').astype(int)\n",
    "# groups are patient_id (multiple timepoints per patient allowed)\n",
    "groups = patient_df['patient_id']\n",
    "# features: numeric columns only (drop identifiers and response)\n",
    "drop_cols = ['patient_id','timepoint','response','response_norm','label']\n",
    "num_cols = patient_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = [c for c in num_cols if c not in drop_cols]\n",
    "if len(feature_cols) == 0:\n",
    "    raise RuntimeError('No numeric features found in patient-level table. Check that patient aggregation produced numeric features.')\n",
    "X = patient_df[feature_cols].fillna(0)\n",
    "y = patient_df['label']\n",
    "\n",
    "n_groups = len(groups.unique())\n",
    "if n_groups < 2:\n",
    "    raise RuntimeError(f'Need at least 2 patients for GroupKFold; found {n_groups}')\n",
    "\n",
    "n_splits = min(5, n_groups)\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "metrics = []\n",
    "all_y_true = []\n",
    "all_y_score = []\n",
    "fold = 0\n",
    "for train_idx, test_idx in gkf.split(X, y, groups):\n",
    "    fold += 1\n",
    "    X_train_df, X_test_df = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    groups_train = groups.iloc[train_idx]\n",
    "\n",
    "    # pipeline with feature selection\n",
    "    k_max = min(50, X.shape[1])\n",
    "    sel_k_choices = sorted(list(set([max(1, min(10, X.shape[1])), max(1, min(20, X.shape[1])), k_max, X.shape[1]])))\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('sel', SelectKBest(f_classif, k=k_max)), ('clf', RandomForestClassifier(class_weight='balanced', random_state=42))])\n",
    "\n",
    "    # nested/randomized search (inner GroupKFold) when sufficient groups available\n",
    "    n_inner = min(3, len(np.unique(groups_train)))\n",
    "    if n_inner >= 2:\n",
    "        param_dist = {\n",
    "            'sel__k': sel_k_choices,\n",
    "            'clf__n_estimators': [100, 200, 500],\n",
    "            'clf__max_depth': [None, 5, 10, 20]\n",
    "        }\n",
    "        search = RandomizedSearchCV(pipe, param_dist, n_iter=8, scoring='roc_auc', cv=GroupKFold(n_splits=n_inner), random_state=42, n_jobs=1)\n",
    "        try:\n",
    "            search.fit(X_train_df, y_train, groups=groups_train)\n",
    "            best_pipe = search.best_estimator_\n",
    "        except Exception as e:\n",
    "            print('Nested search failed, falling back to default pipeline:', e)\n",
    "            best_pipe = pipe\n",
    "            best_pipe.fit(X_train_df, y_train)\n",
    "    else:\n",
    "        best_pipe = pipe\n",
    "        best_pipe.fit(X_train_df, y_train)\n",
    "\n",
    "    # scoring\n",
    "    try:\n",
    "        y_score = best_pipe.predict_proba(X_test_df)[:, 1]\n",
    "    except Exception:\n",
    "        try:\n",
    "            y_score = best_pipe.decision_function(X_test_df)\n",
    "        except Exception:\n",
    "            y_score = np.zeros(len(X_test_df))\n",
    "    y_pred = best_pipe.predict(X_test_df)\n",
    "    try:\n",
    "        auc_score = roc_auc_score(y_test, y_score)\n",
    "    except Exception:\n",
    "        auc_score = float('nan')\n",
    "    # robust confusion matrix\n",
    "    try:\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    except Exception:\n",
    "        tn = fp = fn = tp = np.nan\n",
    "    sens = tp / (tp + fn) if (not np.isnan(tp) and (tp + fn) > 0) else np.nan\n",
    "    spec = tn / (tn + fp) if (not np.isnan(tn) and (tn + fp) > 0) else np.nan\n",
    "    metrics.append({'fold': fold, 'auc': float(auc_score), 'sensitivity': float(sens) if not np.isnan(sens) else np.nan, 'specificity': float(spec) if not np.isnan(spec) else np.nan})\n",
    "    all_y_true.extend(y_test.tolist())\n",
    "    all_y_score.extend(y_score.tolist())\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df.to_csv('../Processed_Data/patient_level_groupcv_results.csv', index=False)\n",
    "print('Saved GroupKFold results to Processed_Data/patient_level_groupcv_results.csv')\n",
    "\n",
    "# train final model on all data (with a small randomized search) and save\n",
    "scaler = StandardScaler()\n",
    "X_all = X\n",
    "pipe_all = Pipeline([('scaler', scaler), ('sel', SelectKBest(f_classif, k=min(50, X.shape[1]))), ('clf', RandomForestClassifier(class_weight='balanced', random_state=42))])\n",
    "param_dist_all = {'sel__k': [min(10, X.shape[1]), min(20, X.shape[1]), min(50, X.shape[1]), X.shape[1]], 'clf__n_estimators': [200, 500], 'clf__max_depth': [None, 10, 20]}\n",
    "try:\n",
    "    rs_all = RandomizedSearchCV(pipe_all, param_dist_all, n_iter=8, scoring='roc_auc', cv=GroupKFold(n_splits=min(3, n_groups)) if n_groups >=3 else 3, random_state=42, n_jobs=1)\n",
    "    if n_groups >= 2:\n",
    "        rs_all.fit(X_all, y, groups=groups)\n",
    "        clf_full = rs_all.best_estimator_\n",
    "    else:\n",
    "        pipe_all.fit(X_all, y)\n",
    "        clf_full = pipe_all\n",
    "except Exception as e:\n",
    "    print('Final model search failed, training default pipeline on all data:', e)\n",
    "    pipe_all.fit(X_all, y)\n",
    "    clf_full = pipe_all\n",
    "\n",
    "dump({'pipeline': clf_full}, '../Processed_Data/patient_level_model_groupcv.joblib')\n",
    "print('Saved trained model to Processed_Data/patient_level_model_groupcv.joblib')\n",
    "\n",
    "# patient-level ROC curve (aggregated across folds)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "if len(set(all_y_true)) > 1:\n",
    "    fpr, tpr, _ = roc_curve(all_y_true, all_y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr, tpr, label=f'GroupKFold ROC (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0,1],[0,1],'--',color='grey')\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../Processed_Data/patient_level_groupcv_roc.png')\n",
    "    plt.show()\n",
    "    print('Saved ROC to Processed_Data/patient_level_groupcv_roc.png')\n",
    "else:\n",
    "    print('Not enough positive/negative samples across folds to compute ROC curve')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dbaced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patient-level rows: 10\n",
      "Unique patient IDs: 6\n",
      "\n",
      "Per-patient response mode counts:\n",
      "response\n",
      "Responder        4\n",
      "Non-Responder    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Row-level response counts:\n",
      "response\n",
      "Responder        7\n",
      "Non-Responder    3\n",
      "Name: count, dtype: int64\n",
      "Patients with responder mode: 6, Non-responder mode: 2\n",
      "Responder patient IDs (sample): ['PT1', 'PT11', 'PT2', 'PT3', 'PT4', 'PT5']\n",
      "Non-responder patient IDs (sample): ['PT2', 'PT4']\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics: check patient/responders distribution\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "pf = Path('../Processed_Data/patient_level_features.csv')\n",
    "df = pd.read_csv(pf)\n",
    "print('Total patient-level rows:', len(df))\n",
    "print('Unique patient IDs:', df['patient_id'].nunique())\n",
    "# compute per-patient response mode\n",
    "patient_mode = df.groupby('patient_id')['response'].agg(lambda x: pd.Series(x).mode().iloc[0] if len(pd.Series(x).mode())>0 else None)\n",
    "print('\\nPer-patient response mode counts:')\n",
    "print(patient_mode.value_counts(dropna=False))\n",
    "print('\\nRow-level response counts:')\n",
    "print(df['response'].value_counts(dropna=False))\n",
    "# list patients by mode\n",
    "responder_patients = patient_mode[patient_mode.astype(str).str.lower().str.contains('respon', na=False)].index.tolist()\n",
    "nonresponder_patients = patient_mode[patient_mode.astype(str).str.lower().str.contains('non', na=False)].index.tolist()\n",
    "print(f'Patients with responder mode: {len(responder_patients)}, Non-responder mode: {len(nonresponder_patients)}')\n",
    "print('Responder patient IDs (sample):', responder_patients[:10])\n",
    "print('Non-responder patient IDs (sample):', nonresponder_patients[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17e2db21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pseudobulk counts to ..\\Processed_Data\\pseudobulk_counts.csv\n",
      "No DE results (too few samples in groups)\n",
      "Saved gene-set scores to ..\\Processed_Data\\pseudobulk_gene_set_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# 3) Pseudobulk differential expression + gene-set scoring (interferon/complement)\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "output_dir = Path('../Processed_Data')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure adata is available\n",
    "if 'adata' not in globals():\n",
    "    processed_path = Path('../Processed_Data/processed_s_rna_seq_data.h5ad')\n",
    "    if processed_path.exists():\n",
    "        adata = sc.read_h5ad(processed_path)\n",
    "    else:\n",
    "        raise FileNotFoundError('`adata` not found; run preprocessing first or provide processed_h5ad file.')\n",
    "\n",
    "# Build pseudobulk sample labels (prefer sample_id; fallback to patient_id::timepoint)\n",
    "if 'sample_id' in adata.obs.columns:\n",
    "    sample_labels = adata.obs['sample_id'].astype(str)\n",
    "else:\n",
    "    tp = adata.obs['timepoint'].astype(str) if 'timepoint' in adata.obs.columns else ''\n",
    "    if 'patient_id' in adata.obs.columns:\n",
    "        sample_labels = adata.obs['patient_id'].astype(str) + '::' + tp\n",
    "    elif 'patient' in adata.obs.columns:\n",
    "        sample_labels = adata.obs['patient'].astype(str) + '::' + tp\n",
    "    else:\n",
    "        sample_labels = pd.Series(adata.obs_names, index=adata.obs_names)\n",
    "\n",
    "# Use adata.raw if available for counts else adata\n",
    "adata_counts = adata.raw if getattr(adata, 'raw', None) is not None else adata\n",
    "if getattr(adata_counts, 'var_names', None) is None:\n",
    "    raise RuntimeError('adata.var_names (gene names) not found in adata or adata.raw. Cannot run pseudobulk DE.')\n",
    "\n",
    "X = adata_counts.X\n",
    "genes = list(adata_counts.var_names)\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Sum counts per sample\n",
    "pseudobulk = []\n",
    "samples = sample_labels.unique()\n",
    "for s in samples:\n",
    "    mask = (sample_labels == s).values\n",
    "    sub = X[mask]\n",
    "    if sp.issparse(sub):\n",
    "        summed = np.asarray(sub.sum(axis=0)).ravel()\n",
    "    else:\n",
    "        summed = np.sum(sub, axis=0)\n",
    "    pseudobulk.append(summed)\n",
    "pseudobulk_counts = pd.DataFrame(np.vstack(pseudobulk).T, index=genes, columns=samples)\n",
    "pseudobulk_counts.to_csv(output_dir / 'pseudobulk_counts.csv')\n",
    "print('Saved pseudobulk counts to', output_dir / 'pseudobulk_counts.csv')\n",
    "\n",
    "# Build sample metadata (response per sample by majority of cells)\n",
    "sample_meta = []\n",
    "for s in samples:\n",
    "    mask = (sample_labels == s).values\n",
    "    if 'response' in adata.obs.columns:\n",
    "        resp = adata.obs.loc[mask, 'response']\n",
    "    else:\n",
    "        resp = pd.Series(dtype=object)\n",
    "    mode = resp.mode() if len(resp) > 0 else pd.Series(dtype=object)\n",
    "    sample_meta.append({'sample': s, 'response': mode.iloc[0] if len(mode) > 0 else None})\n",
    "sample_meta = pd.DataFrame(sample_meta).set_index('sample')\n",
    "\n",
    "# Normalize response labels to canonical 'Responder'/'Non-Responder'\n",
    "def _norm_resp(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x).strip().lower()\n",
    "    if s in ['', 'nan', 'none', 'na']:\n",
    "        return None\n",
    "    if 'respon' in s or s in ('r','1','yes','y','responder'):\n",
    "        return 'Responder'\n",
    "    if 'non' in s or s in ('nr','0','no','n','non-responder','nonresponder'):\n",
    "        return 'Non-Responder'\n",
    "    return s.capitalize()\n",
    "\n",
    "sample_meta['response'] = sample_meta['response'].apply(_norm_resp)\n",
    "\n",
    "# Differential testing (Responder vs Non-Responder) across pseudobulk samples\n",
    "res_samples = sample_meta[sample_meta['response'] == 'Responder'].index.tolist()\n",
    "nonres_samples = sample_meta[sample_meta['response'] == 'Non-Responder'].index.tolist()\n",
    "results = []\n",
    "from scipy.stats import mannwhitneyu\n",
    "for gene in pseudobulk_counts.index:\n",
    "    a = pseudobulk_counts.loc[gene, res_samples].values if len(res_samples) > 0 else np.array([])\n",
    "    b = pseudobulk_counts.loc[gene, nonres_samples].values if len(nonres_samples) > 0 else np.array([])\n",
    "    if len(a) > 0 and len(b) > 0:\n",
    "        # CPM per sample then log2 fold (pseudo-cnt)\n",
    "        a_cpm = (a / (a.sum()+1e-9)) * 1e6\n",
    "        b_cpm = (b / (b.sum()+1e-9)) * 1e6\n",
    "        try:\n",
    "            stat, p = mannwhitneyu(a_cpm, b_cpm, alternative='two-sided')\n",
    "        except Exception:\n",
    "            stat, p = np.nan, np.nan\n",
    "        mean_fc = np.log2((a_cpm.mean() + 1) / (b_cpm.mean() + 1))\n",
    "    else:\n",
    "        p = np.nan\n",
    "        mean_fc = np.nan\n",
    "    results.append({'gene': gene, 'log2FC': float(mean_fc) if not np.isnan(mean_fc) else np.nan, 'pval': float(p) if not np.isnan(p) else np.nan})\n",
    "de_df = pd.DataFrame(results).set_index('gene')\n",
    "de_df = de_df.dropna(subset=['pval'], how='all')\n",
    "if not de_df.empty:\n",
    "    de_df['fdr'] = multipletests(de_df['pval'].fillna(1), method='fdr_bh')[1]\n",
    "    de_df.to_csv(output_dir / 'pseudobulk_DE_results.csv')\n",
    "    print('Saved pseudobulk DE results to', output_dir / 'pseudobulk_DE_results.csv')\n",
    "else:\n",
    "    print('No DE results (too few samples in groups)')\n",
    "\n",
    "# Gene-set scoring (interferon and complement) using Scanpy's score_genes\n",
    "interferon_genes = ['IFIT1','IFIT2','IFIT3','MX1','ISG15','OAS1','OAS2','OASL','IFI44','IFI6']\n",
    "complement_genes = ['C1QA','C1QB','C1QC','C2','C3','C4A','C4B','C5']\n",
    "for name, geneset in [('Interferon',interferon_genes), ('Complement', complement_genes)]:\n",
    "    genes_present = [g for g in geneset if g in adata.var_names]\n",
    "    if len(genes_present) == 0:\n",
    "        print(f'Skipping {name} scoring — none of the genes present in adata.var_names')\n",
    "        continue\n",
    "    use_raw_flag = True if getattr(adata, 'raw', None) is not None and getattr(adata.raw, 'var_names', None) is not None else False\n",
    "    try:\n",
    "        sc.tl.score_genes(adata, gene_list=genes_present, score_name=f'{name}_score', use_raw=use_raw_flag)\n",
    "    except Exception as e:\n",
    "        print(f'Could not score {name} gene set: {e}')\n",
    "\n",
    "# Save sample-level gene set scores (average per sample)\n",
    "scores = []\n",
    "for s in samples:\n",
    "    mask = (sample_labels == s).values\n",
    "    row = {'sample': s}\n",
    "    for name in ['Interferon','Complement']:\n",
    "        col = f'{name}_score'\n",
    "        row[col] = float(adata.obs.loc[mask, col].mean()) if col in adata.obs.columns else np.nan\n",
    "    scores.append(row)\n",
    "scores_df = pd.DataFrame(scores).set_index('sample')\n",
    "scores_df.to_csv(output_dir / 'pseudobulk_gene_set_scores.csv')\n",
    "print('Saved gene-set scores to', output_dir / 'pseudobulk_gene_set_scores.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "242e8dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 10 tumor/all contig files — computing overlaps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tumor–blood overlap table to ..\\Processed_Data\\tumor_blood_overlap.csv\n",
      "Patient-level response not available or overlaps empty; skipping association test.\n"
     ]
    }
   ],
   "source": [
    "# 4) Tumor–blood TCR overlap (Jaccard) — only runs if tumor TCR files are detected\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import mannwhitneyu\n",
    "output_dir = Path('../Processed_Data')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_dir = Path('../Data/GSE300475_RAW')\n",
    "# look for 10x/Cellranger contig annotations files\n",
    "tcr_files = list(data_dir.glob('*_all_contig_annotations.csv')) if data_dir.exists() else []\n",
    "if len(tcr_files) == 0:\n",
    "    print(f'No *_all_contig_annotations.csv files found in {data_dir} (skipping tumor–blood overlap).')\n",
    "else:\n",
    "    # prefer files explicitly mentioning tumor, otherwise use all contig files\n",
    "    tumor_files = [f for f in tcr_files if 'tumor' in f.name.lower() or 'tumour' in f.name.lower()]\n",
    "    if len(tumor_files) == 0:\n",
    "        tumor_files = tcr_files\n",
    "    print(f'Detected {len(tumor_files)} tumor/all contig files — computing overlaps')\n",
    "    tumor_by_patient = {}\n",
    "    for f in tumor_files:\n",
    "        try:\n",
    "            df_t = pd.read_csv(f)\n",
    "            pid = f.stem.split('_')[0]\n",
    "            # find plausible cdr3/sequence column\n",
    "            seq_col = None\n",
    "            for cand in ['cdr3', 'cdr3b', 'productive_cdr3', 'sequence', 'cdr3_nt', 'cdr3aa', 'sequence_complete']:\n",
    "                matches = [c for c in df_t.columns if cand in c.lower()]\n",
    "                if matches:\n",
    "                    seq_col = matches[0]\n",
    "                    break\n",
    "            seqs = df_t[seq_col] if seq_col is not None else df_t.iloc[:, 0]\n",
    "            tumor_by_patient[pid] = set(seqs.dropna().astype(str))\n",
    "        except Exception as e:\n",
    "            print(f'Could not read {f}: {e}')\n",
    "\n",
    "    # Blood clonotypes per sample/patient from adata.obs (use best available column)\n",
    "    clonotype_col = None\n",
    "    for c in ['cdr3_TRB', 'cdr3_TRA', 'cdr3b', 'cdr3a', 'cdr3', 'productive_cdr3', 'clone_id']:\n",
    "        if c in adata.obs.columns:\n",
    "            clonotype_col = c\n",
    "            break\n",
    "    overlaps = []\n",
    "    if clonotype_col is None:\n",
    "        print('No clonotype column found in adata.obs (cdr3_TRB/TRA). Cannot compute overlap.')\n",
    "    else:\n",
    "        # ensure patient_df available for response merging\n",
    "        pf = Path('../Processed_Data/patient_level_features.csv')\n",
    "        patient_df = pd.read_csv(pf) if pf.exists() else None\n",
    "        if patient_df is None:\n",
    "            print('Patient-level features not available; overlap table will be saved but response association skipped.')\n",
    "\n",
    "        # choose grouping columns\n",
    "        group_cols = []\n",
    "        if 'patient_id' in adata.obs.columns:\n",
    "            group_cols.append('patient_id')\n",
    "        if 'timepoint' in adata.obs.columns:\n",
    "            group_cols.append('timepoint')\n",
    "        elif 'sample_id' in adata.obs.columns:\n",
    "            group_cols.append('sample_id')\n",
    "\n",
    "        if len(group_cols) == 0:\n",
    "            print('No patient/sample grouping columns found in adata.obs; skipping overlap.')\n",
    "        else:\n",
    "            grp = adata.obs.groupby(group_cols)\n",
    "            for name, df in grp:\n",
    "                pid = name[0] if isinstance(name, tuple) else name\n",
    "                blood_set = set(df[clonotype_col].dropna().astype(str))\n",
    "                # try to match tumor_by_patient by several heuristics\n",
    "                candidate_keys = [k for k in tumor_by_patient.keys() if pid in k or k in pid or k.split('-')[0] == pid or pid.split('-')[0] == k]\n",
    "                if len(candidate_keys) == 0:\n",
    "                    continue\n",
    "                tumor_set = tumor_by_patient[candidate_keys[0]]\n",
    "                inter = len(blood_set & tumor_set)\n",
    "                union = len(blood_set | tumor_set)\n",
    "                jaccard = inter / union if union > 0 else 0.0\n",
    "                overlaps.append({'patient_id': pid, 'timepoint': name[1] if isinstance(name, tuple) and len(name) > 1 else '', 'jaccard': jaccard, 'n_shared': inter, 'n_blood': len(blood_set), 'n_tumor': len(tumor_set)})\n",
    "\n",
    "            overlaps_df = pd.DataFrame(overlaps)\n",
    "            overlaps_df.to_csv(output_dir / 'tumor_blood_overlap.csv', index=False)\n",
    "            print('Saved tumor–blood overlap table to', output_dir / 'tumor_blood_overlap.csv')\n",
    "\n",
    "            # test association with response if available\n",
    "            if patient_df is not None and 'response' in patient_df.columns and not overlaps_df.empty:\n",
    "                merged = overlaps_df.merge(patient_df[['patient_id','response']].drop_duplicates(), on='patient_id', how='left')\n",
    "                # normalize response labels\n",
    "                merged['response_norm'] = merged['response'].astype(str).str.lower().map(lambda s: 'Responder' if 'respon' in s else ('Non-Responder' if 'non' in s else s))\n",
    "                res = merged[merged['response_norm']=='Responder']['jaccard'].dropna()\n",
    "                non = merged[merged['response_norm']=='Non-Responder']['jaccard'].dropna()\n",
    "                if len(res) > 0 and len(non) > 0:\n",
    "                    stat, p = mannwhitneyu(res, non, alternative='two-sided')\n",
    "                    print(f'Jaccard overlap R vs NR: U={stat:.2f}, p={p:.3e}')\n",
    "                else:\n",
    "                    print('Not enough samples in response groups to test association')\n",
    "            else:\n",
    "                print('Patient-level response not available or overlaps empty; skipping association test.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f7ffaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No k-mer statistics computed (insufficient samples)\n"
     ]
    }
   ],
   "source": [
    "# 5) Multiple-testing corrected k-mer enrichment (pseudobulk per sample; FDR via BH)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "output_dir = Path('../Processed_Data')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build pseudobulk k-mer counts per sample from adata.obs cdr3_TRB (k=3)\n",
    "def kmer_counts_from_seq_list(seqs, k=3):\n",
    "    counts = {}\n",
    "    for s in seqs.dropna().astype(str):\n",
    "        for i in range(len(s)-k+1):\n",
    "            kmer = s[i:i+k]\n",
    "            counts[kmer] = counts.get(kmer, 0) + 1\n",
    "    return counts\n",
    "\n",
    "if 'cdr3_TRB' not in adata.obs.columns and 'cdr3_TRA' not in adata.obs.columns:\n",
    "    print('No CDR3 sequences found in adata.obs (skipping k-mer enrichment).')\n",
    "else:\n",
    "    seq_col = 'cdr3_TRB' if 'cdr3_TRB' in adata.obs.columns else 'cdr3_TRA'\n",
    "    if 'sample_id' in adata.obs.columns:\n",
    "        sample_labels = adata.obs['sample_id'].astype(str)\n",
    "    else:\n",
    "        tp = adata.obs['timepoint'].astype(str) if 'timepoint' in adata.obs.columns else ''\n",
    "        sample_labels = adata.obs['patient_id'].astype(str) + '::' + tp\n",
    "    samples = sample_labels.unique()\n",
    "    k=3\n",
    "    # build dataframe of k-mer counts per sample\n",
    "    rows = []\n",
    "    for s in samples:\n",
    "        mask = sample_labels == s\n",
    "        seqs = adata.obs.loc[mask, seq_col]\n",
    "        kc = kmer_counts_from_seq_list(seqs, k=k)\n",
    "        row = {'sample': s}\n",
    "        row.update(kc)\n",
    "        rows.append(row)\n",
    "    kmer_df = pd.DataFrame(rows).fillna(0).set_index('sample')\n",
    "    # attach response label per sample\n",
    "    sample_meta = []\n",
    "    for s in samples:\n",
    "        mask = sample_labels == s\n",
    "        resp = adata.obs.loc[mask, 'response'] if 'response' in adata.obs.columns else pd.Series(dtype=object)\n",
    "        mode = resp.mode() if len(resp) > 0 else pd.Series(dtype=object)\n",
    "        sample_meta.append({'sample': s, 'response': mode.iloc[0] if len(mode) > 0 else None})\n",
    "    sample_meta = pd.DataFrame(sample_meta).set_index('sample')\n",
    "    # normalize responses\n",
    "    def _norm_resp(x):\n",
    "        if pd.isna(x):\n",
    "            return None\n",
    "        s = str(x).strip().lower()\n",
    "        if 'respon' in s or s in ('r','1','yes','y'):\n",
    "            return 'Responder'\n",
    "        if 'non' in s or s in ('nr','0','no','n'):\n",
    "            return 'Non-Responder'\n",
    "        return None\n",
    "    sample_meta['response'] = sample_meta['response'].apply(_norm_resp)\n",
    "\n",
    "    # perform Mann-Whitney U test per k-mer across samples (Responder vs Non-Responder)\n",
    "    res_samples = sample_meta[sample_meta['response']=='Responder'].index.tolist()\n",
    "    non_samples = sample_meta[sample_meta['response']=='Non-Responder'].index.tolist()\n",
    "    from scipy.stats import mannwhitneyu\n",
    "    results = []\n",
    "    n_samples = len(samples)\n",
    "    # prevalence filter: require k-mer present in at least min_samples samples\n",
    "    min_samples = max(2, int(0.1 * n_samples))\n",
    "    cols = [c for c in kmer_df.columns if (kmer_df[c] > 0).sum() >= min_samples]\n",
    "    if len(cols) == 0:\n",
    "        print('No k-mers pass prevalence filter; relaxing to top-occurring k-mers')\n",
    "        cols = list(kmer_df.columns)\n",
    "    for kmer in cols:\n",
    "        a = kmer_df.loc[res_samples, kmer].values if len(res_samples) > 0 else np.array([])\n",
    "        b = kmer_df.loc[non_samples, kmer].values if len(non_samples) > 0 else np.array([])\n",
    "        if len(a) > 0 and len(b) > 0:\n",
    "            try:\n",
    "                stat, p = mannwhitneyu(a, b, alternative='two-sided')\n",
    "            except Exception:\n",
    "                stat, p = np.nan, np.nan\n",
    "            mean_diff = float(a.mean() - b.mean())\n",
    "        else:\n",
    "            stat, p, mean_diff = np.nan, np.nan, np.nan\n",
    "        results.append({'kmer': kmer, 'stat': stat, 'pval': p, 'mean_diff': mean_diff})\n",
    "    res_df = pd.DataFrame(results).set_index('kmer').dropna(subset=['pval'])\n",
    "    if not res_df.empty:\n",
    "        res_df['fdr'] = multipletests(res_df['pval'], method='fdr_bh')[1]\n",
    "        res_df.sort_values('fdr', inplace=True)\n",
    "        res_df.to_csv(output_dir / 'kmer_enrichment_results.csv')\n",
    "        print('Saved k-mer enrichment results to', output_dir / 'kmer_enrichment_results.csv')\n",
    "        significant = res_df[res_df['fdr'] < 0.05]\n",
    "        print(f'{len(significant)} k-mers with FDR < 0.05')\n",
    "    else:\n",
    "        print('No k-mer statistics computed (insufficient samples)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18cf89c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: adata.X seems to be already log-transformed.\n",
      "HVG selection failed, falling back to variance-based selection: Bin edges must be unique: Index([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan],\n",
      "      dtype='float64').\n",
      "You can drop duplicate edges by setting the 'duplicates' kwarg\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "All gene variances are NaN; check adata.X for valid counts",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 30\u001b[0m\n\u001b[0;32m     29\u001b[0m     sc\u001b[38;5;241m.\u001b[39mpp\u001b[38;5;241m.\u001b[39mlog1p(adata)\n\u001b[1;32m---> 30\u001b[0m     sc\u001b[38;5;241m.\u001b[39mpp\u001b[38;5;241m.\u001b[39mhighly_variable_genes(adata, n_top_genes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\legacy_api_wrap\\__init__.py:82\u001b[0m, in \u001b[0;36mlegacy_api.<locals>.wrapper.<locals>.fn_compatible\u001b[1;34m(*args_all, **kw)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args_all) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_positional:\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs_all, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m     84\u001b[0m args_pos: P\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\scanpy\\preprocessing\\_highly_variable_genes.py:689\u001b[0m, in \u001b[0;36mhighly_variable_genes\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 689\u001b[0m     df \u001b[38;5;241m=\u001b[39m _highly_variable_genes_single_batch(\n\u001b[0;32m    690\u001b[0m         adata, layer\u001b[38;5;241m=\u001b[39mlayer, cutoff\u001b[38;5;241m=\u001b[39mcutoff, n_bins\u001b[38;5;241m=\u001b[39mn_bins, flavor\u001b[38;5;241m=\u001b[39mflavor\n\u001b[0;32m    691\u001b[0m     )\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\scanpy\\preprocessing\\_highly_variable_genes.py:316\u001b[0m, in \u001b[0;36m_highly_variable_genes_single_batch\u001b[1;34m(adata, layer, cutoff, n_bins, flavor)\u001b[0m\n\u001b[0;32m    313\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeans\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdispersions\u001b[39m\u001b[38;5;124m\"\u001b[39m], (mean, dispersion), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    315\u001b[0m )\n\u001b[1;32m--> 316\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_bin\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_mean_bins(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeans\u001b[39m\u001b[38;5;124m\"\u001b[39m], flavor, n_bins)\n\u001b[0;32m    317\u001b[0m disp_stats \u001b[38;5;241m=\u001b[39m _get_disp_stats(df, flavor)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\scanpy\\preprocessing\\_highly_variable_genes.py:343\u001b[0m, in \u001b[0;36m_get_mean_bins\u001b[1;34m(means, flavor, n_bins)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m--> 343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mcut(means, bins\u001b[38;5;241m=\u001b[39mbins)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\tile.py:257\u001b[0m, in \u001b[0;36mcut\u001b[1;34m(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbins must increase monotonically.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 257\u001b[0m fac, bins \u001b[38;5;241m=\u001b[39m _bins_to_cuts(\n\u001b[0;32m    258\u001b[0m     x_idx,\n\u001b[0;32m    259\u001b[0m     bins,\n\u001b[0;32m    260\u001b[0m     right\u001b[38;5;241m=\u001b[39mright,\n\u001b[0;32m    261\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m    262\u001b[0m     precision\u001b[38;5;241m=\u001b[39mprecision,\n\u001b[0;32m    263\u001b[0m     include_lowest\u001b[38;5;241m=\u001b[39minclude_lowest,\n\u001b[0;32m    264\u001b[0m     duplicates\u001b[38;5;241m=\u001b[39mduplicates,\n\u001b[0;32m    265\u001b[0m     ordered\u001b[38;5;241m=\u001b[39mordered,\n\u001b[0;32m    266\u001b[0m )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _postprocess_for_cut(fac, bins, retbins, original)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\tile.py:443\u001b[0m, in \u001b[0;36m_bins_to_cuts\u001b[1;34m(x_idx, bins, right, labels, precision, include_lowest, duplicates, ordered)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m duplicates \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBin edges must be unique: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(bins)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can drop duplicate edges by setting the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduplicates\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m kwarg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n\u001b[0;32m    447\u001b[0m bins \u001b[38;5;241m=\u001b[39m unique_bins\n",
      "\u001b[1;31mValueError\u001b[0m: Bin edges must be unique: Index([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n       nan, nan, nan, nan, nan, nan, nan],\n      dtype='float64').\nYou can drop duplicate edges by setting the 'duplicates' kwarg",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m var \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanvar(Xmat, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39misnan(var)):\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll gene variances are NaN; check adata.X for valid counts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan_to_num(var))[:\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m2000\u001b[39m, Xmat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[0;32m     38\u001b[0m adata \u001b[38;5;241m=\u001b[39m adata[:, idx]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: All gene variances are NaN; check adata.X for valid counts"
     ]
    }
   ],
   "source": [
    "# 6) Batch correction and integration (Harmony primary, bbknn fallback) and re-clustering\n",
    "import scanpy as sc\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "output_dir = Path('../Processed_Data')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Ensure adata is present\n",
    "if 'adata' not in globals():\n",
    "    path = Path('../Processed_Data/processed_s_rna_seq_data.h5ad')\n",
    "    if path.exists():\n",
    "        adata = sc.read_h5ad(path)\n",
    "    else:\n",
    "        raise FileNotFoundError('No adata in memory and no processed file found.')\n",
    "\n",
    "import scipy.sparse as sp\n",
    "# Filter low-prevalence genes to avoid HVG failures\n",
    "try:\n",
    "    sc.pp.filter_genes(adata, min_cells=3)\n",
    "except Exception as e:\n",
    "    print('filter_genes failed or not applicable:', e)\n",
    "\n",
    "if adata.n_vars < 50:\n",
    "    print('Too few genes after filtering; skipping HVG selection and using all genes')\n",
    "else:\n",
    "    try:\n",
    "        sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)\n",
    "    except Exception as e:\n",
    "        print('HVG selection failed, falling back to variance-based selection:', e)\n",
    "        Xmat = adata.X.toarray() if sp.issparse(adata.X) else adata.X\n",
    "        var = np.nanvar(Xmat, axis=0)\n",
    "        if np.all(np.isnan(var)):\n",
    "            raise RuntimeError('All gene variances are NaN; check adata.X for valid counts')\n",
    "        idx = np.argsort(-np.nan_to_num(var))[:min(2000, Xmat.shape[1])]\n",
    "        adata = adata[:, idx].copy()\n",
    "\n",
    "sc.pp.scale(adata, max_value=10)\n",
    "sc.tl.pca(adata, n_comps=50, svd_solver='arpack')\n",
    "\n",
    "use_rep = 'X_pca'\n",
    "# Try Harmony integration if available\n",
    "try:\n",
    "    import harmonypy as hm\n",
    "    print('Running Harmony integration on `sample_id`...')\n",
    "    ho = hm.run_harmony(adata.obsm['X_pca'], adata.obs, 'sample_id')\n",
    "    adata.obsm['X_pca_harmony'] = ho.Z_corr.T\n",
    "    use_rep = 'X_pca_harmony'\n",
    "except Exception as e:\n",
    "    print('Harmony not available or failed, falling back to ComBat/bbknn as available:', e)\n",
    "    try:\n",
    "        sc.pp.combat(adata, key='sample_id')\n",
    "        sc.tl.pca(adata, n_comps=50, svd_solver='arpack')\n",
    "        adata.obsm['X_pca_harmony'] = adata.obsm['X_pca']\n",
    "        use_rep = 'X_pca_harmony'\n",
    "    except Exception as e2:\n",
    "        print('ComBat failed or not appropriate:', e2)\n",
    "\n",
    "# Build neighbors on integrated representation and cluster\n",
    "try:\n",
    "    sc.pp.neighbors(adata, use_rep=use_rep)\n",
    "except Exception as e:\n",
    "    print('sc.pp.neighbors failed; trying bbknn fallback:', e)\n",
    "    try:\n",
    "        import bbknn\n",
    "        if use_rep in adata.obsm:\n",
    "            bbknn.bbknn(adata, batch_key='sample_id', use_rep=use_rep)\n",
    "        else:\n",
    "            print('bbknn fallback not possible; using default neighbors on X_pca')\n",
    "            sc.pp.neighbors(adata, use_rep='X_pca')\n",
    "    except Exception as e3:\n",
    "        print('bbknn not available or failed:', e3)\n",
    "        sc.pp.neighbors(adata, use_rep='X_pca')\n",
    "\n",
    "sc.tl.umap(adata)\n",
    "sc.tl.leiden(adata, resolution=1.0, key_added='leiden_integrated')\n",
    "# Rank markers for integrated clusters\n",
    "try:\n",
    "    sc.tl.rank_genes_groups(adata, groupby='leiden_integrated', method='wilcoxon')\n",
    "    # export marker tables per cluster\n",
    "    for g in adata.obs['leiden_integrated'].cat.categories:\n",
    "        try:\n",
    "            df_mark = sc.get.rank_genes_groups_df(adata, group=g)\n",
    "            df_mark.to_csv(output_dir / f'leiden_integrated_markers_group_{g}.csv', index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    print('Saved integrated clustering and marker tables')\n",
    "except Exception as e:\n",
    "    print('rank_genes_groups failed on integrated data:', e)\n",
    "\n",
    "# Save integrated AnnData\n",
    "adata.write_h5ad(output_dir / 'processed_s_rna_seq_data_integrated.h5ad')\n",
    "print('Saved integrated AnnData to', output_dir / 'processed_s_rna_seq_data_integrated.h5ad')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
