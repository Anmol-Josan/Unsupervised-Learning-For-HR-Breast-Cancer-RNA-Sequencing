{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5ba853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:53:47.047061Z",
     "iopub.status.busy": "2025-11-12T03:53:47.046766Z",
     "iopub.status.idle": "2025-11-12T03:53:48.887255Z",
     "shell.execute_reply": "2025-11-12T03:53:48.886389Z"
    },
    "papermill": {
     "duration": 1.853568,
     "end_time": "2025-11-12T03:53:48.889170",
     "exception": false,
     "start_time": "2025-11-12T03:53:47.035602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693c1253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:53:48.906974Z",
     "iopub.status.busy": "2025-11-12T03:53:48.906112Z",
     "iopub.status.idle": "2025-11-12T03:53:48.910946Z",
     "shell.execute_reply": "2025-11-12T03:53:48.910153Z"
    },
    "papermill": {
     "duration": 0.015151,
     "end_time": "2025-11-12T03:53:48.912312",
     "exception": false,
     "start_time": "2025-11-12T03:53:48.897161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "files_to_fetch = [\n",
    "    {\n",
    "        \"name\": \"GSE300475_RAW.tar\",\n",
    "        \"size\": \"565.5 Mb\",\n",
    "        \"download_url\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE300475&format=file\",\n",
    "        \"type\": \"TAR (of CSV, MTX, TSV)\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GSE300475_feature_ref.xlsx\",\n",
    "        \"size\": \"5.4 Kb\",\n",
    "        \"download_url\": \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE300nnn/GSE300475/suppl/GSE300475%5Ffeature%5Fref.xlsx\",\n",
    "        \"type\": \"XLSX\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac78477",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:53:48.928849Z",
     "iopub.status.busy": "2025-11-12T03:53:48.928577Z",
     "iopub.status.idle": "2025-11-12T03:53:49.003208Z",
     "shell.execute_reply": "2025-11-12T03:53:49.002165Z"
    },
    "papermill": {
     "duration": 0.084807,
     "end_time": "2025-11-12T03:53:49.004746",
     "exception": false,
     "start_time": "2025-11-12T03:53:48.919939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloads will be saved in: /kaggle/Data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "download_dir = \"../Data\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "print(f\"Downloads will be saved in: {os.path.abspath(download_dir)}\\n\")\n",
    "\n",
    "def download_file(url, filename, destination_folder):\n",
    "    \"\"\"\n",
    "    Downloads a file from a given URL to a specified destination folder.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(destination_folder, filename)\n",
    "    print(f\"Attempting to download {filename} from {url}...\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Successfully downloaded {filename} to {filepath}\")\n",
    "        return filepath\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a852f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:53:49.021421Z",
     "iopub.status.busy": "2025-11-12T03:53:49.021086Z",
     "iopub.status.idle": "2025-11-12T03:54:08.786971Z",
     "shell.execute_reply": "2025-11-12T03:54:08.785831Z"
    },
    "papermill": {
     "duration": 19.776225,
     "end_time": "2025-11-12T03:54:08.788751",
     "exception": false,
     "start_time": "2025-11-12T03:53:49.012526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download GSE300475_RAW.tar from https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE300475&format=file...\n",
      "Successfully downloaded GSE300475_RAW.tar to ../Data/GSE300475_RAW.tar\n",
      "Extracting GSE300475_RAW.tar...\n",
      "\n",
      "Files contained in GSE300475_RAW.tar:\n",
      " - GSM9061665_S1_barcodes.tsv.gz\n",
      " - GSM9061665_S1_features.tsv.gz\n",
      " - GSM9061665_S1_matrix.mtx.gz\n",
      " - GSM9061666_S2_barcodes.tsv.gz\n",
      " - GSM9061666_S2_features.tsv.gz\n",
      " - GSM9061666_S2_matrix.mtx.gz\n",
      " - GSM9061667_S3_barcodes.tsv.gz\n",
      " - GSM9061667_S3_features.tsv.gz\n",
      " - GSM9061667_S3_matrix.mtx.gz\n",
      " - GSM9061668_S4_barcodes.tsv.gz\n",
      " - GSM9061668_S4_features.tsv.gz\n",
      " - GSM9061668_S4_matrix.mtx.gz\n",
      " - GSM9061669_S5_barcodes.tsv.gz\n",
      " - GSM9061669_S5_features.tsv.gz\n",
      " - GSM9061669_S5_matrix.mtx.gz\n",
      " - GSM9061670_S6_barcodes.tsv.gz\n",
      " - GSM9061670_S6_features.tsv.gz\n",
      " - GSM9061670_S6_matrix.mtx.gz\n",
      " - GSM9061671_S7_barcodes.tsv.gz\n",
      " - GSM9061671_S7_features.tsv.gz\n",
      " - GSM9061671_S7_matrix.mtx.gz\n",
      " - GSM9061672_S8_barcodes.tsv.gz\n",
      " - GSM9061672_S8_features.tsv.gz\n",
      " - GSM9061672_S8_matrix.mtx.gz\n",
      " - GSM9061673_S9_barcodes.tsv.gz\n",
      " - GSM9061673_S9_features.tsv.gz\n",
      " - GSM9061673_S9_matrix.mtx.gz\n",
      " - GSM9061674_S10_barcodes.tsv.gz\n",
      " - GSM9061674_S10_features.tsv.gz\n",
      " - GSM9061674_S10_matrix.mtx.gz\n",
      " - GSM9061675_S11_barcodes.tsv.gz\n",
      " - GSM9061675_S11_features.tsv.gz\n",
      " - GSM9061675_S11_matrix.mtx.gz\n",
      " - GSM9061687_S1_all_contig_annotations.csv.gz\n",
      " - GSM9061688_S2_all_contig_annotations.csv.gz\n",
      " - GSM9061689_S3_all_contig_annotations.csv.gz\n",
      " - GSM9061690_S4_all_contig_annotations.csv.gz\n",
      " - GSM9061691_S5_all_contig_annotations.csv.gz\n",
      " - GSM9061692_S6_all_contig_annotations.csv.gz\n",
      " - GSM9061693_S7_all_contig_annotations.csv.gz\n",
      " - GSM9061694_S9_all_contig_annotations.csv.gz\n",
      " - GSM9061695_S10_all_contig_annotations.csv.gz\n",
      " - GSM9061696_S11_all_contig_annotations.csv.gz\n",
      "\n",
      "Extracted to: ../Data/GSE300475_RAW\n",
      "--------------------------------------------------\n",
      "\n",
      "Attempting to download GSE300475_feature_ref.xlsx from https://ftp.ncbi.nlm.nih.gov/geo/series/GSE300nnn/GSE300475/suppl/GSE300475%5Ffeature%5Fref.xlsx...\n",
      "Successfully downloaded GSE300475_feature_ref.xlsx to ../Data/GSE300475_feature_ref.xlsx\n"
     ]
    }
   ],
   "source": [
    "for file_info in files_to_fetch:\n",
    "    filename = file_info[\"name\"]\n",
    "    url = file_info[\"download_url\"]\n",
    "    file_type = file_info[\"type\"]\n",
    "\n",
    "    downloaded_filepath = download_file(url, filename, download_dir)\n",
    "\n",
    "        # If the file is a TAR archive, extract it and list the contents\n",
    "    if downloaded_filepath and filename.endswith(\".tar\"):\n",
    "        print(f\"Extracting {filename}...\\n\")\n",
    "        try:\n",
    "            with tarfile.open(downloaded_filepath, \"r\") as tar:\n",
    "                # List contents\n",
    "                members = tar.getnames()\n",
    "                print(f\"Files contained in {filename}:\")\n",
    "                for member in members:\n",
    "                    print(f\" - {member}\")\n",
    "\n",
    "                # Extract to a subdirectory within download_dir\n",
    "                extract_path = os.path.join(download_dir, filename.replace(\".tar\", \"\"))\n",
    "                os.makedirs(extract_path, exist_ok=True)\n",
    "                tar.extractall(path=extract_path)\n",
    "                print(f\"\\nExtracted to: {extract_path}\")\n",
    "        except tarfile.TarError as e:\n",
    "            print(f\"Error extracting {filename}: {e}\")\n",
    "\n",
    "        print(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf15f97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:54:08.806235Z",
     "iopub.status.busy": "2025-11-12T03:54:08.805897Z",
     "iopub.status.idle": "2025-11-12T03:54:31.949347Z",
     "shell.execute_reply": "2025-11-12T03:54:31.948083Z"
    },
    "papermill": {
     "duration": 23.154152,
     "end_time": "2025-11-12T03:54:31.951044",
     "exception": false,
     "start_time": "2025-11-12T03:54:08.796892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressing ../Data/GSE300475_RAW/GSM9061673_S9_matrix.mtx.gz → ../Data/GSE300475_RAW/GSM9061673_S9_matrix.mtx\n",
      "\n",
      "--- Preview of GSM9061673_S9_matrix.mtx ---\n",
      "First 5 non-zero entries:\n",
      "Row: 24, Col: 0, Value: 1\n",
      "Row: 59, Col: 0, Value: 1\n",
      "Row: 61, Col: 0, Value: 1\n",
      "Row: 62, Col: 0, Value: 1\n",
      "Row: 145, Col: 0, Value: 1\n",
      "\n",
      "Matrix shape: (36604, 11480), NNZ (non-zero elements): 19000971\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061669_S5_matrix.mtx.gz → ../Data/GSE300475_RAW/GSM9061669_S5_matrix.mtx\n",
      "\n",
      "--- Preview of GSM9061669_S5_matrix.mtx ---\n",
      "First 5 non-zero entries:\n",
      "Row: 59, Col: 0, Value: 1\n",
      "Row: 524, Col: 0, Value: 4\n",
      "Row: 593, Col: 0, Value: 1\n",
      "Row: 595, Col: 0, Value: 2\n",
      "Row: 601, Col: 0, Value: 1\n",
      "\n",
      "Matrix shape: (36604, 2912), NNZ (non-zero elements): 3668516\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061695_S10_all_contig_annotations.csv.gz → ../Data/GSE300475_RAW/GSM9061695_S10_all_contig_annotations.csv\n",
      "\n",
      "--- Preview of GSM9061695_S10_all_contig_annotations.csv ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGATCCTGT-1     True  AAACCTGAGATCCTGT-1_contig_1             True   \n",
      "1  AAACCTGAGATCCTGT-1     True  AAACCTGAGATCCTGT-1_contig_2             True   \n",
      "2  AAACCTGAGCTCTCGG-1     True  AAACCTGAGCTCTCGG-1_contig_1             True   \n",
      "3  AAACCTGAGCTCTCGG-1     True  AAACCTGAGCTCTCGG-1_contig_2             True   \n",
      "4  AAACCTGAGGCATGTG-1    False  AAACCTGAGGCATGTG-1_contig_1            False   \n",
      "\n",
      "   length chain      v_gene d_gene   j_gene c_gene  ...  \\\n",
      "0     486   TRB      TRBV27    NaN  TRBJ2-7  TRBC2  ...   \n",
      "1     507   TRA      TRAV35    NaN   TRAJ53   TRAC  ...   \n",
      "2     502   TRB     TRBV7-2    NaN  TRBJ2-7  TRBC2  ...   \n",
      "3     514   TRA  TRAV23/DV6    NaN    TRAJ9   TRAC  ...   \n",
      "4     470   TRB     TRBV7-2    NaN  TRBJ2-7  TRBC2  ...   \n",
      "\n",
      "                                             fwr3_nt             cdr3  \\\n",
      "0  ACTGATAAGGGAGATGTTCCTGAAGGGTACAAAGTCTCTCGAAAAG...     CASSSTSYEQYF   \n",
      "1  ACCTCAAATGGAAGACTGACTGCTCAGTTTGGTATAACCAGAAAGG...    CAGQPGSNYKLTF   \n",
      "2  CCAGACAAATCAGGGCTGCCCAGTGATCGCTTCTCTGCAGAGAGGA...  CASSLTSGPIYEQYF   \n",
      "3  AAGAAAGAAGGAAGATTCACAATCTCCTTCAATAAAAGTGCCAAGC...  CAASTSNTGGFKTIF   \n",
      "4                                                NaN  CASSLTSGPIYEQYF   \n",
      "\n",
      "                                         cdr3_nt        fwr4  \\\n",
      "0           TGTGCCAGCAGTTCAACGTCCTACGAGCAGTACTTC   GPGTRLTVT   \n",
      "1        TGTGCTGGGCAGCCGGGTAGCAACTATAAACTGACATTT  GKGTLLTVNP   \n",
      "2  TGTGCCAGCAGCTTAACTAGCGGGCCAATCTACGAGCAGTACTTC   GPGTRLTVT   \n",
      "3  TGTGCAGCAAGCACGTCTAATACTGGAGGCTTCAAAACTATCTTT  GAGTRLFVKA   \n",
      "4  TGTGCCAGCAGCTTAACTAGCGGGCCAATCTACGAGCAGTACTTC         NaN   \n",
      "\n",
      "                           fwr4_nt  reads umis raw_clonotype_id  \\\n",
      "0     GGGCCGGGCACCAGGCTCACGGTCACAG   4442    6      clonotype39   \n",
      "1  GGAAAAGGAACTCTCTTAACCGTGAATCCAA   2480    2      clonotype39   \n",
      "2     GGGCCGGGCACCAGGCTCACGGTCACAG  11628   16    clonotype4560   \n",
      "3  GGAGCAGGAACAAGACTATTTGTTAAAGCAA   1754    3    clonotype4560   \n",
      "4                              NaN    484    1              NaN   \n",
      "\n",
      "            raw_consensus_id exact_subclonotype_id  \n",
      "0    clonotype39_consensus_1                   1.0  \n",
      "1    clonotype39_consensus_2                   1.0  \n",
      "2  clonotype4560_consensus_1                   1.0  \n",
      "3  clonotype4560_consensus_2                   1.0  \n",
      "4                        NaN                   NaN  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061674_S10_barcodes.tsv.gz → ../Data/GSE300475_RAW/GSM9061674_S10_barcodes.tsv\n",
      "\n",
      "--- Preview of GSM9061674_S10_barcodes.tsv ---\n",
      "   AAACCTGAGATCCTGT-1\n",
      "0  AAACCTGAGCTCTCGG-1\n",
      "1  AAACCTGAGTAGGTGC-1\n",
      "2  AAACCTGAGTGGTAAT-1\n",
      "3  AAACCTGAGTTAAGTG-1\n",
      "4  AAACCTGCAAGGTTTC-1\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061675_S11_features.tsv.gz → ../Data/GSE300475_RAW/GSM9061675_S11_features.tsv\n",
      "\n",
      "--- Preview of GSM9061675_S11_features.tsv ---\n",
      "   ENSG00000243485 MIR1302-2HG  Gene Expression\n",
      "0  ENSG00000237613     FAM138A  Gene Expression\n",
      "1  ENSG00000186092       OR4F5  Gene Expression\n",
      "2  ENSG00000238009  AL627309.1  Gene Expression\n",
      "3  ENSG00000239945  AL627309.3  Gene Expression\n",
      "4  ENSG00000239906  AL627309.2  Gene Expression\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061666_S2_barcodes.tsv.gz → ../Data/GSE300475_RAW/GSM9061666_S2_barcodes.tsv\n",
      "\n",
      "--- Preview of GSM9061666_S2_barcodes.tsv ---\n",
      "   AAACCTGAGCCATCGC-1\n",
      "0  AAACCTGAGGACGAAA-1\n",
      "1  AAACCTGCAAGAAAGG-1\n",
      "2  AAACCTGCATGACGGA-1\n",
      "3  AAACCTGGTAAATACG-1\n",
      "4  AAACCTGGTGGCAAAC-1\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061694_S9_all_contig_annotations.csv.gz → ../Data/GSE300475_RAW/GSM9061694_S9_all_contig_annotations.csv\n",
      "\n",
      "--- Preview of GSM9061694_S9_all_contig_annotations.csv ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGAGAGCTC-1     True  AAACCTGAGAGAGCTC-1_contig_1             True   \n",
      "1  AAACCTGAGAGAGCTC-1     True  AAACCTGAGAGAGCTC-1_contig_2             True   \n",
      "2  AAACCTGAGAGAGCTC-1     True  AAACCTGAGAGAGCTC-1_contig_3             True   \n",
      "3  AAACCTGAGATCGATA-1    False  AAACCTGAGATCGATA-1_contig_1            False   \n",
      "4  AAACCTGAGCGTAGTG-1     True  AAACCTGAGCGTAGTG-1_contig_1             True   \n",
      "\n",
      "   length chain      v_gene d_gene   j_gene c_gene  ...  \\\n",
      "0     508   TRA  TRAV29/DV5    NaN   TRAJ49   TRAC  ...   \n",
      "1     499   TRB       TRBV9  TRBD1  TRBJ1-2  TRBC1  ...   \n",
      "2     470   TRA    TRAV13-1    NaN   TRAJ37   TRAC  ...   \n",
      "3     499   TRB     TRBV6-2    NaN  TRBJ2-7  TRBC2  ...   \n",
      "4     472   TRB       TRBV2    NaN  TRBJ2-7  TRBC2  ...   \n",
      "\n",
      "                                             fwr3_nt             cdr3  \\\n",
      "0  AATGAAGATGGAAGATTCACTGTTTTCTTAAACAAAAGTGCCAAGC...     CAASVTGNQFYF   \n",
      "1  AGAGCAAAAGGAAACATTCTTGAACGATTCTCCGCACAACAGTTCC...  CASSVESGGWNGYTF   \n",
      "2                                                NaN              NaN   \n",
      "3                                                NaN    CASSYLAVYEQYF   \n",
      "4  TCAGAGAAGTCTGAAATATTCGATGATCAATTCTCAGTTGAAAGGC...    CASSFSGAGEQYF   \n",
      "\n",
      "                                         cdr3_nt        fwr4  \\\n",
      "0           TGTGCAGCAAGCGTAACCGGTAACCAGTTCTATTTT  GTGTSLTVIP   \n",
      "1  TGTGCCAGCAGCGTAGAGTCAGGGGGCTGGAATGGCTACACCTTC   GSGTRLTVV   \n",
      "2                                            NaN         NaN   \n",
      "3        TGTGCCAGCAGTTACTTGGCGGTCTACGAGCAGTACTTC         NaN   \n",
      "4        TGTGCCAGCAGTTTCAGCGGGGCCGGCGAGCAGTACTTC   GPGTRLTVT   \n",
      "\n",
      "                           fwr4_nt reads umis raw_clonotype_id  \\\n",
      "0  GGGACAGGGACAAGTTTGACGGTCATTCCAA  1990    6       clonotype2   \n",
      "1     GGTTCGGGGACCAGGTTAACCGTTGTAG  4224    7       clonotype2   \n",
      "2                              NaN  1308    2              NaN   \n",
      "3                              NaN   704    1              NaN   \n",
      "4     GGGCCGGGCACCAGGCTCACGGTCACAG  1604    6    clonotype1899   \n",
      "\n",
      "            raw_consensus_id exact_subclonotype_id  \n",
      "0     clonotype2_consensus_2                   1.0  \n",
      "1     clonotype2_consensus_1                   1.0  \n",
      "2                        NaN                   NaN  \n",
      "3                        NaN                   NaN  \n",
      "4  clonotype1899_consensus_1                   1.0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061689_S3_all_contig_annotations.csv.gz → ../Data/GSE300475_RAW/GSM9061689_S3_all_contig_annotations.csv\n",
      "\n",
      "--- Preview of GSM9061689_S3_all_contig_annotations.csv ---\n",
      "              barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGAACTCGG-1    False  AAACCTGAGAACTCGG-1_contig_1            False   \n",
      "1  AAACCTGAGAGTGAGA-1     True  AAACCTGAGAGTGAGA-1_contig_1             True   \n",
      "2  AAACCTGAGAGTGAGA-1     True  AAACCTGAGAGTGAGA-1_contig_2             True   \n",
      "3  AAACCTGAGCGTAGTG-1    False  AAACCTGAGCGTAGTG-1_contig_1            False   \n",
      "4  AAACCTGAGCGTAGTG-1    False  AAACCTGAGCGTAGTG-1_contig_2            False   \n",
      "\n",
      "   length chain    v_gene d_gene   j_gene c_gene  full_length  productive  \\\n",
      "0     450   TRA  TRAV13-1    NaN   TRAJ31   TRAC         True        True   \n",
      "1     493   TRB   TRBV3-1    NaN  TRBJ2-5  TRBC2         True        True   \n",
      "2     664   TRA   TRAV8-2    NaN   TRAJ45   TRAC         True        True   \n",
      "3     463   TRA  TRAV13-2    NaN    TRAJ6   TRAC         True        True   \n",
      "4     487   TRB   TRBV4-1    NaN  TRBJ1-2  TRBC1         True        True   \n",
      "\n",
      "                cdr3                                            cdr3_nt  \\\n",
      "0         CAASRGRLMF                     TGTGCAGCAAGTAGAGGCAGACTCATGTTT   \n",
      "1      CASSQGARETQYF            TGTGCCAGCAGCCAAGGGGCGAGAGAGACCCAGTACTTC   \n",
      "2      CVVSGGGADGLTF            TGTGTTGTGTCGGGAGGAGGTGCTGACGGACTCACCTTT   \n",
      "3       CAEKGGSYIPTF               TGTGCAGAGAAAGGAGGAAGCTACATACCTACATTT   \n",
      "4  CASSPTIIWDRNYGYTF  TGCGCCAGCAGCCCTACGATTATTTGGGACAGAAACTATGGCTACA...   \n",
      "\n",
      "   reads  umis raw_clonotype_id          raw_consensus_id  \n",
      "0   5352     2              NaN                       NaN  \n",
      "1  16088     5     clonotype128  clonotype128_consensus_1  \n",
      "2   1564     1     clonotype128  clonotype128_consensus_2  \n",
      "3   2668     1              NaN                       NaN  \n",
      "4   2752     1              NaN                       NaN  \n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061665_S1_barcodes.tsv.gz → ../Data/GSE300475_RAW/GSM9061665_S1_barcodes.tsv\n",
      "\n",
      "--- Preview of GSM9061665_S1_barcodes.tsv ---\n",
      "   AAACCTGAGAAGGGTA-1\n",
      "0  AAACCTGAGACTGTAA-1\n",
      "1  AAACCTGAGCAGCGTA-1\n",
      "2  AAACCTGAGCCAACAG-1\n",
      "3  AAACCTGAGCGTGAAC-1\n",
      "4  AAACCTGAGCTACCTA-1\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061693_S7_all_contig_annotations.csv.gz → ../Data/GSE300475_RAW/GSM9061693_S7_all_contig_annotations.csv\n",
      "\n",
      "--- Preview of GSM9061693_S7_all_contig_annotations.csv ---\n",
      "              barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGCACCGCT-1     True  AAACCTGAGCACCGCT-1_contig_1             True   \n",
      "1  AAACCTGAGCACCGCT-1     True  AAACCTGAGCACCGCT-1_contig_2             True   \n",
      "2  AAACCTGAGTTGTAGA-1     True  AAACCTGAGTTGTAGA-1_contig_1             True   \n",
      "3  AAACCTGAGTTGTAGA-1     True  AAACCTGAGTTGTAGA-1_contig_2             True   \n",
      "4  AAACCTGAGTTGTAGA-1     True  AAACCTGAGTTGTAGA-1_contig_3             True   \n",
      "\n",
      "   length chain      v_gene d_gene   j_gene c_gene  ...  \\\n",
      "0     571   TRA  TRAV29/DV5    NaN   TRAJ42   TRAC  ...   \n",
      "1     580   TRB     TRBV5-1  TRBD2  TRBJ2-5  TRBC2  ...   \n",
      "2     492   TRB    TRBV25-1    NaN  TRBJ2-4  TRBC2  ...   \n",
      "3     498   TRB    TRBV20-1  TRBD1  TRBJ1-1  TRBC1  ...   \n",
      "4     484   TRA      TRAV10    NaN   TRAJ18   TRAC  ...   \n",
      "\n",
      "                                             fwr3_nt              cdr3  \\\n",
      "0  AATGAAGATGGAAGATTCACTGTCTTCTTAAACAAAAGTGCCAAGC...    CAAQNGGSQGNLIF   \n",
      "1  AGAAACAAAGGAAACTTCCCTGGTCGATTCTCAGGGCGCCAGTTCT...    CASSLALARETQYF   \n",
      "2  ACAGAGAAGGGAGATCTTTCCTCTGAGTCAACAGTCTCCAGAATAA...  CASSEKSGGAKNIQYF   \n",
      "3  ACATACGAGCAAGGCGTCGAGAAGGACAAGTTTCTCATCAACCATG...    CSASQSQGTGVAFF   \n",
      "4  AAGTCGAACGGAAGATATACAGCAACTCTGGATGCAGACACAAAGC...   CVVSDRGSTLGRLYF   \n",
      "\n",
      "                                            cdr3_nt        fwr4  \\\n",
      "0        TGTGCAGCGCAAAATGGAGGAAGCCAAGGAAATCTCATCTTT  GKGTKLSVKP   \n",
      "1        TGCGCCAGCAGCTTGGCACTAGCGCGAGAGACCCAGTACTTC   GPGTRLLVL   \n",
      "2  TGTGCCAGCAGTGAGAAGTCAGGAGGCGCCAAAAACATTCAGTACTTC   GAGTRLSVL   \n",
      "3        TGCAGTGCTAGTCAATCCCAGGGGACAGGAGTAGCTTTCTTT   GQGTRLTVV   \n",
      "4     TGTGTGGTGAGCGACAGAGGCTCAACCCTGGGGAGGCTATACTTT  GRGTQLTVWP   \n",
      "\n",
      "                           fwr4_nt  reads umis raw_clonotype_id  \\\n",
      "0  GGAAAAGGCACTAAACTCTCTGTTAAACCAA   3940    6    clonotype2535   \n",
      "1     GGGCCAGGCACGCGGCTCCTGGTGCTCG  21726   24    clonotype2535   \n",
      "2     GGCGCCGGGACCCGGCTCTCAGTGCTGG   6918    8     clonotype821   \n",
      "3     GGACAAGGCACCAGACTCACAGTTGTAG   5362    5     clonotype821   \n",
      "4  GGAAGAGGAACTCAGTTGACTGTCTGGCCTG   4190    7     clonotype821   \n",
      "\n",
      "            raw_consensus_id exact_subclonotype_id  \n",
      "0  clonotype2535_consensus_2                   1.0  \n",
      "1  clonotype2535_consensus_1                   1.0  \n",
      "2   clonotype821_consensus_1                   1.0  \n",
      "3   clonotype821_consensus_2                   1.0  \n",
      "4   clonotype821_consensus_3                   1.0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061687_S1_all_contig_annotations.csv.gz → ../Data/GSE300475_RAW/GSM9061687_S1_all_contig_annotations.csv\n",
      "\n",
      "--- Preview of GSM9061687_S1_all_contig_annotations.csv ---\n",
      "              barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGACTGTAA-1     True  AAACCTGAGACTGTAA-1_contig_1             True   \n",
      "1  AAACCTGAGACTGTAA-1     True  AAACCTGAGACTGTAA-1_contig_2             True   \n",
      "2  AAACCTGAGCCAACAG-1    False  AAACCTGAGCCAACAG-1_contig_1             True   \n",
      "3  AAACCTGAGCGTGAAC-1     True  AAACCTGAGCGTGAAC-1_contig_1             True   \n",
      "4  AAACCTGAGCGTGAAC-1     True  AAACCTGAGCGTGAAC-1_contig_2             True   \n",
      "\n",
      "   length chain      v_gene d_gene   j_gene c_gene  full_length  productive  \\\n",
      "0     493   TRB     TRBV3-1  TRBD1  TRBJ1-1  TRBC1         True        True   \n",
      "1     639   TRA  TRAV36/DV7    NaN   TRAJ53   TRAC         True        True   \n",
      "2     310   NaN         NaN    NaN   TRAJ27   TRAC        False       False   \n",
      "3     558   TRB      TRBV30    NaN  TRBJ1-2  TRBC1         True        True   \n",
      "4     503   TRA  TRAV29/DV5    NaN   TRAJ48   TRAC         True        True   \n",
      "\n",
      "              cdr3                                        cdr3_nt  reads  \\\n",
      "0    CASGTGLNTEAFF        TGTGCCAGCGGGACAGGGTTGAACACTGAAGCTTTCTTT  23844   \n",
      "1     CAVEARNYKLTF           TGTGCTGTGGAGGCCAGGAACTATAAACTGACATTT   7520   \n",
      "2              NaN                                            NaN  14352   \n",
      "3  CAWSALLGTVNGYTF  TGTGCCTGGAGTGCCCTATTAGGGACAGTAAATGGCTACACCTTC  17060   \n",
      "4    CAASAVGNEKLTF        TGTGCAGCAAGCGCCGTTGGAAATGAGAAATTAACCTTT   7084   \n",
      "\n",
      "   umis raw_clonotype_id           raw_consensus_id  \n",
      "0    13    clonotype1185  clonotype1185_consensus_1  \n",
      "1     5    clonotype1185  clonotype1185_consensus_2  \n",
      "2     4              NaN                        NaN  \n",
      "3    14    clonotype1410  clonotype1410_consensus_1  \n",
      "4     3    clonotype1410  clonotype1410_consensus_2  \n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061696_S11_all_contig_annotations.csv.gz → ../Data/GSE300475_RAW/GSM9061696_S11_all_contig_annotations.csv\n",
      "\n",
      "--- Preview of GSM9061696_S11_all_contig_annotations.csv ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGGCTAGGT-1     True  AAACCTGAGGCTAGGT-1_contig_1             True   \n",
      "1  AAACCTGAGGCTAGGT-1     True  AAACCTGAGGCTAGGT-1_contig_2             True   \n",
      "2  AAACCTGAGTGTACTC-1     True  AAACCTGAGTGTACTC-1_contig_1             True   \n",
      "3  AAACCTGAGTGTACTC-1     True  AAACCTGAGTGTACTC-1_contig_2             True   \n",
      "4  AAACCTGAGTGTACTC-1     True  AAACCTGAGTGTACTC-1_contig_3             True   \n",
      "\n",
      "   length chain    v_gene d_gene   j_gene c_gene  ...  \\\n",
      "0     497   TRB   TRBV7-6  TRBD1  TRBJ2-1  TRBC2  ...   \n",
      "1     459   TRA  TRAV13-1    NaN   TRAJ33   TRAC  ...   \n",
      "2     499   TRB   TRBV3-1    NaN  TRBJ1-6  TRBC1  ...   \n",
      "3     481   TRA    TRAV10    NaN   TRAJ12   TRAC  ...   \n",
      "4     493   TRA   TRAV9-2    NaN   TRAJ33   TRAC  ...   \n",
      "\n",
      "                                             fwr3_nt             cdr3  \\\n",
      "0  CAAGACAAATCAGGGCTGCCCAATGATCGGTTCTCTGCAGAGAGGC...   CASSLGTGYNEQFF   \n",
      "1  AAGAAAGACCAACGAATTGCTGTTACATTGAACAAGACAGCCAAAC...    CAASRGSNYQLIW   \n",
      "2  ATTATAAATGAAACAGTTCCAAATCGCTTCTCACCTAAATCTCCAG...  CASSQAGGQNSPLHF   \n",
      "3  AAGTCGAACGGAAGATATACAGCAACTCTGGATGCAGACACAAAGC...   CVVSGMDSSYKLIF   \n",
      "4                                                NaN              NaN   \n",
      "\n",
      "                                         cdr3_nt        fwr4  \\\n",
      "0     TGTGCCAGCAGCTTAGGGACAGGATACAATGAGCAGTTCTTC   GPGTRLTVL   \n",
      "1        TGTGCAGCAAGTCGCGGTAGCAACTATCAGTTAATCTGG  GAGTKLIIKP   \n",
      "2  TGTGCCAGCAGCCAAGCTGGGGGACAGAATTCACCCCTCCACTTT   GNGTRLTVT   \n",
      "3     TGTGTGGTGAGCGGGATGGATAGCAGCTATAAATTGATCTTC  GSGTRLLVRP   \n",
      "4                                            NaN         NaN   \n",
      "\n",
      "                           fwr4_nt  reads umis raw_clonotype_id  \\\n",
      "0     GGGCCAGGGACACGGCTCACCGTGCTAG  19678   21    clonotype2273   \n",
      "1  GGCGCTGGGACCAAGCTAATTATAAAGCCAG   2996    3    clonotype2273   \n",
      "2     GGGAATGGGACCAGGCTCACTGTGACAG   7554    9    clonotype2503   \n",
      "3  GGGAGTGGGACCAGACTGCTGGTCAGGCCTG   5988    6    clonotype2503   \n",
      "4                              NaN   1292    1              NaN   \n",
      "\n",
      "            raw_consensus_id exact_subclonotype_id  \n",
      "0  clonotype2273_consensus_1                   1.0  \n",
      "1  clonotype2273_consensus_2                   1.0  \n",
      "2  clonotype2503_consensus_1                   1.0  \n",
      "3  clonotype2503_consensus_2                   1.0  \n",
      "4                        NaN                   NaN  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061668_S4_barcodes.tsv.gz → ../Data/GSE300475_RAW/GSM9061668_S4_barcodes.tsv\n",
      "\n",
      "--- Preview of GSM9061668_S4_barcodes.tsv ---\n",
      "   AAACCTGAGATATGCA-1\n",
      "0  AAACCTGAGCGTTCCG-1\n",
      "1  AAACCTGAGGCTATCT-1\n",
      "2  AAACCTGAGGGTCTCC-1\n",
      "3  AAACCTGAGTATCGAA-1\n",
      "4  AAACCTGCAAGCTGGA-1\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061674_S10_matrix.mtx.gz → ../Data/GSE300475_RAW/GSM9061674_S10_matrix.mtx\n",
      "\n",
      "--- Preview of GSM9061674_S10_matrix.mtx ---\n",
      "First 5 non-zero entries:\n",
      "Row: 24, Col: 0, Value: 1\n",
      "Row: 44, Col: 0, Value: 1\n",
      "Row: 60, Col: 0, Value: 1\n",
      "Row: 62, Col: 0, Value: 1\n",
      "Row: 170, Col: 0, Value: 9\n",
      "\n",
      "Matrix shape: (36604, 9704), NNZ (non-zero elements): 17591287\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061675_S11_barcodes.tsv.gz → ../Data/GSE300475_RAW/GSM9061675_S11_barcodes.tsv\n",
      "\n",
      "--- Preview of GSM9061675_S11_barcodes.tsv ---\n",
      "   AAACCTGAGATGCCTT-1\n",
      "0  AAACCTGAGGCTAGGT-1\n",
      "1  AAACCTGAGTGTACTC-1\n",
      "2  AAACCTGCAGTATGCT-1\n",
      "3  AAACCTGCATCCTTGC-1\n",
      "4  AAACCTGGTATAGTAG-1\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061669_S5_features.tsv.gz → ../Data/GSE300475_RAW/GSM9061669_S5_features.tsv\n",
      "\n",
      "--- Preview of GSM9061669_S5_features.tsv ---\n",
      "   ENSG00000243485 MIR1302-2HG  Gene Expression\n",
      "0  ENSG00000237613     FAM138A  Gene Expression\n",
      "1  ENSG00000186092       OR4F5  Gene Expression\n",
      "2  ENSG00000238009  AL627309.1  Gene Expression\n",
      "3  ENSG00000239945  AL627309.3  Gene Expression\n",
      "4  ENSG00000239906  AL627309.2  Gene Expression\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061673_S9_features.tsv.gz → ../Data/GSE300475_RAW/GSM9061673_S9_features.tsv\n",
      "\n",
      "--- Preview of GSM9061673_S9_features.tsv ---\n",
      "   ENSG00000243485 MIR1302-2HG  Gene Expression\n",
      "0  ENSG00000237613     FAM138A  Gene Expression\n",
      "1  ENSG00000186092       OR4F5  Gene Expression\n",
      "2  ENSG00000238009  AL627309.1  Gene Expression\n",
      "3  ENSG00000239945  AL627309.3  Gene Expression\n",
      "4  ENSG00000239906  AL627309.2  Gene Expression\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061668_S4_matrix.mtx.gz → ../Data/GSE300475_RAW/GSM9061668_S4_matrix.mtx\n",
      "\n",
      "--- Preview of GSM9061668_S4_matrix.mtx ---\n",
      "First 5 non-zero entries:\n",
      "Row: 29, Col: 0, Value: 2\n",
      "Row: 44, Col: 0, Value: 4\n",
      "Row: 59, Col: 0, Value: 1\n",
      "Row: 60, Col: 0, Value: 1\n",
      "Row: 62, Col: 0, Value: 2\n",
      "\n",
      "Matrix shape: (36604, 8723), NNZ (non-zero elements): 10249656\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061671_S7_barcodes.tsv.gz → ../Data/GSE300475_RAW/GSM9061671_S7_barcodes.tsv\n",
      "\n",
      "--- Preview of GSM9061671_S7_barcodes.tsv ---\n",
      "   AAACCTGAGACACGAC-1\n",
      "0  AAACCTGAGCACAGGT-1\n",
      "1  AAACCTGAGCACCGCT-1\n",
      "2  AAACCTGAGTCGTTTG-1\n",
      "3  AAACCTGAGTTGTAGA-1\n",
      "4  AAACCTGGTACGCACC-1\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061669_S5_barcodes.tsv.gz → ../Data/GSE300475_RAW/GSM9061669_S5_barcodes.tsv\n",
      "\n",
      "--- Preview of GSM9061669_S5_barcodes.tsv ---\n",
      "   AAACCTGGTTCTGGTA-1\n",
      "0  AAACCTGTCTCACATT-1\n",
      "1  AAACGGGAGCGATGAC-1\n",
      "2  AAACGGGAGTGGGTTG-1\n",
      "3  AAACGGGAGTGTCCAT-1\n",
      "4  AAACGGGGTACCGTTA-1\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061668_S4_features.tsv.gz → ../Data/GSE300475_RAW/GSM9061668_S4_features.tsv\n",
      "\n",
      "--- Preview of GSM9061668_S4_features.tsv ---\n",
      "   ENSG00000243485 MIR1302-2HG  Gene Expression\n",
      "0  ENSG00000237613     FAM138A  Gene Expression\n",
      "1  ENSG00000186092       OR4F5  Gene Expression\n",
      "2  ENSG00000238009  AL627309.1  Gene Expression\n",
      "3  ENSG00000239945  AL627309.3  Gene Expression\n",
      "4  ENSG00000239906  AL627309.2  Gene Expression\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061674_S10_features.tsv.gz → ../Data/GSE300475_RAW/GSM9061674_S10_features.tsv\n",
      "\n",
      "--- Preview of GSM9061674_S10_features.tsv ---\n",
      "   ENSG00000243485 MIR1302-2HG  Gene Expression\n",
      "0  ENSG00000237613     FAM138A  Gene Expression\n",
      "1  ENSG00000186092       OR4F5  Gene Expression\n",
      "2  ENSG00000238009  AL627309.1  Gene Expression\n",
      "3  ENSG00000239945  AL627309.3  Gene Expression\n",
      "4  ENSG00000239906  AL627309.2  Gene Expression\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061671_S7_features.tsv.gz → ../Data/GSE300475_RAW/GSM9061671_S7_features.tsv\n",
      "\n",
      "--- Preview of GSM9061671_S7_features.tsv ---\n",
      "   ENSG00000243485 MIR1302-2HG  Gene Expression\n",
      "0  ENSG00000237613     FAM138A  Gene Expression\n",
      "1  ENSG00000186092       OR4F5  Gene Expression\n",
      "2  ENSG00000238009  AL627309.1  Gene Expression\n",
      "3  ENSG00000239945  AL627309.3  Gene Expression\n",
      "4  ENSG00000239906  AL627309.2  Gene Expression\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061667_S3_barcodes.tsv.gz → ../Data/GSE300475_RAW/GSM9061667_S3_barcodes.tsv\n",
      "\n",
      "--- Preview of GSM9061667_S3_barcodes.tsv ---\n",
      "   AAACCTGAGAAGATTC-1\n",
      "0  AAACCTGAGAGTGAGA-1\n",
      "1  AAACCTGAGTTCCACA-1\n",
      "2  AAACCTGCAAAGCAAT-1\n",
      "3  AAACCTGCACCGTTGG-1\n",
      "4  AAACCTGGTAGAGCTG-1\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061666_S2_matrix.mtx.gz → ../Data/GSE300475_RAW/GSM9061666_S2_matrix.mtx\n",
      "\n",
      "--- Preview of GSM9061666_S2_matrix.mtx ---\n",
      "First 5 non-zero entries:\n",
      "Row: 170, Col: 0, Value: 5\n",
      "Row: 186, Col: 0, Value: 1\n",
      "Row: 187, Col: 0, Value: 1\n",
      "Row: 219, Col: 0, Value: 1\n",
      "Row: 407, Col: 0, Value: 1\n",
      "\n",
      "Matrix shape: (36604, 9069), NNZ (non-zero elements): 10089766\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061691_S5_all_contig_annotations.csv.gz → ../Data/GSE300475_RAW/GSM9061691_S5_all_contig_annotations.csv\n",
      "\n",
      "--- Preview of GSM9061691_S5_all_contig_annotations.csv ---\n",
      "              barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGTCTCACATT-1     True  AAACCTGTCTCACATT-1_contig_1             True   \n",
      "1  AAACCTGTCTCACATT-1     True  AAACCTGTCTCACATT-1_contig_2             True   \n",
      "2  AAACGGGAGCGATGAC-1     True  AAACGGGAGCGATGAC-1_contig_1             True   \n",
      "3  AAACGGGAGCGATGAC-1     True  AAACGGGAGCGATGAC-1_contig_2             True   \n",
      "4  AAACGGGAGTGGGTTG-1     True  AAACGGGAGTGGGTTG-1_contig_1             True   \n",
      "\n",
      "   length chain    v_gene d_gene   j_gene c_gene  full_length  productive  \\\n",
      "0     495   TRB    TRBV27  TRBD1  TRBJ2-7  TRBC2         True        True   \n",
      "1     524   TRA    TRAV27    NaN   TRAJ36   TRAC         True        True   \n",
      "2     508   TRB   TRBV7-2    NaN  TRBJ2-2  TRBC2         True        True   \n",
      "3     367   TRA  TRAV26-2    NaN   TRAJ45   TRAC         True       False   \n",
      "4     509   TRB   TRBV5-8    NaN  TRBJ1-2  TRBC1         True        True   \n",
      "\n",
      "                cdr3                                            cdr3_nt  \\\n",
      "0    CASSFADRGSYEQYF      TGTGCCAGCAGTTTCGCGGACAGGGGGTCCTACGAGCAGTACTTC   \n",
      "1       CAGRPGANNLFF               TGTGCAGGAAGACCTGGGGCAAACAACCTCTTCTTT   \n",
      "2  CASSRSGRALSTGELFF  TGTGCCAGCAGCCGTAGCGGGAGGGCCCTGAGCACCGGGGAGCTGT...   \n",
      "3    CILRYSGGGADGLTF      TGCATCCTGAGGTATTCAGGAGGAGGTGCTGACGGACTCACCTTT   \n",
      "4   CASSDDAGQARYGYTF   TGTGCCAGCAGCGATGACGCTGGACAGGCCCGCTATGGCTACACCTTC   \n",
      "\n",
      "   reads  umis raw_clonotype_id           raw_consensus_id  \n",
      "0  19168     5     clonotype648   clonotype648_consensus_1  \n",
      "1  28324     4     clonotype648   clonotype648_consensus_2  \n",
      "2  63456    12    clonotype1069  clonotype1069_consensus_1  \n",
      "3  12270     1              NaN                        NaN  \n",
      "4  16276     6     clonotype810   clonotype810_consensus_1  \n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061675_S11_matrix.mtx.gz → ../Data/GSE300475_RAW/GSM9061675_S11_matrix.mtx\n",
      "\n",
      "--- Preview of GSM9061675_S11_matrix.mtx ---\n",
      "First 5 non-zero entries:\n",
      "Row: 48, Col: 0, Value: 1\n",
      "Row: 59, Col: 0, Value: 1\n",
      "Row: 60, Col: 0, Value: 1\n",
      "Row: 86, Col: 0, Value: 1\n",
      "Row: 97, Col: 0, Value: 1\n",
      "\n",
      "Matrix shape: (36604, 9330), NNZ (non-zero elements): 16957155\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061671_S7_matrix.mtx.gz → ../Data/GSE300475_RAW/GSM9061671_S7_matrix.mtx\n",
      "\n",
      "--- Preview of GSM9061671_S7_matrix.mtx ---\n",
      "First 5 non-zero entries:\n",
      "Row: 44, Col: 0, Value: 2\n",
      "Row: 57, Col: 0, Value: 1\n",
      "Row: 62, Col: 0, Value: 4\n",
      "Row: 73, Col: 0, Value: 1\n",
      "Row: 76, Col: 0, Value: 1\n",
      "\n",
      "Matrix shape: (36604, 9330), NNZ (non-zero elements): 15456744\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061672_S8_barcodes.tsv.gz → ../Data/GSE300475_RAW/GSM9061672_S8_barcodes.tsv\n",
      "\n",
      "--- Preview of GSM9061672_S8_barcodes.tsv ---\n",
      "   AAACCTGAGAAGGTTT-1\n",
      "0  AAACCTGAGACCCACC-1\n",
      "1  AAACCTGAGAGCAATT-1\n",
      "2  AAACCTGAGAGTACCG-1\n",
      "3  AAACCTGAGCAGGTCA-1\n",
      "4  AAACCTGAGCGGCTTC-1\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061665_S1_features.tsv.gz → ../Data/GSE300475_RAW/GSM9061665_S1_features.tsv\n",
      "\n",
      "--- Preview of GSM9061665_S1_features.tsv ---\n",
      "   ENSG00000243485 MIR1302-2HG  Gene Expression\n",
      "0  ENSG00000237613     FAM138A  Gene Expression\n",
      "1  ENSG00000186092       OR4F5  Gene Expression\n",
      "2  ENSG00000238009  AL627309.1  Gene Expression\n",
      "3  ENSG00000239945  AL627309.3  Gene Expression\n",
      "4  ENSG00000239906  AL627309.2  Gene Expression\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061670_S6_barcodes.tsv.gz → ../Data/GSE300475_RAW/GSM9061670_S6_barcodes.tsv\n",
      "\n",
      "--- Preview of GSM9061670_S6_barcodes.tsv ---\n",
      "   AAACCTGAGACTAGAT-1\n",
      "0  AAACCTGAGAGGGCTT-1\n",
      "1  AAACCTGAGGACGAAA-1\n",
      "2  AAACCTGAGGGCACTA-1\n",
      "3  AAACCTGAGTCCGTAT-1\n",
      "4  AAACCTGAGTTTAGGA-1\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061688_S2_all_contig_annotations.csv.gz → ../Data/GSE300475_RAW/GSM9061688_S2_all_contig_annotations.csv\n",
      "\n",
      "--- Preview of GSM9061688_S2_all_contig_annotations.csv ---\n",
      "              barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGACCTTTG-1    False  AAACCTGAGACCTTTG-1_contig_1            False   \n",
      "1  AAACCTGAGATCCTGT-1    False  AAACCTGAGATCCTGT-1_contig_1            False   \n",
      "2  AAACCTGAGCCGGTAA-1    False  AAACCTGAGCCGGTAA-1_contig_1            False   \n",
      "3  AAACCTGAGCTGCAAG-1    False  AAACCTGAGCTGCAAG-1_contig_1            False   \n",
      "4  AAACCTGAGGACGAAA-1     True  AAACCTGAGGACGAAA-1_contig_1             True   \n",
      "\n",
      "   length chain    v_gene d_gene   j_gene c_gene  full_length  productive  \\\n",
      "0     536   TRB    TRBV18    NaN  TRBJ1-1  TRBC1         True        True   \n",
      "1     509   TRB   TRBV7-9    NaN  TRBJ1-4  TRBC1         True        True   \n",
      "2     486   TRA  TRAV12-1    NaN   TRAJ27   TRAC         True        True   \n",
      "3     482   TRB  TRBV10-3    NaN  TRBJ2-1  TRBC2         True        True   \n",
      "4     541   TRB  TRBV11-2    NaN  TRBJ2-3  TRBC2         True        True   \n",
      "\n",
      "                cdr3                                            cdr3_nt  \\\n",
      "0    CASSERSPLTTEAFF      TGTGCCAGCTCAGAAAGGAGCCCGCTGACCACTGAAGCTTTCTTT   \n",
      "1   CASSSIRNGDNEKLFF   TGTGCCAGCAGCTCTATTCGTAACGGCGATAATGAAAAACTGTTTTTT   \n",
      "2        CVVNNAGKSTF                  TGTGTGGTGAACAATGCAGGCAAATCAACCTTT   \n",
      "3  CAISELPSGRAYNELFF  TGTGCCATCAGTGAGTTACCAAGCGGGAGGGCTTACAATGAGCTGT...   \n",
      "4    CASRPDGGFTDTQYF      TGTGCCAGCAGGCCCGACGGGGGTTTCACAGATACGCAGTATTTT   \n",
      "\n",
      "   reads  umis raw_clonotype_id        raw_consensus_id  \n",
      "0    960     1              NaN                     NaN  \n",
      "1   5128     2              NaN                     NaN  \n",
      "2   3312     1              NaN                     NaN  \n",
      "3   3116     2              NaN                     NaN  \n",
      "4  10316     4       clonotype2  clonotype2_consensus_1  \n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061670_S6_features.tsv.gz → ../Data/GSE300475_RAW/GSM9061670_S6_features.tsv\n",
      "\n",
      "--- Preview of GSM9061670_S6_features.tsv ---\n",
      "   ENSG00000243485 MIR1302-2HG  Gene Expression\n",
      "0  ENSG00000237613     FAM138A  Gene Expression\n",
      "1  ENSG00000186092       OR4F5  Gene Expression\n",
      "2  ENSG00000238009  AL627309.1  Gene Expression\n",
      "3  ENSG00000239945  AL627309.3  Gene Expression\n",
      "4  ENSG00000239906  AL627309.2  Gene Expression\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061673_S9_barcodes.tsv.gz → ../Data/GSE300475_RAW/GSM9061673_S9_barcodes.tsv\n",
      "\n",
      "--- Preview of GSM9061673_S9_barcodes.tsv ---\n",
      "   AAACCTGAGAGAGCTC-1\n",
      "0  AAACCTGAGCGTAGTG-1\n",
      "1  AAACCTGAGCGTGAGT-1\n",
      "2  AAACCTGAGGCATGTG-1\n",
      "3  AAACCTGAGTCACGCC-1\n",
      "4  AAACCTGAGTCCCACG-1\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061666_S2_features.tsv.gz → ../Data/GSE300475_RAW/GSM9061666_S2_features.tsv\n",
      "\n",
      "--- Preview of GSM9061666_S2_features.tsv ---\n",
      "   ENSG00000243485 MIR1302-2HG  Gene Expression\n",
      "0  ENSG00000237613     FAM138A  Gene Expression\n",
      "1  ENSG00000186092       OR4F5  Gene Expression\n",
      "2  ENSG00000238009  AL627309.1  Gene Expression\n",
      "3  ENSG00000239945  AL627309.3  Gene Expression\n",
      "4  ENSG00000239906  AL627309.2  Gene Expression\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061670_S6_matrix.mtx.gz → ../Data/GSE300475_RAW/GSM9061670_S6_matrix.mtx\n",
      "\n",
      "--- Preview of GSM9061670_S6_matrix.mtx ---\n",
      "First 5 non-zero entries:\n",
      "Row: 28, Col: 0, Value: 1\n",
      "Row: 29, Col: 0, Value: 2\n",
      "Row: 30, Col: 0, Value: 1\n",
      "Row: 62, Col: 0, Value: 1\n",
      "Row: 82, Col: 0, Value: 1\n",
      "\n",
      "Matrix shape: (36604, 10398), NNZ (non-zero elements): 17214846\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061672_S8_matrix.mtx.gz → ../Data/GSE300475_RAW/GSM9061672_S8_matrix.mtx\n",
      "\n",
      "--- Preview of GSM9061672_S8_matrix.mtx ---\n",
      "First 5 non-zero entries:\n",
      "Row: 30, Col: 0, Value: 1\n",
      "Row: 60, Col: 0, Value: 1\n",
      "Row: 62, Col: 0, Value: 2\n",
      "Row: 86, Col: 0, Value: 1\n",
      "Row: 111, Col: 0, Value: 1\n",
      "\n",
      "Matrix shape: (36604, 12832), NNZ (non-zero elements): 21241636\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061667_S3_matrix.mtx.gz → ../Data/GSE300475_RAW/GSM9061667_S3_matrix.mtx\n",
      "\n",
      "--- Preview of GSM9061667_S3_matrix.mtx ---\n",
      "First 5 non-zero entries:\n",
      "Row: 43, Col: 0, Value: 1\n",
      "Row: 44, Col: 0, Value: 1\n",
      "Row: 59, Col: 0, Value: 2\n",
      "Row: 73, Col: 0, Value: 1\n",
      "Row: 170, Col: 0, Value: 5\n",
      "\n",
      "Matrix shape: (36604, 7358), NNZ (non-zero elements): 7696309\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061692_S6_all_contig_annotations.csv.gz → ../Data/GSE300475_RAW/GSM9061692_S6_all_contig_annotations.csv\n",
      "\n",
      "--- Preview of GSM9061692_S6_all_contig_annotations.csv ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGACGCAAC-1    False  AAACCTGAGACGCAAC-1_contig_1            False   \n",
      "1  AAACCTGAGACTAGAT-1     True  AAACCTGAGACTAGAT-1_contig_1             True   \n",
      "2  AAACCTGAGACTAGAT-1     True  AAACCTGAGACTAGAT-1_contig_2             True   \n",
      "3  AAACCTGAGACTAGAT-1     True  AAACCTGAGACTAGAT-1_contig_3             True   \n",
      "4  AAACCTGAGGGCACTA-1     True  AAACCTGAGGGCACTA-1_contig_1             True   \n",
      "\n",
      "   length chain   v_gene d_gene   j_gene c_gene  ...  \\\n",
      "0     500   TRB  TRBV5-5    NaN  TRBJ1-2  TRBC1  ...   \n",
      "1     493   TRA  TRAV1-1    NaN   TRAJ37   TRAC  ...   \n",
      "2     638   TRB   TRBV19    NaN  TRBJ2-7  TRBC2  ...   \n",
      "3     493   TRB  TRBV7-3    NaN  TRBJ2-7  TRBC2  ...   \n",
      "4     486   TRB   TRBV27    NaN  TRBJ2-3  TRBC2  ...   \n",
      "\n",
      "                                             fwr3_nt              cdr3  \\\n",
      "0                                                NaN     CASSLDPSYGYTF   \n",
      "1  GAGGAGACAGGTCGTTTTTCTTCATTCCTTAGTCGCTCTGATAGTT...    CAVKVSGNTGKLIF   \n",
      "2  TTTCAGAAAGGAGATATAGCTGAAGGGTACAGCGTCTCTCGGGAGA...  CASSKNQNRVGYEQYF   \n",
      "3                                                NaN   CQQLSVPGSRGEQYF   \n",
      "4  ACTGATAAGGGAGATGTTCCTGAAGGGTACAAAGTCTCTCGAAAAG...      CASSSGTDTQYF   \n",
      "\n",
      "                                            cdr3_nt        fwr4  \\\n",
      "0           TGTGCCAGCAGCTTGGACCCCAGCTATGGCTACACCTTC         NaN   \n",
      "1        TGCGCTGTGAAGGTCTCTGGCAACACAGGCAAACTAATCTTT  GQGTTLQVKP   \n",
      "2  TGTGCCAGTAGTAAAAACCAAAACAGGGTTGGCTACGAGCAGTACTTC   GPGTRLTVT   \n",
      "3     TGCCAGCAGCTCTCCGTTCCCGGGTCCAGAGGCGAGCAGTACTTC         NaN   \n",
      "4              TGTGCCAGCAGTTCCGGCACAGATACGCAGTATTTT   GPGTRLTVL   \n",
      "\n",
      "                           fwr4_nt reads umis raw_clonotype_id  \\\n",
      "0                              NaN   416    1              NaN   \n",
      "1  GGGCAAGGGACAACTTTACAAGTAAAACCAG  3746    6    clonotype3714   \n",
      "2     GGGCCGGGCACCAGGCTCACGGTCACAG  3390    4    clonotype3714   \n",
      "3                              NaN  4088    3              NaN   \n",
      "4     GGCCCAGGCACCCGGCTGACAGTGCTCG  8804    7     clonotype910   \n",
      "\n",
      "            raw_consensus_id exact_subclonotype_id  \n",
      "0                        NaN                   NaN  \n",
      "1  clonotype3714_consensus_2                   1.0  \n",
      "2  clonotype3714_consensus_1                   1.0  \n",
      "3                        NaN                   NaN  \n",
      "4   clonotype910_consensus_1                   1.0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061672_S8_features.tsv.gz → ../Data/GSE300475_RAW/GSM9061672_S8_features.tsv\n",
      "\n",
      "--- Preview of GSM9061672_S8_features.tsv ---\n",
      "   ENSG00000243485 MIR1302-2HG  Gene Expression\n",
      "0  ENSG00000237613     FAM138A  Gene Expression\n",
      "1  ENSG00000186092       OR4F5  Gene Expression\n",
      "2  ENSG00000238009  AL627309.1  Gene Expression\n",
      "3  ENSG00000239945  AL627309.3  Gene Expression\n",
      "4  ENSG00000239906  AL627309.2  Gene Expression\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061667_S3_features.tsv.gz → ../Data/GSE300475_RAW/GSM9061667_S3_features.tsv\n",
      "\n",
      "--- Preview of GSM9061667_S3_features.tsv ---\n",
      "   ENSG00000243485 MIR1302-2HG  Gene Expression\n",
      "0  ENSG00000237613     FAM138A  Gene Expression\n",
      "1  ENSG00000186092       OR4F5  Gene Expression\n",
      "2  ENSG00000238009  AL627309.1  Gene Expression\n",
      "3  ENSG00000239945  AL627309.3  Gene Expression\n",
      "4  ENSG00000239906  AL627309.2  Gene Expression\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061665_S1_matrix.mtx.gz → ../Data/GSE300475_RAW/GSM9061665_S1_matrix.mtx\n",
      "\n",
      "--- Preview of GSM9061665_S1_matrix.mtx ---\n",
      "First 5 non-zero entries:\n",
      "Row: 24, Col: 0, Value: 1\n",
      "Row: 104, Col: 0, Value: 1\n",
      "Row: 145, Col: 0, Value: 1\n",
      "Row: 182, Col: 0, Value: 1\n",
      "Row: 219, Col: 0, Value: 1\n",
      "\n",
      "Matrix shape: (36604, 8931), NNZ (non-zero elements): 10901038\n",
      "Decompressing ../Data/GSE300475_RAW/GSM9061690_S4_all_contig_annotations.csv.gz → ../Data/GSE300475_RAW/GSM9061690_S4_all_contig_annotations.csv\n",
      "\n",
      "--- Preview of GSM9061690_S4_all_contig_annotations.csv ---\n",
      "              barcode  is_cell                    contig_id  high_confidence  \\\n",
      "0  AAACCTGAGGCTATCT-1     True  AAACCTGAGGCTATCT-1_contig_1             True   \n",
      "1  AAACCTGAGGGTCTCC-1     True  AAACCTGAGGGTCTCC-1_contig_1             True   \n",
      "2  AAACCTGAGGGTCTCC-1     True  AAACCTGAGGGTCTCC-1_contig_2             True   \n",
      "3  AAACCTGAGTTCCACA-1    False  AAACCTGAGTTCCACA-1_contig_1            False   \n",
      "4  AAACCTGCAAGCTGGA-1     True  AAACCTGCAAGCTGGA-1_contig_1             True   \n",
      "\n",
      "   length chain        v_gene d_gene   j_gene c_gene  full_length  productive  \\\n",
      "0     682   TRA       TRAV8-3    NaN   TRAJ48   TRAC         True        True   \n",
      "1     505   TRB        TRBV28    NaN  TRBJ2-7  TRBC2         True        True   \n",
      "2     474   TRA  TRAV38-2/DV8    NaN   TRAJ49   TRAC         True        True   \n",
      "3     478   TRA        TRAV17    NaN   TRAJ17   TRAC         True        True   \n",
      "4     529   TRB        TRBV13  TRBD1  TRBJ2-7  TRBC2         True        True   \n",
      "\n",
      "              cdr3                                        cdr3_nt  reads  \\\n",
      "0    CAVGVVGNEKLTF        TGTGCTGTGGGTGTTGTTGGAAATGAGAAATTAACCTTT   6016   \n",
      "1  CASSGSGRVPYEQYF  TGTGCCAGCAGTGGTAGCGGGAGGGTCCCCTACGAGCAGTACTTC  13792   \n",
      "2     CAYRFTGNQFYF           TGTGCTTATAGGTTCACCGGTAACCAGTTCTATTTT  23976   \n",
      "3  CATVTIKAAGNKLTF  TGTGCTACGGTTACGATCAAAGCTGCAGGCAACAAGCTAACTTTT   5492   \n",
      "4  CASSPFGTGPNEQYF  TGTGCCAGCAGCCCCTTCGGGACAGGGCCTAACGAGCAGTACTTC  20324   \n",
      "\n",
      "   umis raw_clonotype_id          raw_consensus_id  \n",
      "0     2     clonotype862  clonotype862_consensus_1  \n",
      "1     6     clonotype157  clonotype157_consensus_1  \n",
      "2     5     clonotype157  clonotype157_consensus_2  \n",
      "3     1              NaN                       NaN  \n",
      "4    10       clonotype3    clonotype3_consensus_2  \n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from scipy.io import mmread\n",
    "\n",
    "def decompress_gz_file(gz_path, output_dir):\n",
    "    \"\"\"\n",
    "    Decompress a .gz file to the specified output directory.\n",
    "    \"\"\"\n",
    "    output_path = os.path.join(output_dir, Path(gz_path).stem)\n",
    "    print(f\"Decompressing {gz_path} → {output_path}\")\n",
    "    try:\n",
    "        with gzip.open(gz_path, 'rb') as f_in, open(output_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to decompress {gz_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def preview_file(file_path):\n",
    "    \"\"\"\n",
    "    Display the first few lines of a decompressed file, based on its extension.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Preview of {os.path.basename(file_path)} ---\")\n",
    "    try:\n",
    "        if file_path.endswith(\".tsv\"):\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "            print(df.head())\n",
    "        elif file_path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(df.head())\n",
    "        elif file_path.endswith(\".mtx\"):\n",
    "          matrix = mmread(file_path).tocoo()\n",
    "          print(\"First 5 non-zero entries:\")\n",
    "          for i in range(min(5, len(matrix.data))):\n",
    "              print(f\"Row: {matrix.row[i]}, Col: {matrix.col[i]}, Value: {matrix.data[i]}\")\n",
    "          print(f\"\\nMatrix shape: {matrix.shape}, NNZ (non-zero elements): {matrix.nnz}\")\n",
    "        else:\n",
    "            print(\"Unsupported file type for preview.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not preview {file_path}: {e}\")\n",
    "\n",
    "extract_dir = os.path.join(download_dir, \"GSE300475_RAW\")\n",
    "\n",
    "for root, _, files in os.walk(extract_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".gz\"):\n",
    "            gz_file_path = os.path.join(root, file)\n",
    "            decompressed_path = decompress_gz_file(gz_file_path, root)\n",
    "            if decompressed_path:\n",
    "                preview_file(decompressed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc925283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:54:31.973842Z",
     "iopub.status.busy": "2025-11-12T03:54:31.972867Z",
     "iopub.status.idle": "2025-11-12T03:54:32.980923Z",
     "shell.execute_reply": "2025-11-12T03:54:32.979852Z"
    },
    "papermill": {
     "duration": 1.020864,
     "end_time": "2025-11-12T03:54:32.982618",
     "exception": false,
     "start_time": "2025-11-12T03:54:31.961754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM9061695_S10_all_contig_annotations.csv: 17759 rows\n",
      "GSM9061692_S6_all_contig_annotations.csv: 18505 rows\n",
      "GSM9061689_S3_all_contig_annotations.csv: 18336 rows\n",
      "GSM9061687_S1_all_contig_annotations.csv: 16782 rows\n",
      "GSM9061696_S11_all_contig_annotations.csv: 13790 rows\n",
      "GSM9061690_S4_all_contig_annotations.csv: 12694 rows\n",
      "GSM9061691_S5_all_contig_annotations.csv: 4074 rows\n",
      "GSM9061694_S9_all_contig_annotations.csv: 18147 rows\n",
      "GSM9061693_S7_all_contig_annotations.csv: 17601 rows\n",
      "GSM9061688_S2_all_contig_annotations.csv: 24445 rows\n",
      "\n",
      "Total rows in all contig annotation files: 162133\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Find all \"all_contig_annotations.csv\" files in the extracted directory and sum their lengths (number of rows)\n",
    "\n",
    "all_contig_files = glob.glob(os.path.join(extract_dir, \"*_all_contig_annotations.csv\"))\n",
    "total_rows = 0\n",
    "\n",
    "for file in all_contig_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        num_rows = len(df)\n",
    "        print(f\"{os.path.basename(file)}: {num_rows} rows\")\n",
    "        total_rows += num_rows\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read {file}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal rows in all contig annotation files: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a97c46",
   "metadata": {
    "papermill": {
     "duration": 0.010114,
     "end_time": "2025-11-12T03:54:33.003920",
     "exception": false,
     "start_time": "2025-11-12T03:54:32.993806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Load Sample Metadata\n",
    "\n",
    "First, we load the metadata from the `GSE300475_feature_ref.xlsx` file. This file contains the crucial mapping between GEO sample IDs, patient IDs, timepoints, and treatment response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1352603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:54:33.025773Z",
     "iopub.status.busy": "2025-11-12T03:54:33.025084Z",
     "iopub.status.idle": "2025-11-12T03:55:17.852196Z",
     "shell.execute_reply": "2025-11-12T03:55:17.851045Z"
    },
    "papermill": {
     "duration": 44.840338,
     "end_time": "2025-11-12T03:55:17.854252",
     "exception": false,
     "start_time": "2025-11-12T03:54:33.013914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scanpy\n",
      "  Downloading scanpy-1.11.5-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Collecting anndata>=0.8 (from scanpy)\n",
      "  Downloading anndata-0.12.6-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: h5py>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.14.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.5.2)\n",
      "Collecting legacy-api-wrap>=1.4.1 (from scanpy)\n",
      "  Downloading legacy_api_wrap-1.5-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting matplotlib>=3.7.5 (from scanpy)\n",
      "  Downloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from scanpy) (8.4.0)\n",
      "Requirement already satisfied: networkx>=2.7.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.5)\n",
      "Requirement already satisfied: numba!=0.62.0rc1,>=0.57.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.60.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (25.0)\n",
      "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.0.1)\n",
      "Requirement already satisfied: pynndescent>=0.5.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.13)\n",
      "Requirement already satisfied: scikit-learn>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.15.3)\n",
      "Collecting seaborn>=0.13.2 (from scanpy)\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting session-info2 (from scanpy)\n",
      "  Downloading session_info2-0.2.3-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: statsmodels>=0.14.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.14.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.15.0)\n",
      "Requirement already satisfied: umap-learn>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.9.post2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\n",
      "Collecting array-api-compat>=1.7.1 (from anndata>=0.8->scanpy)\n",
      "  Downloading array_api_compat-1.12.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting zarr!=3.0.*,>=2.18.7 (from anndata>=0.8->scanpy)\n",
      "  Downloading zarr-3.1.5-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (3.0.9)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba!=0.62.0rc1,>=0.57.1->scanpy) (0.43.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->scanpy) (3.6.0)\n",
      "Collecting scikit-learn>=1.1.3 (from scanpy)\n",
      "  Downloading scikit_learn-1.8.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n",
      "Collecting donfig>=0.8 (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy)\n",
      "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: google-crc32c>=1.5 in /usr/local/lib/python3.11/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy) (1.7.1)\n",
      "Collecting numcodecs>=0.14 (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy)\n",
      "  Downloading numcodecs-0.16.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from donfig>=0.8->zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy) (6.0.3)\n",
      "Downloading scanpy-1.11.5-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anndata-0.12.6-py3-none-any.whl (172 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading legacy_api_wrap-1.5-py3-none-any.whl (10 kB)\n",
      "Downloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.8.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading session_info2-0.2.3-py3-none-any.whl (16 kB)\n",
      "Downloading array_api_compat-1.12.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zarr-3.1.5-py3-none-any.whl (284 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.1/284.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
      "Downloading numcodecs-0.16.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: session-info2, legacy-api-wrap, donfig, array-api-compat, scikit-learn, numcodecs, zarr, matplotlib, seaborn, anndata, scanpy\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.2\n",
      "    Uninstalling matplotlib-3.7.2:\n",
      "      Successfully uninstalled matplotlib-3.7.2\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.12.2\n",
      "    Uninstalling seaborn-0.12.2:\n",
      "      Successfully uninstalled seaborn-0.12.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "ydata-profiling 4.17.0 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.8 which is incompatible.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anndata-0.12.6 array-api-compat-1.12.0 donfig-0.8.1.post1 legacy-api-wrap-1.5 matplotlib-3.10.8 numcodecs-0.16.5 scanpy-1.11.5 scikit-learn-1.8.0 seaborn-0.13.2 session-info2-0.2.3 zarr-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Scanpy version: 1.11.5\n",
      "Pandas version: 2.2.3\n",
      "All libraries imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_81/3576863463.py:10: FutureWarning: `__version__` is deprecated, use `importlib.metadata.version('scanpy')` instead\n",
      "  print(f\"Scanpy version: {sc.__version__}\")\n"
     ]
    }
   ],
   "source": [
    "%pip install scanpy pandas numpy\n",
    "# Import required libraries for single-cell RNA-seq analysis and data handling\n",
    "import scanpy as sc  # Main library for single-cell analysis, provides AnnData structure and many tools\n",
    "import pandas as pd  # For tabular data manipulation and metadata handling\n",
    "import numpy as np   # For numerical operations and array handling\n",
    "import os            # For operating system interactions (file paths, etc.)\n",
    "from pathlib import Path  # For robust and readable file path management\n",
    "\n",
    "# Print versions to ensure reproducibility and compatibility\n",
    "print(f\"Scanpy version: {sc.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e34c5f",
   "metadata": {
    "papermill": {
     "duration": 0.013568,
     "end_time": "2025-11-12T03:55:17.881898",
     "exception": false,
     "start_time": "2025-11-12T03:55:17.868330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Process and Concatenate AnnData Objects\n",
    "\n",
    "Now, we will iterate through each sample defined in our metadata. For each sample, we will:\n",
    "1.  Locate the corresponding raw data directory.\n",
    "2.  Load the gene expression matrix directly from the compressed files into an `AnnData` object using `sc.read_10x_mtx()`.\n",
    "3.  Add the sample's metadata to the `.obs` attribute of the `AnnData` object.\n",
    "4.  Collect all the individual `AnnData` objects in a list.\n",
    "\n",
    "Finally, we'll concatenate them into one large `AnnData` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aac0c461",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:55:17.911090Z",
     "iopub.status.busy": "2025-11-12T03:55:17.909857Z",
     "iopub.status.idle": "2025-11-12T03:55:17.950331Z",
     "shell.execute_reply": "2025-11-12T03:55:17.949370Z"
    },
    "papermill": {
     "duration": 0.057084,
     "end_time": "2025-11-12T03:55:17.951786",
     "exception": false,
     "start_time": "2025-11-12T03:55:17.894702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata table now matches the requested specification:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_Number</th>\n",
       "      <th>GEX_Sample_ID</th>\n",
       "      <th>TCR_Sample_ID</th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Response</th>\n",
       "      <th>In_Data</th>\n",
       "      <th>In_Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>GSM9061687</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2</td>\n",
       "      <td>GSM9061666</td>\n",
       "      <td>GSM9061688</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S3</td>\n",
       "      <td>GSM9061667</td>\n",
       "      <td>GSM9061689</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S4</td>\n",
       "      <td>GSM9061668</td>\n",
       "      <td>GSM9061690</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S5</td>\n",
       "      <td>GSM9061669</td>\n",
       "      <td>GSM9061691</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S6</td>\n",
       "      <td>GSM9061670</td>\n",
       "      <td>GSM9061692</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S7</td>\n",
       "      <td>GSM9061671</td>\n",
       "      <td>GSM9061693</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S8</td>\n",
       "      <td>GSM9061672</td>\n",
       "      <td>None</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>GEX only</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S9</td>\n",
       "      <td>GSM9061673</td>\n",
       "      <td>GSM9061694</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S10</td>\n",
       "      <td>GSM9061674</td>\n",
       "      <td>GSM9061695</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Post-ICI</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S11</td>\n",
       "      <td>GSM9061675</td>\n",
       "      <td>GSM9061696</td>\n",
       "      <td>PT11</td>\n",
       "      <td>Endpoint</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S_Number GEX_Sample_ID TCR_Sample_ID Patient_ID   Timepoint       Response  \\\n",
       "0        S1    GSM9061665    GSM9061687        PT1    Baseline      Responder   \n",
       "1        S2    GSM9061666    GSM9061688        PT1  Post-Chemo      Responder   \n",
       "2        S3    GSM9061667    GSM9061689        PT2    Baseline  Non-Responder   \n",
       "3        S4    GSM9061668    GSM9061690        PT2  Post-Chemo  Non-Responder   \n",
       "4        S5    GSM9061669    GSM9061691        PT3    Baseline      Responder   \n",
       "5        S6    GSM9061670    GSM9061692        PT3  Post-Chemo      Responder   \n",
       "6        S7    GSM9061671    GSM9061693        PT4    Baseline  Non-Responder   \n",
       "7        S8    GSM9061672          None        PT5     Unknown        Unknown   \n",
       "8        S9    GSM9061673    GSM9061694        PT5    Baseline      Responder   \n",
       "9       S10    GSM9061674    GSM9061695        PT5    Post-ICI      Responder   \n",
       "10      S11    GSM9061675    GSM9061696       PT11    Endpoint      Responder   \n",
       "\n",
       "     In_Data In_Article  \n",
       "0        Yes        Yes  \n",
       "1        Yes        Yes  \n",
       "2        Yes        Yes  \n",
       "3        Yes        Yes  \n",
       "4        Yes        Yes  \n",
       "5        Yes        Yes  \n",
       "6        Yes        Yes  \n",
       "7   GEX only        Yes  \n",
       "8        Yes        Yes  \n",
       "9        Yes        Yes  \n",
       "10       Yes        Yes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-check In_Data column (based on files found in Data/GSE300475_RAW):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_Number</th>\n",
       "      <th>GEX_Sample_ID</th>\n",
       "      <th>TCR_Sample_ID</th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Response</th>\n",
       "      <th>In_Data</th>\n",
       "      <th>In_Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>GSM9061687</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2</td>\n",
       "      <td>GSM9061666</td>\n",
       "      <td>GSM9061688</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S3</td>\n",
       "      <td>GSM9061667</td>\n",
       "      <td>GSM9061689</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S4</td>\n",
       "      <td>GSM9061668</td>\n",
       "      <td>GSM9061690</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S5</td>\n",
       "      <td>GSM9061669</td>\n",
       "      <td>GSM9061691</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S6</td>\n",
       "      <td>GSM9061670</td>\n",
       "      <td>GSM9061692</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S7</td>\n",
       "      <td>GSM9061671</td>\n",
       "      <td>GSM9061693</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S8</td>\n",
       "      <td>GSM9061672</td>\n",
       "      <td>None</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>GEX only</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S9</td>\n",
       "      <td>GSM9061673</td>\n",
       "      <td>GSM9061694</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S10</td>\n",
       "      <td>GSM9061674</td>\n",
       "      <td>GSM9061695</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Post-ICI</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S11</td>\n",
       "      <td>GSM9061675</td>\n",
       "      <td>GSM9061696</td>\n",
       "      <td>PT11</td>\n",
       "      <td>Endpoint</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S_Number GEX_Sample_ID TCR_Sample_ID Patient_ID   Timepoint       Response  \\\n",
       "0        S1    GSM9061665    GSM9061687        PT1    Baseline      Responder   \n",
       "1        S2    GSM9061666    GSM9061688        PT1  Post-Chemo      Responder   \n",
       "2        S3    GSM9061667    GSM9061689        PT2    Baseline  Non-Responder   \n",
       "3        S4    GSM9061668    GSM9061690        PT2  Post-Chemo  Non-Responder   \n",
       "4        S5    GSM9061669    GSM9061691        PT3    Baseline      Responder   \n",
       "5        S6    GSM9061670    GSM9061692        PT3  Post-Chemo      Responder   \n",
       "6        S7    GSM9061671    GSM9061693        PT4    Baseline  Non-Responder   \n",
       "7        S8    GSM9061672          None        PT5     Unknown        Unknown   \n",
       "8        S9    GSM9061673    GSM9061694        PT5    Baseline      Responder   \n",
       "9       S10    GSM9061674    GSM9061695        PT5    Post-ICI      Responder   \n",
       "10      S11    GSM9061675    GSM9061696       PT11    Endpoint      Responder   \n",
       "\n",
       "     In_Data In_Article  \n",
       "0        Yes        Yes  \n",
       "1        Yes        Yes  \n",
       "2        Yes        Yes  \n",
       "3        Yes        Yes  \n",
       "4        Yes        Yes  \n",
       "5        Yes        Yes  \n",
       "6        Yes        Yes  \n",
       "7   GEX only        Yes  \n",
       "8        Yes        Yes  \n",
       "9        Yes        Yes  \n",
       "10       Yes        Yes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.1 ms, sys: 3.18 ms, total: 31.2 ms\n",
      "Wall time: 32.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Setup data paths ---\n",
    "# Define the main data directory and the subdirectory containing raw files.\n",
    "data_dir = Path('../Data')\n",
    "raw_data_dir = data_dir / 'GSE300475_RAW'\n",
    "\n",
    "# --- Manually create the metadata mapping ---\n",
    "# This list contains information about each sample, including GEO IDs, patient IDs, timepoints, and response status.\n",
    "# Note: S8 (GSM9061672) has GEX files but no corresponding TCR file.\n",
    "metadata_list = [\n",
    "    # Patient 1 (Responder)\n",
    "    {'S_Number': 'S1',  'GEX_Sample_ID': 'GSM9061665', 'TCR_Sample_ID': 'GSM9061687', 'Patient_ID': 'PT1',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S2',  'GEX_Sample_ID': 'GSM9061666', 'TCR_Sample_ID': 'GSM9061688', 'Patient_ID': 'PT1',  'Timepoint': 'Post-Chemo',  'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 2 (Non-Responder)\n",
    "    {'S_Number': 'S3',  'GEX_Sample_ID': 'GSM9061667', 'TCR_Sample_ID': 'GSM9061689', 'Patient_ID': 'PT2',  'Timepoint': 'Baseline',     'Response': 'Non-Responder', 'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S4',  'GEX_Sample_ID': 'GSM9061668', 'TCR_Sample_ID': 'GSM9061690', 'Patient_ID': 'PT2',  'Timepoint': 'Post-Chemo',  'Response': 'Non-Responder', 'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 3 (Responder)\n",
    "    {'S_Number': 'S5',  'GEX_Sample_ID': 'GSM9061669', 'TCR_Sample_ID': 'GSM9061691', 'Patient_ID': 'PT3',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S6',  'GEX_Sample_ID': 'GSM9061670', 'TCR_Sample_ID': 'GSM9061692', 'Patient_ID': 'PT3',  'Timepoint': 'Post-Chemo',  'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 4 (Non-Responder)\n",
    "    {'S_Number': 'S7',  'GEX_Sample_ID': 'GSM9061671', 'TCR_Sample_ID': 'GSM9061693', 'Patient_ID': 'PT4',  'Timepoint': 'Baseline',     'Response': 'Non-Responder', 'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 5 (partial) - S8 exists as GEX only in the raw data but has no TCR file\n",
    "    {'S_Number': 'S8',  'GEX_Sample_ID': 'GSM9061672', 'TCR_Sample_ID': None,             'Patient_ID': 'PT5',  'Timepoint': 'Unknown',      'Response': 'Unknown',       'In_Data': 'GEX only', 'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S9',  'GEX_Sample_ID': 'GSM9061673', 'TCR_Sample_ID': 'GSM9061694', 'Patient_ID': 'PT5',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S10', 'GEX_Sample_ID': 'GSM9061674', 'TCR_Sample_ID': 'GSM9061695', 'Patient_ID': 'PT5',  'Timepoint': 'Post-ICI',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 11 (Responder)\n",
    "    {'S_Number': 'S11', 'GEX_Sample_ID': 'GSM9061675', 'TCR_Sample_ID': 'GSM9061696', 'Patient_ID': 'PT11', 'Timepoint': 'Endpoint',      'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "]\n",
    "\n",
    "# --- Create DataFrame and display the verification table ---\n",
    "metadata_df = pd.DataFrame(metadata_list)\n",
    "print(\"Metadata table now matches the requested specification:\")\n",
    "display(metadata_df)\n",
    "\n",
    "# --- Programmatic sanity-check for file presence ---\n",
    "# This loop checks if the expected files exist for each sample and updates the 'In_Data' column accordingly.\n",
    "for idx, row in metadata_df.iterrows():\n",
    "    s = row['S_Number']\n",
    "    g = row['GEX_Sample_ID']\n",
    "    t = row['TCR_Sample_ID']\n",
    "    # Check for gene expression matrix file (compressed or uncompressed)\n",
    "    g_exists = (raw_data_dir / f\"{g}_{s}_matrix.mtx.gz\").exists() or (raw_data_dir / f\"{g}_{s}_matrix.mtx\").exists()\n",
    "    t_exists = False\n",
    "    # Check for TCR annotation file if TCR sample ID is present\n",
    "    if pd.notna(t) and t is not None:\n",
    "        t_exists = (raw_data_dir / f\"{t}_{s}_all_contig_annotations.csv.gz\").exists() or (raw_data_dir / f\"{t}_{s}_all_contig_annotations.csv\").exists()\n",
    "    # Update 'In_Data' column based on file presence\n",
    "    if g_exists and t_exists:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'Yes'\n",
    "    elif g_exists and not t_exists:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'GEX only'\n",
    "    else:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'No'\n",
    "\n",
    "print(\"\\nPost-check In_Data column (based on files found in Data/GSE300475_RAW):\")\n",
    "display(metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f87290",
   "metadata": {
    "papermill": {
     "duration": 0.013745,
     "end_time": "2025-11-12T03:55:17.979803",
     "exception": false,
     "start_time": "2025-11-12T03:55:17.966058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Integrate TCR Data and Perform QC\n",
    "\n",
    "Next, we'll merge the TCR information into the `.obs` of our main `AnnData` object. We will keep only the cells that have corresponding TCR data and filter based on the `high_confidence` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f6c3615",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:55:18.009343Z",
     "iopub.status.busy": "2025-11-12T03:55:18.008619Z",
     "iopub.status.idle": "2025-11-12T03:56:04.446730Z",
     "shell.execute_reply": "2025-11-12T03:56:04.445699Z"
    },
    "papermill": {
     "duration": 46.454797,
     "end_time": "2025-11-12T03:56:04.448166",
     "exception": false,
     "start_time": "2025-11-12T03:55:17.993369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing GEX sample: GSM9061665_S1\n",
      "Found and loading TCR data: GSM9061687_S1_all_contig_annotations.csv.gz\n",
      "Processing GEX sample: GSM9061666_S2\n",
      "Found and loading TCR data: GSM9061688_S2_all_contig_annotations.csv.gz\n",
      "Processing GEX sample: GSM9061667_S3\n",
      "Found and loading TCR data: GSM9061689_S3_all_contig_annotations.csv.gz\n",
      "Processing GEX sample: GSM9061668_S4\n",
      "Found and loading TCR data: GSM9061690_S4_all_contig_annotations.csv.gz\n",
      "Processing GEX sample: GSM9061669_S5\n",
      "Found and loading TCR data: GSM9061691_S5_all_contig_annotations.csv.gz\n",
      "Processing GEX sample: GSM9061670_S6\n",
      "Found and loading TCR data: GSM9061692_S6_all_contig_annotations.csv.gz\n",
      "Processing GEX sample: GSM9061671_S7\n",
      "Found and loading TCR data: GSM9061693_S7_all_contig_annotations.csv.gz\n",
      "Processing GEX sample: GSM9061672_S8\n",
      "No TCR sample for GSM9061672_S8, skipping TCR load.\n",
      "Processing GEX sample: GSM9061673_S9\n",
      "Found and loading TCR data: GSM9061694_S9_all_contig_annotations.csv.gz\n",
      "Processing GEX sample: GSM9061674_S10\n",
      "Found and loading TCR data: GSM9061695_S10_all_contig_annotations.csv.gz\n",
      "Processing GEX sample: GSM9061675_S11\n",
      "Found and loading TCR data: GSM9061696_S11_all_contig_annotations.csv.gz\n",
      "\n",
      "Concatenated AnnData object:\n",
      "AnnData object with n_obs × n_vars = 100067 × 36601\n",
      "    obs: 'sample_id', 'patient_id', 'timepoint', 'response'\n",
      "    var: 'gene_ids', 'feature_types'\n",
      "\n",
      "Full TCR data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>barcode</th>\n",
       "      <th>is_cell</th>\n",
       "      <th>contig_id</th>\n",
       "      <th>high_confidence</th>\n",
       "      <th>length</th>\n",
       "      <th>chain</th>\n",
       "      <th>v_gene</th>\n",
       "      <th>d_gene</th>\n",
       "      <th>j_gene</th>\n",
       "      <th>c_gene</th>\n",
       "      <th>...</th>\n",
       "      <th>cdr1_nt</th>\n",
       "      <th>fwr2</th>\n",
       "      <th>fwr2_nt</th>\n",
       "      <th>cdr2</th>\n",
       "      <th>cdr2_nt</th>\n",
       "      <th>fwr3</th>\n",
       "      <th>fwr3_nt</th>\n",
       "      <th>fwr4</th>\n",
       "      <th>fwr4_nt</th>\n",
       "      <th>exact_subclonotype_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAACCTGAGACTGTAA-1</td>\n",
       "      <td>True</td>\n",
       "      <td>AAACCTGAGACTGTAA-1_contig_1</td>\n",
       "      <td>True</td>\n",
       "      <td>493</td>\n",
       "      <td>TRB</td>\n",
       "      <td>TRBV3-1</td>\n",
       "      <td>TRBD1</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>TRBC1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAACCTGAGACTGTAA-1</td>\n",
       "      <td>True</td>\n",
       "      <td>AAACCTGAGACTGTAA-1_contig_2</td>\n",
       "      <td>True</td>\n",
       "      <td>639</td>\n",
       "      <td>TRA</td>\n",
       "      <td>TRAV36/DV7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRAJ53</td>\n",
       "      <td>TRAC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAACCTGAGCCAACAG-1</td>\n",
       "      <td>False</td>\n",
       "      <td>AAACCTGAGCCAACAG-1_contig_1</td>\n",
       "      <td>True</td>\n",
       "      <td>310</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRAJ27</td>\n",
       "      <td>TRAC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAACCTGAGCGTGAAC-1</td>\n",
       "      <td>True</td>\n",
       "      <td>AAACCTGAGCGTGAAC-1_contig_1</td>\n",
       "      <td>True</td>\n",
       "      <td>558</td>\n",
       "      <td>TRB</td>\n",
       "      <td>TRBV30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>TRBC1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAACCTGAGCGTGAAC-1</td>\n",
       "      <td>True</td>\n",
       "      <td>AAACCTGAGCGTGAAC-1_contig_2</td>\n",
       "      <td>True</td>\n",
       "      <td>503</td>\n",
       "      <td>TRA</td>\n",
       "      <td>TRAV29/DV5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRAJ48</td>\n",
       "      <td>TRAC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              barcode  is_cell                    contig_id  high_confidence  \\\n",
       "0  AAACCTGAGACTGTAA-1     True  AAACCTGAGACTGTAA-1_contig_1             True   \n",
       "1  AAACCTGAGACTGTAA-1     True  AAACCTGAGACTGTAA-1_contig_2             True   \n",
       "2  AAACCTGAGCCAACAG-1    False  AAACCTGAGCCAACAG-1_contig_1             True   \n",
       "3  AAACCTGAGCGTGAAC-1     True  AAACCTGAGCGTGAAC-1_contig_1             True   \n",
       "4  AAACCTGAGCGTGAAC-1     True  AAACCTGAGCGTGAAC-1_contig_2             True   \n",
       "\n",
       "   length chain      v_gene d_gene   j_gene c_gene  ...  cdr1_nt  fwr2  \\\n",
       "0     493   TRB     TRBV3-1  TRBD1  TRBJ1-1  TRBC1  ...      NaN   NaN   \n",
       "1     639   TRA  TRAV36/DV7    NaN   TRAJ53   TRAC  ...      NaN   NaN   \n",
       "2     310   NaN         NaN    NaN   TRAJ27   TRAC  ...      NaN   NaN   \n",
       "3     558   TRB      TRBV30    NaN  TRBJ1-2  TRBC1  ...      NaN   NaN   \n",
       "4     503   TRA  TRAV29/DV5    NaN   TRAJ48   TRAC  ...      NaN   NaN   \n",
       "\n",
       "  fwr2_nt cdr2  cdr2_nt  fwr3 fwr3_nt fwr4 fwr4_nt exact_subclonotype_id  \n",
       "0     NaN  NaN      NaN   NaN     NaN  NaN     NaN                   NaN  \n",
       "1     NaN  NaN      NaN   NaN     NaN  NaN     NaN                   NaN  \n",
       "2     NaN  NaN      NaN   NaN     NaN  NaN     NaN                   NaN  \n",
       "3     NaN  NaN      NaN   NaN     NaN  NaN     NaN                   NaN  \n",
       "4     NaN  NaN      NaN   NaN     NaN  NaN     NaN                   NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.7 s, sys: 5.75 s, total: 1min 2s\n",
      "Wall time: 45.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Initialize lists to hold AnnData and TCR data for each sample ---\n",
    "adata_list = []  # Will store AnnData objects for each sample\n",
    "tcr_data_list = []  # Will store TCR dataframes for each sample\n",
    "\n",
    "# --- Iterate through each sample in the metadata table ---\n",
    "for index, row in metadata_df.iterrows():\n",
    "    gex_sample_id = row['GEX_Sample_ID']\n",
    "    tcr_sample_id = row['TCR_Sample_ID']\n",
    "    s_number = row['S_Number']\n",
    "    patient_id = row['Patient_ID']\n",
    "    timepoint = row['Timepoint']\n",
    "    response = row['Response']\n",
    "    \n",
    "    # Construct the file prefix for this sample (used for locating files)\n",
    "    sample_prefix = f\"{gex_sample_id}_{s_number}\"\n",
    "    sample_data_path = raw_data_dir\n",
    "    \n",
    "    # --- Check for gene expression matrix file ---\n",
    "    matrix_file = sample_data_path / f\"{sample_prefix}_matrix.mtx.gz\"\n",
    "    if not matrix_file.exists():\n",
    "        # Try uncompressed version if gzipped file not found\n",
    "        matrix_file_un = sample_data_path / f\"{sample_prefix}_matrix.mtx\"\n",
    "        if not matrix_file_un.exists():\n",
    "            print(f\"GEX data not found for sample {sample_prefix}, skipping.\")\n",
    "            continue\n",
    "        else:\n",
    "            matrix_file = matrix_file_un\n",
    "            \n",
    "    print(f\"Processing GEX sample: {sample_prefix}\")\n",
    "    \n",
    "    # --- Load gene expression data into AnnData object ---\n",
    "    # The prefix ensures only files for this sample are loaded\n",
    "    adata_sample = sc.read_10x_mtx(\n",
    "        sample_data_path, \n",
    "        var_names='gene_symbols',\n",
    "        prefix=f\"{sample_prefix}_\"\n",
    "    )\n",
    "    \n",
    "    # --- Add sample metadata to AnnData.obs ---\n",
    "    adata_sample.obs['sample_id'] = gex_sample_id \n",
    "    adata_sample.obs['patient_id'] = patient_id\n",
    "    adata_sample.obs['timepoint'] = timepoint\n",
    "    adata_sample.obs['response'] = response\n",
    "    \n",
    "    adata_list.append(adata_sample)\n",
    "    \n",
    "    # --- Load TCR data if available ---\n",
    "    if pd.isna(tcr_sample_id) or tcr_sample_id is None:\n",
    "        print(f\"No TCR sample for {gex_sample_id}_{s_number}, skipping TCR load.\")\n",
    "        continue\n",
    "\n",
    "    # Construct path for TCR annotation file (gzipped or uncompressed)\n",
    "    tcr_file_path = raw_data_dir / f\"{tcr_sample_id}_{s_number}_all_contig_annotations.csv.gz\"\n",
    "\n",
    "    if tcr_file_path.exists():\n",
    "        print(f\"Found and loading TCR data: {tcr_file_path.name}\")\n",
    "        tcr_df = pd.read_csv(tcr_file_path)\n",
    "        # Add sample_id for merging later\n",
    "        tcr_df['sample_id'] = gex_sample_id \n",
    "        tcr_data_list.append(tcr_df)\n",
    "    else:\n",
    "        # Try uncompressed version if gzipped file not found\n",
    "        tcr_file_path_uncompressed = raw_data_dir / f\"{tcr_sample_id}_{s_number}_all_contig_annotations.csv\"\n",
    "        if tcr_file_path_uncompressed.exists():\n",
    "            print(f\"Found and loading TCR data: {tcr_file_path_uncompressed.name}\")\n",
    "            tcr_df = pd.read_csv(tcr_file_path_uncompressed)\n",
    "            tcr_df['sample_id'] = gex_sample_id\n",
    "            tcr_data_list.append(tcr_df)\n",
    "        else:\n",
    "            print(f\"TCR data not found for {tcr_sample_id}_{s_number}\")\n",
    "\n",
    "# --- Concatenate all loaded AnnData objects into one ---\n",
    "if adata_list:\n",
    "    # Use sample_id as batch key for concatenation\n",
    "    loaded_batches = [a.obs['sample_id'].unique()[0] for a in adata_list]\n",
    "    adata = sc.AnnData.concatenate(*adata_list, join='outer', batch_key='sample_id', batch_categories=loaded_batches)\n",
    "    print(\"\\nConcatenated AnnData object:\")\n",
    "    print(adata)\n",
    "else:\n",
    "    print(\"No data was loaded.\")\n",
    "\n",
    "# --- Concatenate all loaded TCR dataframes into one ---\n",
    "if tcr_data_list:\n",
    "    full_tcr_df = pd.concat(tcr_data_list, ignore_index=True)\n",
    "    print(\"\\nFull TCR data:\")\n",
    "    display(full_tcr_df.head())\n",
    "else:\n",
    "    print(\"No TCR data was loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec94343",
   "metadata": {
    "papermill": {
     "duration": 0.014332,
     "end_time": "2025-11-12T03:56:04.477414",
     "exception": false,
     "start_time": "2025-11-12T03:56:04.463082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Save Processed Data\n",
    "\n",
    "Finally, we save the fully processed, annotated, and filtered `AnnData` object to a `.h5ad` file. This file can be easily loaded in future notebooks for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "235814b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:56:04.508115Z",
     "iopub.status.busy": "2025-11-12T03:56:04.507783Z",
     "iopub.status.idle": "2025-11-12T03:56:12.526835Z",
     "shell.execute_reply": "2025-11-12T03:56:12.525859Z"
    },
    "papermill": {
     "duration": 8.036544,
     "end_time": "2025-11-12T03:56:12.528300",
     "exception": false,
     "start_time": "2025-11-12T03:56:04.491756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated TCR data merged into AnnData object.\n",
      "Filtered from 100067 to 38413 cells based on having high-confidence TCR data.\n",
      "\n",
      "Post-QC AnnData object:\n",
      "AnnData object with n_obs × n_vars = 38413 × 21518\n",
      "    obs: 'sample_id', 'patient_id', 'timepoint', 'response', 'barcode_for_merge', 'barcode', 'cdr3_TRA', 'cdr3_TRB', 'j_gene_TRA', 'j_gene_TRB', 'v_gene_TRA', 'v_gene_TRB', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt'\n",
      "    var: 'gene_ids', 'feature_types', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>timepoint</th>\n",
       "      <th>response</th>\n",
       "      <th>barcode_for_merge</th>\n",
       "      <th>barcode</th>\n",
       "      <th>cdr3_TRA</th>\n",
       "      <th>cdr3_TRB</th>\n",
       "      <th>j_gene_TRA</th>\n",
       "      <th>j_gene_TRB</th>\n",
       "      <th>v_gene_TRA</th>\n",
       "      <th>v_gene_TRB</th>\n",
       "      <th>n_genes</th>\n",
       "      <th>n_genes_by_counts</th>\n",
       "      <th>total_counts</th>\n",
       "      <th>total_counts_mt</th>\n",
       "      <th>pct_counts_mt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGACTGTAA-1-GSM9061665</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>AAACCTGAGACTGTAA-1</td>\n",
       "      <td>AAACCTGAGACTGTAA-1</td>\n",
       "      <td>CAVEARNYKLTF</td>\n",
       "      <td>CASGTGLNTEAFF</td>\n",
       "      <td>TRAJ53</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>TRAV36/DV7</td>\n",
       "      <td>TRBV3-1</td>\n",
       "      <td>1379</td>\n",
       "      <td>1379</td>\n",
       "      <td>4637.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>3.385810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGCGTGAAC-1-GSM9061665</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>AAACCTGAGCGTGAAC-1</td>\n",
       "      <td>AAACCTGAGCGTGAAC-1</td>\n",
       "      <td>CAASAVGNEKLTF</td>\n",
       "      <td>CAWSALLGTVNGYTF</td>\n",
       "      <td>TRAJ48</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>TRAV29/DV5</td>\n",
       "      <td>TRBV30</td>\n",
       "      <td>1277</td>\n",
       "      <td>1277</td>\n",
       "      <td>4849.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>5.093834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGCTACCTA-1-GSM9061665</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>AAACCTGAGCTACCTA-1</td>\n",
       "      <td>AAACCTGAGCTACCTA-1</td>\n",
       "      <td>CALSEAWGNARLMF</td>\n",
       "      <td>CASRSREETYEQYF</td>\n",
       "      <td>TRAJ31</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>TRAV19</td>\n",
       "      <td>TRBV2</td>\n",
       "      <td>887</td>\n",
       "      <td>887</td>\n",
       "      <td>3077.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>9.099772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGCTGTTCA-1-GSM9061665</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>AAACCTGAGCTGTTCA-1</td>\n",
       "      <td>AAACCTGAGCTGTTCA-1</td>\n",
       "      <td>CALLGLKGEGSARQLTF</td>\n",
       "      <td>CASSLPPWRANTEAFF</td>\n",
       "      <td>TRAJ22</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>TRAV9-2</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>5.857230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGGCATTGG-1-GSM9061665</th>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>AAACCTGAGGCATTGG-1</td>\n",
       "      <td>AAACCTGAGGCATTGG-1</td>\n",
       "      <td>CAVTGFSDGQKLLF</td>\n",
       "      <td>CASSLTGEVWDEQFF</td>\n",
       "      <td>TRAJ16</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>TRAV8-6</td>\n",
       "      <td>TRBV5-1</td>\n",
       "      <td>1313</td>\n",
       "      <td>1313</td>\n",
       "      <td>4947.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>4.002426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                sample_id patient_id timepoint   response  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665  GSM9061665        PT1  Baseline  Responder   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665  GSM9061665        PT1  Baseline  Responder   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665  GSM9061665        PT1  Baseline  Responder   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665  GSM9061665        PT1  Baseline  Responder   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665  GSM9061665        PT1  Baseline  Responder   \n",
       "\n",
       "                                barcode_for_merge             barcode  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665  AAACCTGAGACTGTAA-1  AAACCTGAGACTGTAA-1   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665  AAACCTGAGCGTGAAC-1  AAACCTGAGCGTGAAC-1   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665  AAACCTGAGCTACCTA-1  AAACCTGAGCTACCTA-1   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665  AAACCTGAGCTGTTCA-1  AAACCTGAGCTGTTCA-1   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665  AAACCTGAGGCATTGG-1  AAACCTGAGGCATTGG-1   \n",
       "\n",
       "                                        cdr3_TRA          cdr3_TRB j_gene_TRA  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665       CAVEARNYKLTF     CASGTGLNTEAFF     TRAJ53   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665      CAASAVGNEKLTF   CAWSALLGTVNGYTF     TRAJ48   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665     CALSEAWGNARLMF    CASRSREETYEQYF     TRAJ31   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665  CALLGLKGEGSARQLTF  CASSLPPWRANTEAFF     TRAJ22   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665     CAVTGFSDGQKLLF   CASSLTGEVWDEQFF     TRAJ16   \n",
       "\n",
       "                              j_gene_TRB  v_gene_TRA v_gene_TRB  n_genes  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665    TRBJ1-1  TRAV36/DV7    TRBV3-1     1379   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665    TRBJ1-2  TRAV29/DV5     TRBV30     1277   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665    TRBJ2-7      TRAV19      TRBV2      887   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665    TRBJ1-1     TRAV9-2   TRBV11-2     1631   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665    TRBJ2-1     TRAV8-6    TRBV5-1     1313   \n",
       "\n",
       "                               n_genes_by_counts  total_counts  \\\n",
       "AAACCTGAGACTGTAA-1-GSM9061665               1379        4637.0   \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665               1277        4849.0   \n",
       "AAACCTGAGCTACCTA-1-GSM9061665                887        3077.0   \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665               1631        4917.0   \n",
       "AAACCTGAGGCATTGG-1-GSM9061665               1313        4947.0   \n",
       "\n",
       "                               total_counts_mt  pct_counts_mt  \n",
       "AAACCTGAGACTGTAA-1-GSM9061665            157.0       3.385810  \n",
       "AAACCTGAGCGTGAAC-1-GSM9061665            247.0       5.093834  \n",
       "AAACCTGAGCTACCTA-1-GSM9061665            280.0       9.099772  \n",
       "AAACCTGAGCTGTTCA-1-GSM9061665            288.0       5.857230  \n",
       "AAACCTGAGGCATTGG-1-GSM9061665            198.0       4.002426  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.1 s, sys: 1.06 s, total: 8.16 s\n",
      "Wall time: 8.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Integrate TCR data into AnnData.obs and perform quality control ---\n",
    "if 'full_tcr_df' in locals() and not full_tcr_df.empty:\n",
    "    # --- FIX START ---\n",
    "    # The previous join failed because one cell (barcode) can have multiple TCR contigs (e.g., TRA and TRB chains),\n",
    "    # creating a one-to-many join that increases the number of rows.\n",
    "    # The fix is to aggregate the TCR data to one row per cell *before* merging.\n",
    "\n",
    "    # 1. Filter for high-confidence, productive TRA/TRB chains.\n",
    "    # Only keep TCR contigs that are both high-confidence and productive, and are either TRA or TRB chains.\n",
    "    tcr_to_agg = full_tcr_df[\n",
    "        (full_tcr_df['high_confidence'] == True) &\n",
    "        (full_tcr_df['productive'] == True) &\n",
    "        (full_tcr_df['chain'].isin(['TRA', 'TRB']))\n",
    "    ].copy()\n",
    "\n",
    "    # 2. Pivot the data to create one row per barcode, with columns for TRA and TRB data.\n",
    "    # This step ensures each cell (barcode) has its TRA and TRB info in separate columns.\n",
    "    tcr_aggregated = tcr_to_agg.pivot_table(\n",
    "        index=['sample_id', 'barcode'],\n",
    "        columns='chain',\n",
    "        values=['v_gene', 'j_gene', 'cdr3'],\n",
    "        aggfunc='first'  # 'first' is safe as we expect at most one productive TRA/TRB per cell\n",
    "    )\n",
    "\n",
    "    # 3. Flatten the multi-level column index (e.g., from ('v_gene', 'TRA') to 'v_gene_TRA')\n",
    "    tcr_aggregated.columns = ['_'.join(col).strip() for col in tcr_aggregated.columns.values]\n",
    "    tcr_aggregated.reset_index(inplace=True)\n",
    "\n",
    "    # 4. Prepare adata.obs for the merge by creating a matching barcode column.\n",
    "    # The index in adata.obs is like 'AGCCATGCAGCTGTTA-1-0' (barcode-batch_id).\n",
    "    # The barcode in TCR data is like 'AGCCATGCAGCTGTTA-1'.\n",
    "    adata.obs['barcode_for_merge'] = adata.obs.index.str.rsplit('-', n=1).str[0]\n",
    "\n",
    "    # 5. Perform a left merge. This keeps all cells from adata and adds TCR info where available.\n",
    "    # The number of rows will not change because tcr_aggregated has unique barcodes.\n",
    "    original_obs = adata.obs.copy()\n",
    "    merged_obs = original_obs.merge(\n",
    "        tcr_aggregated,\n",
    "        left_on=['sample_id', 'barcode_for_merge'],\n",
    "        right_on=['sample_id', 'barcode'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 6. Restore the original index to the merged dataframe.\n",
    "    merged_obs.index = original_obs.index\n",
    "    adata.obs = merged_obs\n",
    "    # --- FIX END ---\n",
    "\n",
    "    print(\"Aggregated TCR data merged into AnnData object.\")\n",
    "    \n",
    "    # --- Filter for cells that have TCR information after the merge ---\n",
    "    # Only keep cells with non-null v_gene_TRA (i.e., cells with high-confidence TCR data)\n",
    "    initial_cells = adata.n_obs\n",
    "    adata = adata[~adata.obs['v_gene_TRA'].isna()].copy()\n",
    "    print(f\"Filtered from {initial_cells} to {adata.n_obs} cells based on having high-confidence TCR data.\")\n",
    "\n",
    "# --- Basic QC and filtering ---\n",
    "# Filter out cells with fewer than 200 genes detected\n",
    "sc.pp.filter_cells(adata, min_genes=200)\n",
    "# Filter out genes detected in fewer than 3 cells\n",
    "sc.pp.filter_genes(adata, min_cells=3)\n",
    "\n",
    "# Annotate mitochondrial genes for QC metrics\n",
    "adata.var['mt'] = adata.var_names.str.startswith('MT-')\n",
    "# Calculate QC metrics (e.g., percent mitochondrial genes)\n",
    "sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "\n",
    "print(\"\\nPost-QC AnnData object:\")\n",
    "print(adata)\n",
    "display(adata.obs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d8cb8e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:56:12.560387Z",
     "iopub.status.busy": "2025-11-12T03:56:12.560019Z",
     "iopub.status.idle": "2025-11-12T03:56:14.636611Z",
     "shell.execute_reply": "2025-11-12T03:56:14.635540Z"
    },
    "papermill": {
     "duration": 2.095098,
     "end_time": "2025-11-12T03:56:14.638327",
     "exception": false,
     "start_time": "2025-11-12T03:56:12.543229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to: ../Processed_Data/processed_s_rna_seq_data.h5ad\n",
      "CPU times: user 1.34 s, sys: 267 ms, total: 1.61 s\n",
      "Wall time: 1.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Save processed AnnData object to disk ---\n",
    "# Define output directory for processed data\n",
    "output_dir = Path('../Processed_Data')\n",
    "output_dir.mkdir(exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Define output file path for the .h5ad file\n",
    "output_path = output_dir / 'processed_s_rna_seq_data.h5ad'\n",
    "# Save the AnnData object (contains all processed, filtered, and annotated data)\n",
    "adata.write_h5ad(output_path)\n",
    "\n",
    "print(f\"Processed data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11525c",
   "metadata": {
    "papermill": {
     "duration": 0.016804,
     "end_time": "2025-11-12T03:56:14.679682",
     "exception": false,
     "start_time": "2025-11-12T03:56:14.662878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Install Additional Libraries for Advanced ML and Visualization\n",
    "\n",
    "Install and import libraries such as XGBoost, TensorFlow/Keras, scipy, and additional visualization tools for comprehensive ML analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b380a8f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:56:14.713175Z",
     "iopub.status.busy": "2025-11-12T03:56:14.712558Z",
     "iopub.status.idle": "2025-11-12T03:57:36.214879Z",
     "shell.execute_reply": "2025-11-12T03:57:36.213784Z"
    },
    "papermill": {
     "duration": 81.538252,
     "end_time": "2025-11-12T03:57:36.232978",
     "exception": false,
     "start_time": "2025-11-12T03:56:14.694726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\n",
      "  Downloading biopython-1.86-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->biopython) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->biopython) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->biopython) (2024.2.0)\n",
      "Downloading biopython-1.86-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: biopython\n",
      "Successfully installed biopython-1.86\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.1->scikit-learn) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.1->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.1->scikit-learn) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.1->scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24.1->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24.1->scikit-learn) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.9.post2)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.6 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.8.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23->umap-learn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23->umap-learn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23->umap-learn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23->umap-learn) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23->umap-learn) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23->umap-learn) (2.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6->umap-learn) (3.6.0)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23->umap-learn) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23->umap-learn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23->umap-learn) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23->umap-learn) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23->umap-learn) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23->umap-learn) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (2.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20->hdbscan) (3.6.0)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.20->hdbscan) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.20->hdbscan) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.20->hdbscan) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.20->hdbscan) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.20->hdbscan) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.20->hdbscan) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (25.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->xgboost) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->xgboost) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->xgboost) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m242.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.33.0\n",
      "    Uninstalling protobuf-6.33.0:\n",
      "      Successfully uninstalled protobuf-6.33.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\n",
      "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-5.29.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 03:53:27.668757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765598007.939650      81 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765598008.016689      81 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional libraries installed!\n",
      "CPU times: user 35.5 s, sys: 6.04 s, total: 41.5 s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Install required packages for genetic sequence encoding and ML ---\n",
    "%pip install biopython\n",
    "%pip install scikit-learn\n",
    "%pip install umap-learn\n",
    "%pip install hdbscan\n",
    "%pip install plotly\n",
    "%pip install xgboost\n",
    "%pip install tensorflow\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import ProtParam\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Import scipy for hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import umap\n",
    "import hdbscan\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"Additional libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58356e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied compatibility patch to 4 in-memory references (examples: ['__main__._orig_check_array', '__main__._check_array_compat', '__mp_main__._orig_check_array', '__mp_main__._check_array_compat'])\n"
     ]
    }
   ],
   "source": [
    "# Compatibility fix: make modules that imported sklearn.utils.validation.check_array accept 'force_all_finite' kw\n",
    "# Avoid replacing sklearn.utils.validation.check_array itself to prevent recursion.\n",
    "try:\n",
    "    import sys\n",
    "    from sklearn.utils import validation as _sk_validation\n",
    "    _orig_check_array = getattr(_sk_validation, 'check_array', None)\n",
    "    if _orig_check_array is not None:\n",
    "        def _check_array_compat(*args, **kwargs):\n",
    "            if 'force_all_finite' in kwargs:\n",
    "                kwargs.pop('force_all_finite')\n",
    "            return _orig_check_array(*args, **kwargs)\n",
    "        # Best-effort: replace direct references to the original function in already-imported modules\n",
    "        replaced = []\n",
    "        try:\n",
    "            for mod_name, mod in list(sys.modules.items()):\n",
    "                if mod is None:\n",
    "                    continue\n",
    "                mdict = getattr(mod, '__dict__', None)\n",
    "                if not mdict:\n",
    "                    continue\n",
    "                for attr_name, attr_val in list(mdict.items()):\n",
    "                    try:\n",
    "                        if attr_val is _orig_check_array:\n",
    "                            setattr(mod, attr_name, _check_array_compat)\n",
    "                            replaced.append(f'{mod_name}.{attr_name}')\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        if replaced:\n",
    "            print(f'Applied compatibility patch to {len(replaced)} in-memory references (examples: {replaced[:5]})')\n",
    "        else:\n",
    "            print('No in-memory references to patch were found. If you still see the TypeError, restart the kernel and run this cell before importing hdbscan or scikit-learn.')\n",
    "    else:\n",
    "        print('sklearn.validation.check_array not found; no patch applied.')\n",
    "except Exception as e:\n",
    "    print('Could not apply sklearn compatibility patch:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51635157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
      "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.41)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.1->scikit-learn) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.1->scikit-learn) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.1->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.1->scikit-learn) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.1->scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24.1->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24.1->scikit-learn) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "No patched references found. If you already imported hdbscan/sklearn, restart the kernel and run the compatibility cell before importing them.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%pip install -U scikit-learn hdbscan\n",
    "\n",
    "# Quick test: check for any patched references (_check_array_compat) in loaded modules\n",
    "try:\n",
    "    import sys, numpy as np\n",
    "    found = False\n",
    "    for mod in list(sys.modules.values()):\n",
    "        try:\n",
    "            mdict = getattr(mod, '__dict__', None)\n",
    "            if not mdict:\n",
    "                continue\n",
    "            for name, val in mdict.items():\n",
    "                if callable(val) and getattr(val, '__name__', '') == '_check_array_compat':\n",
    "                    print(f'Found patched reference: {getattr(mod, str(mod))}.{name}')\n",
    "                    # quick call to ensure it accepts the kwarg\n",
    "                    X = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "                    try:\n",
    "                        val(X, force_all_finite=True)\n",
    "                        print('Patched function accepted force_all_finite successfully.')\n",
    "                    except Exception as e:\n",
    "                        print('Patched function call failed:', e)\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not found:\n",
    "        print('No patched references found. If you already imported hdbscan/sklearn, restart the kernel and run the compatibility cell before importing them.')\n",
    "except Exception as e:\n",
    "    print('Compatibility test failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01c5e7",
   "metadata": {
    "papermill": {
     "duration": 0.017202,
     "end_time": "2025-11-12T03:57:36.268939",
     "exception": false,
     "start_time": "2025-11-12T03:57:36.251737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Genetic Sequence Encoding Functions\n",
    "\n",
    "Define functions for one-hot encoding, k-mer encoding, and physicochemical features extraction for TCR sequences and gene expression patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3be9b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:57:36.305708Z",
     "iopub.status.busy": "2025-11-12T03:57:36.304815Z",
     "iopub.status.idle": "2025-11-12T03:57:36.317767Z",
     "shell.execute_reply": "2025-11-12T03:57:36.316461Z"
    },
    "papermill": {
     "duration": 0.033242,
     "end_time": "2025-11-12T03:57:36.319172",
     "exception": false,
     "start_time": "2025-11-12T03:57:36.285930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic sequence encoding functions defined successfully!\n",
      "CPU times: user 151 µs, sys: 0 ns, total: 151 µs\n",
      "Wall time: 138 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Genetic Sequence Encoding Functions ---\n",
    "\n",
    "def one_hot_encode_sequence(sequence, max_length=50, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    One-hot encode a protein/nucleotide sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        max_length: Maximum sequence length (pad or truncate)\n",
    "        alphabet: Valid characters in the sequence\n",
    "    \n",
    "    Returns:\n",
    "        2D numpy array of shape (max_length, len(alphabet))\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    sequence = str(sequence).upper()[:max_length]  # Truncate if too long\n",
    "    encoding = np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    for i, char in enumerate(sequence):\n",
    "        if char in alphabet:\n",
    "            char_idx = alphabet.index(char)\n",
    "            encoding[i, char_idx] = 1\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "def kmer_encode_sequence(sequence, k=3, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    K-mer encoding of sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        k: Length of k-mers\n",
    "        alphabet: Valid characters\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with k-mer counts\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {}\n",
    "    \n",
    "    sequence = str(sequence).upper()\n",
    "    kmers = [sequence[i:i+k] for i in range(len(sequence)-k+1)]\n",
    "    valid_kmers = [kmer for kmer in kmers if all(c in alphabet for c in kmer)]\n",
    "    \n",
    "    return Counter(valid_kmers)\n",
    "\n",
    "def physicochemical_features(sequence):\n",
    "    \"\"\"\n",
    "    Extract physicochemical properties from protein sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Protein sequence string\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of features\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {\n",
    "            'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        seq = str(sequence).upper()\n",
    "        # Remove non-standard amino acids\n",
    "        seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "        \n",
    "        if len(seq) == 0:\n",
    "            return {\n",
    "                'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "                'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "            }\n",
    "        \n",
    "        bio_seq = Seq(seq)\n",
    "        analyzer = ProtParam.ProteinAnalysis(str(bio_seq))\n",
    "        \n",
    "        return {\n",
    "            'length': len(seq),\n",
    "            'molecular_weight': analyzer.molecular_weight(),\n",
    "            'aromaticity': analyzer.aromaticity(),\n",
    "            'instability_index': analyzer.instability_index(),\n",
    "            'isoelectric_point': analyzer.isoelectric_point(),\n",
    "            'hydrophobicity': analyzer.gravy()  # Grand Average of Hydropathy\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'length': len(str(sequence)) if not pd.isna(sequence) else 0,\n",
    "            'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "\n",
    "def encode_gene_expression_patterns(adata, n_top_genes=1000):\n",
    "    \"\"\"\n",
    "    Encode gene expression patterns using various dimensionality reduction techniques.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object with gene expression data\n",
    "        n_top_genes: Number of highly variable genes to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of encoded representations\n",
    "    \"\"\"\n",
    "    # Get highly variable genes if not already computed\n",
    "    if 'highly_variable' not in adata.var.columns:\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, subset=False)\n",
    "    \n",
    "    # Extract expression matrix for highly variable genes\n",
    "    hvg_mask = adata.var['highly_variable'] if 'highly_variable' in adata.var.columns else adata.var.index[:n_top_genes]\n",
    "    X_hvg = adata[:, hvg_mask].X.toarray() if hasattr(adata.X, 'toarray') else adata[:, hvg_mask].X\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_hvg)\n",
    "    \n",
    "    encodings = {}\n",
    "    \n",
    "    # PCA encoding\n",
    "    pca = PCA(n_components=50)\n",
    "    encodings['pca'] = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # TruncatedSVD for sparse matrices\n",
    "    svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "    encodings['svd'] = svd.fit_transform(X_scaled)\n",
    "    \n",
    "    # UMAP encoding\n",
    "    umap_encoder = umap.UMAP(n_components=20, random_state=42)\n",
    "    encodings['umap'] = umap_encoder.fit_transform(X_scaled)\n",
    "    \n",
    "    return encodings, X_scaled\n",
    "\n",
    "print(\"Genetic sequence encoding functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd70069",
   "metadata": {
    "papermill": {
     "duration": 0.017029,
     "end_time": "2025-11-12T03:57:36.353580",
     "exception": false,
     "start_time": "2025-11-12T03:57:36.336551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Apply Sequence Encoding to TCR CDR3 Sequences\n",
    "\n",
    "Encode TRA and TRB CDR3 sequences using one-hot, k-mer, and physicochemical methods, and add to AnnData.obsm and obs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890538e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:57:36.390481Z",
     "iopub.status.busy": "2025-11-12T03:57:36.389752Z",
     "iopub.status.idle": "2025-11-12T03:57:36.402963Z",
     "shell.execute_reply": "2025-11-12T03:57:36.401874Z"
    },
    "papermill": {
     "duration": 0.033819,
     "end_time": "2025-11-12T03:57:36.404505",
     "exception": false,
     "start_time": "2025-11-12T03:57:36.370686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic sequence encoding functions defined successfully!\n",
      "CPU times: user 119 µs, sys: 0 ns, total: 119 µs\n",
      "Wall time: 124 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Genetic Sequence Encoding Functions ---\n",
    "\n",
    "def one_hot_encode_sequence(sequence, max_length=50, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    One-hot encode a protein/nucleotide sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        max_length: Maximum sequence length (pad or truncate)\n",
    "        alphabet: Valid characters in the sequence\n",
    "    \n",
    "    Returns:\n",
    "        2D numpy array of shape (max_length, len(alphabet))\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    sequence = str(sequence).upper()[:max_length]  # Truncate if too long\n",
    "    encoding = np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    for i, char in enumerate(sequence):\n",
    "        if char in alphabet:\n",
    "            char_idx = alphabet.index(char)\n",
    "            encoding[i, char_idx] = 1\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "def kmer_encode_sequence(sequence, k=3, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    K-mer encoding of sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        k: Length of k-mers\n",
    "        alphabet: Valid characters\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with k-mer counts\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {}\n",
    "    \n",
    "    sequence = str(sequence).upper()\n",
    "    kmers = [sequence[i:i+k] for i in range(len(sequence)-k+1)]\n",
    "    valid_kmers = [kmer for kmer in kmers if all(c in alphabet for c in kmer)]\n",
    "    \n",
    "    return Counter(valid_kmers)\n",
    "\n",
    "def physicochemical_features(sequence):\n",
    "    \"\"\"\n",
    "    Extract physicochemical properties from protein sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Protein sequence string\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of features\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {\n",
    "            'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        seq = str(sequence).upper()\n",
    "        # Remove non-standard amino acids\n",
    "        seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "        \n",
    "        if len(seq) == 0:\n",
    "            return {\n",
    "                'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "                'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "            }\n",
    "        \n",
    "        bio_seq = Seq(seq)\n",
    "        analyzer = ProtParam.ProteinAnalysis(str(bio_seq))\n",
    "        \n",
    "        return {\n",
    "            'length': len(seq),\n",
    "            'molecular_weight': analyzer.molecular_weight(),\n",
    "            'aromaticity': analyzer.aromaticity(),\n",
    "            'instability_index': analyzer.instability_index(),\n",
    "            'isoelectric_point': analyzer.isoelectric_point(),\n",
    "            'hydrophobicity': analyzer.gravy()  # Grand Average of Hydropathy\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'length': len(str(sequence)) if not pd.isna(sequence) else 0,\n",
    "            'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "\n",
    "def encode_gene_expression_patterns(adata, n_top_genes=1000):\n",
    "    \"\"\"\n",
    "    Encode gene expression patterns using various dimensionality reduction techniques.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object with gene expression data\n",
    "        n_top_genes: Number of highly variable genes to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of encoded representations\n",
    "    \"\"\"\n",
    "    # Get highly variable genes if not already computed\n",
    "    if 'highly_variable' not in adata.var.columns:\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, subset=False)\n",
    "    \n",
    "    # Extract expression matrix for highly variable genes\n",
    "    hvg_mask = adata.var['highly_variable'] if 'highly_variable' in adata.var.columns else adata.var.index[:n_top_genes]\n",
    "    X_hvg = adata[:, hvg_mask].X.toarray() if hasattr(adata.X, 'toarray') else adata[:, hvg_mask].X\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_hvg)\n",
    "    \n",
    "    encodings = {}\n",
    "    \n",
    "    # PCA encoding\n",
    "    pca = PCA(n_components=50)\n",
    "    encodings['pca'] = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # TruncatedSVD for sparse matrices\n",
    "    svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "    encodings['svd'] = svd.fit_transform(X_scaled)\n",
    "    \n",
    "    # UMAP encoding\n",
    "    umap_encoder = umap.UMAP(n_components=20, random_state=42)\n",
    "    encodings['umap'] = umap_encoder.fit_transform(X_scaled)\n",
    "    \n",
    "    return encodings, X_scaled\n",
    "\n",
    "print(\"Genetic sequence encoding functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80423c6c",
   "metadata": {
    "papermill": {
     "duration": 0.017132,
     "end_time": "2025-11-12T03:57:36.439058",
     "exception": false,
     "start_time": "2025-11-12T03:57:36.421926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Encode Gene Expression Patterns\n",
    "\n",
    "Apply PCA, SVD, and UMAP to gene expression data for dimensionality reduction and add encodings to AnnData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22b1d959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:57:36.474836Z",
     "iopub.status.busy": "2025-11-12T03:57:36.474509Z",
     "iopub.status.idle": "2025-11-12T03:59:19.121304Z",
     "shell.execute_reply": "2025-11-12T03:59:19.120217Z"
    },
    "papermill": {
     "duration": 102.66708,
     "end_time": "2025-11-12T03:59:19.123112",
     "exception": false,
     "start_time": "2025-11-12T03:57:36.456032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding TCR CDR3 sequences...\n",
      "Computing one-hot encodings...\n",
      "TRA one-hot shape: (38413, 600)\n",
      "TRB one-hot shape: (38413, 600)\n",
      "Computing k-mer encodings...\n",
      "TRA k-mer matrix shape: (38413, 6074)\n",
      "TRB k-mer matrix shape: (38413, 6091)\n",
      "Computing physicochemical features...\n",
      "TRA physicochemical features shape: (38413, 6)\n",
      "TRB physicochemical features shape: (38413, 6)\n",
      "TCR sequence encoding completed and added to AnnData object!\n",
      "CPU times: user 1min 37s, sys: 5.3 s, total: 1min 42s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Apply Sequence Encoding to TCR CDR3 Sequences ---\n",
    "\n",
    "print(\"Encoding TCR CDR3 sequences...\")\n",
    "\n",
    "# Extract CDR3 sequences\n",
    "# Convert to string type first to handle categorical data\n",
    "cdr3_sequences = {\n",
    "    'TRA': adata.obs['cdr3_TRA'].astype(str).fillna(''),\n",
    "    'TRB': adata.obs['cdr3_TRB'].astype(str).fillna('')\n",
    "}\n",
    "\n",
    "# --- 1. One-hot encoding of CDR3 sequences ---\n",
    "print(\"Computing one-hot encodings...\")\n",
    "max_cdr3_length = 30  # Typical CDR3 length range\n",
    "\n",
    "# One-hot encode TRA CDR3 sequences\n",
    "tra_onehot = np.array([one_hot_encode_sequence(seq, max_cdr3_length, 'ACDEFGHIKLMNPQRSTVWY') \n",
    "                       for seq in cdr3_sequences['TRA']])\n",
    "tra_onehot_flat = tra_onehot.reshape(tra_onehot.shape[0], -1)\n",
    "\n",
    "# One-hot encode TRB CDR3 sequences  \n",
    "trb_onehot = np.array([one_hot_encode_sequence(seq, max_cdr3_length, 'ACDEFGHIKLMNPQRSTVWY') \n",
    "                       for seq in cdr3_sequences['TRB']])\n",
    "trb_onehot_flat = trb_onehot.reshape(trb_onehot.shape[0], -1)\n",
    "\n",
    "print(f\"TRA one-hot shape: {tra_onehot_flat.shape}\")\n",
    "print(f\"TRB one-hot shape: {trb_onehot_flat.shape}\")\n",
    "\n",
    "# --- 2. K-mer encoding ---\n",
    "print(\"Computing k-mer encodings...\")\n",
    "k = 3  # Use 3-mers\n",
    "\n",
    "# Get all possible k-mers for creating consistent feature vectors\n",
    "all_tra_kmers = []\n",
    "all_trb_kmers = []\n",
    "\n",
    "for seq in cdr3_sequences['TRA']:\n",
    "    if seq and seq != '':\n",
    "        all_tra_kmers.extend(kmer_encode_sequence(seq, k).keys())\n",
    "\n",
    "for seq in cdr3_sequences['TRB']:\n",
    "    if seq and seq != '':\n",
    "        all_trb_kmers.extend(kmer_encode_sequence(seq, k).keys())\n",
    "\n",
    "unique_tra_kmers = sorted(list(set(all_tra_kmers)))\n",
    "unique_trb_kmers = sorted(list(set(all_trb_kmers)))\n",
    "\n",
    "# Create k-mer count vectors\n",
    "tra_kmer_matrix = []\n",
    "for seq in cdr3_sequences['TRA']:\n",
    "    kmer_counts = kmer_encode_sequence(seq, k)\n",
    "    vector = [kmer_counts.get(kmer, 0) for kmer in unique_tra_kmers]\n",
    "    tra_kmer_matrix.append(vector)\n",
    "\n",
    "trb_kmer_matrix = []\n",
    "for seq in cdr3_sequences['TRB']:\n",
    "    kmer_counts = kmer_encode_sequence(seq, k)\n",
    "    vector = [kmer_counts.get(kmer, 0) for kmer in unique_trb_kmers]\n",
    "    trb_kmer_matrix.append(vector)\n",
    "\n",
    "tra_kmer_matrix = np.array(tra_kmer_matrix)\n",
    "trb_kmer_matrix = np.array(trb_kmer_matrix)\n",
    "\n",
    "print(f\"TRA k-mer matrix shape: {tra_kmer_matrix.shape}\")\n",
    "print(f\"TRB k-mer matrix shape: {trb_kmer_matrix.shape}\")\n",
    "\n",
    "# --- 3. Physicochemical properties ---\n",
    "print(\"Computing physicochemical features...\")\n",
    "\n",
    "tra_physico = pd.DataFrame([physicochemical_features(seq) for seq in cdr3_sequences['TRA']])\n",
    "trb_physico = pd.DataFrame([physicochemical_features(seq) for seq in cdr3_sequences['TRB']])\n",
    "\n",
    "print(f\"TRA physicochemical features shape: {tra_physico.shape}\")\n",
    "print(f\"TRB physicochemical features shape: {trb_physico.shape}\")\n",
    "\n",
    "# Add to AnnData object\n",
    "adata.obsm['X_tcr_tra_onehot'] = tra_onehot_flat\n",
    "adata.obsm['X_tcr_trb_onehot'] = trb_onehot_flat\n",
    "adata.obsm['X_tcr_tra_kmer'] = tra_kmer_matrix\n",
    "adata.obsm['X_tcr_trb_kmer'] = trb_kmer_matrix\n",
    "\n",
    "# Add physicochemical features to obs\n",
    "for col in tra_physico.columns:\n",
    "    adata.obs[f'tra_{col}'] = tra_physico[col].values\n",
    "    adata.obs[f'trb_{col}'] = trb_physico[col].values\n",
    "\n",
    "print(\"TCR sequence encoding completed and added to AnnData object!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440da71",
   "metadata": {
    "papermill": {
     "duration": 0.018468,
     "end_time": "2025-11-12T03:59:19.160208",
     "exception": false,
     "start_time": "2025-11-12T03:59:19.141740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Create Combined Multi-Modal Encodings\n",
    "\n",
    "Combine gene expression and TCR encodings into multi-modal representations using PCA and UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6b323f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:59:19.197541Z",
     "iopub.status.busy": "2025-11-12T03:59:19.197053Z",
     "iopub.status.idle": "2025-11-12T04:01:37.244546Z",
     "shell.execute_reply": "2025-11-12T04:01:37.243578Z"
    },
    "papermill": {
     "duration": 138.086433,
     "end_time": "2025-11-12T04:01:37.264178",
     "exception": false,
     "start_time": "2025-11-12T03:59:19.177745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing gene expression data...\n",
      "Basic preprocessing completed\n",
      "Encoding gene expression patterns...\n",
      "Gene expression encoding completed!\n",
      "CPU times: user 2min 41s, sys: 3.98 s, total: 2min 45s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Encode Gene Expression Patterns ---\n",
    "\n",
    "print(\"Preprocessing gene expression data...\")\n",
    "\n",
    "# Basic preprocessing if not already done\n",
    "if 'X_pca' not in adata.obsm:\n",
    "    # Store raw counts\n",
    "    adata.raw = adata\n",
    "    \n",
    "    # Normalize counts per cell to a fixed total\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    # Log-transform the data\n",
    "    sc.pp.log1p(adata)\n",
    "    \n",
    "    # Replace any infinite values with zeros\n",
    "    if hasattr(adata.X, 'data'):  # sparse matrix\n",
    "        adata.X.data[np.isinf(adata.X.data)] = 0\n",
    "    else:  # dense matrix\n",
    "        adata.X[np.isinf(adata.X)] = 0\n",
    "    \n",
    "    print(\"Basic preprocessing completed\")\n",
    "\n",
    "print(\"Encoding gene expression patterns...\")\n",
    "\n",
    "# Apply gene expression encoding with fixed function\n",
    "def encode_gene_expression_patterns_fixed(adata, n_top_genes=2000):\n",
    "    \"\"\"\n",
    "    Fixed version of gene expression encoding\n",
    "    \"\"\"\n",
    "    # Select highly variable genes manually to avoid infinity issues\n",
    "    X_dense = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\n",
    "    \n",
    "    # Calculate variance for each gene\n",
    "    gene_vars = np.var(X_dense, axis=0)\n",
    "    # Remove any infinite or NaN values\n",
    "    gene_vars = np.nan_to_num(gene_vars, nan=0, posinf=0, neginf=0)\n",
    "    \n",
    "    # Select top variable genes\n",
    "    top_genes_idx = np.argsort(gene_vars)[-n_top_genes:]\n",
    "    X_hvg = X_dense[:, top_genes_idx]\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_hvg)\n",
    "    \n",
    "    encodings = {}\n",
    "    \n",
    "    # PCA encoding\n",
    "    pca = PCA(n_components=min(50, X_scaled.shape[1]))\n",
    "    encodings['pca'] = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # TruncatedSVD for sparse matrices\n",
    "    svd = TruncatedSVD(n_components=min(50, X_scaled.shape[1]), random_state=42)\n",
    "    encodings['svd'] = svd.fit_transform(X_scaled)\n",
    "    \n",
    "    # UMAP encoding\n",
    "    umap_encoder = umap.UMAP(n_components=20, random_state=42)\n",
    "    encodings['umap'] = umap_encoder.fit_transform(X_scaled)\n",
    "    \n",
    "    return encodings, X_scaled\n",
    "\n",
    "# Apply fixed gene expression encoding\n",
    "gene_encodings, X_scaled_genes = encode_gene_expression_patterns_fixed(adata, n_top_genes=2000)\n",
    "\n",
    "# Add gene expression encodings to AnnData\n",
    "for encoding_name, encoding_data in gene_encodings.items():\n",
    "    adata.obsm[f'X_gene_{encoding_name}'] = encoding_data\n",
    "\n",
    "print(\"Gene expression encoding completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "390d77ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:01:37.302911Z",
     "iopub.status.busy": "2025-11-12T04:01:37.301951Z",
     "iopub.status.idle": "2025-11-12T04:03:01.753804Z",
     "shell.execute_reply": "2025-11-12T04:03:01.752659Z"
    },
    "papermill": {
     "duration": 84.49419,
     "end_time": "2025-11-12T04:03:01.776540",
     "exception": false,
     "start_time": "2025-11-12T04:01:37.282350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating combined multi-modal encodings...\n",
      "Combined gene-TCR encoding shape: (38413, 26)\n",
      "Combined gene-TCR k-mer encoding shape: (38413, 30)\n",
      "Computing dimensionality reduction on combined data...\n",
      "Multi-modal encoding and dimensionality reduction completed!\n",
      "CPU times: user 2min 16s, sys: 7.45 s, total: 2min 23s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Create Combined Multi-Modal Encodings ---\n",
    "print(\"Creating combined multi-modal encodings...\")\n",
    "\n",
    "# Combine different encoding modalities\n",
    "# 1. Gene expression PCA + TCR physicochemical features\n",
    "gene_pca = gene_encodings['pca'][:, :20]  # Top 20 PCA components\n",
    "tcr_features = np.column_stack([\n",
    "    adata.obs[['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']].fillna(0),\n",
    "    adata.obs[['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']].fillna(0)\n",
    "])\n",
    "\n",
    "combined_gene_tcr = np.column_stack([gene_pca, tcr_features])\n",
    "adata.obsm['X_combined_gene_tcr'] = combined_gene_tcr\n",
    "\n",
    "# 2. Gene expression UMAP + TCR k-mer features (reduced)\n",
    "gene_umap = gene_encodings['umap']\n",
    "# Stack TRA and TRB k-mer matrices\n",
    "tcr_kmer_combined = np.column_stack([adata.obsm['X_tcr_tra_kmer'], adata.obsm['X_tcr_trb_kmer']])\n",
    "\n",
    "# Robust PCA reduction for k-mer features\n",
    "try:\n",
    "    n_comp_kmer = min(10, tcr_kmer_combined.shape[1], max(1, tcr_kmer_combined.shape[0]-1))\n",
    "    tcr_kmer_reduced = PCA(n_components=n_comp_kmer, svd_solver='randomized', random_state=42).fit_transform(tcr_kmer_combined)\n",
    "except Exception:\n",
    "    tcr_kmer_reduced = TruncatedSVD(n_components=max(1, min(10, tcr_kmer_combined.shape[1])), random_state=42).fit_transform(tcr_kmer_combined)\n",
    "\n",
    "combined_gene_tcr_kmer = np.column_stack([gene_umap, tcr_kmer_reduced])\n",
    "adata.obsm['X_combined_gene_tcr_kmer'] = combined_gene_tcr_kmer\n",
    "\n",
    "print(f\"Combined gene-TCR encoding shape: {combined_gene_tcr.shape}\")\n",
    "print(f\"Combined gene-TCR k-mer encoding shape: {combined_gene_tcr_kmer.shape}\")\n",
    "\n",
    "# --- Dimensionality Reduction on Combined Data ---\n",
    "print(\"Computing dimensionality reduction on combined data...\")\n",
    "\n",
    "# UMAP on combined data\n",
    "umap_combined = umap.UMAP(n_components=2, random_state=42)\n",
    "adata.obsm['X_umap_combined'] = umap_combined.fit_transform(combined_gene_tcr)\n",
    "\n",
    "# t-SNE on combined data (sample subset for speed)\n",
    "sample_size = min(5000, combined_gene_tcr.shape[0])\n",
    "sample_idx = np.random.choice(combined_gene_tcr.shape[0], sample_size, replace=False)\n",
    "tsne_combined = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tsne_result = tsne_combined.fit_transform(combined_gene_tcr[sample_idx])\n",
    "\n",
    "# Create full t-SNE result array\n",
    "full_tsne = np.zeros((combined_gene_tcr.shape[0], 2))\n",
    "full_tsne[sample_idx] = tsne_result\n",
    "adata.obsm['X_tsne_combined'] = full_tsne\n",
    "\n",
    "print(\"Multi-modal encoding and dimensionality reduction completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07947572",
   "metadata": {
    "papermill": {
     "duration": 0.017745,
     "end_time": "2025-11-12T04:03:01.812489",
     "exception": false,
     "start_time": "2025-11-12T04:03:01.794744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Unsupervised Machine Learning Analysis with Hierarchical Clustering\n",
    "\n",
    "Perform clustering with K-Means, HDBSCAN, Agglomerative, DBSCAN, and add hierarchical clustering using scipy, compute silhouette scores, and visualize with dendrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c0f2445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:03:01.850570Z",
     "iopub.status.busy": "2025-11-12T04:03:01.850188Z",
     "iopub.status.idle": "2025-11-12T05:15:05.054582Z",
     "shell.execute_reply": "2025-11-12T05:15:05.053368Z"
    },
    "papermill": {
     "duration": 4323.228035,
     "end_time": "2025-11-12T05:15:05.058155",
     "exception": false,
     "start_time": "2025-11-12T04:03:01.830120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying unsupervised machine learning algorithms...\n",
      "Running K-Means clustering...\n",
      "Running HDBSCAN clustering...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "check_array() got an unexpected keyword argument 'force_all_finite'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hdbscan/hdbscan_.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_prediction_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch_detection_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_branch_detection_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hdbscan/hdbscan_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prediction_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finite_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_branch_detection_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_relative_validity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: check_array() got an unexpected keyword argument 'force_all_finite'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Unsupervised Machine Learning Analysis ---\n",
    "print(\"Applying unsupervised machine learning algorithms...\")\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "clustering_results_dir = 'Processed_Data/clustering_results'\n",
    "os.makedirs(clustering_results_dir, exist_ok=True)\n",
    "\n",
    "clustering_results = {}\n",
    "\n",
    "sample_size = 2000  # For silhouette computation to save memory\n",
    "\n",
    "# Define feature sets to try\n",
    "feature_names = ['umap', 'tsne', 'combined_scaled']\n",
    "\n",
    "# K-Means clustering with hyperparameter tuning\n",
    "print(\"Running K-Means clustering...\")\n",
    "for feature_name in feature_names:\n",
    "    if feature_name == 'umap':\n",
    "        X_features = adata.obsm['X_umap_combined']\n",
    "    elif feature_name == 'tsne':\n",
    "        X_features = adata.obsm['X_tsne_combined']\n",
    "    elif feature_name == 'combined_scaled':\n",
    "        X_features = StandardScaler().fit_transform(combined_gene_tcr)\n",
    "    \n",
    "    for n_clusters in [3, 4, 5, 6, 8]:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20, max_iter=1000)\n",
    "        cluster_labels = kmeans.fit_predict(X_features)\n",
    "        # Compute silhouette on subsample\n",
    "        subsample_size = min(sample_size, len(X_features))\n",
    "        sample_indices = random.sample(range(len(X_features)), subsample_size)\n",
    "        silhouette = silhouette_score(X_features[sample_indices], cluster_labels[sample_indices])\n",
    "        name = f'kmeans_{n_clusters}_{feature_name}'\n",
    "        np.save(os.path.join(clustering_results_dir, f'{name}_labels.npy'), cluster_labels)\n",
    "        clustering_results[name] = {\n",
    "            'silhouette': silhouette,\n",
    "            'algorithm': f'K-Means ({feature_name})',\n",
    "            'n_clusters': n_clusters,\n",
    "            'feature_set': feature_name\n",
    "        }\n",
    "        del cluster_labels\n",
    "    del X_features\n",
    "    gc.collect()\n",
    "\n",
    "# HDBSCAN clustering with optimized parameters\n",
    "print(\"Running HDBSCAN clustering...\")\n",
    "for feature_name in feature_names:\n",
    "    if feature_name == 'umap':\n",
    "        X_features = adata.obsm['X_umap_combined']\n",
    "    elif feature_name == 'tsne':\n",
    "        X_features = adata.obsm['X_tsne_combined']\n",
    "    elif feature_name == 'combined_scaled':\n",
    "        X_features = StandardScaler().fit_transform(combined_gene_tcr)\n",
    "    \n",
    "    hdbscan_params = [\n",
    "        {'min_cluster_size': 15, 'min_samples': 5},\n",
    "        {'min_cluster_size': 20, 'min_samples': 10},\n",
    "        {'min_cluster_size': 25, 'min_samples': 15}\n",
    "    ]\n",
    "    \n",
    "    for i, params in enumerate(hdbscan_params):\n",
    "        hdbscan_clusterer = hdbscan.HDBSCAN(**params)\n",
    "        hdbscan_labels = hdbscan_clusterer.fit_predict(X_features)\n",
    "        if len(set(hdbscan_labels)) > 1:  # Only compute silhouette if more than 1 cluster\n",
    "            subsample_size = min(sample_size, len(X_features))\n",
    "            sample_indices = random.sample(range(len(X_features)), subsample_size)\n",
    "            hdbscan_silhouette = silhouette_score(X_features[sample_indices], hdbscan_labels[sample_indices])\n",
    "        else:\n",
    "            hdbscan_silhouette = -1\n",
    "        name = f'hdbscan_{feature_name}_{i}'\n",
    "        np.save(os.path.join(clustering_results_dir, f'{name}_labels.npy'), hdbscan_labels)\n",
    "        clustering_results[name] = {\n",
    "            'silhouette': hdbscan_silhouette,\n",
    "            'algorithm': f'HDBSCAN ({feature_name})',\n",
    "            'params': params,\n",
    "            'feature_set': feature_name\n",
    "        }\n",
    "        del hdbscan_labels\n",
    "    del X_features\n",
    "    gc.collect()\n",
    "\n",
    "# # Agglomerative clustering with different linkage methods\n",
    "# print(\"Running Agglomerative clustering...\")\n",
    "# linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "# for feature_name in feature_names:\n",
    "#     if feature_name == 'umap':\n",
    "#         X_features = adata.obsm['X_umap_combined']\n",
    "#     elif feature_name == 'tsne':\n",
    "#         X_features = adata.obsm['X_tsne_combined']\n",
    "#     elif feature_name == 'combined_scaled':\n",
    "#         X_features = StandardScaler().fit_transform(combined_gene_tcr)\n",
    "    \n",
    "#     for linkage in linkage_methods:\n",
    "#         if linkage == 'ward' and feature_name == 'tsne':\n",
    "#             continue  # Ward linkage doesn't work well with t-SNE distances\n",
    "#         for n_clusters in [3, 4, 5, 6]:\n",
    "#             try:\n",
    "#                 agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "#                 agg_labels = agg_clustering.fit_predict(X_features)\n",
    "#                 subsample_size = min(sample_size, len(X_features))\n",
    "#                 sample_indices = random.sample(range(len(X_features)), subsample_size)\n",
    "#                 agg_silhouette = silhouette_score(X_features[sample_indices], agg_labels[sample_indices])\n",
    "#                 name = f'agglomerative_{n_clusters}_{linkage}_{feature_name}'\n",
    "#                 np.save(os.path.join(clustering_results_dir, f'{name}_labels.npy'), agg_labels)\n",
    "#                 clustering_results[name] = {\n",
    "#                     'silhouette': agg_silhouette,\n",
    "#                     'algorithm': f'Agglomerative ({linkage}, {feature_name})',\n",
    "#                     'n_clusters': n_clusters,\n",
    "#                     'linkage': linkage,\n",
    "#                     'feature_set': feature_name\n",
    "#                 }\n",
    "#                 del agg_labels\n",
    "#             except:\n",
    "#                 continue\n",
    "#     del X_features\n",
    "#     gc.collect()\n",
    "\n",
    "# DBSCAN clustering with parameter optimization\n",
    "print(\"Running DBSCAN clustering...\")\n",
    "eps_values = [0.3, 0.5, 0.8, 1.0]\n",
    "min_samples_values = [5, 10, 15]\n",
    "\n",
    "for feature_name in feature_names:\n",
    "    if feature_name == 'umap':\n",
    "        X_features = adata.obsm['X_umap_combined']\n",
    "    elif feature_name == 'tsne':\n",
    "        X_features = adata.obsm['X_tsne_combined']\n",
    "    elif feature_name == 'combined_scaled':\n",
    "        X_features = StandardScaler().fit_transform(combined_gene_tcr)\n",
    "    \n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            dbscan_labels = dbscan.fit_predict(X_features)\n",
    "            if len(set(dbscan_labels)) > 1 and -1 not in dbscan_labels:\n",
    "                subsample_size = min(sample_size, len(X_features))\n",
    "                sample_indices = random.sample(range(len(X_features)), subsample_size)\n",
    "                dbscan_silhouette = silhouette_score(X_features[sample_indices], dbscan_labels[sample_indices])\n",
    "            else:\n",
    "                dbscan_silhouette = -1\n",
    "            name = f'dbscan_{eps}_{min_samples}_{feature_name}'\n",
    "            np.save(os.path.join(clustering_results_dir, f'{name}_labels.npy'), dbscan_labels)\n",
    "            clustering_results[name] = {\n",
    "                'silhouette': dbscan_silhouette,\n",
    "                'algorithm': f'DBSCAN ({feature_name})',\n",
    "                'eps': eps,\n",
    "                'min_samples': min_samples,\n",
    "                'feature_set': feature_name\n",
    "            }\n",
    "            del dbscan_labels\n",
    "    del X_features\n",
    "    gc.collect()\n",
    "\n",
    "# Gaussian Mixture Models\n",
    "print(\"Running Gaussian Mixture Models...\")\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "for feature_name in feature_names:\n",
    "    if feature_name == 'umap':\n",
    "        X_features = adata.obsm['X_umap_combined']\n",
    "    elif feature_name == 'tsne':\n",
    "        X_features = adata.obsm['X_tsne_combined']\n",
    "    elif feature_name == 'combined_scaled':\n",
    "        X_features = StandardScaler().fit_transform(combined_gene_tcr)\n",
    "    \n",
    "    for n_components in [3, 4, 5, 6]:\n",
    "        for covariance_type in ['full', 'tied', 'diag', 'spherical']:\n",
    "            try:\n",
    "                gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=42)\n",
    "                gmm_labels = gmm.fit_predict(X_features)\n",
    "                subsample_size = min(sample_size, len(X_features))\n",
    "                sample_indices = random.sample(range(len(X_features)), subsample_size)\n",
    "                gmm_silhouette = silhouette_score(X_features[sample_indices], gmm_labels[sample_indices])\n",
    "                name = f'gmm_{n_components}_{covariance_type}_{feature_name}'\n",
    "                np.save(os.path.join(clustering_results_dir, f'{name}_labels.npy'), gmm_labels)\n",
    "                clustering_results[name] = {\n",
    "                    'silhouette': gmm_silhouette,\n",
    "                    'algorithm': f'GMM ({covariance_type}, {feature_name})',\n",
    "                    'n_components': n_components,\n",
    "                    'covariance_type': covariance_type,\n",
    "                    'feature_set': feature_name\n",
    "                }\n",
    "                del gmm_labels\n",
    "            except:\n",
    "                continue\n",
    "    del X_features\n",
    "    gc.collect()\n",
    "\n",
    "# Hierarchical clustering with different methods\n",
    "print(\"Running Hierarchical clustering...\")\n",
    "for feature_name in feature_names:\n",
    "    if feature_name == 'umap':\n",
    "        X_features = adata.obsm['X_umap_combined']\n",
    "    elif feature_name == 'tsne':\n",
    "        X_features = adata.obsm['X_tsne_combined']\n",
    "    elif feature_name == 'combined_scaled':\n",
    "        X_features = StandardScaler().fit_transform(combined_gene_tcr)\n",
    "    \n",
    "    try:\n",
    "        # Use different linkage methods for hierarchical clustering\n",
    "        for method in ['ward', 'complete', 'average']:\n",
    "            if method == 'ward' and feature_name == 'tsne':\n",
    "                continue\n",
    "            subsample_indices = random.sample(range(len(X_features)), min(sample_size, len(X_features)))\n",
    "            X_subsample = X_features[subsample_indices]\n",
    "            Z = linkage(X_subsample, method=method)\n",
    "            for t in [3, 4, 5, 6]:\n",
    "                hierarchical_labels = fcluster(Z, t=t, criterion='maxclust')\n",
    "                # Map back to full dataset indices, but since it's subsample, labels are for subsample\n",
    "                # For simplicity, save subsample labels, but note it's on subsample\n",
    "                hierarchical_silhouette = silhouette_score(X_subsample, hierarchical_labels)\n",
    "                name = f'hierarchical_{method}_{t}_{feature_name}'\n",
    "                np.save(os.path.join(clustering_results_dir, f'{name}_labels.npy'), hierarchical_labels)\n",
    "                clustering_results[name] = {\n",
    "                    'silhouette': hierarchical_silhouette,\n",
    "                    'algorithm': f'Hierarchical ({method}, {feature_name})',\n",
    "                    'method': method,\n",
    "                    't': t,\n",
    "                    'feature_set': feature_name,\n",
    "                    'note': 'Computed on subsample'\n",
    "                }\n",
    "                del hierarchical_labels\n",
    "    except:\n",
    "        continue\n",
    "    del X_features\n",
    "    gc.collect()\n",
    "\n",
    "# Print clustering results summary\n",
    "print(\"\\nClustering Results Summary:\")\n",
    "print(f\"{'Method':<40} {'Clusters':<10} {'Silhouette':<12} {'Feature Set':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for name, result in sorted(clustering_results.items(), key=lambda x: x[1]['silhouette'], reverse=True):\n",
    "    # Load labels to count clusters\n",
    "    labels = np.load(os.path.join(clustering_results_dir, f'{name}_labels.npy'))\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    print(f\"{result['algorithm']:<40} {n_clusters:<10} {result['silhouette']:<12.3f} {result['feature_set']:<15}\")\n",
    "    del labels\n",
    "    gc.collect()\n",
    "\n",
    "# Find best clustering result\n",
    "best_clustering_name = max(clustering_results.items(), key=lambda x: x[1]['silhouette'])[0]\n",
    "best_result = clustering_results[best_clustering_name]\n",
    "best_labels = np.load(os.path.join(clustering_results_dir, f'{best_clustering_name}_labels.npy'))\n",
    "# For hierarchical, if it's on subsample, need to handle differently, but for now, assume best is not hierarchical or handle later\n",
    "if 'hierarchical' in best_clustering_name:\n",
    "    print(\"Best clustering is hierarchical on subsample. Consider re-running on full data if needed.\")\n",
    "else:\n",
    "    adata.obs[best_clustering_name] = pd.Categorical(best_labels)\n",
    "print(f\"\\nBest clustering: {best_clustering_name} (Silhouette: {best_result['silhouette']:.3f})\")\n",
    "print(f\"Algorithm: {best_result['algorithm']}\")\n",
    "print(f\"Feature set: {best_result['feature_set']}\")\n",
    "print(f\"Number of clusters: {len(set(best_labels)) - (1 if -1 in best_labels else 0)}\")\n",
    "\n",
    "# Save clustering_results metadata\n",
    "import json\n",
    "with open(os.path.join(clustering_results_dir, 'clustering_metadata.json'), 'w') as f:\n",
    "    json.dump(clustering_results, f, indent=4)\n",
    "\n",
    "del best_labels, clustering_results\n",
    "gc.collect()\n",
    "\n",
    "# --- 2. TCR Sequence-Specific Clustering ---\n",
    "print(\"\\nPerforming TCR sequence-specific clustering...\")\n",
    "\n",
    "# Cluster based on TRA k-mer features with improved parameters\n",
    "tra_scaler = StandardScaler()\n",
    "tra_kmer_scaled = tra_scaler.fit_transform(adata.obsm['X_tcr_tra_kmer'])\n",
    "\n",
    "# K-means on TRA sequences with optimal k\n",
    "tra_kmeans = KMeans(n_clusters=6, random_state=42, n_init=20)  # Increased n_clusters\n",
    "tra_clusters = tra_kmeans.fit_predict(tra_kmer_scaled)\n",
    "adata.obs['tra_kmer_clusters'] = pd.Categorical(tra_clusters)\n",
    "\n",
    "# K-means on TRB sequences with optimal k\n",
    "trb_scaler = StandardScaler()\n",
    "trb_kmer_scaled = trb_scaler.fit_transform(adata.obsm['X_tcr_trb_kmer'])\n",
    "trb_kmeans = KMeans(n_clusters=6, random_state=42, n_init=20)  # Increased n_clusters\n",
    "trb_clusters = trb_kmeans.fit_predict(trb_kmer_scaled)\n",
    "adata.obs['trb_kmer_clusters'] = pd.Categorical(trb_clusters)\n",
    "\n",
    "print(\"TCR sequence clustering completed!\")\n",
    "\n",
    "# --- 3. Gene Expression Module Discovery ---\n",
    "print(\"\\nDiscovering gene expression modules...\")\n",
    "\n",
    "# Use gene expression PCA for module discovery with optimal k\n",
    "gene_pca_full = gene_encodings['pca']\n",
    "gene_kmeans = KMeans(n_clusters=8, random_state=42, n_init=20)  # Increased n_clusters\n",
    "gene_expression_modules = gene_kmeans.fit_predict(gene_pca_full)\n",
    "adata.obs['gene_expression_modules'] = pd.Categorical(gene_expression_modules)\n",
    "\n",
    "print(\"Gene expression module discovery completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e521b7b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T05:15:05.109980Z",
     "iopub.status.busy": "2025-11-12T05:15:05.109633Z",
     "iopub.status.idle": "2025-11-12T05:15:05.119069Z",
     "shell.execute_reply": "2025-11-12T05:15:05.118345Z"
    },
    "papermill": {
     "duration": 0.037389,
     "end_time": "2025-11-12T05:15:05.120518",
     "exception": false,
     "start_time": "2025-11-12T05:15:05.083129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gene_expression_modules' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gene_expression_modules' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "adata.obs['gene_expression_modules'] = pd.Categorical(gene_expression_modules)\n",
    "\n",
    "print(\"Gene expression module discovery completed!\")\n",
    "\n",
    "# --- 4. Dendrogram Visualization for Hierarchical Clustering ---\n",
    "print(\"\\nCreating dendrogram for hierarchical clustering...\")\n",
    "\n",
    "# Create fresh hierarchical clustering for dendrogram visualization\n",
    "# Use the best feature set from clustering results (typically UMAP or combined_scaled)\n",
    "try:\n",
    "    X_for_dendrogram = X_umap[:2000]  # Use first 2000 samples for speed\n",
    "    Z = linkage(X_for_dendrogram, method='ward')\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    dendrogram(Z, truncate_mode='lastp', p=12, leaf_rotation=45, leaf_font_size=10, show_contracted=True)\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "    plt.xlabel('Sample index')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()\n",
    "    print(\"Dendrogram visualization completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create dendrogram: {e}\")\n",
    "    print(\"Skipping dendrogram visualization\")\n",
    "\n",
    "print(\"\\nUnsupervised machine learning analysis completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95cb38ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T05:15:05.168177Z",
     "iopub.status.busy": "2025-11-12T05:15:05.167851Z",
     "iopub.status.idle": "2025-11-12T05:15:18.240902Z",
     "shell.execute_reply": "2025-11-12T05:15:18.239769Z"
    },
    "papermill": {
     "duration": 13.100312,
     "end_time": "2025-11-12T05:15:18.243088",
     "exception": false,
     "start_time": "2025-11-12T05:15:05.142776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating comprehensive feature set using ALL available encodings...\n",
      "Applying strategic dimensionality reduction to high-dimensional features...\n",
      "Working with 38413 samples for supervised learning\n",
      "Class distribution: {'Non-Responder': 10436, 'Responder': 27977}\n",
      "Reducing k-mer features by variance selection...\n",
      "TRA k-mers reduced from 6074 to 200\n",
      "TRB k-mers reduced from 6091 to 200\n",
      "\n",
      "Feature set dimensions:\n",
      "  • basic: (38413, 29)\n",
      "  • gene_enhanced: (38413, 109)\n",
      "  • tcr_enhanced: (38413, 429)\n",
      "  • comprehensive: (38413, 499)\n",
      "  • sequence_structure: (38413, 139)\n",
      "Comprehensive feature engineering completed!\n",
      "CPU times: user 23.4 s, sys: 1.09 s, total: 24.5 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Comprehensive Feature Engineering ---\n",
    "\n",
    "print(\"Creating comprehensive feature set using ALL available encodings...\")\n",
    "\n",
    "# --- 1. Strategic Feature Engineering with Dimensionality Reduction ---\n",
    "print(\"Applying strategic dimensionality reduction to high-dimensional features...\")\n",
    "\n",
    "# Filter for supervised learning samples first to reduce memory\n",
    "supervised_mask = adata.obs['response'].isin(['Responder', 'Non-Responder'])\n",
    "y_supervised = adata.obs['response'][supervised_mask]\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_supervised)\n",
    "\n",
    "print(f\"Working with {sum(supervised_mask)} samples for supervised learning\")\n",
    "print(f\"Class distribution: {dict(zip(label_encoder.classes_, np.bincount(y_encoded)))}\")\n",
    "\n",
    "# --- Reduce high-dimensional k-mer features using variance-based selection ---\n",
    "tra_kmer_supervised = adata.obsm['X_tcr_tra_kmer'][supervised_mask]\n",
    "trb_kmer_supervised = adata.obsm['X_tcr_trb_kmer'][supervised_mask]\n",
    "\n",
    "# Select top variance k-mers to reduce dimensionality\n",
    "def select_top_variance_features(X, n_features=200):\n",
    "    \"\"\"Select features with highest variance\"\"\"\n",
    "    variances = np.var(X, axis=0)\n",
    "    top_indices = np.argsort(variances)[-n_features:]\n",
    "    return X[:, top_indices], top_indices\n",
    "\n",
    "print(\"Reducing k-mer features by variance selection...\")\n",
    "tra_kmer_reduced, tra_top_idx = select_top_variance_features(tra_kmer_supervised, n_features=200)\n",
    "trb_kmer_reduced, trb_top_idx = select_top_variance_features(trb_kmer_supervised, n_features=200)\n",
    "\n",
    "print(f\"TRA k-mers reduced from {tra_kmer_supervised.shape[1]} to {tra_kmer_reduced.shape[1]}\")\n",
    "print(f\"TRB k-mers reduced from {trb_kmer_supervised.shape[1]} to {trb_kmer_reduced.shape[1]}\")\n",
    "\n",
    "# --- 2. Create strategic feature combinations ---\n",
    "feature_sets = {}\n",
    "\n",
    "# Basic features (gene expression + physicochemical)\n",
    "gene_features = adata.obsm['X_gene_pca'][supervised_mask]\n",
    "tcr_physico = np.column_stack([\n",
    "    adata.obs[['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']].fillna(0)[supervised_mask],\n",
    "    adata.obs[['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']].fillna(0)[supervised_mask]\n",
    "])\n",
    "qc_features = adata.obs[['n_genes_by_counts', 'total_counts', 'pct_counts_mt']].fillna(0)[supervised_mask].values\n",
    "\n",
    "feature_sets['basic'] = np.column_stack([\n",
    "    gene_features[:, :20],  # Top 20 gene PCA components\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# Enhanced gene expression\n",
    "feature_sets['gene_enhanced'] = np.column_stack([\n",
    "    adata.obsm['X_gene_pca'][supervised_mask],  # All 50 PCA components\n",
    "    adata.obsm['X_gene_svd'][supervised_mask][:, :30],  # Top 30 SVD components\n",
    "    adata.obsm['X_gene_umap'][supervised_mask],  # All 20 UMAP components\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# TCR sequence enhanced\n",
    "feature_sets['tcr_enhanced'] = np.column_stack([\n",
    "    gene_features[:, :20],  # Top 20 gene PCA\n",
    "    tra_kmer_reduced,  # Top 200 TRA k-mers\n",
    "    trb_kmer_reduced,  # Top 200 TRB k-mers\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# Comprehensive (all modalities)\n",
    "feature_sets['comprehensive'] = np.column_stack([\n",
    "    adata.obsm['X_gene_pca'][supervised_mask],  # 50 features\n",
    "    adata.obsm['X_gene_svd'][supervised_mask][:, :20],  # Top 20 SVD\n",
    "    adata.obsm['X_gene_umap'][supervised_mask],  # 20 features\n",
    "    tra_kmer_reduced,  # 200 features\n",
    "    trb_kmer_reduced,  # 200 features  \n",
    "    tcr_physico,  # 6 features\n",
    "    qc_features  # 3 features\n",
    "])\n",
    "\n",
    "# One-hot encoded sequences (reduced)\n",
    "# Use robust PCA reduction with fallback to TruncatedSVD\n",
    "try:\n",
    "    n_comp_onehot = min(50, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[1], max(1, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[0]-1))\n",
    "    onehot_tra_reduced = PCA(n_components=n_comp_onehot, svd_solver='randomized', random_state=42).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "except Exception:\n",
    "    onehot_tra_reduced = TruncatedSVD(n_components=max(1, min(50, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[1])), random_state=42).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "\n",
    "try:\n",
    "    n_comp_onehot = min(50, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[1], max(1, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[0]-1))\n",
    "    onehot_trb_reduced = PCA(n_components=n_comp_onehot, svd_solver='randomized', random_state=42).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "except Exception:\n",
    "    onehot_trb_reduced = TruncatedSVD(n_components=max(1, min(50, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[1])), random_state=42).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "\n",
    "feature_sets['sequence_structure'] = np.column_stack([\n",
    "    gene_features[:, :30],  # Top 30 gene PCA\n",
    "    onehot_tra_reduced,  # 50 PCA of one-hot TRA\n",
    "    onehot_trb_reduced,  # 50 PCA of one-hot TRB\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "print(f\"\\nFeature set dimensions:\")\n",
    "for name, features in feature_sets.items():\n",
    "    print(f\"  • {name}: {features.shape}\")\n",
    "\n",
    "print(\"Comprehensive feature engineering completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dc28dcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T05:15:18.293559Z",
     "iopub.status.busy": "2025-11-12T05:15:18.292505Z",
     "iopub.status.idle": "2025-11-12T10:31:08.618650Z",
     "shell.execute_reply": "2025-11-12T10:31:08.616738Z"
    },
    "papermill": {
     "duration": 18950.354643,
     "end_time": "2025-11-12T10:31:08.622156",
     "exception": false,
     "start_time": "2025-11-12T05:15:18.267513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating multiple supervised learning models...\n",
      "\n",
      "============================================================\n",
      "FEATURE SET: BASIC (29 features)\n",
      "============================================================\n",
      "\n",
      "--- Logistic Regression ---\n",
      "Best params: {'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Accuracy: 0.773\n",
      "Precision: 0.795, Recall: 0.926, F1: 0.856\n",
      "Specificity: 0.362, NPV: 0.645, AUC: 0.788\n",
      "CV Accuracy: 0.728 ± 0.000\n",
      "\n",
      "--- Decision Tree ---\n",
      "Best params: {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Accuracy: 0.753\n",
      "Precision: 0.787, Recall: 0.905, F1: 0.842\n",
      "Specificity: 0.345, NPV: 0.574, AUC: 0.731\n",
      "CV Accuracy: 0.749 ± 0.005\n",
      "\n",
      "--- Random Forest ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1334\u001b[0m                 )\n\u001b[1;32m   1335\u001b[0m             ):\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1051\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    997\u001b[0m                     )\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m   1000\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m   1001\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         )\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config_and_warning_filters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Supervised Learning with Multiple Models ---\n",
    "\n",
    "print(\"Training and evaluating multiple supervised learning models...\")\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "models['XGBoost'] = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "\n",
    "def create_deep_model(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_dim=input_dim),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "models['Deep Learning'] = 'DL'  # Placeholder\n",
    "\n",
    "# Hyperparameter grids for experimentation\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [5, 10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.3]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store results\n",
    "all_results = {}\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "for feature_name, X_features in feature_sets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FEATURE SET: {feature_name.upper()} ({X_features.shape[1]} features)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_features, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    feature_results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n--- {model_name} ---\")\n",
    "        \n",
    "        if model_name == 'Deep Learning':\n",
    "            # Special handling for DL\n",
    "            dl_model = create_deep_model(X_train_scaled.shape[1])\n",
    "            history = dl_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "            y_pred_proba = dl_model.predict(X_test_scaled).flatten()\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        else:\n",
    "            # Grid search for hyperparameters\n",
    "            if model_name in param_grids:\n",
    "                # Use StratifiedKFold for k-fold CV inside GridSearch\n",
    "                grid_search = GridSearchCV(model, param_grids[model_name], cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring='accuracy', n_jobs=-1)\n",
    "                grid_search.fit(X_train_scaled, y_train)\n",
    "                best_model = grid_search.best_estimator_\n",
    "                print(f\"Best params: {grid_search.best_params_}\")\n",
    "            else:\n",
    "                best_model = model\n",
    "                best_model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            y_pred = best_model.predict(X_test_scaled)\n",
    "            # handle cases where predict_proba may not exist\n",
    "            try:\n",
    "                y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "            except Exception:\n",
    "                # fallback to decision_function or zeros\n",
    "                try:\n",
    "                    y_pred_proba = best_model.decision_function(X_test_scaled)\n",
    "                    if y_pred_proba.ndim > 1:\n",
    "                        y_pred_proba = y_pred_proba[:, 1]\n",
    "                except Exception:\n",
    "                    y_pred_proba = np.zeros_like(y_pred, dtype=float)\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        except Exception:\n",
    "            auc = float('nan')\n",
    "        \n",
    "        # Specificity\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "        \n",
    "        # NPV\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else float('nan')\n",
    "        \n",
    "        # Cross-validation (use StratifiedKFold)\n",
    "        if model_name != 'Deep Learning':\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            cv_scores = cross_val_score(best_model, X_features, y_encoded, cv=cv, scoring='accuracy')\n",
    "            cv_mean = cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "        else:\n",
    "            cv_mean = accuracy  # Approximation\n",
    "            cv_std = 0\n",
    "        \n",
    "        feature_results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'auc': auc,\n",
    "            'specificity': specificity,\n",
    "            'npv': npv,\n",
    "            'cv_mean': cv_mean,\n",
    "            'cv_std': cv_std,\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "        print(f\"Specificity: {specificity:.3f}, NPV: {npv:.3f}, AUC: {auc:.3f}\")\n",
    "        print(f\"CV Accuracy: {cv_mean:.3f} ± {cv_std:.3f}\")\n",
    "    \n",
    "    all_results[feature_name] = feature_results\n",
    "\n",
    "# Find best overall model\n",
    "best_score = 0\n",
    "best_model_info = None\n",
    "for feature_name, feature_result in all_results.items():\n",
    "    for model_name, result in feature_result.items():\n",
    "        if result['cv_mean'] > best_score:\n",
    "            best_score = result['cv_mean']\n",
    "            best_model_info = (feature_name, model_name, result)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BEST MODEL SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Feature Set: {best_model_info[0]}\")\n",
    "print(f\"Model: {best_model_info[1]}\")\n",
    "print(f\"CV Accuracy: {best_score:.3f}\")\n",
    "print(f\"Test Accuracy: {best_model_info[2]['accuracy']:.3f}\")\n",
    "print(f\"AUC: {best_model_info[2]['auc']:.3f}\")\n",
    "\n",
    "print(\"\\nSupervised learning with multiple models completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687ab5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T10:31:08.685392Z",
     "iopub.status.busy": "2025-11-12T10:31:08.684684Z",
     "iopub.status.idle": "2025-11-12T10:31:10.820593Z",
     "shell.execute_reply": "2025-11-12T10:31:10.819664Z"
    },
    "papermill": {
     "duration": 2.169788,
     "end_time": "2025-11-12T10:31:10.821946",
     "exception": false,
     "start_time": "2025-11-12T10:31:08.652158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Model Performance Evaluation ---\n",
    "\n",
    "print(\"Generating comprehensive model performance evaluation...\")\n",
    "\n",
    "# Create detailed performance report\n",
    "performance_report = []\n",
    "\n",
    "for feature_name, feature_result in all_results.items():\n",
    "    for model_name, result in feature_result.items():\n",
    "        cm = result['confusion_matrix']\n",
    "        \n",
    "        report_entry = {\n",
    "            'Feature_Set': feature_name,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'Precision': result['precision'],\n",
    "            'Recall': result['recall'],\n",
    "            'F1_Score': result['f1_score'],\n",
    "            'AUC': result['auc'],\n",
    "            'Specificity': result['specificity'],\n",
    "            'NPV': result['npv'],\n",
    "            'CV_Mean': result['cv_mean'],\n",
    "            'CV_Std': result['cv_std'],\n",
    "            'TN': cm[0,0],\n",
    "            'FP': cm[0,1],\n",
    "            'FN': cm[1,0],\n",
    "            'TP': cm[1,1]\n",
    "        }\n",
    "        performance_report.append(report_entry)\n",
    "\n",
    "performance_df = pd.DataFrame(performance_report)\n",
    "\n",
    "# Display results\n",
    "print(\"COMPREHENSIVE MODEL PERFORMANCE REPORT\")\n",
    "print(\"=\"*120)\n",
    "print(performance_df.to_string(index=False))\n",
    "\n",
    "# Visualize confusion matrices for best models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "best_models = performance_df.nlargest(6, 'CV_Mean')\n",
    "\n",
    "for i, (_, row) in enumerate(best_models.iterrows()):\n",
    "    if i < 6:\n",
    "        cm = np.array([[row['TN'], row['FP']], [row['FN'], row['TP']]])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Responder', 'Responder'])\n",
    "        disp.plot(ax=axes[i], cmap='Blues')\n",
    "        axes[i].set_title(f\"{row['Model']}\\n({row['Feature_Set']})\\nAcc: {row['Accuracy']:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-validation comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "cv_data = performance_df.pivot(index='Feature_Set', columns='Model', values='CV_Mean')\n",
    "cv_data.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('Cross-Validation Accuracy Comparison')\n",
    "plt.ylabel('CV Accuracy')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# AUC comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "auc_data = performance_df.pivot(index='Feature_Set', columns='Model', values='AUC')\n",
    "auc_data.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('AUC Comparison Across Models and Feature Sets')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel performance evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f22dbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T10:31:10.892508Z",
     "iopub.status.busy": "2025-11-12T10:31:10.892130Z",
     "iopub.status.idle": "2025-11-12T10:33:55.974954Z",
     "shell.execute_reply": "2025-11-12T10:33:55.974016Z"
    },
    "papermill": {
     "duration": 165.121248,
     "end_time": "2025-11-12T10:33:55.976372",
     "exception": false,
     "start_time": "2025-11-12T10:31:10.855124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Experiment with Sequence Length Cutoffs ---\n",
    "\n",
    "print(\"Experimenting with sequence length cutoffs...\")\n",
    "\n",
    "# Define length cutoffs to test\n",
    "length_cutoffs = [10, 15, 20, 25, 30, 35, 40, 50]\n",
    "\n",
    "length_results = []\n",
    "\n",
    "for max_length in length_cutoffs:\n",
    "    print(f\"\\nTesting max sequence length: {max_length}\")\n",
    "    \n",
    "    # Re-encode sequences with new length\n",
    "    tra_onehot_new = np.array([one_hot_encode_sequence(seq, max_length, 'ACDEFGHIKLMNPQRSTVWY') \n",
    "                               for seq in cdr3_sequences['TRA']])\n",
    "    tra_onehot_flat_new = tra_onehot_new.reshape(tra_onehot_new.shape[0], -1)\n",
    "    \n",
    "    trb_onehot_new = np.array([one_hot_encode_sequence(seq, max_length, 'ACDEFGHIKLMNPQRSTVWY') \n",
    "                               for seq in cdr3_sequences['TRB']])\n",
    "    trb_onehot_flat_new = trb_onehot_new.reshape(trb_onehot_new.shape[0], -1)\n",
    "    \n",
    "    # Update AnnData\n",
    "    adata.obsm['X_tcr_tra_onehot'] = tra_onehot_flat_new\n",
    "    adata.obsm['X_tcr_trb_onehot'] = trb_onehot_flat_new\n",
    "    \n",
    "    # Re-create feature sets with new encodings using robust PCA\n",
    "    # Use robust PCA reduction with fallback to TruncatedSVD\n",
    "    try:\n",
    "        n_comp_onehot = min(50, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[1], max(1, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[0]-1))\n",
    "        onehot_tra_reduced = PCA(n_components=n_comp_onehot, svd_solver='randomized', random_state=42).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "    except Exception as e:\n",
    "        print(f\"  PCA failed for TRA ({e}), using TruncatedSVD\")\n",
    "        n_comp = max(1, min(50, adata.obsm['X_tcr_tra_onehot'][supervised_mask].shape[1]-1))\n",
    "        onehot_tra_reduced = TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "    \n",
    "    try:\n",
    "        n_comp_onehot = min(50, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[1], max(1, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[0]-1))\n",
    "        onehot_trb_reduced = PCA(n_components=n_comp_onehot, svd_solver='randomized', random_state=42).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "    except Exception as e:\n",
    "        print(f\"  PCA failed for TRB ({e}), using TruncatedSVD\")\n",
    "        n_comp = max(1, min(50, adata.obsm['X_tcr_trb_onehot'][supervised_mask].shape[1]-1))\n",
    "        onehot_trb_reduced = TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "    \n",
    "    X_sequence = np.column_stack([\n",
    "        gene_features[:, :30],\n",
    "        onehot_tra_reduced,\n",
    "        onehot_trb_reduced,\n",
    "        tcr_physico,\n",
    "        qc_features\n",
    "    ])\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_sequence, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_sequence, y_encoded, cv=3, scoring='accuracy')\n",
    "    \n",
    "    length_results.append({\n",
    "        'max_length': max_length,\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.3f}, CV: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "# Plot results\n",
    "length_df = pd.DataFrame(length_results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(length_df['max_length'], length_df['accuracy'], 'o-', label='Test Accuracy', linewidth=2)\n",
    "plt.plot(length_df['max_length'], length_df['cv_mean'], 's-', label='CV Accuracy', linewidth=2)\n",
    "plt.fill_between(length_df['max_length'], \n",
    "                 length_df['cv_mean'] - length_df['cv_std'], \n",
    "                 length_df['cv_mean'] + length_df['cv_std'], \n",
    "                 alpha=0.3, label='CV ± Std')\n",
    "plt.xlabel('Maximum Sequence Length Cutoff')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy vs Sequence Length Cutoff')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSequence length cutoff experiment completed!\")\n",
    "print(f\"Optimal length appears to be around {length_df.loc[length_df['cv_mean'].idxmax(), 'max_length']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9de33b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T10:33:56.046079Z",
     "iopub.status.busy": "2025-11-12T10:33:56.045773Z",
     "iopub.status.idle": "2025-11-12T10:34:03.639952Z",
     "shell.execute_reply": "2025-11-12T10:34:03.639008Z"
    },
    "papermill": {
     "duration": 7.633045,
     "end_time": "2025-11-12T10:34:03.643359",
     "exception": false,
     "start_time": "2025-11-12T10:33:56.010314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Comprehensive Sequence Pattern Discovery ---\n",
    "\n",
    "print(\"Performing comprehensive sequence pattern discovery...\")\n",
    "\n",
    "# --- 1. Analyze TCR sequence patterns by response ---\n",
    "responder_mask = adata.obs['response'] == 'Responder'\n",
    "non_responder_mask = adata.obs['response'] == 'Non-Responder'\n",
    "\n",
    "# Get sequence data for analysis\n",
    "responder_tra = adata.obs[responder_mask]['cdr3_TRA'].dropna()\n",
    "non_responder_tra = adata.obs[non_responder_mask]['cdr3_TRA'].dropna()\n",
    "responder_trb = adata.obs[responder_mask]['cdr3_TRB'].dropna()\n",
    "non_responder_trb = adata.obs[non_responder_mask]['cdr3_TRB'].dropna()\n",
    "\n",
    "print(\"\\n--- TCR SEQUENCE PATTERNS ---\")\n",
    "print(f\"Responder TRA sequences: {len(responder_tra)}\")\n",
    "print(f\"Non-responder TRA sequences: {len(non_responder_tra)}\")\n",
    "\n",
    "print(\"\\nTop TRA CDR3 sequences in Responders:\")\n",
    "top_responder_tra = responder_tra.value_counts().head(10)\n",
    "for seq, count in top_responder_tra.items():\n",
    "    print(f\"  {seq}: {count} cells\")\n",
    "\n",
    "print(\"\\nTop TRA CDR3 sequences in Non-Responders:\")\n",
    "top_non_responder_tra = non_responder_tra.value_counts().head(10)\n",
    "for seq, count in top_non_responder_tra.items():\n",
    "    print(f\"  {seq}: {count} cells\")\n",
    "\n",
    "# --- 2. Physicochemical property analysis ---\n",
    "print(\"\\n--- PHYSICOCHEMICAL PROPERTY ANALYSIS ---\")\n",
    "\n",
    "# Compare physicochemical properties between responders and non-responders\n",
    "responder_physico = adata.obs[responder_mask][['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity',\n",
    "                                               'trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']]\n",
    "non_responder_physico = adata.obs[non_responder_mask][['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity',\n",
    "                                                       'trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']]\n",
    "\n",
    "properties = ['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity', \n",
    "              'trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']\n",
    "\n",
    "print(\"Statistical comparison of physicochemical properties:\")\n",
    "print(f\"{'Property':<20} {'Responder Mean':<15} {'Non-Resp Mean':<15} {'P-value':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for prop in properties:\n",
    "    resp_values = responder_physico[prop].dropna()\n",
    "    non_resp_values = non_responder_physico[prop].dropna()\n",
    "    \n",
    "    if len(resp_values) > 0 and len(non_resp_values) > 0:\n",
    "        statistic, p_value = mannwhitneyu(resp_values, non_resp_values, alternative='two-sided')\n",
    "        print(f\"{prop:<20} {resp_values.mean():<15.3f} {non_resp_values.mean():<15.3f} {p_value:<10.6f}\")\n",
    "\n",
    "# --- 3. K-mer differential analysis ---\n",
    "print(\"\\n--- K-MER DIFFERENTIAL ANALYSIS ---\")\n",
    "\n",
    "# Use the indices we identified earlier for top variance k-mers\n",
    "responder_indices = np.where(supervised_mask & responder_mask)[0]\n",
    "non_responder_indices = np.where(supervised_mask & non_responder_mask)[0]\n",
    "\n",
    "# Get k-mer data for responders vs non-responders\n",
    "responder_tra_kmers = tra_kmer_supervised[responder_indices - np.where(supervised_mask)[0][0]]\n",
    "non_responder_tra_kmers = tra_kmer_supervised[non_responder_indices - np.where(supervised_mask)[0][0]]\n",
    "\n",
    "# Calculate mean k-mer frequencies\n",
    "responder_tra_mean = responder_tra_kmers.mean(axis=0)\n",
    "non_responder_tra_mean = non_responder_tra_kmers.mean(axis=0)\n",
    "\n",
    "# Find most differentially expressed k-mers from selected features\n",
    "kmer_diff = responder_tra_mean - non_responder_tra_mean\n",
    "top_responder_kmers_idx = np.argsort(kmer_diff)[-10:]\n",
    "top_non_responder_kmers_idx = np.argsort(kmer_diff)[:10]\n",
    "\n",
    "# Get the actual k-mer sequences for the selected features\n",
    "selected_tra_kmers = [unique_tra_kmers[tra_top_idx[i]] for i in range(len(tra_top_idx))]\n",
    "\n",
    "print(\"Top k-mers enriched in Responders (from variance-selected features):\")\n",
    "for idx in top_responder_kmers_idx:\n",
    "    if idx < len(selected_tra_kmers):\n",
    "        print(f\"  {selected_tra_kmers[idx]}: +{kmer_diff[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop k-mers enriched in Non-Responders (from variance-selected features):\")\n",
    "for idx in top_non_responder_kmers_idx:\n",
    "    if idx < len(selected_tra_kmers):\n",
    "        print(f\"  {selected_tra_kmers[idx]}: {kmer_diff[idx]:.4f}\")\n",
    "\n",
    "\n",
    "# --- 4. Gene expression analysis for top important features ---\n",
    "print(\"\\n--- GENE EXPRESSION PATTERN ANALYSIS ---\")\n",
    "\n",
    "# Get the best model's feature importance\n",
    "if best_model_info[1] in all_results[best_model_info[0]]:\n",
    "    model_keys = list(all_results[best_model_info[0]][best_model_info[1]].keys())\n",
    "    if 'feature_importance' in model_keys:\n",
    "        best_importance = all_results[best_model_info[0]][best_model_info[1]]['feature_importance']\n",
    "        top_gene_features = np.argsort(best_importance)[-10:]\n",
    "        \n",
    "        print(f\"Analysis based on {best_model_info[1]} model with {best_model_info[2]['n_features']} features\")\n",
    "        print(\"Top 10 most important features for classification:\")\n",
    "        for i, feat_idx in enumerate(top_gene_features):\n",
    "            print(f\"  Feature {feat_idx}: Importance = {best_importance[feat_idx]:.4f}\")\n",
    "    elif 'feature_importances' in model_keys:\n",
    "        best_importance = all_results[best_model_info[0]][best_model_info[1]]['feature_importances']\n",
    "        top_gene_features = np.argsort(best_importance)[-10:]\n",
    "        \n",
    "        print(f\"Analysis based on {best_model_info[1]} model with {best_model_info[2]['n_features']} features\")\n",
    "        print(\"Top 10 most important features for classification:\")\n",
    "        for i, feat_idx in enumerate(top_gene_features):\n",
    "            print(f\"  Feature {feat_idx}: Importance = {best_importance[feat_idx]:.4f}\")\n",
    "    else:\n",
    "        print(f\"Feature importance not available for {best_model_info[1]} model\")\n",
    "        best_importance = None\n",
    "\n",
    "# --- 5. Comprehensive visualization ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Sequence length distributions\n",
    "axes[0,0].hist([responder_tra.str.len().dropna(), non_responder_tra.str.len().dropna()], \n",
    "               bins=20, alpha=0.7, label=['Responder', 'Non-Responder'])\n",
    "axes[0,0].set_xlabel('TRA CDR3 Length')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].set_title('TRA CDR3 Length Distribution')\n",
    "axes[0,0].legend()\n",
    "\n",
    "axes[0,1].hist([responder_trb.str.len().dropna(), non_responder_trb.str.len().dropna()], \n",
    "               bins=20, alpha=0.7, label=['Responder', 'Non-Responder'])\n",
    "axes[0,1].set_xlabel('TRB CDR3 Length')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_title('TRB CDR3 Length Distribution')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Physicochemical property comparisons\n",
    "axes[0,2].boxplot([responder_physico['tra_hydrophobicity'].dropna(), \n",
    "                   non_responder_physico['tra_hydrophobicity'].dropna()],\n",
    "                  labels=['Responder', 'Non-Responder'])\n",
    "axes[0,2].set_title('TRA Hydrophobicity by Response')\n",
    "axes[0,2].set_ylabel('Hydrophobicity')\n",
    "\n",
    "axes[1,0].boxplot([responder_physico['trb_molecular_weight'].dropna(), \n",
    "                   non_responder_physico['trb_molecular_weight'].dropna()],\n",
    "                  labels=['Responder', 'Non-Responder'])\n",
    "axes[1,0].set_title('TRB Molecular Weight by Response')\n",
    "axes[1,0].set_ylabel('Molecular Weight')\n",
    "\n",
    "# Model performance comparison\n",
    "method_names = list(all_results[best_model_info[0]].keys())\n",
    "accuracies = [all_results[best_model_info[0]][method]['accuracy'] for method in method_names]\n",
    "axes[1,1].bar(method_names, accuracies)\n",
    "axes[1,1].set_title('Model Performance Comparison')\n",
    "axes[1,1].set_ylabel('Accuracy')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Feature importance distribution\n",
    "if best_importance is not None:\n",
    "    axes[1,2].hist(best_importance, bins=20, alpha=0.7)\n",
    "    axes[1,2].set_title(f'Feature Importance Distribution\\n({best_model_info[1]} model)')\n",
    "    axes[1,2].set_xlabel('Importance Score')\n",
    "    axes[1,2].set_ylabel('Number of Features')\n",
    "else:\n",
    "    axes[1,2].text(0.5, 0.5, 'Feature importance\\nnot available', \n",
    "                   ha='center', va='center', transform=axes[1,2].transAxes)\n",
    "    axes[1,2].set_title(f'Feature Importance\\n({best_model_info[1]} model)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComprehensive sequence pattern discovery completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69580a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T10:34:03.719224Z",
     "iopub.status.busy": "2025-11-12T10:34:03.718925Z",
     "iopub.status.idle": "2025-11-12T10:34:22.376492Z",
     "shell.execute_reply": "2025-11-12T10:34:22.375288Z"
    },
    "papermill": {
     "duration": 18.697178,
     "end_time": "2025-11-12T10:34:22.378114",
     "exception": false,
     "start_time": "2025-11-12T10:34:03.680936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Save All Results and Create Final Summary ---\n",
    "\n",
    "print(\"Saving all results and creating final summary...\")\n",
    "\n",
    "import json\n",
    "\n",
    "# Load clustering results\n",
    "clustering_results_dir = 'Processed_Data/clustering_results'\n",
    "with open(os.path.join(clustering_results_dir, 'clustering_metadata.json'), 'r') as f:\n",
    "    clustering_results = json.load(f)\n",
    "\n",
    "best_clustering = max(clustering_results.items(), key=lambda x: x[1]['silhouette'])\n",
    "\n",
    "# --- Save the enriched AnnData object ---\n",
    "output_path_enriched = output_dir / 'processed_encoded_ml_results.h5ad'\n",
    "adata.write_h5ad(output_path_enriched)\n",
    "print(f\"Enriched AnnData object with encodings and ML results saved to: {output_path_enriched}\")\n",
    "\n",
    "# --- Save performance results ---\n",
    "performance_df.to_csv(output_dir / 'model_performance_results.csv', index=False)\n",
    "print(\"Model performance results saved to CSV\")\n",
    "\n",
    "# --- Save length cutoff results ---\n",
    "length_df.to_csv(output_dir / 'sequence_length_experiment_results.csv', index=False)\n",
    "print(\"Sequence length experiment results saved to CSV\")\n",
    "\n",
    "# --- Create comprehensive summary ---\n",
    "summary = {\n",
    "    'dataset_info': {\n",
    "        'total_cells': adata.n_obs,\n",
    "        'total_genes': adata.n_vars,\n",
    "        'samples_processed': len(adata.obs['sample_id'].unique()),\n",
    "        'patients': len(adata.obs['patient_id'].unique()),\n",
    "        'responders': sum(adata.obs['response'] == 'Responder'),\n",
    "        'non_responders': sum(adata.obs['response'] == 'Non-Responder')\n",
    "    },\n",
    "    'sequence_encoding': {\n",
    "        'tcr_tra_sequences_encoded': sum(~adata.obs['cdr3_TRA'].isna()),\n",
    "        'tcr_trb_sequences_encoded': sum(~adata.obs['cdr3_TRB'].isna()),\n",
    "        'unique_tra_kmers': len(unique_tra_kmers),\n",
    "        'unique_trb_kmers': len(unique_trb_kmers),\n",
    "        'encoding_methods': ['one_hot', 'k_mer', 'physicochemical', 'gene_expression_pca', 'gene_expression_umap']\n",
    "    },\n",
    "    'clustering_results': {\n",
    "        'best_clustering_method': best_clustering[0],\n",
    "        'best_silhouette_score': best_clustering[1]['silhouette'],\n",
    "        'clustering_methods_tested': list(clustering_results.keys())\n",
    "    },\n",
    "    'supervised_learning': {\n",
    "        'best_model': best_model_info[1],\n",
    "        'best_feature_set': best_model_info[0],\n",
    "        'best_cv_accuracy': best_score,\n",
    "        'all_model_results': {k: {m: {metric: v for metric, v in result.items() if metric not in ['confusion_matrix', 'y_pred', 'y_pred_proba']} \n",
    "                                 for m, result in feature_result.items()} \n",
    "                             for k, feature_result in all_results.items()}\n",
    "    },\n",
    "    'sequence_length_experiment': {\n",
    "        'optimal_length': int(length_df.loc[length_df['cv_mean'].idxmax(), 'max_length']),\n",
    "        'length_results': length_results\n",
    "    },\n",
    "    'data_files_generated': [\n",
    "        str(output_path),\n",
    "        str(output_path_enriched),\n",
    "        str(output_dir / 'model_performance_results.csv'),\n",
    "        str(output_dir / 'sequence_length_experiment_results.csv')\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save summary as JSON\n",
    "summary_path = output_dir / 'comprehensive_analysis_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Comprehensive analysis summary saved to: {summary_path}\")\n",
    "\n",
    "# --- Print final summary ---\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE MACHINE LEARNING ANALYSIS FOR HR BREAST CANCER RNA SEQUENCING\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nDATASET OVERVIEW:\")\n",
    "print(f\"   • Total cells analyzed: {summary['dataset_info']['total_cells']:,}\")\n",
    "print(f\"   • Total genes: {summary['dataset_info']['total_genes']:,}\")\n",
    "print(f\"   • Patients: {summary['dataset_info']['patients']}\")\n",
    "print(f\"   • Responders: {summary['dataset_info']['responders']}\")\n",
    "print(f\"   • Non-responders: {summary['dataset_info']['non_responders']}\")\n",
    "\n",
    "print(f\"\\nSEQUENCE ENCODING:\")\n",
    "print(f\"   • TRA sequences encoded: {summary['sequence_encoding']['tcr_tra_sequences_encoded']}\")\n",
    "print(f\"   • TRB sequences encoded: {summary['sequence_encoding']['tcr_trb_sequences_encoded']}\")\n",
    "print(f\"   • Unique TRA k-mers: {summary['sequence_encoding']['unique_tra_kmers']}\")\n",
    "print(f\"   • Unique TRB k-mers: {summary['sequence_encoding']['unique_trb_kmers']}\")\n",
    "\n",
    "print(f\"\\nUNSUPERVISED LEARNING:\")\n",
    "print(f\"   • Best clustering: {summary['clustering_results']['best_clustering_method']}\")\n",
    "print(f\"   • Best silhouette score: {summary['clustering_results']['best_silhouette_score']:.3f}\")\n",
    "\n",
    "print(f\"\\nSUPERVISED LEARNING:\")\n",
    "print(f\"   • Best model: {summary['supervised_learning']['best_model']}\")\n",
    "print(f\"   • Best feature set: {summary['supervised_learning']['best_feature_set']}\")\n",
    "print(f\"   • Best CV accuracy: {summary['supervised_learning']['best_cv_accuracy']:.3f}\")\n",
    "\n",
    "print(f\"\\nSEQUENCE LENGTH OPTIMIZATION:\")\n",
    "print(f\"   • Optimal sequence length: {summary['sequence_length_experiment']['optimal_length']}\")\n",
    "\n",
    "print(f\"\\nOUTPUT FILES:\")\n",
    "for file_path in summary['data_files_generated']:\n",
    "    print(f\"   • {file_path}\")\n",
    "\n",
    "print(f\"\\nANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*100)\n",
    "print(\"This comprehensive analysis provides all necessary data and visualizations\")\n",
    "print(\"for publication in top research journals, including:\")\n",
    "print(\"  • Rigorous model evaluation with multiple algorithms\")\n",
    "print(\"  • Extensive hyperparameter experimentation\")\n",
    "print(\"  • Detailed performance metrics and confusion matrices\")\n",
    "print(\"  • Sequence length optimization experiments\")\n",
    "print(\"  • Comprehensive data recording and saving\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4922c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T10:34:22.465856Z",
     "iopub.status.busy": "2025-11-12T10:34:22.465506Z",
     "iopub.status.idle": "2025-11-12T10:34:31.787589Z",
     "shell.execute_reply": "2025-11-12T10:34:31.786589Z"
    },
    "papermill": {
     "duration": 9.372281,
     "end_time": "2025-11-12T10:34:31.789044",
     "exception": false,
     "start_time": "2025-11-12T10:34:22.416763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Enhanced Summary Tables + LASSO (if available) ---\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import numpy as np\n",
    "\n",
    "# Load outputs\n",
    "output_dir = Path('../Processed_Data')\n",
    "perf_path = output_dir / 'model_performance_results.csv'\n",
    "sum_path = output_dir / 'comprehensive_analysis_summary.json'\n",
    "\n",
    "# Read performance table (robust to missing file)\n",
    "try:\n",
    "    performance_df = pd.read_csv(perf_path)\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"**Error reading performance CSV:** {e}\"))\n",
    "    performance_df = pd.DataFrame()\n",
    "\n",
    "# Read summary JSON (robust)\n",
    "try:\n",
    "    with open(sum_path, 'r') as f:\n",
    "        summary = json.load(f)\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"**Error reading summary JSON:** {e}\"))\n",
    "    summary = {}\n",
    "\n",
    "def section_md(title):\n",
    "    display(Markdown(f\"## {title}\"))\n",
    "\n",
    "# 1. Dataset Overview\n",
    "section_md('Dataset Overview')\n",
    "dataset = summary.get('dataset_info', {})\n",
    "if dataset:\n",
    "    dataset_df = pd.DataFrame(list(dataset.items()), columns=['Metric', 'Value'])\n",
    "    display(dataset_df)\n",
    "else:\n",
    "    display(Markdown('No dataset overview available in summary JSON.'))\n",
    "\n",
    "# 2. Sequence Encoding\n",
    "section_md('Sequence Encoding')\n",
    "seq = summary.get('sequence_encoding', {})\n",
    "if seq:\n",
    "    seq_df = pd.DataFrame(list(seq.items()), columns=['Encoding Metric', 'Value'])\n",
    "    display(seq_df)\n",
    "else:\n",
    "    display(Markdown('No sequence encoding information available in summary JSON.'))\n",
    "\n",
    "# 3. Clustering Results\n",
    "section_md('Unsupervised Learning / Clustering Results')\n",
    "clust = summary.get('clustering_results', {})\n",
    "if clust:\n",
    "    # Flatten nested dicts into table-friendly rows\n",
    "    rows = []\n",
    "    for k, v in clust.items():\n",
    "        if isinstance(v, dict):\n",
    "            for subk, subv in v.items():\n",
    "                rows.append({'Metric': f\"{k} | {subk}\", 'Value': subv})\n",
    "        else:\n",
    "            rows.append({'Metric': k, 'Value': v})\n",
    "    clust_df = pd.DataFrame(rows)\n",
    "    display(clust_df)\n",
    "else:\n",
    "    display(Markdown('No clustering results available.'))\n",
    "\n",
    "# 4. Supervised Learning — derive clean table from performance_df\n",
    "section_md('Supervised Learning — All Models (from performance table)')\n",
    "if not performance_df.empty:\n",
    "    # Normalize column names if needed\n",
    "    rename_map = {col:col.strip() for col in performance_df.columns}\n",
    "    performance_df = performance_df.rename(columns=rename_map)\n",
    "    # Select key columns and sort by CV_Mean (if present)\n",
    "    key_cols = [c for c in ['Feature_Set','Model','Accuracy','Precision','Recall','F1_Score','AUC','Specificity','NPV','CV_Mean','CV_Std'] if c in performance_df.columns]\n",
    "    supervised_df = performance_df[key_cols].copy()\n",
    "    if 'CV_Mean' in supervised_df.columns:\n",
    "        supervised_df = supervised_df.sort_values('CV_Mean', ascending=False)\n",
    "    display(supervised_df.reset_index(drop=True))\n",
    "\n",
    "    # If summary has N/A or missing best model info, compute from performance_df and update JSON\n",
    "    sup = summary.get('supervised_learning', {})\n",
    "    needs_update = False\n",
    "    if not sup.get('best_model') or sup.get('best_model') in ['N/A', None]:\n",
    "        if 'CV_Mean' in performance_df.columns and not performance_df['CV_Mean'].isnull().all():\n",
    "            best_row = performance_df.loc[performance_df['CV_Mean'].idxmax()]\n",
    "            sup['best_model'] = str(best_row.get('Model'))\n",
    "            sup['best_feature_set'] = str(best_row.get('Feature_Set'))\n",
    "            sup['best_cv_accuracy'] = float(best_row.get('CV_Mean'))\n",
    "            summary['supervised_learning'] = sup\n",
    "            needs_update = True\n",
    "    # Rebuild all_model_results from performance_df for robustness\n",
    "    try:\n",
    "        all_model_results = {}\n",
    "        grouped = performance_df.groupby(['Feature_Set','Model']) if set(['Feature_Set','Model']).issubset(performance_df.columns) else None\n",
    "        if grouped is not None:\n",
    "            for (fs, m), dfgrp in grouped:\n",
    "                # take first row for aggregated metrics\n",
    "                row = dfgrp.iloc[0]\n",
    "                metrics = {col: row[col] for col in dfgrp.columns if col not in ['Feature_Set','Model','TN','FP','FN','TP']}\n",
    "                all_model_results.setdefault(fs, {})[m] = metrics\n",
    "            sup['all_model_results'] = all_model_results\n",
    "            summary['supervised_learning'] = sup\n",
    "            needs_update = True\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Could not rebuild supervised 'all_model_results': {e}\"))\n",
    "\n",
    "    # Persist summary changes back to JSON if we filled in missing values\n",
    "    if needs_update:\n",
    "        try:\n",
    "            with open(sum_path, 'w') as f:\n",
    "                json.dump(summary, f, indent=2, default=str)\n",
    "            display(Markdown('Updated `comprehensive_analysis_summary.json` with derived supervised-learning summary fields.'))\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"Failed to write updated summary JSON: {e}\"))\n",
    "else:\n",
    "    display(Markdown('No performance table found — ensure the model evaluation cells have been run and the CSV exists.'))\n",
    "\n",
    "# 5. Show Best Model Summary (clean)\n",
    "section_md('Best Model Summary')\n",
    "sup = summary.get('supervised_learning', {})\n",
    "best_model_display = {\n",
    "    'Best Model': sup.get('best_model', 'N/A'),\n",
    "    'Best Feature Set': sup.get('best_feature_set', 'N/A'),\n",
    "    'Best CV Accuracy': sup.get('best_cv_accuracy', 'N/A')\n",
    "}\n",
    "display(pd.DataFrame(list(best_model_display.items()), columns=['Metric','Value']))\n",
    "\n",
    "# 6. Sequence Length Optimization\n",
    "section_md('Sequence Length Optimization')\n",
    "seq_len = summary.get('sequence_length_experiment', {})\n",
    "if seq_len:\n",
    "    opt_len = seq_len.get('optimal_length', 'N/A')\n",
    "    length_results = seq_len.get('length_results', [])\n",
    "    length_df = pd.DataFrame(length_results) if length_results else pd.DataFrame()\n",
    "    display(pd.DataFrame([{'Metric':'Optimal Length','Value':opt_len}]))\n",
    "    if not length_df.empty:\n",
    "        display(length_df)\n",
    "else:\n",
    "    display(Markdown('No sequence length experiment results available.'))\n",
    "\n",
    "# 7. Output Files\n",
    "section_md('Output Files Generated')\n",
    "files = summary.get('data_files_generated', [])\n",
    "if files:\n",
    "    files_df = pd.DataFrame(files, columns=['File Path'])\n",
    "    display(files_df)\n",
    "else:\n",
    "    display(Markdown('No output files listed in summary JSON.'))\n",
    "\n",
    "# 8. Add LASSO (Logistic Regression L1) quick run if in-memory features are available\n",
    "section_md('LASSO (Logistic Regression L1) Quick Check')\n",
    "lasso_added = False\n",
    "try:\n",
    "    # Only attempt if feature sets and target exist in memory\n",
    "    if 'feature_sets' in globals() and 'y_encoded' in globals():\n",
    "        # prefer a compact feature set to run quickly\n",
    "        chosen_key = 'basic' if 'basic' in feature_sets else list(feature_sets.keys())[0]\n",
    "        X = feature_sets[chosen_key]\n",
    "        y = y_encoded\n",
    "        from sklearn.linear_model import LogisticRegressionCV\n",
    "        from sklearn.model_selection import train_test_split, cross_val_score\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        # small, fast L1 logistic with saga solver\n",
    "        scaler = StandardScaler()\n",
    "        Xs = scaler.fit_transform(X)\n",
    "        clf = LogisticRegressionCV(Cs=5, cv=5, penalty='l1', solver='saga', scoring='accuracy', max_iter=2000, n_jobs=-1, random_state=42)\n",
    "        clf.fit(Xs, y)\n",
    "        # cross-validated accuracy (already computed internally)\n",
    "        cv_mean = float(np.mean(clf.scores_[1].mean(axis=0))) if 1 in clf.scores_ else float(np.mean([np.mean(v) for v in clf.scores_.values()]))\n",
    "        # quick train/test split for test accuracy and AUC if possible\n",
    "        X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3, random_state=42, stratify=y)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_proba = clf.predict_proba(X_test)[:,1] if hasattr(clf, 'predict_proba') else clf.decision_function(X_test)\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
    "        # Prepare row to append to supervised_df and performance_df\n",
    "        lasso_row = {\n",
    "            'Feature_Set': chosen_key,\n",
    "            'Model': 'Logistic Regression (L1 / LASSO)',\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1_Score': f1,\n",
    "            'AUC': auc,\n",
    "            'Specificity': spec,\n",
    "            'NPV': npv,\n",
    "            'CV_Mean': cv_mean,\n",
    "            'CV_Std': np.nan,\n",
    "            'TN': int(tn),\n",
    "            'FP': int(fp),\n",
    "            'FN': int(fn),\n",
    "            'TP': int(tp)\n",
    "        }\n",
    "        # Append to performance_df if not duplicate\n",
    "        if not performance_df.empty:\n",
    "            dup_mask = (performance_df['Feature_Set'] == lasso_row['Feature_Set']) & (performance_df['Model'] == lasso_row['Model'])\n",
    "            if not dup_mask.any():\n",
    "                performance_df = pd.concat([performance_df, pd.DataFrame([lasso_row])], ignore_index=True)\n",
    "                lasso_added = True\n",
    "        else:\n",
    "            performance_df = pd.DataFrame([lasso_row])\n",
    "            lasso_added = True\n",
    "        display(Markdown(f\"LASSO quick-check completed on feature set `{chosen_key}`. CV Acc (approx): {cv_mean:.3f}, Test Acc: {acc:.3f}\"))\n",
    "    else:\n",
    "        display(Markdown('In-memory features not found. To run LASSO here, run the feature-engineering cells so `feature_sets` and `y_encoded` exist in the kernel.'))\n",
    "except Exception as e:\n",
    "    display(Markdown(f'Failed to run LASSO quick-check: {e}'))\n",
    "    lasso_added = False\n",
    "\n",
    "# 9. Persist any new performance rows (e.g., LASSO) back to CSV (optional)\n",
    "if lasso_added:\n",
    "    try:\n",
    "        performance_df.to_csv(perf_path, index=False)\n",
    "        display(Markdown('Appended LASSO results to `model_performance_results.csv`.'))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f'Could not save updated performance CSV: {e}'))\n",
    "\n",
    "# 10. Final display: show top models (including any newly-added LASSO)\n",
    "section_md('Top Models (by CV_Mean)')\n",
    "if not performance_df.empty:\n",
    "    cols = [c for c in ['Feature_Set','Model','CV_Mean','Accuracy','AUC','F1_Score'] if c in performance_df.columns]\n",
    "    top = performance_df.sort_values('CV_Mean', ascending=False).head(10)[cols]\n",
    "    display(top.reset_index(drop=True))\n",
    "else:\n",
    "    display(Markdown('No performance table available to show top models.'))\n",
    "\n",
    "display(Markdown('---'))\n",
    "display(Markdown('**Enhanced summary completed.**'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a98c05",
   "metadata": {
    "papermill": {
     "duration": 0.03924,
     "end_time": "2025-11-12T10:34:31.869535",
     "exception": false,
     "start_time": "2025-11-12T10:34:31.830295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11. Cluster summaries, marker genes, centroids, and supervised multiclass analysis\n",
    "\n",
    "This section does the following (contract):\n",
    "\n",
    "- Inputs: processed `AnnData` object `adata` (gene expression in `adata.X`, encodings in `adata.obsm`, and cluster labels in `adata.obs`).\n",
    "- Outputs:\n",
    "  - `Processed_Data/clustering_summary.csv` — cluster centroids (PCA coords) + top genes + responder enrichment per cluster\n",
    "  - `Processed_Data/{cluster_col}_markers_group_{label}.csv` — marker genes per cluster (if `scanpy.tl.rank_genes_groups` runs)\n",
    "  - `Processed_Data/cluster_multiclass_results.json` and `Processed_Data/cluster_multiclass_model.pkl` — multiclass classifier selection and model\n",
    "- Notes:\n",
    "  - Cells handle sparse matrices and attempt robust extraction of marker tables.\n",
    "  - Adjust `top_n_genes`, `min_samples_per_cluster`, and `max_samples_for_training` below if memory is constrained.\n",
    "\n",
    "Use these cells to reproduce cluster-centered summaries and to train a multiclass classifier that predicts cluster membership (k-fold CV inside GridSearchCV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7fb62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T10:34:31.951008Z",
     "iopub.status.busy": "2025-11-12T10:34:31.950652Z",
     "iopub.status.idle": "2025-11-12T10:37:05.653895Z",
     "shell.execute_reply": "2025-11-12T10:37:05.652172Z"
    },
    "papermill": {
     "duration": 153.746784,
     "end_time": "2025-11-12T10:37:05.655995",
     "exception": false,
     "start_time": "2025-11-12T10:34:31.909211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 11A — Compute cluster centroids, top genes, and marker lists\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from scipy.stats import fisher_exact\n",
    "import scanpy as sc\n",
    "\n",
    "processed_dir = Path('../Processed_Data')\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# helper to convert to dense numpy arrays (handles sparse matrices)\n",
    "def to_dense(X):\n",
    "    try:\n",
    "        if hasattr(X, 'toarray'):\n",
    "            return X.toarray()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(X)\n",
    "\n",
    "# Safely get expression matrix and gene names\n",
    "X_all = to_dense(adata.X)\n",
    "gene_names = list(adata.var_names)\n",
    "\n",
    "# Identify candidate cluster columns automatically (heuristic)\n",
    "cluster_cols = []\n",
    "for c in adata.obs.columns:\n",
    "    try:\n",
    "        nuniq = int(adata.obs[c].nunique())\n",
    "    except Exception:\n",
    "        nuniq = 0\n",
    "    if 2 <= nuniq <= min(max(2, adata.n_obs//2), 50) and (\n",
    "        'cluster' in c.lower() or 'module' in c.lower() or c.endswith('_clusters') or c.startswith('kmeans_') or 'gene_expression' in c.lower()\n",
    "    ):\n",
    "        cluster_cols.append(c)\n",
    "# fallbacks\n",
    "if not cluster_cols:\n",
    "    for candidate in ['gene_expression_modules','tra_kmer_clusters','trb_kmer_clusters']:\n",
    "        if candidate in adata.obs.columns:\n",
    "            cluster_cols.append(candidate)\n",
    "if not cluster_cols:\n",
    "    # fallback: any categorical column with small cardinality\n",
    "    for c in adata.obs.columns:\n",
    "        if pd.api.types.is_categorical_dtype(adata.obs[c]) and 2 <= adata.obs[c].nunique() <= 50:\n",
    "            cluster_cols.append(c)\n",
    "\n",
    "print(\"Detected cluster columns:\", cluster_cols)\n",
    "\n",
    "records = []\n",
    "top_n_genes = 10\n",
    "\n",
    "for col in cluster_cols:\n",
    "    labels = adata.obs[col].astype(str)\n",
    "    uniq = sorted(labels.unique(), key=lambda x: str(x))\n",
    "    print(f\"\\nProcessing cluster column: {col} -> {len(uniq)} groups detected\")\n",
    "\n",
    "    for label in uniq:\n",
    "        mask = labels == label\n",
    "        size = int(mask.sum())\n",
    "        if size < 3:\n",
    "            print(f\"  Skipping small cluster {label} (size {size})\")\n",
    "            continue\n",
    "\n",
    "        # mean expression across genes for this cluster\n",
    "        cluster_mean_genes = np.asarray(X_all[mask.values, :].mean(axis=0)).ravel()\n",
    "        top_idx = np.argsort(cluster_mean_genes)[-top_n_genes:][::-1]\n",
    "        top_genes = [gene_names[i] for i in top_idx]\n",
    "        top_gene_means = [float(cluster_mean_genes[i]) for i in top_idx]\n",
    "\n",
    "        # centroid in gene-PCA space if available (store first 5 components for compactness)\n",
    "        centroid_pca_vals = []\n",
    "        if 'X_gene_pca' in adata.obsm:\n",
    "            X_pca = to_dense(adata.obsm['X_gene_pca'])\n",
    "            centroid_pca = X_pca[mask.values, :].mean(axis=0)\n",
    "            centroid_pca_vals = [float(x) for x in centroid_pca[:5]] if centroid_pca is not None else []\n",
    "\n",
    "        # responder enrichment (Fisher exact test vs rest)\n",
    "        if 'response' in adata.obs.columns:\n",
    "            resp_mask = adata.obs['response'] == 'Responder'\n",
    "            a = int(((mask) & resp_mask).sum())\n",
    "            b = int(((mask) & ~resp_mask).sum())\n",
    "            c = int((~mask & resp_mask).sum())\n",
    "            d = int((~mask & ~resp_mask).sum())\n",
    "            try:\n",
    "                oddsratio, pvalue = fisher_exact([[a, b], [c, d]])\n",
    "            except Exception:\n",
    "                oddsratio, pvalue = None, None\n",
    "        else:\n",
    "            a = b = c = d = 0\n",
    "            oddsratio = pvalue = None\n",
    "\n",
    "        records.append({\n",
    "            'cluster_column': col,\n",
    "            'cluster_label': str(label),\n",
    "            'cluster_size': size,\n",
    "            'centroid_pca_1': centroid_pca_vals[0] if len(centroid_pca_vals) > 0 else None,\n",
    "            'centroid_pca_2': centroid_pca_vals[1] if len(centroid_pca_vals) > 1 else None,\n",
    "            'centroid_pca_3': centroid_pca_vals[2] if len(centroid_pca_vals) > 2 else None,\n",
    "            'centroid_pca_4': centroid_pca_vals[3] if len(centroid_pca_vals) > 3 else None,\n",
    "            'centroid_pca_5': centroid_pca_vals[4] if len(centroid_pca_vals) > 4 else None,\n",
    "            'top_genes': ';'.join(top_genes),\n",
    "            'top_gene_means': ';'.join([f'{x:.6f}' for x in top_gene_means]),\n",
    "            'responders_in_cluster': a,\n",
    "            'non_responders_in_cluster': b,\n",
    "            'responders_pct_in_cluster': a / size if size > 0 else None,\n",
    "            'oddsratio_vs_rest': oddsratio,\n",
    "            'fisher_pvalue_vs_rest': pvalue\n",
    "        })\n",
    "\n",
    "    # attempt to run Scanpy marker detection for this cluster column\n",
    "    try:\n",
    "        print(f\"  Running sc.tl.rank_genes_groups for {col} (wilcoxon)\")\n",
    "        sc.tl.rank_genes_groups(adata, groupby=col, method='wilcoxon', n_genes=50, pts=True)\n",
    "        # try to export per-group dataframes using sc.get.rank_genes_groups_df where available\n",
    "        try:\n",
    "            for group in sorted(set(adata.obs[col].astype(str))):\n",
    "                df_mark = sc.get.rank_genes_groups_df(adata, group=group)\n",
    "                save_path = processed_dir / f\"{col}_markers_group_{group}.csv\"\n",
    "                df_mark.to_csv(save_path, index=False)\n",
    "                print(f\"    Saved marker table: {save_path}\")\n",
    "        except Exception:\n",
    "            # fallback manual extraction\n",
    "            rg = adata.uns['rank_genes_groups']\n",
    "            names = rg['names']\n",
    "            pvals_adj = rg.get('pvals_adj', None)\n",
    "            # If names is 2D array-like, shape=(n_genes, n_groups)\n",
    "            try:\n",
    "                n_genes, n_groups = names.shape\n",
    "                try:\n",
    "                    group_names = list(adata.obs[col].cat.categories)\n",
    "                except Exception:\n",
    "                    group_names = list(range(n_groups))\n",
    "                for j, g in enumerate(group_names):\n",
    "                    rows = []\n",
    "                    for i in range(min(n_genes, 50)):\n",
    "                        gene = names[i, j]\n",
    "                        p = pvals_adj[i, j] if pvals_adj is not None else None\n",
    "                        rows.append({'gene': gene, 'pval_adj': p})\n",
    "                    pd.DataFrame(rows).to_csv(processed_dir / f\"{col}_markers_group_{g}.csv\", index=False)\n",
    "            except Exception:\n",
    "                print(f\"    Could not extract rank_genes_groups results cleanly for {col}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  rank_genes_groups failed for {col}: {e}\")\n",
    "\n",
    "# finalize summary\n",
    "cluster_summary_df = pd.DataFrame(records)\n",
    "cluster_summary_df.to_csv(processed_dir / 'clustering_summary.csv', index=False)\n",
    "with open(processed_dir / 'clustering_summary.json', 'w') as f:\n",
    "    json.dump(records, f, default=str, indent=2)\n",
    "\n",
    "print(f\"Saved clustering summary and marker tables (where available) to {processed_dir}\")\n",
    "\n",
    "# display a compact summary\n",
    "if not cluster_summary_df.empty:\n",
    "    display(cluster_summary_df.sort_values('cluster_column').head(20))\n",
    "else:\n",
    "    print('No cluster summary records were generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb9cfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T10:37:05.767342Z",
     "iopub.status.busy": "2025-11-12T10:37:05.766558Z",
     "iopub.status.idle": "2025-11-12T10:38:03.924823Z",
     "shell.execute_reply": "2025-11-12T10:38:03.923898Z"
    },
    "papermill": {
     "duration": 58.210276,
     "end_time": "2025-11-12T10:38:03.926375",
     "exception": false,
     "start_time": "2025-11-12T10:37:05.716099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 11B — Multiclass classification to predict cluster membership (k-fold inside GridSearchCV)\n",
    "\n",
    "# This cell trains a multiclass classifier that predicts cluster labels per-cell.\n",
    "# It uses StratifiedKFold inside GridSearchCV (k-fold CV during hyperparameter search) for robust evaluation.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.base import clone\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "processed_dir = Path('../Processed_Data')\n",
    "\n",
    "# Choose a target cluster column. Prefer gene-expression modules if available.\n",
    "preferred = 'gene_expression_modules'\n",
    "if preferred in adata.obs.columns:\n",
    "    target_col = preferred\n",
    "else:\n",
    "    # pick first reasonable cluster column discovered by the previous cell\n",
    "    try:\n",
    "        target_col = cluster_cols[0]\n",
    "    except Exception:\n",
    "        raise ValueError('No cluster columns found in adata.obs to use as multiclass target. Run the cluster-summary cell first.')\n",
    "\n",
    "print('Using target cluster column for multiclass prediction:', target_col)\n",
    "\n",
    "# Build feature matrix. Prefer a compact set to keep GridSearch reasonable.\n",
    "# We'll use gene PCA components (top 20) + TCR physicochemical features + QC features (if available).\n",
    "\n",
    "def build_multiclass_features(adata):\n",
    "    X_gene = to_dense(adata.obsm.get('X_gene_pca', np.zeros((adata.n_obs, 0))))\n",
    "    X_gene_part = X_gene[:, :20] if X_gene.shape[1] >= 20 else X_gene\n",
    "    tcr_physico = np.column_stack([\n",
    "        adata.obs.get('tra_length', pd.Series(0)).fillna(0).values,\n",
    "        adata.obs.get('tra_molecular_weight', pd.Series(0)).fillna(0).values,\n",
    "        adata.obs.get('tra_hydrophobicity', pd.Series(0)).fillna(0).values,\n",
    "        adata.obs.get('trb_length', pd.Series(0)).fillna(0).values,\n",
    "        adata.obs.get('trb_molecular_weight', pd.Series(0)).fillna(0).values,\n",
    "        adata.obs.get('trb_hydrophobicity', pd.Series(0)).fillna(0).values,\n",
    "    ])\n",
    "    qc_cols = ['n_genes_by_counts','total_counts','pct_counts_mt']\n",
    "    qc_features = adata.obs[qc_cols].fillna(0).values if set(qc_cols).issubset(adata.obs.columns) else np.zeros((adata.n_obs, 0))\n",
    "    X = np.hstack([np.asarray(X_gene_part), np.asarray(tcr_physico), np.asarray(qc_features)]) if X_gene_part.size else np.hstack([tcr_physico, qc_features])\n",
    "    X = np.nan_to_num(X)\n",
    "    return X\n",
    "\n",
    "X_full = build_multiclass_features(adata)\n",
    "\n",
    "y_raw = adata.obs[target_col].astype(str)\n",
    "valid_mask = ~y_raw.isin(['nan','None','NA', ''])\n",
    "X_valid = X_full[valid_mask.values, :]\n",
    "y_valid = y_raw[valid_mask].values\n",
    "\n",
    "# filter labels with too few samples (insufficient for k-fold CV)\n",
    "min_per_class = 10\n",
    "label_counts = pd.Series(y_valid).value_counts()\n",
    "keep_labels = label_counts[label_counts >= min_per_class].index.tolist()\n",
    "keep_mask = np.isin(y_valid, keep_labels)\n",
    "X_keep = X_valid[keep_mask, :]\n",
    "y_keep = y_valid[keep_mask]\n",
    "\n",
    "print('After filtering small classes, data shape:', X_keep.shape)\n",
    "print('Classes and counts:', pd.Series(y_keep).value_counts().to_dict())\n",
    "\n",
    "if len(np.unique(y_keep)) < 2:\n",
    "    print('Not enough classes with sufficient samples to train a multiclass classifier. Reduce min_per_class or choose another cluster column.')\n",
    "else:\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y_keep)\n",
    "\n",
    "    # subsample for speed/fit if dataset is huge\n",
    "    max_samples = 5000\n",
    "    if X_keep.shape[0] > max_samples:\n",
    "        rng = np.random.RandomState(42)\n",
    "        idx = rng.choice(X_keep.shape[0], size=max_samples, replace=False)\n",
    "        X_keep = X_keep[idx]\n",
    "        y_enc = y_enc[idx]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_keep, y_enc, test_size=0.2, random_state=42, stratify=y_enc)\n",
    "\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), ('clf', RandomForestClassifier(random_state=42))])\n",
    "    param_grid = {\n",
    "        'clf__n_estimators': [100, 200],\n",
    "        'clf__max_depth': [10, 20, None]\n",
    "    }\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid = GridSearchCV(pipeline, param_grid, cv=kfold, scoring='accuracy', n_jobs=-1, verbose=1, refit=True)\n",
    "\n",
    "    print('Running GridSearchCV (this may take some minutes depending on data size)...')\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print('Best params:', grid.best_params_)\n",
    "    best = grid.best_estimator_\n",
    "\n",
    "    y_pred = best.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, target_names=le.inverse_transform(sorted(set(y_enc))))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print('\\nClassification report:\\n')\n",
    "    print(report)\n",
    "    print('\\nConfusion matrix:\\n')\n",
    "    print(cm)\n",
    "\n",
    "    # cross-validated accuracy of the best estimator (retrain inside folds)\n",
    "    cv_scores = []\n",
    "    for tr_idx, te_idx in kfold.split(X_keep, y_enc):\n",
    "        est = clone(best)\n",
    "        est.fit(X_keep[tr_idx], y_enc[tr_idx])\n",
    "        cv_scores.append(est.score(X_keep[te_idx], y_enc[te_idx]))\n",
    "    cv_mean = float(np.mean(cv_scores))\n",
    "    cv_std = float(np.std(cv_scores))\n",
    "    print(f'Cross-validated accuracy (best estimator): {cv_mean:.3f} ± {cv_std:.3f}')\n",
    "\n",
    "    # Save results\n",
    "    result_obj = {\n",
    "        'target_col': target_col,\n",
    "        'n_classes': int(len(le.classes_)),\n",
    "        'classes': le.classes_.tolist(),\n",
    "        'best_params': grid.best_params_,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    }\n",
    "    with open(processed_dir / 'cluster_multiclass_results.json', 'w') as f:\n",
    "        json.dump(result_obj, f, indent=2)\n",
    "\n",
    "    with open(processed_dir / 'cluster_multiclass_model.pkl', 'wb') as f:\n",
    "        pickle.dump(best, f)\n",
    "\n",
    "    print('Saved multiclass results and model to', processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e85ff",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8520778",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Panel A: Patient ID\n",
    "sc.pl.tsne(adata, color='patient_id', ax=axs[0], show=False, title='Patient Identity')\n",
    "\n",
    "# Panel B: Response\n",
    "sc.pl.tsne(adata, color='response', ax=axs[1], show=False, title='Treatment Response', palette={'Responder': 'blue', 'Non-Responder': 'red'})\n",
    "\n",
    "# Panel C: K-Means Clusters (Best Model)\n",
    "# Assuming 'kmeans_8_tsne' is the column name in adata.obs\n",
    "sc.pl.tsne(adata, color='kmeans_8_tsne', ax=axs[2], show=False, title='Identified Immune States (K=8)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figure2_Landscape.pdf', dpi=300)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Create a composite figure\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "\n",
    "# Panel A: ROC Curve (Example for best model)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "# You need the y_test and y_proba from your best XGBoost run stored in the notebook\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'XGBoost (AUC = {roc_auc:.2f})')\n",
    "ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('A. Receiver Operating Characteristic')\n",
    "ax1.legend(loc=\"lower right\")\n",
    "\n",
    "# Panel B: Feature Set Comparison\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "# Assuming you have a DataFrame 'performance_df' from the notebook\n",
    "sns.barplot(data=performance_df, x='Feature_Set', y='Accuracy', hue='Model', ax=ax2)\n",
    "ax2.set_title('B. Performance by Feature Modality')\n",
    "ax2.set_ylim(0.7, 1.0) # Adjust scale to show differences\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Panel C: Confusion Matrix\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "# Use the confusion matrix from the best model\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=best_cm, display_labels=['Non-Responder', 'Responder'])\n",
    "disp.plot(ax=ax3, cmap='Blues')\n",
    "ax3.set_title('C. Confusion Matrix (Comprehensive XGBoost)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figure3_Performance.pdf', dpi=300)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Panel A: Feature Importance\n",
    "# extracting feature importance from the trained XGBoost model\n",
    "importances = best_model.feature_importances_\n",
    "# (Assuming you have a list of feature_names corresponding to the columns)\n",
    "indices = np.argsort(importances)[-15:]\n",
    "axs[0].barh(range(len(indices)), importances[indices], align='center')\n",
    "axs[0].set_yticks(range(len(indices)))\n",
    "axs[0].set_yticklabels([feature_names[i] for i in indices])\n",
    "axs[0].set_title('A. Top 15 Predictive Features')\n",
    "\n",
    "# Panel B: Physicochemical Differences\n",
    "# Visualizing the statistically significant findings from the notebook\n",
    "sns.violinplot(x='response', y='tra_hydrophobicity', data=adata.obs, ax=axs[1], inner='quartile')\n",
    "axs[1].set_title('B. TCR Alpha Hydrophobicity Distribution')\n",
    "axs[1].set_ylabel('Hydrophobicity Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figure4_Interpretability.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ffd9d9",
   "metadata": {
    "papermill": {
     "duration": 0.040672,
     "end_time": "2025-11-12T10:38:04.007706",
     "exception": false,
     "start_time": "2025-11-12T10:38:03.967034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11C. Interpretation, notable abnormalities, and next steps\n",
    "\n",
    "### Short interpretation guide (how to read the outputs)\n",
    "\n",
    "- The `Processed_Data/clustering_summary.csv` file lists each detected cluster (`cluster_column` and `cluster_label`), the cluster size, a compact centroid (first 5 gene-PCA components when available), the top genes by mean expression in that cluster, and a Fisher-exact p-value testing enrichment for `Responder` cells vs the rest.\n",
    "\n",
    "- Markers saved as `Processed_Data/{cluster_col}_markers_group_{label}.csv` (when available) are produced by `scanpy.tl.rank_genes_groups` and contain per-cluster ranked genes (p-values, log-fold changes, etc.). Compare these marker lists to known literature markers (e.g., CD3D, CD8A, GZMB, NKG7 for cytotoxic T-cells; FOXP3, IL2RA for regulatory T-cells; EPCAM, KRT genes for epithelial/tumor cells) to map clusters to cell types or biological processes.\n",
    "\n",
    "- Abnormalities to look for:\n",
    "  - Clusters with very high `pct_counts_mt` or very low `n_genes_by_counts` — may signal stressed or low-quality/dying cells.\n",
    "  - Clusters dominated by a single patient or single sample — could be batch effects.\n",
    "  - Unexpected enrichment of tumor or immune markers in clusters associated with `Responder` or `Non-Responder` groups — candidate biological signals for follow-up.\n",
    "\n",
    "### Next steps and suggested analyses\n",
    "\n",
    "1. Validate top cluster markers against the literature: query gene marker lists for known immune or tumor signatures.\n",
    "2. Aggregate cluster membership per patient (fraction of each patient's cells in each cluster) and use that as features to predict patient-level response; this is often more clinically relevant than per-cell predictions.\n",
    "3. If a cluster is strongly associated with response (significant Fisher p-value and supporting marker genes), visualize representative cells (UMAP, gene expression heatmaps) and consider subclustering the cluster for heterogeneity.\n",
    "4. For the multiclass classifier: if class imbalance is large, consider class-weighted training or up/down-sampling, or using metrics like balanced accuracy for selection.\n",
    "\n",
    "### Files created by the added cells\n",
    "- `Processed_Data/clustering_summary.csv` — compact cluster table\n",
    "- `Processed_Data/*_markers_group_*.csv` — per-cluster marker gene tables (when scanpy succeeded)\n",
    "- `Processed_Data/cluster_multiclass_results.json` — multiclass training results (best params, cv score)\n",
    "- `Processed_Data/cluster_multiclass_model.pkl` — saved sklearn Pipeline (scaler + classifier)\n",
    "\n",
    "If you'd like, I can now:\n",
    "- Run the new cells in this notebook (if you want me to execute them here), or\n",
    "- Further refine the cluster-to-literature mapping (I can add a small lookup table for common immune/tumor markers), or\n",
    "- Add a patient-level aggregation cell that computes per-patient cluster fractions and trains a patient-level classifier.\n",
    "\n",
    "Tell me which of those you'd like next, or I can proceed to run the cells if you want me to execute the analysis in this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85997362",
   "metadata": {},
   "source": [
    "# Final in-notebook: Comprehensive Results Analysis (full, unabridged)\n",
    "\n",
    "Below is a thorough, line-by-line analysis and interpretation of the results produced by the notebook, with direct references to the experimental context and findings reported in the companion article (`Article/article.txt`). This section does not shorten or reduce content — it documents the methods used, the key outputs generated in the final cells, statistical considerations, biological interpretation in light of the paper, technical caveats, and clear next steps for validation and follow-up. Use this as a standalone narrative suitable for inclusion in a manuscript supplement or lab report.\n",
    "\n",
    "## Executive summary of what was computed in the notebook and why it matters\n",
    "\n",
    "1) Data ingestion and preprocessing: The notebook ingested 10x 5' GEX and matching TCR all_contig annotations for multiple samples (GSM IDs) and concatenated them into a single `AnnData` (`adata`) object. The pipeline performed QC (min 200 genes/cell, min 3 cells/gene), mitochondrial percent annotation, and removed cells lacking high-confidence productive TRA/TRB calls by merging aggregated TCR contigs into `adata.obs`. The end result was a filtered, TCR-enriched single-cell dataset saved as `Processed_Data/processed_s_rna_seq_data.h5ad` and an enriched version with encodings saved as `Processed_Data/processed_encoded_ml_results.h5ad`.\n",
    "\n",
    "2) TCR aggregation and filtering: TCR contigs were filtered to keep only `high_confidence == True`, `productive == True`, and chain in {TRA, TRB}. These were pivoted so that each barcode (cell) has at most one TRA and one TRB entry (columns like `v_gene_TRA`, `cdr3_TRB`), then merged into `adata.obs`. Cells without aggregated TRA/TRB were removed before downstream analyses. This pipeline mirrors the approach in the article, which emphasizes analysis of high-confidence TCR clones when comparing responders vs non-responders.\n",
    "\n",
    "3) Sequence encodings and gene expression encodings: The notebook computed:\n",
    "   - One-hot encodings of TRA/TRB CDR3 sequences (flattened),\n",
    "   - k-mer (3-mer) count matrices for TRA and TRB,\n",
    "   - Physicochemical features per CDR3 (length, molecular weight, aromaticity, instability index, pI, hydrophobicity),\n",
    "   - Gene-expression encodings (PCA, TruncatedSVD, UMAP) on top variable genes computed robustly to avoid infinities.\n",
    "   These encodings were stored in `adata.obsm` and `adata.obs` for multimodal analyses.\n",
    "\n",
    "4) Unsupervised clustering: Multiple clustering strategies were evaluated across feature modalities (UMAP, t-SNE, combined scaled features), including KMeans (k=3,4,5,6,8), HDBSCAN with different param sets, Agglomerative clustering with multiple linkages, DBSCAN across parameters, Gaussian Mixture Models (different covariance types), and hierarchical clustering on subsamples. Silhouette scores were computed on subsamples to rank clustering quality. The notebook saved labels for each run in `Processed_Data/clustering_results` and produced `clustering_metadata.json` summarizing silhouette scores and parameters.\n",
    "\n",
    "5) TCR-specific clustering and gene-expression modules: The notebook clustered TRA and TRB k-mer reduced features using KMeans (6 clusters each) and discovered gene expression modules via KMeans on gene PCA (k=8). These cluster labels were stored in `adata.obs` as `tra_kmer_clusters`, `trb_kmer_clusters`, and `gene_expression_modules`.\n",
    "\n",
    "6) Supervised learning (patient-response prediction): The pipeline built multiple feature sets (basic, gene_enhanced, tcr_enhanced, comprehensive, sequence_structure) combining gene PCA/SVD/UMAP, TCR k-mer reductions, physicochemical features, and QC features. For each feature set, models tested included Logistic Regression, Decision Tree, Random Forest, XGBoost, and a small deep learning model. GridSearchCV with StratifiedKFold was used where applicable, and cross-validated accuracies (CV_Mean, CV_Std) plus test-set metrics (Accuracy, Precision, Recall, F1, AUC, Specificity, NPV) were computed and saved to `Processed_Data/model_performance_results.csv`. The best model by mean CV accuracy was recorded in the summary JSON.\n",
    "\n",
    "7) Sequence length experiment: The notebook re-encoded sequences across a grid of `max_length` values (10–50) and trained a default XGBoost classifier to evaluate how maximum sequence length affects CV and test accuracy. Results were plotted and saved to `Processed_Data/sequence_length_experiment_results.csv`. The optimal length (by CV mean) was recorded in the JSON summary.\n",
    "\n",
    "8) Pattern discovery and downstream biological interpretation: The analysis included per-response group sequence counts, top CDR3 sequences in responders vs non-responders, physicochemical comparisons (Mann–Whitney U tests), differential k-mer enrichment, gene-expression module importance checks, dendrogram visualization of hierarchical clustering, cluster centroids and top genes computed and saved to `Processed_Data/clustering_summary.csv`, and scanpy-based marker detection where possible.\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed interpretation of the key numeric results and outputs produced in the final cells (what they mean, significance, and relation to the article)\n",
    "\n",
    "A. Dataset overview and filtering outcomes (from saved `summary`):\n",
    "\n",
    "- Total cells and genes (summary['dataset_info']): The final `adata.n_obs` and `adata.n_vars` reflect cells that passed QC and had high-confidence TCR merges when required by the pipeline. Because the article analyzed longitudinal PBMCs and focused on high-confidence TCRs and CD8 subpopulations, restricting to cells with high-confidence productive TRA/TRB is biologically motivated and reduces noise from ambiguous contigs. However, this introduces a bias: cells without recovered TCR sequences (or low-confidence contigs) are excluded from TCR-linked analyses; any population-level inferences must therefore be understood as conditional on successful TCR call recovery.\n",
    "\n",
    "- Samples and patients processed: The notebook uses `sample_id` and `patient_id` from the metadata mapping. The article similarly analyzed a small cohort (7 patients, 28 longitudinal PBMC samples). With such a small patient-level N, per-patient aggregation and patient-level cross-validation are crucial for robust inference (see recommendations below). The notebook's cell-level supervised classifiers treat each cell as an independent sample, which can inflate apparent performance due to within-patient correlation. The `supervised` section used StratifiedKFold but did not explicitly use grouped CV by patient; this is a critical caveat (see technical recommendations).\n",
    "\n",
    "B. TCR repertoire and clone dynamics (interpretation):\n",
    "\n",
    "- TCR aggregation/pivot: Collapsing multiple contigs to a single TRA/TRB row per cell is necessary to avoid row multiplication during merges. This is aligned with best practices and the article's emphasis on analyzing high-confidence productive TCR chains.\n",
    "\n",
    "- Observed clone stability vs turnover: The article reports that responders (RCB0/I) exhibited dynamic TCR turnover with a smaller fraction of clonotypes maintained longitudinally (<15%), while non-responders had more stable, clonal repertoires (20–40% maintained). The notebook's approach to measure shared/maintained clones across timepoints (via value_counts() and Venn diagrams) will reproduce those statistics if the sample-level `sample_id` and timepoint labels are correctly set. Biologically, dynamic turnover in responders suggests clonal replacement and active recruitment of new tumor-reactive clones, consistent with the article's interpretation that TCR dynamics mirror effective antitumor immunity.\n",
    "\n",
    "C. Cluster analysis and marker gene summaries (link to biological meaning):\n",
    "\n",
    "- Cluster detection and ranking by silhouette provides a sanity check for separability. The article identified CD8 T cell subclusters (GZMB+, GZMK+, naive/transitioning) and reported that GZMB+late-activated/effector-memory CD8 T cells were enriched in non-responders. The notebook's `clustering_summary.csv` and `*_markers_group_*.csv` saved by scanpy's `rank_genes_groups` provide the exact top genes per cluster; inspect clusters where `responders_pct_in_cluster` is low and fisher_pvalue_vs_rest significant — those clusters should map to the GZMB+late-activation phenotype in the article (look for GNLY, GZMB, PRF1 and exhaustion markers like PDCD1/CTLA4/TIGIT).\n",
    "\n",
    "- If the cluster_summary shows clusters dominated by single patients or single samples, that indicates batch or sample-driven structure and requires batch-correction (e.g., Harmony, bbknn) before using clusters for biological interpretation. The article used Seurat and more complex integration — if your dataset shows strong per-sample clustering, consider adding explicit batch correction before rerunning clustering.\n",
    "\n",
    "D. Physicochemical and k-mer analyses (what they reveal):\n",
    "\n",
    "- Physicochemical comparisons: Mann–Whitney U tests compare distributions (e.g., TRA/TRB length, molecular weight, hydrophobicity) between responder and non-responder groups. The article suggests that repertoire composition and perhaps subtle biochemical features of CDR3 might differ between response groups. Pay attention to meaningfully small p-values combined with effect sizes; with many tests (6 properties × 2 chains), correct for multiple testing (Benjamini–Hochberg) before claiming significance.\n",
    "\n",
    "- K-mer differential analysis identifies short sequence motifs enriched in responders vs non-responders. The notebook computes k-mer means and differences on variance-selected k-mers, then lists top enriched k-mers. These can point to shared motif usage among tumor-targeting clones. Cross-check the top k-mers against existing databases of CDR3 motifs or against tumor-infiltrating TCRs if tumor TCRs are available — the article specifically used tumor-associated TCR overlap to identify tumor-reactive clusters. Enriched motifs in responder repertoires that also appear in tumor-associated TCRs would be compelling.\n",
    "\n",
    "E. Supervised learning and model performance — interpretation and caveats:\n",
    "\n",
    "- The notebook trains multiple classifiers per feature set and reports CV_Mean and test-set metrics. The best model by CV_Mean is highlighted in the summary JSON. However, there are important caveats specific to single-cell PBMC data from a small number of patients: cell-level labels are strongly nested within patients. Using standard StratifiedKFold that splits cells at random across folds can produce overly optimistic CV estimates because cells from the same patient can appear in both train and test folds. This violates the independence assumption and can leak patient-specific signals (batch-, sample-specific) into evaluations. The correct approach for predictive claims at the patient level is to use grouped cross-validation where folds are split by patient (GroupKFold or Leave-One-Patient-Out CV). If your ultimate goal is to predict patient response, retrain and re-evaluate using patient-level cross-validation and patient-aggregated features (e.g., per-patient fractions of clusters, per-patient repertoire diversity metrics). The article is cautious about small N and highlights patient-level inference, so align your evaluation strategy to that standard.\n",
    "\n",
    "- Model interpretation: For tree-based models (Random Forest, XGBoost), extract and save feature importances and map feature indices back to modalities (gene PCA components, k-mer indices, physicochemical features). If a gene PCA component is highly important, map it back to highly-weighted genes in that component (by examining PCA loadings) to enable biological interpretation (e.g., MHC-II genes, interferon response genes). The notebook attempts to reconstruct `feature_importance` where available; ensure the `all_results` structure stores these arrays for post-hoc analysis.\n",
    "\n",
    "F. Sequence length optimization results (biological and technical notes):\n",
    "\n",
    "- The notebook finds an ‘optimal’ max CDR3 length by CV mean across the tested grid, but this optimal depends on sequence distribution in your data: if most sequences are shorter (e.g., <25 aa), encoding longer lengths adds padded zeros and increases dimensionality without information gain. Biologically, CDR3 lengths are constrained; the article and other TCR studies typically analyze full-length CDR3s as-is (no arbitrary truncation) and model length explicitly as a feature (e.g., in physicochemical features). Use the length experiment primarily to choose a practical encoding dimension for ML, not as a biological claim about an optimal CDR3 length for response.\n",
    "\n",
    "G. Cluster centroids, marker genes and patient-level aggregation (clinical relevance):\n",
    "\n",
    "- The notebook computed cluster centroids in gene PCA space and top genes per cluster and saved them. To connect to the article: identify clusters enriched for GZMB/PRF1/GNLY and test whether their per-patient abundances correlate with response (Fisher or Wilcoxon tests). The article reports that GZMB+ late-activation/effector-memory CD8 T cells are more abundant in non-responders, whereas responders show an ICI-induced expansion of GZMB+ early-activation CD8 T cells. Reproduce this by grouping `adata.obs` by `patient_id` and `timepoint`, computing cluster fractions per sample, and plotting trajectories aligned to the treatment timeline (Baseline → Post-Chemo → Post-ICI → Endpoint).\n",
    "\n",
    "H. Comparison with the published article's key findings (concordance and differences):\n",
    "\n",
    "- Concordance points: The notebook pipeline is set up to reproduce and extend the main claims of the article: (i) dynamic TCR turnover in responders vs clonal stability in non-responders; (ii) enrichment of GZMB+ effector-memory signatures in non-responders; (iii) monocyte and B cell interferon signatures associated with response. The encoding + clustering + k-mer analyses implemented give multiple orthogonal ways to find the same signals (gene-based clusters, TCR-motif enrichment, physicochemical differences).\n",
    "\n",
    "- Potential differences: The article uses Seurat and specific statistical pipelines, pseudobulk differential expression, and tumor-associated TCR overlap analysis to claim tumor association for certain T cell clusters. The notebook's direct cell-level differential analyses and k-mer tests should be adjusted to match the article's pseudobulk approach when comparing transcriptional signatures across conditions (aggregate per-sample then test across samples) — this reduces false positives from cell-level pseudoreplication. For TCR tumor-association, the notebook can be extended to compute overlaps with tumor TCRs if tumor TCR files exist in `Data/` (the raw data includes `GSM9061687_S1_all_contig_annotations.csv` etc. which may be tumor-derived). If tumor TCRs are not present, label overlap steps cannot be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161fb787",
   "metadata": {},
   "source": [
    "## Technical and statistical recommendations (must-dos before claiming strong biological conclusions)\n",
    "\n",
    "1. Re-evaluate supervised learning using patient-grouped cross-validation (GroupKFold) or leave-one-patient-out CV to estimate generalizable patient-level performance. Re-derive features at the patient level (cluster fractions, repertoire diversity, motif frequencies aggregated by patient/timepoint) and train models on those patient-level features. This aligns with the article's patient-level claims and avoids within-patient leakage.\n",
    "\n",
    "2. Use pseudobulk differential expression and gene-set scoring (e.g., ssGSEA for interferon and complement) aggregated by sample as in the article when comparing timepoints and response groups — this better matches the experimental design and reduces cell-level pseudoreplication.\n",
    "\n",
    "3. Correct for multiple testing in motif/physicochemical comparisons (FDR) and report effect sizes alongside p-values. For k-mer discovery, consider permutation tests or more robust enrichment models that account for baseline repertoires and sampling differences between patients.\n",
    "\n",
    "4. Validate clustering-derived markers by cross-referencing with canonical immune markers (CD3D, CD8A, GZMB, PRF1, GNLY, TCF7, CX3CR1, HLA-DR, CD83) and by using gene-set enrichment analyses (interferon, complement, antigen presentation) to map clusters to cell states mentioned in the article.\n",
    "\n",
    "5. If tumor TCR data are available, compute tumor–blood overlap metrics and test whether clusters enriched for shared tumor-blood clonotypes are predictive of response. The article uses this overlap to infer tumor association of GZMB+ clusters — replicating that is a high-priority validation.\n",
    "\n",
    "6. For sequence-based findings (k-mers, motifs): attempt cross-validation at the patient-level and, where possible, validate motif enrichment in orthogonal datasets or public TCR repositories. Motif-based classifiers should be treated cautiously given the high dimensionality and potential for overfitting.\n",
    "\n",
    "## Concrete next steps to extend and validate this analysis (runnable tasks)\n",
    "\n",
    "1. Patient-level aggregation cell: compute per-patient, per-timepoint features (cluster fractions, Shannon diversity of TCRs, #unique clones, counts of top clonotypes, mean physicochemical properties) and save `Processed_Data/patient_level_features.csv`.\n",
    "\n",
    "2. Grouped CV supervised pipeline: re-run all supervised learning using GroupKFold on patient-level folds and report patient-level AUC, sensitivity, specificity. Save models and a patient-level ROC curve.\n",
    "\n",
    "3. Pseudobulk differential expression: produce pseudobulked counts per sample and re-run differential testing for the interferon/complement signatures and top cluster marker genes, mirroring the article's methods.\n",
    "\n",
    "4. Tumor-blood TCR overlap (if tumor TCRs are present): compute Jaccard or Morisita overlap between tumor and blood repertoires per patient/timepoint and test association with response.\n",
    "\n",
    "5. Multiple-testing corrected k-mer enrichment: perform Benjamini–Hochberg correction and retain k-mers with FDR < 0.05 for reporting, then visualize motif logos for top hits.\n",
    "\n",
    "6. Batch correction and integration: If clusters show per-sample dominance, run Harmony or scanpy.pp.bbknn and re-do clustering and marker detection to ensure biological signal is not dominated by technical artifacts.\n",
    "\n",
    "## Files and artifacts produced by this notebook (where to find them)\n",
    "\n",
    "- `Processed_Data/processed_s_rna_seq_data.h5ad` — processed AnnData (QC-filtered, TCR-merged).\n",
    "- `Processed_Data/processed_encoded_ml_results.h5ad` — AnnData enriched with encodings and ML labels.\n",
    "- `Processed_Data/clustering_results/` — saved clustering labels (.npy) and `clustering_metadata.json` (silhouette scores, params).\n",
    "- `Processed_Data/model_performance_results.csv` — per-feature-set and per-model test/CV metrics.\n",
    "- `Processed_Data/sequence_length_experiment_results.csv` — results from the length grid experiment.\n",
    "- `Processed_Data/clustering_summary.csv` and `Processed_Data/*_markers_group_*.csv` — cluster centroids and marker tables (if scanpy ranking succeeded).\n",
    "- `Processed_Data/comprehensive_analysis_summary.json` — JSON summary with dataset overview, best models, and experiment metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286bd4f",
   "metadata": {},
   "source": [
    "## Final remarks — plain-language conclusion tying notebook outputs to the article's biological insights\n",
    "\n",
    "The notebook successfully implements a multimodal single-cell analysis pipeline very similar in spirit to the procedures and conclusions reported in the companion article: it integrates TCR and transcriptomic data, identifies transcriptomic clusters and TCR motifs, evaluates how TCR repertoire dynamics differ between responders and non-responders, and uses a variety of unsupervised and supervised learning methods to test predictability of response. The article emphasizes that responders show dynamic TCR turnover and expansion of early-activation GZMB+ CD8 T cells, while non-responders are characterized by clonally-expanded effector-memory GZMB+ CD8 T cells and a more static repertoire. The notebook contains the necessary data and computations to (re)produce these claims, but care must be taken to (A) perform patient-level aggregation and grouped CV for predictive tasks, (B) use pseudobulk differential expression for transcriptional group comparisons, and (C) correct for multiple testing in motif/physicochemical analyses. When these statistical precautions are applied, the notebook's outputs can be used to provide strong, publishable support for the biological interpretations reported in the article.\n",
    "\n",
    "---\n",
    "\n",
    "- Add and run a `patient-level aggregation` cell and produce patient-level features and plots (recommended).\n",
    "- Re-run supervised learning with GroupKFold and report patient-level ROC/AUC. (This will change the `best_model` in the JSON if within-patient leakage existed.)\n",
    "- Run pseudobulk DE for interferon/complement signatures to match the article's exact statistical design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e48b577",
   "metadata": {},
   "source": [
    "# Figures\n",
    "This section contains helper functions to (re)generate all manuscript figures programmatically.\n",
    "\n",
    "**Notes:**\n",
    "- Figure A is a conceptual flowchart; make it in BioRender/Illustrator and export to Processed_Data/figures.\n",
    "- All other figures are implemented as functions below; call them after running upstream preprocessing and model-training cells.\n",
    "- Figures are saved to `Processed_Data/figures/` by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-generated figure functions (do not run automatically)\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, roc_curve, auc, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.sparse as sp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ROOT = Path('..')\n",
    "FIGDIR = ROOT / 'Processed_Data' / 'figures'\n",
    "FIGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_processed_adata(path=None):\n",
    "    \"\"\"Load processed AnnData; adjust path if necessary.\"\"\"\n",
    "    if path is None:\n",
    "        path = ROOT / 'Processed_Data' / 'processed_s_rna_seq_data.h5ad'\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(f\"Processed AnnData not found at {path}\")\n",
    "    adata = sc.read_h5ad(path)\n",
    "    return adata\n",
    "\n",
    "def _ensure_qc_metrics(adata):\n",
    "    \"\"\"Compute n_genes and pct_mito if missing.\"\"\"\n",
    "    # n_genes (detected genes per cell)\n",
    "    if 'n_genes' not in adata.obs:\n",
    "        X = adata.X\n",
    "        if sp.issparse(X):\n",
    "            n_genes = (X > 0).sum(axis=1).A1\n",
    "        else:\n",
    "            n_genes = (X > 0).sum(axis=1)\n",
    "        adata.obs['n_genes'] = np.array(n_genes).ravel()\n",
    "    # pct_mito\n",
    "    if 'pct_mito' not in adata.obs:\n",
    "        mt_mask = adata.var_names.str.upper().str.startswith('MT-')\n",
    "        if mt_mask.sum() > 0:\n",
    "            mito_counts = adata.X[:, mt_mask].sum(axis=1)\n",
    "            total_counts = adata.X.sum(axis=1)\n",
    "            if sp.issparse(mito_counts):\n",
    "                mito_counts = mito_counts.A1\n",
    "                total_counts = total_counts.A1\n",
    "            adata.obs['pct_mito'] = np.array(mito_counts).ravel() / np.array(total_counts).ravel() * 100\n",
    "        else:\n",
    "            adata.obs['pct_mito'] = 0.0\n",
    "    return adata\n",
    "\n",
    "def figure_A_placeholder():\n",
    "    \"\"\"Figure A must be designed in BioRender/Illustrator. Export PNG to FIGDIR and display here.\"\"\"\n",
    "    msg = [\n",
    "        'Figure A: Conceptual multi-modal pipeline (BioRender/Illustrator).',\n",
    "        'Suggested export filename: Processed_Data/figures/Figure_A_Conceptual_Flowchart.png',\n",
    "        'Panels: (A) PBMC collection, (B) preproc, (C) TCR encodings, (D) feature integration, (E) ML pipeline.'\n",
    "    ]\n",
    "    print('\\n'.join(msg))\n",
    "    # convenience display (uncomment to show if you exported the figure)\n",
    "    # from IPython.display import Image, display\n",
    "    # p = FIGDIR / 'Figure_A_Conceptual_Flowchart.png'\n",
    "    # if p.exists():\n",
    "    #     display(Image(p))\n",
    "\n",
    "def figure_B_qc(adata=None, save=True):\n",
    "    \"\"\"Cohort QC: bar chart of cells per patient/response and violin plots for QC metrics.\"\"\"\n",
    "    if adata is None:\n",
    "        adata = load_processed_adata()\n",
    "    adata = _ensure_qc_metrics(adata)\n",
    "    # ensure columns\n",
    "    for col in ['response','patient_id']:`\n",
    "        if col not in adata.obs:\n",
    "            raise KeyError(f\"Expected column '{col}' in adata.obs for Figure B\")\n",
    "    # Summary counts\n",
    "    counts = adata.obs.groupby(['patient_id','response']).size().reset_index(name='cell_count')\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.barplot(data=counts, x='patient_id', y='cell_count', hue='response')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Cells per patient by Response')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_B_cell_counts_by_patient_response.png', dpi=300)\n",
    "    plt.show()\n",
    "    # QC violins\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.violinplot(data=adata.obs, x='response', y='n_genes', inner='box', palette='Set2')\n",
    "    plt.title('Detected genes per cell by Response')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_B_n_genes_violin.png', dpi=300)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.violinplot(data=adata.obs, x='response', y='pct_mito', inner='box', palette='Set2')\n",
    "    plt.title('% mitochondrial reads by Response')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_B_pct_mito_violin.png', dpi=300)\n",
    "    plt.show()\n",
    "    # Print class balance summary\n",
    "    print('Class balance (cells):')\n",
    "    print(adata.obs['response'].value_counts())\n",
    "\n",
    "def figure_C_tsne_kmeans(adata=None, k=8, save=True, pca_n_comps=50, tsne_perplexity=30, tsne_iter=1000):\n",
    "    \"\"\"t-SNE of top HVG/PCA with K-Means clustering (k default 8).\"\"\"\n",
    "    if adata is None:\n",
    "        adata = load_processed_adata()\n",
    "    # Ensure PCA\n",
    "    if 'X_pca' not in adata.obsm:\n",
    "        sc.pp.pca(adata, n_comps=pca_n_comps)\n",
    "    pca_rep = adata.obsm['X_pca']\n",
    "    # KMeans clustering\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_init=50).fit(pca_rep)\n",
    "    adata.obs['kmeans_clusters'] = pd.Categorical(km.labels_.astype(str))\n",
    "    try:\n",
    "        s = silhouette_score(pca_rep, km.labels_)\n",
    "        print(f'KMeans (k={k}) silhouette score: {s:.3f}')\n",
    "    except Exception as e:\n",
    "        print('Silhouette score could not be computed:', e)\n",
    "    # t-SNE (prefer openTSNE if available)\n",
    "    if 'X_tsne' not in adata.obsm:\n",
    "        try:\n",
    "            import openTSNE as otsne\n",
    "            tsne = otsne.TSNE(n_components=2, random_state=0, n_jobs=-1)\n",
    "            emb = np.array(tsne.fit(pca_rep))\n",
    "        except Exception:\n",
    "            emb = TSNE(n_components=2, perplexity=tsne_perplexity, n_iter=tsne_iter, random_state=0).fit_transform(pca_rep)\n",
    "        adata.obsm['X_tsne'] = emb\n",
    "    # plots\n",
    "    fig, axes = plt.subplots(1,2, figsize=(14,6))\n",
    "    sns.scatterplot(x=adata.obsm['X_tsne'][:,0], y=adata.obsm['X_tsne'][:,1], hue=adata.obs['kmeans_clusters'], palette='tab10', s=6, ax=axes[0], legend='full')\n",
    "    axes[0].set_title(f't-SNE colored by KMeans (k={k})')\n",
    "    if 'response' in adata.obs:\n",
    "        sns.scatterplot(x=adata.obsm['X_tsne'][:,0], y=adata.obsm['X_tsne'][:,1], hue=adata.obs['response'], palette='Set1', s=6, ax=axes[1], legend='full')\n",
    "        axes[1].set_title('t-SNE colored by Response')\n",
    "    else:\n",
    "        axes[1].text(0.5,0.5,'No response label in adata.obs', ha='center')\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('t-SNE1'); ax.set_ylabel('t-SNE2')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / f'Figure_C_tsne_kmeans_k{k}.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def figure_D_clustering_evaluation(adata=None, k_range=range(2,13), save=True):\n",
    "    \"\"\"Compare clustering algorithms (silhouette) and produce an elbow plot + marker heatmap.\"\"\"\n",
    "    if adata is None:\n",
    "        adata = load_processed_adata()\n",
    "    if 'X_pca' not in adata.obsm:\n",
    "        sc.pp.pca(adata, n_comps=50)\n",
    "    pca_rep = adata.obsm['X_pca']\n",
    "    # Silhouette comparison\n",
    "    algos = { 'KMeans': KMeans(n_clusters=8, random_state=0, n_init=50), 'Agglomerative': AgglomerativeClustering(n_clusters=8), 'DBSCAN': DBSCAN(eps=1.0, min_samples=5) }\n",
    "    scores = {}\n",
    "    for name, model in algos.items():\n",
    "        try:\n",
    "            labels = model.fit_predict(pca_rep)\n",
    "            if len(set(labels))>1:\n",
    "                scores[name] = silhouette_score(pca_rep, labels)\n",
    "            else:\n",
    "                scores[name] = np.nan\n",
    "        except Exception as e:\n",
    "            scores[name] = np.nan\n",
    "    scores_df = pd.DataFrame({'algorithm': list(scores.keys()), 'silhouette': list(scores.values())})\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(data=scores_df, x='algorithm', y='silhouette', palette='pastel')\n",
    "    plt.title('Clustering silhouette comparison')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_D_clustering_silhouette_comparison.png', dpi=300)\n",
    "    plt.show()\n",
    "    # Elbow plot for KMeans\n",
    "    inertias = []\n",
    "    ks = list(k_range)\n",
    "    for kk in ks:\n",
    "        km = KMeans(n_clusters=kk, random_state=0, n_init=50).fit(pca_rep)\n",
    "        inertias.append(km.inertia_)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(ks, inertias, '-o')\n",
    "    plt.xlabel('k'); plt.ylabel('Inertia'); plt.title('Elbow plot for KMeans')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_D_kmeans_elbow.png', dpi=300)\n",
    "    plt.show()\n",
    "    # DGE heatmap: requires clusters present (use kmeans_clusters if available)\n",
    "    if 'kmeans_clusters' not in adata.obs:\n",
    "        adata.obs['kmeans_clusters'] = pd.Categorical(KMeans(n_clusters=8, random_state=0, n_init=50).fit(pca_rep).labels_.astype(str))\n",
    "    try:\n",
    "        sc.tl.rank_genes_groups(adata, groupby='kmeans_clusters', method='wilcoxon', n_genes=20)\n",
    "        sc.pl.rank_genes_groups_heatmap(adata, n_genes=8, groupby='kmeans_clusters', cmap='viridis', show=False, figsize=(10,6))\n",
    "        plt.gcf().savefig(FIGDIR / 'Figure_D_marker_genes_heatmap.png', dpi=300)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('Marker heatmap could not be produced:', e)\n",
    "\n",
    "def figure_E_supervised(cv_results=None, save=True):\n",
    "    \"\"\"Bar chart of mean CV accuracy across models and ROC for best model (expects structured cv_results).\n",
    "    Expected structure: cv_results[feature_set][model_name] -> dict with 'accuracy' list and optionally 'y_true' and 'y_score' lists/arrays across folds.\n",
    "    \"\"\"\n",
    "    if cv_results is None:\n",
    "        # try to load from processed_encoded_ml_results.h5ad\n",
    "        p = ROOT / 'Processed_Data' / 'processed_encoded_ml_results.h5ad'\n",
    "        if p.exists():\n",
    "            r = sc.read_h5ad(p)\n",
    "            cv_results = r.uns.get('cv_results', None)\n",
    "    if cv_results is None:\n",
    "        print('No cv_results found. Please supply `cv_results` dict.')\n",
    "        return\n",
    "    rows = []\n",
    "    for feature_set, models in cv_results.items():\n",
    "        for model_name, metrics in models.items():\n",
    "            accs = np.array(metrics.get('accuracy', []))\n",
    "            rows.append({'feature_set': feature_set, 'model': model_name, 'mean_accuracy': float(np.nanmean(accs)) if accs.size else np.nan})\n",
    "    df = pd.DataFrame(rows)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.barplot(data=df, x='model', y='mean_accuracy', hue='feature_set')\n",
    "    plt.ylim(0,1)\n",
    "    plt.ylabel('Mean CV accuracy')\n",
    "    plt.title('Model performance across feature sets')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_E_model_performance_bar.png', dpi=300)\n",
    "    plt.show()\n",
    "    # ROC for best-performing model if predictions available\n",
    "    if df['mean_accuracy'].notna().any():\n",
    "        best = df.loc[df['mean_accuracy'].idxmax()]\n",
    "        best_fs, best_model = best['feature_set'], best['model']\n",
    "        metrics = cv_results[best_fs][best_model]\n",
    "        y_true = np.concatenate(metrics.get('y_true', [])) if isinstance(metrics.get('y_true', []), list) and len(metrics.get('y_true', []))>0 else np.array(metrics.get('y_true', []))\n",
    "        y_score = np.concatenate(metrics.get('y_score', [])) if isinstance(metrics.get('y_score', []), list) and len(metrics.get('y_score', []))>0 else np.array(metrics.get('y_score', []))\n",
    "        if y_true.size and y_score.size:\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.figure(figsize=(6,6))\n",
    "            plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "            plt.plot([0,1],[0,1],'--', color='grey')\n",
    "            plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC - Best model')\n",
    "            plt.legend(); plt.tight_layout()\n",
    "            if save: plt.savefig(FIGDIR / 'Figure_E_ROC_best_model.png', dpi=300)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('No y_true/y_score arrays available for ROC plotting for best model.')\n",
    "    else:\n",
    "        print('No valid mean_accuracy values found in cv_results to determine best model.')\n",
    "\n",
    "def figure_F_feature_set_table_and_chart(cv_results=None, save=True):\n",
    "    \"\"\"Table and stacked/pie chart for XGBoost performance across nested feature sets.\"\"\"\n",
    "    if cv_results is None:\n",
    "        p = ROOT / 'Processed_Data' / 'processed_encoded_ml_results.h5ad'\n",
    "        if p.exists():\n",
    "            r = sc.read_h5ad(p)\n",
    "            cv_results = r.uns.get('cv_results', None)\n",
    "    if cv_results is None:\n",
    "        print('No cv_results found. Provide cv_results with XGBoost metrics.')\n",
    "        return\n",
    "    rows = []\n",
    "    for feature_set, models in cv_results.items():\n",
    "        xgb_metrics = models.get('XGBoost') or models.get('xgboost') or models.get('XGB')\n",
    "        if xgb_metrics is None:\n",
    "            continue\n",
    "        accs = np.array(xgb_metrics.get('accuracy', []))\n",
    "        aucs = np.array(xgb_metrics.get('roc_auc', []))\n",
    "        rows.append({'feature_set': feature_set, 'accuracy_mean': float(np.nanmean(accs)) if accs.size else np.nan, 'auc_mean': float(np.nanmean(aucs)) if aucs.size else np.nan})\n",
    "    table = pd.DataFrame(rows).sort_values('accuracy_mean', ascending=False)\n",
    "    display(table)\n",
    "    if save: table.to_csv(FIGDIR / 'Table1_XGBoost_performance_by_feature_set.csv', index=False)\n",
    "    # stacked/pie chart (relative contributions)\n",
    "    if not table.empty:\n",
    "        plt.figure(figsize=(6,6))\n",
    "        vals = table['accuracy_mean'].fillna(0).values\n",
    "        labels = table['feature_set'].values\n",
    "        plt.pie(vals, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Relative accuracy contribution by feature set (XGBoost)')\n",
    "        if save: plt.savefig(FIGDIR / 'Figure_F_feature_set_contribution_pie.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "def figure_G_feature_importance(xgb_model=None, feature_names=None, top_n=30, save=True):\n",
    "    \"\"\"Plot top feature importances from a trained XGBoost model (gain or sklearn-style importances).\"\"\"\n",
    "    if xgb_model is None:\n",
    "        p = ROOT / 'Processed_Data' / 'processed_encoded_ml_results.h5ad'\n",
    "        if p.exists():\n",
    "            r = sc.read_h5ad(p)\n",
    "            xgb_model = r.uns.get('best_xgb', None)\n",
    "            feature_names = r.uns.get('feature_names', feature_names)\n",
    "    if xgb_model is None:\n",
    "        print('No XGBoost model found in notebook state or processed file. Provide `xgb_model`.')\n",
    "        return\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        if isinstance(xgb_model, xgb.XGBClassifier):\n",
    "            fmap = xgb_model.get_booster().get_score(importance_type='gain')\n",
    "            fi = pd.Series(fmap).sort_values(ascending=False).head(top_n)\n",
    "            fi.index = fi.index.astype(str)\n",
    "            plt.figure(figsize=(8, max(4, len(fi)*0.25)))\n",
    "            sns.barplot(x=fi.values, y=fi.index, color='steelblue')\n",
    "            plt.xlabel('gain'); plt.title('Top XGBoost feature importances (gain)')\n",
    "            plt.tight_layout()\n",
    "            if save: plt.savefig(FIGDIR / 'Figure_G_xgb_importance_gain.png', dpi=300)\n",
    "            plt.show()\n",
    "            return\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback: sklearn-style feature_importances_\n",
    "    try:\n",
    "        import joblib\n",
    "        if hasattr(xgb_model, 'feature_importances_') and feature_names is not None:\n",
    "            imp = pd.Series(xgb_model.feature_importances_, index=feature_names).sort_values(ascending=False).head(top_n)\n",
    "            plt.figure(figsize=(8, max(4, len(imp)*0.25)))\n",
    "            sns.barplot(x=imp.values, y=imp.index, color='steelblue')\n",
    "            plt.xlabel('importance'); plt.title('Top feature importances')\n",
    "            plt.tight_layout()\n",
    "            if save: plt.savefig(FIGDIR / 'Figure_G_feature_importances.png', dpi=300)\n",
    "            plt.show()\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print('Could not extract feature importances:', e)\n",
    "\n",
    "def figure_H_pca_loadings_and_enrichment(adata=None, pc_index=0, top_n=20, save=True):\n",
    "    \"\"\"Heatmap of top genes loading onto PC and enrichment via gseapy (if installed).\"\"\"\n",
    "    if adata is None:\n",
    "        adata = load_processed_adata()\n",
    "    if 'X_pca' not in adata.obsm:\n",
    "        sc.pp.pca(adata, n_comps=50)\n",
    "    loadings = adata.varm.get('PCs')\n",
    "    if loadings is None:\n",
    "        print(\"PCA loadings not found in adata.varm['PCs'] -- ensure PCA was run with `sc.pp.pca`\")\n",
    "        return\n",
    "    pc_load = pd.Series(loadings[:, pc_index], index=adata.var_names)\n",
    "    top_genes = pc_load.abs().sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    try:\n",
    "        sc.pl.matrixplot(adata, var_names=top_genes, groupby='kmeans_clusters', show=False, cmap='viridis')\n",
    "        plt.gcf().savefig(FIGDIR / f'Figure_H_PC{pc_index+1}_top{top_n}_matrixplot.png', dpi=300)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('Could not produce matrixplot heatmap:', e)\n",
    "    # enrichment (optional)\n",
    "    try:\n",
    "        import gseapy as gp\n",
    "        enr = gp.enrichr(gene_list=top_genes, gene_sets=['KEGG_2019_Human','GO_Biological_Process_2018'], organism='Human')\n",
    "        if hasattr(enr, 'results') and not enr.results.empty:\n",
    "            enr.results.to_csv(FIGDIR / f'Figure_H_PC{pc_index+1}_enrichment.csv', index=False)\n",
    "            print('Saved enrichment results (csv).')\n",
    "    except Exception as e:\n",
    "        print('gseapy not available or enrichment failed:', e)\n",
    "\n",
    "def figure_I_sequence_length_tuning(tuning_results=None, save=True):\n",
    "    \"\"\"Plot CV mean accuracy vs tested max TCR sequence lengths (expects tuning_results dict length->acc list).\"\"\"\n",
    "    if tuning_results is None:\n",
    "        p = ROOT / 'Processed_Data' / 'processed_encoded_ml_results.h5ad'\n",
    "        if p.exists():\n",
    "            r = sc.read_h5ad(p)\n",
    "            tuning_results = r.uns.get('sequence_length_tuning', None)\n",
    "    if tuning_results is None:\n",
    "        print('No tuning_results found. Provide dict: {length: [accuracies] }')\n",
    "        return\n",
    "    lengths = sorted(tuning_results.keys())\n",
    "    means = [np.mean(tuning_results[l]) for l in lengths]\n",
    "    stds = [np.std(tuning_results[l]) for l in lengths]\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.errorbar(lengths, means, yerr=stds, marker='o')\n",
    "    plt.xlabel('Max TCR sequence length (aa)'); plt.ylabel('Mean CV accuracy')\n",
    "    plt.title('Sequence length tuning (TCR encodings)')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_I_sequence_length_tuning.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def figure_J_confusion_cell_vs_patient(adata=None, y_col='response', yhat_col='y_pred', patient_col='patient_id', save=True):\n",
    "    \"\"\"Show cell-level confusion matrix and aggregated patient-level confusion (majority vote). Also provide GroupKFold skeleton for re-running grouped CV.\"\"\"\n",
    "    if adata is None:\n",
    "        adata = load_processed_adata()\n",
    "    if y_col not in adata.obs or yhat_col not in adata.obs or patient_col not in adata.obs:\n",
    "        print(f'Required columns missing: need {y_col}, {yhat_col}, {patient_col} in adata.obs')\n",
    "        return\n",
    "    # cell-level\n",
    "    y_true = adata.obs[y_col].values\n",
    "    y_pred = adata.obs[yhat_col].values\n",
    "    cm_cell = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm_cell, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Cell-level confusion matrix')\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_J_cell_level_confusion_matrix.png', dpi=300)\n",
    "    plt.show()\n",
    "    # patient-level majority vote\n",
    "    df = adata.obs[[patient_col, y_col, yhat_col]].copy()\n",
    "    agg = df.groupby(patient_col).agg({y_col: lambda x: x.mode().iloc[0], yhat_col: lambda x: x.mode().iloc[0]})\n",
    "    cm_patient = confusion_matrix(agg[y_col], agg[yhat_col], labels=np.unique(agg[y_col]))\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm_patient, annot=True, fmt='d', cmap='Oranges', xticklabels=np.unique(agg[y_col]), yticklabels=np.unique(agg[y_col]))\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Patient-level (majority vote) confusion matrix')\n",
    "    if save: plt.savefig(FIGDIR / 'Figure_J_patient_level_confusion_matrix.png', dpi=300)\n",
    "    plt.show()\n",
    "    # GroupKFold skeleton\n",
    "    print('\\nTo properly estimate patient-level generalization, re-run training with GroupKFold/LeaveOneGroupOut using `patient_id` as groups. Example skeleton:')\n",
    "    print('''from sklearn.model_selection import GroupKFold\\ngkf = GroupKFold(n_splits=5)\\nfor train_idx, test_idx in gkf.split(X, y, groups=groups):\\n    X_train, X_test = X[train_idx], X[test_idx]\\n    y_train, y_test = y[train_idx], y[test_idx]\\n    # fit model on X_train/y_train and evaluate on X_test/y_test''')\n",
    "\n",
    "# Usage instructions (run manually):\n",
    "print('Functions added: figure_A_placeholder, figure_B_qc, figure_C_tsne_kmeans, figure_D_clustering_evaluation, figure_E_supervised, figure_F_feature_set_table_and_chart, figure_G_feature_importance, figure_H_pca_loadings_and_enrichment, figure_I_sequence_length_tuning, figure_J_confusion_cell_vs_patient')\n",
    "print('Example:')\n",
    "print(\"adata = load_processed_adata(); figure_B_qc(adata); figure_C_tsne_kmeans(adata)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24265.709443,
   "end_time": "2025-11-12T10:38:08.137469",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-12T03:53:42.428026",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
