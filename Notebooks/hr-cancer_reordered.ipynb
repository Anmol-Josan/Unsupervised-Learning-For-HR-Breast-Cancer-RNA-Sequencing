{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6795d8f5",
   "metadata": {},
   "source": [
    "# Introduction to HR Breast Cancer RNA Sequencing Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of single-cell RNA sequencing data from HR (Hormone Receptor) breast cancer patients. The dataset (GSE300475) includes gene expression profiles and T-cell receptor (TCR) sequences from responders and non-responders to immunotherapy.\n",
    "\n",
    "## Objectives\n",
    "- Process and integrate single-cell RNA-seq and TCR data\n",
    "- Perform unsupervised clustering to identify cell populations\n",
    "- Develop supervised models to predict treatment response\n",
    "- Analyze sequence patterns and gene expression signatures\n",
    "- Identify cluster-specific markers and associations with clinical outcomes\n",
    "\n",
    "## Methodology\n",
    "- Data loading and quality control using Scanpy\n",
    "- Sequence encoding (one-hot, k-mer, physicochemical) for TCR data\n",
    "- Dimensionality reduction (PCA, UMAP, t-SNE) for gene expression\n",
    "- Unsupervised clustering (K-Means, HDBSCAN, etc.)\n",
    "- Supervised classification with cross-validation\n",
    "- Interpretation of clusters and biomarkers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d71f68",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659906c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_fetch = [\n",
    "    {\n",
    "        \"name\": \"GSE300475_RAW.tar\",\n",
    "        \"size\": \"565.5 Mb\",\n",
    "        \"download_url\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE300475&format=file\",\n",
    "        \"type\": \"TAR (of CSV, MTX, TSV)\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GSE300475_feature_ref.xlsx\",\n",
    "        \"size\": \"5.4 Kb\",\n",
    "        \"download_url\": \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE300nnn/GSE300475/suppl/GSE300475%5Ffeature%5Fref.xlsx\",\n",
    "        \"type\": \"XLSX\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea8846",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = \"../Data\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "print(f\"Downloads will be saved in: {os.path.abspath(download_dir)}\\n\")\n",
    "\n",
    "def download_file(url, filename, destination_folder):\n",
    "    \"\"\"\n",
    "    Downloads a file from a given URL to a specified destination folder.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(destination_folder, filename)\n",
    "    print(f\"Attempting to download {filename} from {url}...\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Successfully downloaded {filename} to {filepath}\")\n",
    "        return filepath\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc53ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_info in files_to_fetch:\n",
    "    filename = file_info[\"name\"]\n",
    "    url = file_info[\"download_url\"]\n",
    "    file_type = file_info[\"type\"]\n",
    "\n",
    "    downloaded_filepath = download_file(url, filename, download_dir)\n",
    "\n",
    "        # If the file is a TAR archive, extract it and list the contents\n",
    "    if downloaded_filepath and filename.endswith(\".tar\"):\n",
    "        print(f\"Extracting {filename}...\\n\")\n",
    "        try:\n",
    "            with tarfile.open(downloaded_filepath, \"r\") as tar:\n",
    "                # List contents\n",
    "                members = tar.getnames()\n",
    "                print(f\"Files contained in {filename}:\")\n",
    "                for member in members:\n",
    "                    print(f\" - {member}\")\n",
    "\n",
    "                # Extract to a subdirectory within download_dir\n",
    "                extract_path = os.path.join(download_dir, filename.replace(\".tar\", \"\"))\n",
    "                os.makedirs(extract_path, exist_ok=True)\n",
    "                tar.extractall(path=extract_path)\n",
    "                print(f\"\\nExtracted to: {extract_path}\")\n",
    "        except tarfile.TarError as e:\n",
    "            print(f\"Error extracting {filename}: {e}\")\n",
    "\n",
    "        print(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c21e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from scipy.io import mmread\n",
    "\n",
    "def decompress_gz_file(gz_path, output_dir):\n",
    "    \"\"\"\n",
    "    Decompress a .gz file to the specified output directory.\n",
    "    \"\"\"\n",
    "    output_path = os.path.join(output_dir, Path(gz_path).stem)\n",
    "    print(f\"Decompressing {gz_path} â†’ {output_path}\")\n",
    "    try:\n",
    "        with gzip.open(gz_path, 'rb') as f_in, open(output_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to decompress {gz_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def preview_file(file_path):\n",
    "    \"\"\"\n",
    "    Display the first few lines of a decompressed file, based on its extension.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Preview of {os.path.basename(file_path)} ---\")\n",
    "    try:\n",
    "        if file_path.endswith(\".tsv\"):\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "            print(df.head())\n",
    "        elif file_path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(df.head())\n",
    "        elif file_path.endswith(\".mtx\"):\n",
    "          matrix = mmread(file_path).tocoo()\n",
    "          print(\"First 5 non-zero entries:\")\n",
    "          for i in range(min(5, len(matrix.data))):\n",
    "              print(f\"Row: {matrix.row[i]}, Col: {matrix.col[i]}, Value: {matrix.data[i]}\")\n",
    "          print(f\"\\nMatrix shape: {matrix.shape}, NNZ (non-zero elements): {matrix.nnz}\")\n",
    "        else:\n",
    "            print(\"Unsupported file type for preview.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not preview {file_path}: {e}\")\n",
    "\n",
    "extract_dir = os.path.join(download_dir, \"GSE300475_RAW\")\n",
    "\n",
    "for root, _, files in os.walk(extract_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".gz\"):\n",
    "            gz_file_path = os.path.join(root, file)\n",
    "            decompressed_path = decompress_gz_file(gz_file_path, root)\n",
    "            if decompressed_path:\n",
    "                preview_file(decompressed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Find all \"all_contig_annotations.csv\" files in the extracted directory and sum their lengths (number of rows)\n",
    "\n",
    "all_contig_files = glob.glob(os.path.join(extract_dir, \"*_all_contig_annotations.csv\"))\n",
    "total_rows = 0\n",
    "\n",
    "for file in all_contig_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        num_rows = len(df)\n",
    "        print(f\"{os.path.basename(file)}: {num_rows} rows\")\n",
    "        total_rows += num_rows\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read {file}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal rows in all contig annotation files: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0e0a0",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821420a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scanpy pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc6dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for single-cell RNA-seq analysis and data handling\n",
    "import scanpy as sc  # Main library for single-cell analysis, provides AnnData structure and many tools\n",
    "import pandas as pd  # For tabular data manipulation and metadata handling\n",
    "import numpy as np   # For numerical operations and array handling\n",
    "import os            # For operating system interactions (file paths, etc.)\n",
    "from pathlib import Path  # For robust and readable file path management\n",
    "\n",
    "# Print versions to ensure reproducibility and compatibility\n",
    "print(f\"Scanpy version: {sc.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c34ad",
   "metadata": {},
   "source": [
    "### Load Sample Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd77bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Setup data paths ---\n",
    "# Define the main data directory and the subdirectory containing raw files.\n",
    "data_dir = Path('../Data')\n",
    "raw_data_dir = data_dir / 'GSE300475_RAW'\n",
    "\n",
    "# --- Manually create the metadata mapping ---\n",
    "# This list contains information about each sample, including GEO IDs, patient IDs, timepoints, and response status.\n",
    "# Note: S8 (GSM9061672) has GEX files but no corresponding TCR file.\n",
    "metadata_list = [\n",
    "    # Patient 1 (Responder)\n",
    "    {'S_Number': 'S1',  'GEX_Sample_ID': 'GSM9061665', 'TCR_Sample_ID': 'GSM9061687', 'Patient_ID': 'PT1',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S2',  'GEX_Sample_ID': 'GSM9061666', 'TCR_Sample_ID': 'GSM9061688', 'Patient_ID': 'PT1',  'Timepoint': 'Post-Chemo',  'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 2 (Non-Responder)\n",
    "    {'S_Number': 'S3',  'GEX_Sample_ID': 'GSM9061667', 'TCR_Sample_ID': 'GSM9061689', 'Patient_ID': 'PT2',  'Timepoint': 'Baseline',     'Response': 'Non-Responder', 'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S4',  'GEX_Sample_ID': 'GSM9061668', 'TCR_Sample_ID': 'GSM9061690', 'Patient_ID': 'PT2',  'Timepoint': 'Post-Chemo',  'Response': 'Non-Responder', 'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 3 (Responder)\n",
    "    {'S_Number': 'S5',  'GEX_Sample_ID': 'GSM9061669', 'TCR_Sample_ID': 'GSM9061691', 'Patient_ID': 'PT3',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S6',  'GEX_Sample_ID': 'GSM9061670', 'TCR_Sample_ID': 'GSM9061692', 'Patient_ID': 'PT3',  'Timepoint': 'Post-Chemo',  'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 4 (Non-Responder)\n",
    "    {'S_Number': 'S7',  'GEX_Sample_ID': 'GSM9061671', 'TCR_Sample_ID': 'GSM9061693', 'Patient_ID': 'PT4',  'Timepoint': 'Baseline',     'Response': 'Non-Responder', 'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 5 (partial) - S8 exists as GEX only in the raw data but has no TCR file\n",
    "    {'S_Number': 'S8',  'GEX_Sample_ID': 'GSM9061672', 'TCR_Sample_ID': None,             'Patient_ID': 'PT5',  'Timepoint': 'Unknown',      'Response': 'Unknown',       'In_Data': 'GEX only', 'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S9',  'GEX_Sample_ID': 'GSM9061673', 'TCR_Sample_ID': 'GSM9061694', 'Patient_ID': 'PT5',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S10', 'GEX_Sample_ID': 'GSM9061674', 'TCR_Sample_ID': 'GSM9061695', 'Patient_ID': 'PT5',  'Timepoint': 'Post-ICI',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 11 (Responder)\n",
    "    {'S_Number': 'S11', 'GEX_Sample_ID': 'GSM9061675', 'TCR_Sample_ID': 'GSM9061696', 'Patient_ID': 'PT11', 'Timepoint': 'Endpoint',      'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "]\n",
    "\n",
    "# --- Create DataFrame and display the verification table ---\n",
    "metadata_df = pd.DataFrame(metadata_list)\n",
    "print(\"Metadata table now matches the requested specification:\")\n",
    "display(metadata_df)\n",
    "\n",
    "# --- Programmatic sanity-check for file presence ---\n",
    "# This loop checks if the expected files exist for each sample and updates the 'In_Data' column accordingly.\n",
    "for idx, row in metadata_df.iterrows():\n",
    "    s = row['S_Number']\n",
    "    g = row['GEX_Sample_ID']\n",
    "    t = row['TCR_Sample_ID']\n",
    "    # Check for gene expression matrix file (compressed or uncompressed)\n",
    "    g_exists = (raw_data_dir / f\"{g}_{s}_matrix.mtx.gz\").exists() or (raw_data_dir / f\"{g}_{s}_matrix.mtx\").exists()\n",
    "    t_exists = False\n",
    "    # Check for TCR annotation file if TCR sample ID is present\n",
    "    if pd.notna(t) and t is not None:\n",
    "        t_exists = (raw_data_dir / f\"{t}_{s}_all_contig_annotations.csv.gz\").exists() or (raw_data_dir / f\"{t}_{s}_all_contig_annotations.csv\").exists()\n",
    "    # Update 'In_Data' column based on file presence\n",
    "    if g_exists and t_exists:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'Yes'\n",
    "    elif g_exists and not t_exists:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'GEX only'\n",
    "    else:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'No'\n",
    "\n",
    "print(\"\\nPost-check In_Data column (based on files found in Data/GSE300475_RAW):\")\n",
    "display(metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdf83b",
   "metadata": {},
   "source": [
    "### Process and Concatenate AnnData Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Initialize lists to hold AnnData and TCR data for each sample ---\n",
    "adata_list = []  # Will store AnnData objects for each sample\n",
    "tcr_data_list = []  # Will store TCR dataframes for each sample\n",
    "\n",
    "# --- Iterate through each sample in the metadata table ---\n",
    "for index, row in metadata_df.iterrows():\n",
    "    gex_sample_id = row['GEX_Sample_ID']\n",
    "    tcr_sample_id = row['TCR_Sample_ID']\n",
    "    s_number = row['S_Number']\n",
    "    patient_id = row['Patient_ID']\n",
    "    timepoint = row['Timepoint']\n",
    "    response = row['Response']\n",
    "    \n",
    "    # Construct the file prefix for this sample (used for locating files)\n",
    "    sample_prefix = f\"{gex_sample_id}_{s_number}\"\n",
    "    sample_data_path = raw_data_dir\n",
    "    \n",
    "    # --- Check for gene expression matrix file ---\n",
    "    matrix_file = sample_data_path / f\"{sample_prefix}_matrix.mtx.gz\"\n",
    "    if not matrix_file.exists():\n",
    "        # Try uncompressed version if gzipped file not found\n",
    "        matrix_file_un = sample_data_path / f\"{sample_prefix}_matrix.mtx\"\n",
    "        if not matrix_file_un.exists():\n",
    "            print(f\"GEX data not found for sample {sample_prefix}, skipping.\")\n",
    "            continue\n",
    "        else:\n",
    "            matrix_file = matrix_file_un\n",
    "            \n",
    "    print(f\"Processing GEX sample: {sample_prefix}\")\n",
    "    \n",
    "    # --- Load gene expression data into AnnData object ---\n",
    "    # The prefix ensures only files for this sample are loaded\n",
    "    adata_sample = sc.read_10x_mtx(\n",
    "        sample_data_path, \n",
    "        var_names='gene_symbols',\n",
    "        prefix=f\"{sample_prefix}_\",\n",
    "    )\n",
    "    \n",
    "    # --- Add sample metadata to AnnData.obs ---\n",
    "    adata_sample.obs['sample_id'] = gex_sample_id \n",
    "    adata_sample.obs['patient_id'] = patient_id\n",
    "    adata_sample.obs['timepoint'] = timepoint\n",
    "    adata_sample.obs['response'] = response\n",
    "    \n",
    "    adata_list.append(adata_sample)\n",
    "    \n",
    "    # --- Load TCR data if available ---\n",
    "    if pd.isna(tcr_sample_id) or tcr_sample_id is None:\n",
    "        print(f\"No TCR sample for {gex_sample_id}_{s_number}, skipping TCR load.\")\n",
    "        continue\n",
    "\n",
    "    # Construct path for TCR annotation file (gzipped or uncompressed)\n",
    "    tcr_file_path = raw_data_dir / f\"{tcr_sample_id}_{s_number}_all_contig_annotations.csv.gz\"\n",
    "\n",
    "    if tcr_file_path.exists():\n",
    "        print(f\"Found and loading TCR data: {tcr_file_path.name}\")\n",
    "        tcr_df = pd.read_csv(tcr_file_path)\n",
    "        # Add sample_id for merging later\n",
    "        tcr_df['sample_id'] = gex_sample_id \n",
    "        tcr_data_list.append(tcr_df)\n",
    "    else:\n",
    "        # Try uncompressed version if gzipped file not found\n",
    "        tcr_file_path_uncompressed = raw_data_dir / f\"{tcr_sample_id}_{s_number}_all_contig_annotations.csv\"\n",
    "        if tcr_file_path_uncompressed.exists():\n",
    "            print(f\"Found and loading TCR data: {tcr_file_path_uncompressed.name}\")\n",
    "            tcr_df = pd.read_csv(tcr_file_path_uncompressed)\n",
    "            tcr_df['sample_id'] = gex_sample_id\n",
    "            tcr_data_list.append(tcr_df)\n",
    "        else:\n",
    "            print(f\"TCR data not found for {tcr_sample_id}_{s_number}\")\n",
    "\n",
    "# --- Concatenate all loaded AnnData objects into one ---\n",
    "if adata_list:\n",
    "    # Use sample_id as batch key for concatenation\n",
    "    loaded_batches = [a.obs['sample_id'].unique()[0] for a in adata_list]\n",
    "    adata = sc.AnnData.concatenate(*adata_list, join='outer', batch_key='sample_id', batch_categories=loaded_batches)\n",
    "    print(\"\\nConcatenated AnnData object:\")\n",
    "    print(adata)\n",
    "else:\n",
    "    print(\"No data was loaded.\")\n",
    "\n",
    "# --- Concatenate all loaded TCR dataframes into one ---\n",
    "if tcr_data_list:\n",
    "    full_tcr_df = pd.concat(tcr_data_list, ignore_index=True)\n",
    "    print(\"\\nFull TCR data:\")\n",
    "    display(full_tcr_df.head())\n",
    "else:\n",
    "    print(\"No TCR data was loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ab087",
   "metadata": {},
   "source": [
    "### Integrate TCR Data and Perform QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169dffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Integrate TCR data into AnnData.obs and perform quality control ---\n",
    "if 'full_tcr_df' in locals() and not full_tcr_df.empty:\n",
    "    # --- FIX START ---\n",
    "    # The previous join failed because one cell (barcode) can have multiple TCR contigs (e.g., TRA and TRB chains),\n",
    "    # creating a one-to-many join that increases the number of rows.\n",
    "    # The fix is to aggregate the TCR data to one row per cell *before* merging.\n",
    "\n",
    "    # 1. Filter for high-confidence, productive TRA/TRB chains.\n",
    "    # Only keep TCR contigs that are both high-confidence and productive, and are either TRA or TRB chains.\n",
    "    tcr_to_agg = full_tcr_df[\n",
    "        (full_tcr_df['high_confidence'] == True) &\n",
    "        (full_tcr_df['productive'] == True) &\n",
    "        (full_tcr_df['chain'].isin(['TRA', 'TRB']))\n",
    "    ].copy()\n",
    "\n",
    "    # 2. Pivot the data to create one row per barcode, with columns for TRA and TRB data.\n",
    "    # This step ensures each cell (barcode) has its TRA and TRB info in separate columns.\n",
    "    tcr_aggregated = tcr_to_agg.pivot_table(\n",
    "        index=['sample_id', 'barcode'],\n",
    "        columns='chain',\n",
    "        values=['v_gene', 'j_gene', 'cdr3'],\n",
    "        aggfunc='first'  # 'first' is safe as we expect at most one productive TRA/TRB per cell\n",
    "    )\n",
    "\n",
    "    # 3. Flatten the multi-level column index (e.g., from ('v_gene', 'TRA') to 'v_gene_TRA')\n",
    "    tcr_aggregated.columns = ['_'.join(col).strip() for col in tcr_aggregated.columns.values]\n",
    "    tcr_aggregated.reset_index(inplace=True)\n",
    "\n",
    "    # 4. Prepare adata.obs for the merge by creating a matching barcode column.\n",
    "    # The index in adata.obs is like 'AGCCATGCAGCTGTTA-1-0' (barcode-batch_id).\n",
    "    # The barcode in TCR data is like 'AGCCATGCAGCTGTTA-1'.\n",
    "    adata.obs['barcode_for_merge'] = adata.obs.index.str.rsplit('-', n=1).str[0]\n",
    "\n",
    "    # 5. Perform a left merge. This keeps all cells from adata and adds TCR info where available.\n",
    "    # The number of rows will not change because tcr_aggregated has unique barcodes.\n",
    "    original_obs = adata.obs.copy()\n",
    "    merged_obs = original_obs.merge(\n",
    "        tcr_aggregated,\n",
    "        left_on=['sample_id', 'barcode_for_merge'],\n",
    "        right_on=['sample_id', 'barcode'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 6. Restore the original index to the merged dataframe.\n",
    "    merged_obs.index = original_obs.index\n",
    "    adata.obs = merged_obs\n",
    "    # --- FIX END ---\n",
    "\n",
    "    print(\"Aggregated TCR data merged into AnnData object.\")\n",
    "    \n",
    "    # --- Filter for cells that have TCR information after the merge ---\n",
    "    # Only keep cells with non-null v_gene_TRA (i.e., cells with high-confidence TCR data)\n",
    "    initial_cells = adata.n_obs\n",
    "    adata = adata[~adata.obs['v_gene_TRA'].isna()].copy()\n",
    "    print(f\"Filtered from {initial_cells} to {adata.n_obs} cells based on having high-confidence TCR data.\")\n",
    "\n",
    "# --- Basic QC and filtering ---\n",
    "# Filter out cells with fewer than 200 genes detected\n",
    "sc.pp.filter_cells(adata, min_genes=200)\n",
    "# Filter out genes detected in fewer than 3 cells\n",
    "sc.pp.filter_genes(adata, min_cells=3)\n",
    "\n",
    "# Annotate mitochondrial genes for QC metrics\n",
    "adata.var['mt'] = adata.var_names.str.startswith('MT-')\n",
    "# Calculate QC metrics (e.g., percent mitochondrial genes)\n",
    "sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "\n",
    "print(\"\\nPost-QC AnnData object:\")\n",
    "print(adata)\n",
    "display(adata.obs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90620a68",
   "metadata": {},
   "source": [
    "### Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Save processed AnnData object to disk ---\n",
    "# Define output directory for processed data\n",
    "output_dir = Path('../Processed_Data')\n",
    "output_dir.mkdir(exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Define output file path for the .h5ad file\n",
    "output_path = output_dir / 'processed_s_rna_seq_data.h5ad'\n",
    "# Save the AnnData object (contains all processed, filtered, and annotated data)\n",
    "adata.write_h5ad(output_path)\n",
    "\n",
    "print(f\"Processed data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e14b40e",
   "metadata": {},
   "source": [
    "## 3. Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Install required packages for genetic sequence encoding and ML ---\n",
    "%pip install biopython\n",
    "%pip install scikit-learn\n",
    "%pip install umap-learn\n",
    "%pip install hdbscan\n",
    "%pip install plotly\n",
    "%pip install xgboost\n",
    "%pip install tensorflow\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import ProtParam\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Import scipy for hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import umap\n",
    "import hdbscan\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"Additional libraries installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9db459",
   "metadata": {},
   "source": [
    "## 4. Defining Sequence Encoding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b4272",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Genetic Sequence Encoding Functions ---\n",
    "\n",
    "def one_hot_encode_sequence(sequence, max_length=50, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    One-hot encode a protein/nucleotide sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        max_length: Maximum sequence length (pad or truncate)\n",
    "        alphabet: Valid characters in the sequence\n",
    "    \n",
    "    Returns:\n",
    "        2D numpy array of shape (max_length, len(alphabet))\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    sequence = str(sequence).upper()[:max_length]  # Truncate if too long\n",
    "    encoding = np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    for i, char in enumerate(sequence):\n",
    "        if char in alphabet:\n",
    "            char_idx = alphabet.index(char)\n",
    "            encoding[i, char_idx] = 1\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "def kmer_encode_sequence(sequence, k=3, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    K-mer encoding of sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        k: Length of k-mers\n",
    "        alphabet: Valid characters\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with k-mer counts\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {}\n",
    "    \n",
    "    sequence = str(sequence).upper()\n",
    "    kmers = [sequence[i:i+k] for i in range(len(sequence)-k+1)]\n",
    "    valid_kmers = [kmer for kmer in kmers if all(c in alphabet for c in kmer)]\n",
    "    \n",
    "    return Counter(valid_kmers)\n",
    "\n",
    "def physicochemical_features(sequence):\n",
    "    \"\"\"\n",
    "    Extract physicochemical properties from protein sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Protein sequence string\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of features\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {\n",
    "            'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        seq = str(sequence).upper()\n",
    "        # Remove non-standard amino acids\n",
    "        seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "        \n",
    "        if len(seq) == 0:\n",
    "            return {\n",
    "                'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "                'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "            }\n",
    "        \n",
    "        bio_seq = Seq(seq)\n",
    "        analyzer = ProtParam.ProteinAnalysis(str(bio_seq))\n",
    "        \n",
    "        return {\n",
    "            'length': len(seq),\n",
    "            'molecular_weight': analyzer.molecular_weight(),\n",
    "            'aromaticity': analyzer.aromaticity(),\n",
    "            'instability_index': analyzer.instability_index(),\n",
    "            'isoelectric_point': analyzer.isoelectric_point(),\n",
    "            'hydrophobicity': analyzer.gravy()  # Grand Average of Hydropathy\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'length': len(str(sequence)) if not pd.isna(sequence) else 0,\n",
    "            'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "\n",
    "def encode_gene_expression_patterns(adata, n_top_genes=1000):\n",
    "    \"\"\"\n",
    "    Encode gene expression patterns using various dimensionality reduction techniques.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object with gene expression data\n",
    "        n_top_genes: Number of highly variable genes to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of encoded representations\n",
    "    \"\"\"\n",
    "    # Get highly variable genes if not already computed\n",
    "    if 'highly_variable' not in adata.var.columns:\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, subset=False)\n",
    "    \n",
    "    # Extract expression matrix for highly variable genes\n",
    "    hvg_mask = adata.var['highly_variable'] if 'highly_variable' in adata.var.columns else adata.var.index[:n_top_genes]\n",
    "    X_hvg = adata[:, hvg_mask].X.toarray() if hasattr(adata.X, 'toarray') else adata[:, hvg_mask].X\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_hvg)\n",
    "    \n",
    "    encodings = {}\n",
    "    \n",
    "    # PCA encoding\n",
    "    pca = PCA(n_components=50)\n",
    "    encodings['pca'] = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # TruncatedSVD for sparse matrices\n",
    "    svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "    encodings['svd'] = svd.fit_transform(X_scaled)\n",
    "    \n",
    "    # UMAP encoding\n",
    "    umap_encoder = umap.UMAP(n_components=20, random_state=42)\n",
    "    encodings['umap'] = umap_encoder.fit_transform(X_scaled)\n",
    "    \n",
    "    return encodings, X_scaled\n",
    "\n",
    "print(\"Genetic sequence encoding functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be794b0",
   "metadata": {},
   "source": [
    "## 5. Applying Encodings to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a46496",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Applying Encodings to Combined Data ---\n",
    "\n",
    "print(\"Applying encodings to combined data...\")\n",
    "\n",
    "# Apply gene expression pattern encodings\n",
    "print(\"Encoding gene expression patterns...\")\n",
    "expression_encodings, X_scaled = encode_gene_expression_patterns(combined_adata, n_top_genes=1000)\n",
    "\n",
    "# Apply sequence encodings to contig data\n",
    "print(\"Encoding genetic sequences...\")\n",
    "sequence_features = []\n",
    "\n",
    "for idx, row in combined_contigs.iterrows():\n",
    "    if idx % 1000 == 0:\n",
    "        print(f\"Processing sequence {idx}/{len(combined_contigs)}\")\n",
    "    \n",
    "    # One-hot encoding\n",
    "    oh_encoding = one_hot_encode_sequence(row['cdr3_aa'], max_length=30)\n",
    "    oh_flat = oh_encoding.flatten()\n",
    "    \n",
    "    # K-mer encoding (3-mers)\n",
    "    kmer_counts = kmer_encode_sequence(row['cdr3_aa'], k=3)\n",
    "    kmer_features = [kmer_counts.get(kmer, 0) for kmer in all_possible_kmers]\n",
    "    \n",
    "    # Physicochemical features\n",
    "    phys_features = physicochemical_features(row['cdr3_aa'])\n",
    "    phys_values = list(phys_features.values())\n",
    "    \n",
    "    # Combine all features\n",
    "    combined_features = np.concatenate([\n",
    "        oh_flat,\n",
    "        np.array(kmer_features),\n",
    "        np.array(phys_values)\n",
    "    ])\n",
    "    \n",
    "    sequence_features.append(combined_features)\n",
    "\n",
    "sequence_features = np.array(sequence_features)\n",
    "print(f\"Sequence features shape: {sequence_features.shape}\")\n",
    "\n",
    "# Combine expression and sequence features\n",
    "print(\"Combining expression and sequence features...\")\n",
    "combined_features = []\n",
    "\n",
    "for i in range(len(combined_adata)):\n",
    "    # Get expression encodings for this cell\n",
    "    expr_pca = expression_encodings['pca'][i]\n",
    "    expr_umap = expression_encodings['umap'][i]\n",
    "    \n",
    "    # For now, we'll use a simple approach: average sequence features for cells with multiple contigs\n",
    "    # In a more sophisticated approach, you might want to aggregate or select representative contigs\n",
    "    cell_contigs = combined_contigs[combined_contigs['sample'] == combined_adata.obs.index[i]]\n",
    "    \n",
    "    if len(cell_contigs) > 0:\n",
    "        # Average sequence features across contigs for this cell\n",
    "        cell_seq_features = sequence_features[cell_contigs.index]\n",
    "        avg_seq_features = np.mean(cell_seq_features, axis=0)\n",
    "    else:\n",
    "        # If no contigs, use zeros\n",
    "        avg_seq_features = np.zeros(sequence_features.shape[1])\n",
    "    \n",
    "    # Combine expression and sequence features\n",
    "    combined = np.concatenate([expr_pca, expr_umap, avg_seq_features])\n",
    "    combined_features.append(combined)\n",
    "\n",
    "combined_features = np.array(combined_features)\n",
    "print(f\"Combined features shape: {combined_features.shape}\")\n",
    "\n",
    "# Standardize combined features\n",
    "scaler_combined = StandardScaler()\n",
    "X_combined_scaled = scaler_combined.fit_transform(combined_features)\n",
    "\n",
    "print(\"Encodings applied successfully!\")\n",
    "print(f\"Final feature matrix shape: {X_combined_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36369a",
   "metadata": {},
   "source": [
    "## 6. Unsupervised Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Unsupervised Clustering Analysis ---\n",
    "\n",
    "print(\"Performing unsupervised clustering analysis...\")\n",
    "\n",
    "# Define clustering algorithms to test\n",
    "clustering_algorithms = {\n",
    "    'K-Means': KMeans(n_clusters=5, random_state=42, n_init=10),\n",
    "    'HDBSCAN': hdbscan.HDBSCAN(min_cluster_size=50, min_samples=10),\n",
    "    'Agglomerative': AgglomerativeClustering(n_clusters=5),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=10),\n",
    "    'GaussianMixture': GaussianMixture(n_components=5, random_state=42)\n",
    "}\n",
    "\n",
    "# Store clustering results\n",
    "clustering_results = {}\n",
    "silhouette_scores = {}\n",
    "\n",
    "# Sample data for faster computation (optional)\n",
    "sample_size = min(5000, len(X_combined_scaled))\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(X_combined_scaled), sample_size, replace=False)\n",
    "X_sample = X_combined_scaled[sample_indices]\n",
    "\n",
    "print(f\"Clustering on {sample_size} samples...\")\n",
    "\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    print(f\"Running {name}...\")\n",
    "    try:\n",
    "        if name == 'GaussianMixture':\n",
    "            labels = algorithm.fit_predict(X_sample)\n",
    "        else:\n",
    "            labels = algorithm.fit_predict(X_sample)\n",
    "        \n",
    "        clustering_results[name] = labels\n",
    "        \n",
    "        # Calculate silhouette score if more than 1 cluster\n",
    "        if len(np.unique(labels)) > 1:\n",
    "            sil_score = silhouette_score(X_sample, labels)\n",
    "            silhouette_scores[name] = sil_score\n",
    "            print(f\"{name} silhouette score: {sil_score:.3f}\")\n",
    "        else:\n",
    "            silhouette_scores[name] = -1\n",
    "            print(f\"{name} found only 1 cluster\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {name}: {e}\")\n",
    "        clustering_results[name] = None\n",
    "        silhouette_scores[name] = -1\n",
    "\n",
    "# Find best clustering algorithm\n",
    "best_algorithm = max(silhouette_scores, key=silhouette_scores.get)\n",
    "best_labels = clustering_results[best_algorithm]\n",
    "\n",
    "print(f\"\\nBest clustering algorithm: {best_algorithm} (silhouette: {silhouette_scores[best_algorithm]:.3f})\")\n",
    "\n",
    "# Apply best clustering to full dataset\n",
    "print(\"Applying best clustering to full dataset...\")\n",
    "if best_algorithm == 'GaussianMixture':\n",
    "    final_labels = GaussianMixture(n_components=5, random_state=42).fit_predict(X_combined_scaled)\n",
    "else:\n",
    "    final_labels = clustering_algorithms[best_algorithm].fit_predict(X_combined_scaled)\n",
    "\n",
    "# Add clustering results to AnnData\n",
    "combined_adata.obs['cluster'] = final_labels.astype(str)\n",
    "\n",
    "print(\"Clustering analysis completed!\")\n",
    "print(f\"Number of clusters found: {len(np.unique(final_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d30b7e",
   "metadata": {},
   "source": [
    "## 7. Visualization of Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Visualization of Clustering Results ---\n",
    "\n",
    "print(\"Creating visualizations of clustering results...\")\n",
    "\n",
    "# Compute UMAP for visualization\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "umap_coords = umap_reducer.fit_transform(X_combined_scaled)\n",
    "\n",
    "# Add to AnnData\n",
    "combined_adata.obsm['X_umap'] = umap_coords\n",
    "\n",
    "# Create visualization plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: UMAP colored by cluster\n",
    "scatter = axes[0, 0].scatter(umap_coords[:, 0], umap_coords[:, 1],\n",
    "                           c=final_labels, cmap='tab10', alpha=0.7, s=10)\n",
    "axes[0, 0].set_title('UMAP - Clusters')\n",
    "axes[0, 0].set_xlabel('UMAP1')\n",
    "axes[0, 0].set_ylabel('UMAP2')\n",
    "plt.colorbar(scatter, ax=axes[0, 0], label='Cluster')\n",
    "\n",
    "# Plot 2: UMAP colored by sample\n",
    "sample_colors = plt.cm.rainbow(np.linspace(0, 1, len(combined_adata.obs['sample'].unique())))\n",
    "sample_color_map = dict(zip(combined_adata.obs['sample'].unique(), sample_colors))\n",
    "\n",
    "for sample in combined_adata.obs['sample'].unique():\n",
    "    mask = combined_adata.obs['sample'] == sample\n",
    "    axes[0, 1].scatter(umap_coords[mask, 0], umap_coords[mask, 1],\n",
    "                      color=sample_color_map[sample], alpha=0.7, s=10, label=sample)\n",
    "\n",
    "axes[0, 1].set_title('UMAP - Samples')\n",
    "axes[0, 1].set_xlabel('UMAP1')\n",
    "axes[0, 1].set_ylabel('UMAP2')\n",
    "axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Plot 3: Cluster distribution\n",
    "cluster_counts = pd.Series(final_labels).value_counts().sort_index()\n",
    "axes[1, 0].bar(range(len(cluster_counts)), cluster_counts.values)\n",
    "axes[1, 0].set_title('Cluster Size Distribution')\n",
    "axes[1, 0].set_xlabel('Cluster')\n",
    "axes[1, 0].set_ylabel('Number of Cells')\n",
    "axes[1, 0].set_xticks(range(len(cluster_counts)))\n",
    "axes[1, 0].set_xticklabels([f'Cluster {i}' for i in cluster_counts.index])\n",
    "\n",
    "# Plot 4: Silhouette scores comparison\n",
    "algorithms = list(silhouette_scores.keys())\n",
    "scores = list(silhouette_scores.values())\n",
    "bars = axes[1, 1].bar(algorithms, scores, color=['red' if s == max(scores) else 'blue' for s in scores])\n",
    "axes[1, 1].set_title('Clustering Algorithm Comparison')\n",
    "axes[1, 1].set_ylabel('Silhouette Score')\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   '.3f', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(processed_data_dir, 'clustering_visualization.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Additional visualization: t-SNE\n",
    "print(\"Computing t-SNE for additional visualization...\")\n",
    "tsne_reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tsne_coords = tsne_reducer.fit_transform(X_combined_scaled[:5000])  # t-SNE on subsample for speed\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter_tsne = plt.scatter(tsne_coords[:, 0], tsne_coords[:, 1],\n",
    "                          c=final_labels[:5000], cmap='tab10', alpha=0.7, s=10)\n",
    "plt.title('t-SNE - Clusters ( subsample)')\n",
    "plt.xlabel('t-SNE1')\n",
    "plt.ylabel('t-SNE2')\n",
    "plt.colorbar(scatter_tsne, label='Cluster')\n",
    "plt.savefig(os.path.join(processed_data_dir, 'tsne_clusters.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b27afc",
   "metadata": {},
   "source": [
    "## 8. Supervised Learning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Supervised Learning Analysis ---\n",
    "\n",
    "print(\"Performing supervised learning analysis...\")\n",
    "\n",
    "# Prepare target variable (clusters as labels)\n",
    "y = final_labels\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='mlogloss'),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Define parameter grids for GridSearchCV\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.1, 0.2]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2']\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform k-fold cross-validation with GridSearchCV\n",
    "cv_results = {}\n",
    "best_models = {}\n",
    "\n",
    "print(\"Performing k-fold cross-validation with hyperparameter tuning...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "    \n",
    "    # Use StratifiedKFold for balanced class distribution\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grids[name], cv=cv, scoring='accuracy',\n",
    "        n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'cv_results': grid_search.cv_results_\n",
    "    }\n",
    "    \n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate best models on test set\n",
    "test_results = {}\n",
    "\n",
    "print(\"\\nEvaluating models on test set...\")\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    test_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} - Test Accuracy: {accuracy:.3f}, F1-Score: {f1:.3f}\")\n",
    "\n",
    "# Find best performing model\n",
    "best_model_name = max(test_results, key=lambda x: test_results[x]['f1_score'])\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "print(f\"Test F1-Score: {test_results[best_model_name]['f1_score']:.3f}\")\n",
    "\n",
    "# Save best model\n",
    "model_path = os.path.join(processed_data_dir, 'best_supervised_model.pkl')\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Best model saved to: {model_path}\")\n",
    "\n",
    "print(\"Supervised learning analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f4186",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c1689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Model Evaluation and Interpretation ---\n",
    "\n",
    "print(\"Evaluating and interpreting model results...\")\n",
    "\n",
    "# Create evaluation plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Model comparison\n",
    "model_names = list(test_results.keys())\n",
    "accuracies = [test_results[name]['accuracy'] for name in model_names]\n",
    "f1_scores = [test_results[name]['f1_score'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0, 0].bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "bars2 = axes[0, 0].bar(x + width/2, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "axes[0, 0].set_title('Model Performance Comparison')\n",
    "axes[0, 0].set_xlabel('Model')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(model_names, rotation=45)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       '.3f', ha='center', va='bottom')\n",
    "\n",
    "# Plot 2: Confusion matrix for best model\n",
    "best_pred = test_results[best_model_name]['predictions']\n",
    "cm = confusion_matrix(y_test, best_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
    "disp.plot(ax=axes[0, 1], cmap='Blues', values_format='d')\n",
    "axes[0, 1].set_title(f'Confusion Matrix - {best_model_name}')\n",
    "\n",
    "# Plot 3: Feature importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    # Get top 20 features\n",
    "    top_indices = np.argsort(feature_importance)[-20:]\n",
    "    top_features = feature_importance[top_indices]\n",
    "    feature_names = [f'Feature_{i}' for i in top_indices]\n",
    "    \n",
    "    axes[1, 0].barh(range(len(top_features)), top_features)\n",
    "    axes[1, 0].set_yticks(range(len(top_features)))\n",
    "    axes[1, 0].set_yticklabels(feature_names)\n",
    "    axes[1, 0].set_xlabel('Importance')\n",
    "    axes[1, 0].set_title('Top 20 Feature Importances')\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'Feature importance\\nnot available\\nfor this model',\n",
    "                   ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    axes[1, 0].set_title('Feature Importances')\n",
    "\n",
    "# Plot 4: ROC curves (if binary or adapted for multiclass)\n",
    "if len(np.unique(y)) == 2:\n",
    "    # Binary classification\n",
    "    y_proba = test_results[best_model_name]['probabilities'][:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    axes[1, 1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    axes[1, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    axes[1, 1].set_xlim([0.0, 1.0])\n",
    "    axes[1, 1].set_ylim([0.0, 1.05])\n",
    "    axes[1, 1].set_xlabel('False Positive Rate')\n",
    "    axes[1, 1].set_ylabel('True Positive Rate')\n",
    "    axes[1, 1].set_title('ROC Curve')\n",
    "    axes[1, 1].legend(loc=\"lower right\")\n",
    "else:\n",
    "    # Multiclass - plot precision-recall curves\n",
    "    y_test_bin = label_binarize(y_test, classes=np.unique(y))\n",
    "    n_classes = y_test_bin.shape[1]\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    if test_results[best_model_name]['probabilities'] is not None:\n",
    "        y_proba = test_results[best_model_name]['probabilities']\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_proba[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Plot micro-average ROC curve\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_proba.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        \n",
    "        axes[1, 1].plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                       label=f'micro-average ROC curve (AUC = {roc_auc[\"micro\"]:.2f})',\n",
    "                       color='deeppink', linestyle=':', linewidth=4)\n",
    "        \n",
    "        axes[1, 1].plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        axes[1, 1].set_xlim([0.0, 1.0])\n",
    "        axes[1, 1].set_ylim([0.0, 1.05])\n",
    "        axes[1, 1].set_xlabel('False Positive Rate')\n",
    "        axes[1, 1].set_ylabel('True Positive Rate')\n",
    "        axes[1, 1].set_title('Multiclass ROC Curves')\n",
    "        axes[1, 1].legend(loc=\"lower right\")\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'ROC curves require\\nprobability estimates',\n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('ROC Curves')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(processed_data_dir, 'model_evaluation.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"Detailed Classification Report for Best Model:\")\n",
    "print(classification_report(y_test, best_pred, target_names=[f'Cluster_{i}' for i in np.unique(y)]))\n",
    "\n",
    "# Save evaluation results\n",
    "evaluation_summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'test_accuracy': test_results[best_model_name]['accuracy'],\n",
    "    'test_f1_score': test_results[best_model_name]['f1_score'],\n",
    "    'cv_results': cv_results,\n",
    "    'test_results': {k: {kk: vv for kk, vv in v.items() if kk not in ['predictions', 'probabilities']}\n",
    "                    for k, v in test_results.items()}\n",
    "}\n",
    "\n",
    "with open(os.path.join(processed_data_dir, 'model_evaluation_summary.json'), 'w') as f:\n",
    "    json.dump(evaluation_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"Model evaluation and interpretation completed!\")\n",
    "print(f\"Results saved to: {os.path.join(processed_data_dir, 'model_evaluation_summary.json')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e251f7",
   "metadata": {},
   "source": [
    "## 10. Cluster Analysis and Biological Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b02d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Cluster Analysis and Biological Interpretation ---\n",
    "\n",
    "print(\"Performing cluster analysis and biological interpretation...\")\n",
    "\n",
    "# Identify marker genes for each cluster using Scanpy\n",
    "print(\"Identifying marker genes for each cluster...\")\n",
    "sc.tl.rank_genes_groups(combined_adata, 'cluster', method='wilcoxon', use_raw=False)\n",
    "\n",
    "# Get top marker genes for each cluster\n",
    "marker_genes = {}\n",
    "for cluster in combined_adata.obs['cluster'].unique():\n",
    "    cluster_markers = sc.get.rank_genes_groups_df(combined_adata, group=cluster)\n",
    "    top_markers = cluster_markers.head(20)  # Top 20 markers per cluster\n",
    "    marker_genes[cluster] = top_markers\n",
    "\n",
    "# Save marker genes\n",
    "marker_df = pd.concat([df.assign(cluster=cluster) for cluster, df in marker_genes.items()])\n",
    "marker_df.to_csv(os.path.join(processed_data_dir, 'cluster_marker_genes.csv'), index=False)\n",
    "\n",
    "print(\"Marker genes identified and saved!\")\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "cluster_stats = {}\n",
    "\n",
    "for cluster in np.unique(final_labels):\n",
    "    cluster_mask = final_labels == cluster\n",
    "    cluster_data = combined_adata[cluster_mask]\n",
    "    \n",
    "    # Basic statistics\n",
    "    n_cells = len(cluster_data)\n",
    "    sample_dist = cluster_data.obs['sample'].value_counts()\n",
    "    \n",
    "    # Expression statistics\n",
    "    mean_expr = np.mean(cluster_data.X.toarray(), axis=0)\n",
    "    var_expr = np.var(cluster_data.X.toarray(), axis=0)\n",
    "    \n",
    "    # Most variable genes in this cluster\n",
    "    top_var_indices = np.argsort(var_expr)[-10:]\n",
    "    top_var_genes = combined_adata.var.index[top_var_indices]\n",
    "    \n",
    "    cluster_stats[f'Cluster_{cluster}'] = {\n",
    "        'n_cells': n_cells,\n",
    "        'sample_distribution': sample_dist.to_dict(),\n",
    "        'top_variable_genes': top_var_genes.tolist(),\n",
    "        'mean_expression_top10': mean_expr[top_var_indices].tolist()\n",
    "    }\n",
    "\n",
    "# Save cluster statistics\n",
    "with open(os.path.join(processed_data_dir, 'cluster_statistics.json'), 'w') as f:\n",
    "    json.dump(cluster_stats, f, indent=2)\n",
    "\n",
    "print(\"Cluster statistics computed!\")\n",
    "\n",
    "# Biological interpretation\n",
    "print(\"Performing biological interpretation...\")\n",
    "\n",
    "# Literature comparison (simplified - in practice, you'd use databases like GO, KEGG, etc.)\n",
    "biological_interpretation = {}\n",
    "\n",
    "for cluster in np.unique(final_labels):\n",
    "    cluster_name = f'Cluster_{cluster}'\n",
    "    stats = cluster_stats[cluster_name]\n",
    "    \n",
    "    # Simple interpretation based on marker genes and characteristics\n",
    "    interpretation = {\n",
    "        'size': 'Large' if stats['n_cells'] > np.mean([s['n_cells'] for s in cluster_stats.values()]) else 'Small',\n",
    "        'sample_preference': max(stats['sample_distribution'], key=stats['sample_distribution'].get),\n",
    "        'potential_biological_role': 'To be determined based on marker gene analysis',\n",
    "        'key_characteristics': f\"Contains {stats['n_cells']} cells, prefers {max(stats['sample_distribution'], key=stats['sample_distribution'].get)}\"\n",
    "    }\n",
    "    \n",
    "    biological_interpretation[cluster_name] = interpretation\n",
    "\n",
    "# Save biological interpretation\n",
    "with open(os.path.join(processed_data_dir, 'biological_interpretation.json'), 'w') as f:\n",
    "    json.dump(biological_interpretation, f, indent=2)\n",
    "\n",
    "# Create summary visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Cluster sizes\n",
    "cluster_sizes = [stats['n_cells'] for stats in cluster_stats.values()]\n",
    "axes[0, 0].bar(range(len(cluster_sizes)), cluster_sizes)\n",
    "axes[0, 0].set_title('Cluster Sizes')\n",
    "axes[0, 0].set_xlabel('Cluster')\n",
    "axes[0, 0].set_ylabel('Number of Cells')\n",
    "axes[0, 0].set_xticks(range(len(cluster_sizes)))\n",
    "axes[0, 0].set_xticklabels([f'Cluster {i}' for i in range(len(cluster_sizes))])\n",
    "\n",
    "# Plot 2: Sample distribution across clusters\n",
    "sample_cluster_dist = pd.DataFrame({\n",
    "    cluster: pd.Series(stats['sample_distribution'])\n",
    "    for cluster, stats in cluster_stats.items()\n",
    "}).fillna(0)\n",
    "\n",
    "sample_cluster_dist.plot(kind='bar', ax=axes[0, 1], stacked=True)\n",
    "axes[0, 1].set_title('Sample Distribution Across Clusters')\n",
    "axes[0, 1].set_xlabel('Sample')\n",
    "axes[0, 1].set_ylabel('Number of Cells')\n",
    "axes[0, 1].legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Plot 3: Top marker genes heatmap (simplified)\n",
    "if len(marker_genes) > 0:\n",
    "    # Get top 5 genes per cluster\n",
    "    top5_per_cluster = {}\n",
    "    for cluster, df in marker_genes.items():\n",
    "        top5_per_cluster[cluster] = df.head(5)['names'].tolist()\n",
    "    \n",
    "    # Find unique genes\n",
    "    all_top_genes = list(set([gene for genes in top5_per_cluster.values() for gene in genes]))\n",
    "    \n",
    "    # Create expression matrix for top genes\n",
    "    gene_expr_matrix = combined_adata[:, all_top_genes].X.toarray()\n",
    "    \n",
    "    # Create cluster-averaged expression\n",
    "    cluster_avg_expr = []\n",
    "    for cluster in np.unique(final_labels):\n",
    "        cluster_mask = final_labels == cluster\n",
    "        cluster_expr = np.mean(gene_expr_matrix[cluster_mask], axis=0)\n",
    "        cluster_avg_expr.append(cluster_expr)\n",
    "    \n",
    "    cluster_avg_expr = np.array(cluster_avg_expr)\n",
    "    \n",
    "    sns.heatmap(cluster_avg_expr, xticklabels=all_top_genes, yticklabels=[f'Cluster {i}' for i in np.unique(final_labels)],\n",
    "                ax=axes[1, 0], cmap='viridis')\n",
    "    axes[1, 0].set_title('Top Marker Genes Expression by Cluster')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No marker genes\\navailable', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "\n",
    "# Plot 4: Biological interpretation summary\n",
    "interpretation_text = \"\\n\".join([\n",
    "    f\"{cluster}: {interp['key_characteristics']}\"\n",
    "    for cluster, interp in biological_interpretation.items()\n",
    "])\n",
    "\n",
    "axes[1, 1].text(0.1, 0.9, \"Biological Interpretation Summary:\", fontsize=12, fontweight='bold',\n",
    "               transform=axes[1, 1].transAxes)\n",
    "axes[1, 1].text(0.1, 0.8, interpretation_text, fontsize=10, verticalalignment='top',\n",
    "               transform=axes[1, 1].transAxes)\n",
    "axes[1, 1].set_title('Cluster Biological Interpretation')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(processed_data_dir, 'cluster_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create comprehensive analysis summary\n",
    "analysis_summary = {\n",
    "    'clustering': {\n",
    "        'algorithm': best_algorithm,\n",
    "        'n_clusters': len(np.unique(final_labels)),\n",
    "        'silhouette_score': silhouette_scores[best_algorithm]\n",
    "    },\n",
    "    'supervised_learning': {\n",
    "        'best_model': best_model_name,\n",
    "        'test_accuracy': test_results[best_model_name]['accuracy'],\n",
    "        'test_f1_score': test_results[best_model_name]['f1_score']\n",
    "    },\n",
    "    'biological_insights': biological_interpretation,\n",
    "    'marker_genes': {k: v.head(5).to_dict('records') for k, v in marker_genes.items()},\n",
    "    'cluster_statistics': cluster_stats\n",
    "}\n",
    "\n",
    "with open(os.path.join(processed_data_dir, 'analysis_summary.json'), 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"Cluster analysis and biological interpretation completed!\")\n",
    "print(f\"Comprehensive analysis summary saved to: {os.path.join(processed_data_dir, 'analysis_summary.json')}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: {len(combined_adata)} cells from {len(combined_adata.obs['sample'].unique())} samples\")\n",
    "print(f\"Clusters identified: {len(np.unique(final_labels))} using {best_algorithm}\")\n",
    "print(f\"Best supervised model: {best_model_name} (F1: {test_results[best_model_name]['f1_score']:.3f})\")\n",
    "print(f\"Results saved in: {processed_data_dir}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec755b3",
   "metadata": {},
   "source": [
    "## 11. Conclusion and Future Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd0b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conclusion and Future Directions ---\n",
    "\n",
    "print(\"Analysis completed successfully!\")\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"â€¢ Identified {len(np.unique(final_labels))} distinct cell clusters using {best_algorithm}\")\n",
    "print(f\"â€¢ Best supervised classification model: {best_model_name} (F1-score: {test_results[best_model_name]['f1_score']:.3f})\")\n",
    "print(f\"â€¢ Combined gene expression and TCR sequence features for comprehensive analysis\")\n",
    "print(\"â€¢ Generated marker genes and biological interpretations for each cluster\")\n",
    "\n",
    "print(\"\\nFiles Generated:\")\n",
    "print(f\"â€¢ Processed data: {processed_data_dir}\")\n",
    "print(\"  - analysis_summary.json: Complete analysis results\")\n",
    "print(\"  - cluster_marker_genes.csv: Marker genes for each cluster\")\n",
    "print(\"  - cluster_statistics.json: Detailed cluster statistics\")\n",
    "print(\"  - biological_interpretation.json: Biological insights\")\n",
    "print(\"  - best_supervised_model.pkl: Trained classification model\")\n",
    "print(\"  - Various visualization plots (PNG files)\")\n",
    "\n",
    "print(\"\\nFuture Directions:\")\n",
    "print(\"1. Validate findings with additional biological experiments\")\n",
    "print(\"2. Integrate pathway analysis (GO, KEGG) for deeper biological insights\")\n",
    "print(\"3. Apply deep learning approaches for improved classification\")\n",
    "print(\"4. Extend analysis to larger datasets or additional cancer types\")\n",
    "print(\"5. Develop predictive models for treatment response based on cluster membership\")\n",
    "\n",
    "print(\"\\nNotebook completed! All results saved in the Processed_Data directory.\")\n",
    "print(\"The reordered notebook provides a logical, end-to-end analysis workflow.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
