{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be93f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scanpy pandas numpy\n",
    "# Import required libraries for single-cell RNA-seq analysis and data handling\n",
    "import scanpy as sc  # Main library for single-cell analysis, provides AnnData structure and many tools\n",
    "import pandas as pd  # For tabular data manipulation and metadata handling\n",
    "import numpy as np   # For numerical operations and array handling\n",
    "import os            # For operating system interactions (file paths, etc.)\n",
    "from pathlib import Path  # For robust and readable file path management\n",
    "\n",
    "# Print versions to ensure reproducibility and compatibility\n",
    "print(f\"Scanpy version: {sc.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c54f45",
   "metadata": {},
   "source": [
    "## 1. Load Sample Metadata\n",
    "\n",
    "First, we load the metadata from the `GSE300475_feature_ref.xlsx` file. This file contains the crucial mapping between GEO sample IDs, patient IDs, timepoints, and treatment response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16dfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata table now matches the requested specification:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_Number</th>\n",
       "      <th>GEX_Sample_ID</th>\n",
       "      <th>TCR_Sample_ID</th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Response</th>\n",
       "      <th>In_Data</th>\n",
       "      <th>In_Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>GSM9061687</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2</td>\n",
       "      <td>GSM9061666</td>\n",
       "      <td>GSM9061688</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S3</td>\n",
       "      <td>GSM9061667</td>\n",
       "      <td>GSM9061689</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S4</td>\n",
       "      <td>GSM9061668</td>\n",
       "      <td>GSM9061690</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S5</td>\n",
       "      <td>GSM9061669</td>\n",
       "      <td>GSM9061691</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S6</td>\n",
       "      <td>GSM9061670</td>\n",
       "      <td>GSM9061692</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S7</td>\n",
       "      <td>GSM9061671</td>\n",
       "      <td>GSM9061693</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S8</td>\n",
       "      <td>GSM9061672</td>\n",
       "      <td>None</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>GEX only</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S9</td>\n",
       "      <td>GSM9061673</td>\n",
       "      <td>GSM9061694</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S10</td>\n",
       "      <td>GSM9061674</td>\n",
       "      <td>GSM9061695</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Post-ICI</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S11</td>\n",
       "      <td>GSM9061675</td>\n",
       "      <td>GSM9061696</td>\n",
       "      <td>PT11</td>\n",
       "      <td>Endpoint</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S_Number GEX_Sample_ID TCR_Sample_ID Patient_ID   Timepoint       Response  \\\n",
       "0        S1    GSM9061665    GSM9061687        PT1    Baseline      Responder   \n",
       "1        S2    GSM9061666    GSM9061688        PT1  Post-Chemo      Responder   \n",
       "2        S3    GSM9061667    GSM9061689        PT2    Baseline  Non-Responder   \n",
       "3        S4    GSM9061668    GSM9061690        PT2  Post-Chemo  Non-Responder   \n",
       "4        S5    GSM9061669    GSM9061691        PT3    Baseline      Responder   \n",
       "5        S6    GSM9061670    GSM9061692        PT3  Post-Chemo      Responder   \n",
       "6        S7    GSM9061671    GSM9061693        PT4    Baseline  Non-Responder   \n",
       "7        S8    GSM9061672          None        PT5     Unknown        Unknown   \n",
       "8        S9    GSM9061673    GSM9061694        PT5    Baseline      Responder   \n",
       "9       S10    GSM9061674    GSM9061695        PT5    Post-ICI      Responder   \n",
       "10      S11    GSM9061675    GSM9061696       PT11    Endpoint      Responder   \n",
       "\n",
       "     In_Data In_Article  \n",
       "0        Yes        Yes  \n",
       "1        Yes        Yes  \n",
       "2        Yes        Yes  \n",
       "3        Yes        Yes  \n",
       "4        Yes        Yes  \n",
       "5        Yes        Yes  \n",
       "6        Yes        Yes  \n",
       "7   GEX only        Yes  \n",
       "8        Yes        Yes  \n",
       "9        Yes        Yes  \n",
       "10       Yes        Yes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-check In_Data column (based on files found in Data/GSE300475_RAW):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_Number</th>\n",
       "      <th>GEX_Sample_ID</th>\n",
       "      <th>TCR_Sample_ID</th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Response</th>\n",
       "      <th>In_Data</th>\n",
       "      <th>In_Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>GSM9061665</td>\n",
       "      <td>GSM9061687</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2</td>\n",
       "      <td>GSM9061666</td>\n",
       "      <td>GSM9061688</td>\n",
       "      <td>PT1</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S3</td>\n",
       "      <td>GSM9061667</td>\n",
       "      <td>GSM9061689</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S4</td>\n",
       "      <td>GSM9061668</td>\n",
       "      <td>GSM9061690</td>\n",
       "      <td>PT2</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S5</td>\n",
       "      <td>GSM9061669</td>\n",
       "      <td>GSM9061691</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S6</td>\n",
       "      <td>GSM9061670</td>\n",
       "      <td>GSM9061692</td>\n",
       "      <td>PT3</td>\n",
       "      <td>Post-Chemo</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S7</td>\n",
       "      <td>GSM9061671</td>\n",
       "      <td>GSM9061693</td>\n",
       "      <td>PT4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Non-Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S8</td>\n",
       "      <td>GSM9061672</td>\n",
       "      <td>None</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>GEX only</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S9</td>\n",
       "      <td>GSM9061673</td>\n",
       "      <td>GSM9061694</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S10</td>\n",
       "      <td>GSM9061674</td>\n",
       "      <td>GSM9061695</td>\n",
       "      <td>PT5</td>\n",
       "      <td>Post-ICI</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S11</td>\n",
       "      <td>GSM9061675</td>\n",
       "      <td>GSM9061696</td>\n",
       "      <td>PT11</td>\n",
       "      <td>Endpoint</td>\n",
       "      <td>Responder</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S_Number GEX_Sample_ID TCR_Sample_ID Patient_ID   Timepoint       Response  \\\n",
       "0        S1    GSM9061665    GSM9061687        PT1    Baseline      Responder   \n",
       "1        S2    GSM9061666    GSM9061688        PT1  Post-Chemo      Responder   \n",
       "2        S3    GSM9061667    GSM9061689        PT2    Baseline  Non-Responder   \n",
       "3        S4    GSM9061668    GSM9061690        PT2  Post-Chemo  Non-Responder   \n",
       "4        S5    GSM9061669    GSM9061691        PT3    Baseline      Responder   \n",
       "5        S6    GSM9061670    GSM9061692        PT3  Post-Chemo      Responder   \n",
       "6        S7    GSM9061671    GSM9061693        PT4    Baseline  Non-Responder   \n",
       "7        S8    GSM9061672          None        PT5     Unknown        Unknown   \n",
       "8        S9    GSM9061673    GSM9061694        PT5    Baseline      Responder   \n",
       "9       S10    GSM9061674    GSM9061695        PT5    Post-ICI      Responder   \n",
       "10      S11    GSM9061675    GSM9061696       PT11    Endpoint      Responder   \n",
       "\n",
       "     In_Data In_Article  \n",
       "0        Yes        Yes  \n",
       "1        Yes        Yes  \n",
       "2        Yes        Yes  \n",
       "3        Yes        Yes  \n",
       "4        Yes        Yes  \n",
       "5        Yes        Yes  \n",
       "6        Yes        Yes  \n",
       "7   GEX only        Yes  \n",
       "8        Yes        Yes  \n",
       "9        Yes        Yes  \n",
       "10       Yes        Yes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 53.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Setup data paths ---\n",
    "# Define the main data directory and the subdirectory containing raw files.\n",
    "data_dir = Path('../Data')\n",
    "raw_data_dir = data_dir / 'GSE300475_RAW'\n",
    "\n",
    "# --- Manually create the metadata mapping ---\n",
    "# This list contains information about each sample, including GEO IDs, patient IDs, timepoints, and response status.\n",
    "# Note: S8 (GSM9061672) has GEX files but no corresponding TCR file.\n",
    "metadata_list = [\n",
    "    # Patient 1 (Responder)\n",
    "    {'S_Number': 'S1',  'GEX_Sample_ID': 'GSM9061665', 'TCR_Sample_ID': 'GSM9061687', 'Patient_ID': 'PT1',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S2',  'GEX_Sample_ID': 'GSM9061666', 'TCR_Sample_ID': 'GSM9061688', 'Patient_ID': 'PT1',  'Timepoint': 'Post-Chemo',  'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 2 (Non-Responder)\n",
    "    {'S_Number': 'S3',  'GEX_Sample_ID': 'GSM9061667', 'TCR_Sample_ID': 'GSM9061689', 'Patient_ID': 'PT2',  'Timepoint': 'Baseline',     'Response': 'Non-Responder', 'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S4',  'GEX_Sample_ID': 'GSM9061668', 'TCR_Sample_ID': 'GSM9061690', 'Patient_ID': 'PT2',  'Timepoint': 'Post-Chemo',  'Response': 'Non-Responder', 'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 3 (Responder)\n",
    "    {'S_Number': 'S5',  'GEX_Sample_ID': 'GSM9061669', 'TCR_Sample_ID': 'GSM9061691', 'Patient_ID': 'PT3',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S6',  'GEX_Sample_ID': 'GSM9061670', 'TCR_Sample_ID': 'GSM9061692', 'Patient_ID': 'PT3',  'Timepoint': 'Post-Chemo',  'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 4 (Non-Responder)\n",
    "    {'S_Number': 'S7',  'GEX_Sample_ID': 'GSM9061671', 'TCR_Sample_ID': 'GSM9061693', 'Patient_ID': 'PT4',  'Timepoint': 'Baseline',     'Response': 'Non-Responder', 'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 5 (partial) - S8 exists as GEX only in the raw data but has no TCR file\n",
    "    {'S_Number': 'S8',  'GEX_Sample_ID': 'GSM9061672', 'TCR_Sample_ID': None,             'Patient_ID': 'PT5',  'Timepoint': 'Unknown',      'Response': 'Unknown',       'In_Data': 'GEX only', 'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S9',  'GEX_Sample_ID': 'GSM9061673', 'TCR_Sample_ID': 'GSM9061694', 'Patient_ID': 'PT5',  'Timepoint': 'Baseline',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    {'S_Number': 'S10', 'GEX_Sample_ID': 'GSM9061674', 'TCR_Sample_ID': 'GSM9061695', 'Patient_ID': 'PT5',  'Timepoint': 'Post-ICI',     'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "    # Patient 11 (Responder)\n",
    "    {'S_Number': 'S11', 'GEX_Sample_ID': 'GSM9061675', 'TCR_Sample_ID': 'GSM9061696', 'Patient_ID': 'PT11', 'Timepoint': 'Endpoint',      'Response': 'Responder',     'In_Data': 'Yes',      'In_Article': 'Yes'},\n",
    "]\n",
    "\n",
    "# --- Create DataFrame and display the verification table ---\n",
    "metadata_df = pd.DataFrame(metadata_list)\n",
    "print(\"Metadata table now matches the requested specification:\")\n",
    "display(metadata_df)\n",
    "\n",
    "# --- Programmatic sanity-check for file presence ---\n",
    "# This loop checks if the expected files exist for each sample and updates the 'In_Data' column accordingly.\n",
    "for idx, row in metadata_df.iterrows():\n",
    "    s = row['S_Number']\n",
    "    g = row['GEX_Sample_ID']\n",
    "    t = row['TCR_Sample_ID']\n",
    "    # Check for gene expression matrix file (compressed or uncompressed)\n",
    "    g_exists = (raw_data_dir / f\"{g}_{s}_matrix.mtx.gz\").exists() or (raw_data_dir / f\"{g}_{s}_matrix.mtx\").exists()\n",
    "    t_exists = False\n",
    "    # Check for TCR annotation file if TCR sample ID is present\n",
    "    if pd.notna(t) and t is not None:\n",
    "        t_exists = (raw_data_dir / f\"{t}_{s}_all_contig_annotations.csv.gz\").exists() or (raw_data_dir / f\"{t}_{s}_all_contig_annotations.csv\").exists()\n",
    "    # Update 'In_Data' column based on file presence\n",
    "    if g_exists and t_exists:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'Yes'\n",
    "    elif g_exists and not t_exists:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'GEX only'\n",
    "    else:\n",
    "        metadata_df.at[idx, 'In_Data'] = 'No'\n",
    "\n",
    "print(\"\\nPost-check In_Data column (based on files found in Data/GSE300475_RAW):\")\n",
    "display(metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe565f3e",
   "metadata": {},
   "source": [
    "## 2. Process and Concatenate AnnData Objects\n",
    "\n",
    "Now, we will iterate through each sample defined in our metadata. For each sample, we will:\n",
    "1.  Locate the corresponding raw data directory.\n",
    "2.  Load the gene expression matrix directly from the compressed files into an `AnnData` object using `sc.read_10x_mtx()`.\n",
    "3.  Add the sample's metadata to the `.obs` attribute of the `AnnData` object.\n",
    "4.  Collect all the individual `AnnData` objects in a list.\n",
    "\n",
    "Finally, we'll concatenate them into one large `AnnData` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Initialize lists to hold AnnData and TCR data for each sample ---\n",
    "adata_list = []  # Will store AnnData objects for each sample\n",
    "tcr_data_list = []  # Will store TCR dataframes for each sample\n",
    "\n",
    "# --- Iterate through each sample in the metadata table ---\n",
    "for index, row in metadata_df.iterrows():\n",
    "    gex_sample_id = row['GEX_Sample_ID']\n",
    "    tcr_sample_id = row['TCR_Sample_ID']\n",
    "    s_number = row['S_Number']\n",
    "    patient_id = row['Patient_ID']\n",
    "    timepoint = row['Timepoint']\n",
    "    response = row['Response']\n",
    "    \n",
    "    # Construct the file prefix for this sample (used for locating files)\n",
    "    sample_prefix = f\"{gex_sample_id}_{s_number}\"\n",
    "    sample_data_path = raw_data_dir\n",
    "    \n",
    "    # --- Check for gene expression matrix file ---\n",
    "    matrix_file = sample_data_path / f\"{sample_prefix}_matrix.mtx.gz\"\n",
    "    if not matrix_file.exists():\n",
    "        # Try uncompressed version if gzipped file not found\n",
    "        matrix_file_un = sample_data_path / f\"{sample_prefix}_matrix.mtx\"\n",
    "        if not matrix_file_un.exists():\n",
    "            print(f\"GEX data not found for sample {sample_prefix}, skipping.\")\n",
    "            continue\n",
    "        else:\n",
    "            matrix_file = matrix_file_un\n",
    "            \n",
    "    print(f\"Processing GEX sample: {sample_prefix}\")\n",
    "    \n",
    "    # --- Load gene expression data into AnnData object ---\n",
    "    # The prefix ensures only files for this sample are loaded\n",
    "    adata_sample = sc.read_10x_mtx(\n",
    "        sample_data_path, \n",
    "        var_names='gene_symbols',\n",
    "        prefix=f\"{sample_prefix}_\"\n",
    "    )\n",
    "    \n",
    "    # --- Add sample metadata to AnnData.obs ---\n",
    "    adata_sample.obs['sample_id'] = gex_sample_id \n",
    "    adata_sample.obs['patient_id'] = patient_id\n",
    "    adata_sample.obs['timepoint'] = timepoint\n",
    "    adata_sample.obs['response'] = response\n",
    "    \n",
    "    adata_list.append(adata_sample)\n",
    "    \n",
    "    # --- Load TCR data if available ---\n",
    "    if pd.isna(tcr_sample_id) or tcr_sample_id is None:\n",
    "        print(f\"No TCR sample for {gex_sample_id}_{s_number}, skipping TCR load.\")\n",
    "        continue\n",
    "\n",
    "    # Construct path for TCR annotation file (gzipped or uncompressed)\n",
    "    tcr_file_path = raw_data_dir / f\"{tcr_sample_id}_{s_number}_all_contig_annotations.csv.gz\"\n",
    "\n",
    "    if tcr_file_path.exists():\n",
    "        print(f\"Found and loading TCR data: {tcr_file_path.name}\")\n",
    "        tcr_df = pd.read_csv(tcr_file_path)\n",
    "        # Add sample_id for merging later\n",
    "        tcr_df['sample_id'] = gex_sample_id \n",
    "        tcr_data_list.append(tcr_df)\n",
    "    else:\n",
    "        # Try uncompressed version if gzipped file not found\n",
    "        tcr_file_path_uncompressed = raw_data_dir / f\"{tcr_sample_id}_{s_number}_all_contig_annotations.csv\"\n",
    "        if tcr_file_path_uncompressed.exists():\n",
    "            print(f\"Found and loading TCR data: {tcr_file_path_uncompressed.name}\")\n",
    "            tcr_df = pd.read_csv(tcr_file_path_uncompressed)\n",
    "            tcr_df['sample_id'] = gex_sample_id\n",
    "            tcr_data_list.append(tcr_df)\n",
    "        else:\n",
    "            print(f\"TCR data not found for {tcr_sample_id}_{s_number}\")\n",
    "\n",
    "# --- Concatenate all loaded AnnData objects into one ---\n",
    "if adata_list:\n",
    "    # Use sample_id as batch key for concatenation\n",
    "    loaded_batches = [a.obs['sample_id'].unique()[0] for a in adata_list]\n",
    "    adata = sc.AnnData.concatenate(*adata_list, join='outer', batch_key='sample_id', batch_categories=loaded_batches)\n",
    "    print(\"\\nConcatenated AnnData object:\")\n",
    "    print(adata)\n",
    "else:\n",
    "    print(\"No data was loaded.\")\n",
    "\n",
    "# --- Concatenate all loaded TCR dataframes into one ---\n",
    "if tcr_data_list:\n",
    "    full_tcr_df = pd.concat(tcr_data_list, ignore_index=True)\n",
    "    print(\"\\nFull TCR data:\")\n",
    "    display(full_tcr_df.head())\n",
    "else:\n",
    "    print(\"No TCR data was loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18451740",
   "metadata": {},
   "source": [
    "## 3. Integrate TCR Data and Perform QC\n",
    "\n",
    "Next, we'll merge the TCR information into the `.obs` of our main `AnnData` object. We will keep only the cells that have corresponding TCR data and filter based on the `high_confidence` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Integrate TCR data into AnnData.obs and perform quality control ---\n",
    "if 'full_tcr_df' in locals() and not full_tcr_df.empty:\n",
    "    # --- FIX START ---\n",
    "    # The previous join failed because one cell (barcode) can have multiple TCR contigs (e.g., TRA and TRB chains),\n",
    "    # creating a one-to-many join that increases the number of rows.\n",
    "    # The fix is to aggregate the TCR data to one row per cell *before* merging.\n",
    "\n",
    "    # 1. Filter for high-confidence, productive TRA/TRB chains.\n",
    "    # Only keep TCR contigs that are both high-confidence and productive, and are either TRA or TRB chains.\n",
    "    tcr_to_agg = full_tcr_df[\n",
    "        (full_tcr_df['high_confidence'] == True) &\n",
    "        (full_tcr_df['productive'] == True) &\n",
    "        (full_tcr_df['chain'].isin(['TRA', 'TRB']))\n",
    "    ].copy()\n",
    "\n",
    "    # 2. Pivot the data to create one row per barcode, with columns for TRA and TRB data.\n",
    "    # This step ensures each cell (barcode) has its TRA and TRB info in separate columns.\n",
    "    tcr_aggregated = tcr_to_agg.pivot_table(\n",
    "        index=['sample_id', 'barcode'],\n",
    "        columns='chain',\n",
    "        values=['v_gene', 'j_gene', 'cdr3'],\n",
    "        aggfunc='first'  # 'first' is safe as we expect at most one productive TRA/TRB per cell\n",
    "    )\n",
    "\n",
    "    # 3. Flatten the multi-level column index (e.g., from ('v_gene', 'TRA') to 'v_gene_TRA')\n",
    "    tcr_aggregated.columns = ['_'.join(col).strip() for col in tcr_aggregated.columns.values]\n",
    "    tcr_aggregated.reset_index(inplace=True)\n",
    "\n",
    "    # 4. Prepare adata.obs for the merge by creating a matching barcode column.\n",
    "    # The index in adata.obs is like 'AGCCATGCAGCTGTTA-1-0' (barcode-batch_id).\n",
    "    # The barcode in TCR data is like 'AGCCATGCAGCTGTTA-1'.\n",
    "    adata.obs['barcode_for_merge'] = adata.obs.index.str.rsplit('-', n=1).str[0]\n",
    "\n",
    "    # 5. Perform a left merge. This keeps all cells from adata and adds TCR info where available.\n",
    "    # The number of rows will not change because tcr_aggregated has unique barcodes.\n",
    "    original_obs = adata.obs.copy()\n",
    "    merged_obs = original_obs.merge(\n",
    "        tcr_aggregated,\n",
    "        left_on=['sample_id', 'barcode_for_merge'],\n",
    "        right_on=['sample_id', 'barcode'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 6. Restore the original index to the merged dataframe.\n",
    "    merged_obs.index = original_obs.index\n",
    "    adata.obs = merged_obs\n",
    "    # --- FIX END ---\n",
    "\n",
    "    print(\"Aggregated TCR data merged into AnnData object.\")\n",
    "    \n",
    "    # --- Filter for cells that have TCR information after the merge ---\n",
    "    # Only keep cells with non-null v_gene_TRA (i.e., cells with high-confidence TCR data)\n",
    "    initial_cells = adata.n_obs\n",
    "    adata = adata[~adata.obs['v_gene_TRA'].isna()].copy()\n",
    "    print(f\"Filtered from {initial_cells} to {adata.n_obs} cells based on having high-confidence TCR data.\")\n",
    "\n",
    "# --- Basic QC and filtering ---\n",
    "# Filter out cells with fewer than 200 genes detected\n",
    "sc.pp.filter_cells(adata, min_genes=200)\n",
    "# Filter out genes detected in fewer than 3 cells\n",
    "sc.pp.filter_genes(adata, min_cells=3)\n",
    "\n",
    "# Annotate mitochondrial genes for QC metrics\n",
    "adata.var['mt'] = adata.var_names.str.startswith('MT-')\n",
    "# Calculate QC metrics (e.g., percent mitochondrial genes)\n",
    "sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "\n",
    "print(\"\\nPost-QC AnnData object:\")\n",
    "print(adata)\n",
    "display(adata.obs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687edbaf",
   "metadata": {},
   "source": [
    "## 4. Save Processed Data\n",
    "\n",
    "Finally, we save the fully processed, annotated, and filtered `AnnData` object to a `.h5ad` file. This file can be easily loaded in future notebooks for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Save processed AnnData object to disk ---\n",
    "# Define output directory for processed data\n",
    "output_dir = Path('../Processed_Data')\n",
    "output_dir.mkdir(exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Define output file path for the .h5ad file\n",
    "output_path = output_dir / 'processed_s_rna_seq_data.h5ad'\n",
    "# Save the AnnData object (contains all processed, filtered, and annotated data)\n",
    "adata.write_h5ad(output_path)\n",
    "\n",
    "print(f\"Processed data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b1d9e",
   "metadata": {},
   "source": [
    "## 5. Install Additional Libraries for Advanced ML and Visualization\n",
    "\n",
    "Install and import libraries such as XGBoost, TensorFlow/Keras, scipy, and additional visualization tools for comprehensive ML analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Install required packages for genetic sequence encoding and ML ---\n",
    "%pip install biopython\n",
    "%pip install scikit-learn\n",
    "%pip install umap-learn\n",
    "%pip install hdbscan\n",
    "%pip install plotly\n",
    "%pip install xgboost\n",
    "%pip install tensorflow\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import ProtParam\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Import scipy for hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import umap\n",
    "import hdbscan\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"Additional libraries installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3b728",
   "metadata": {},
   "source": [
    "## 6. Genetic Sequence Encoding Functions\n",
    "\n",
    "Define functions for one-hot encoding, k-mer encoding, and physicochemical features extraction for TCR sequences and gene expression patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c855cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Genetic Sequence Encoding Functions ---\n",
    "\n",
    "def one_hot_encode_sequence(sequence, max_length=50, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    One-hot encode a protein/nucleotide sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        max_length: Maximum sequence length (pad or truncate)\n",
    "        alphabet: Valid characters in the sequence\n",
    "    \n",
    "    Returns:\n",
    "        2D numpy array of shape (max_length, len(alphabet))\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    sequence = str(sequence).upper()[:max_length]  # Truncate if too long\n",
    "    encoding = np.zeros((max_length, len(alphabet)))\n",
    "    \n",
    "    for i, char in enumerate(sequence):\n",
    "        if char in alphabet:\n",
    "            char_idx = alphabet.index(char)\n",
    "            encoding[i, char_idx] = 1\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "def kmer_encode_sequence(sequence, k=3, alphabet='ACDEFGHIKLMNPQRSTVWY'):\n",
    "    \"\"\"\n",
    "    K-mer encoding of sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequence: String sequence to encode\n",
    "        k: Length of k-mers\n",
    "        alphabet: Valid characters\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with k-mer counts\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {}\n",
    "    \n",
    "    sequence = str(sequence).upper()\n",
    "    kmers = [sequence[i:i+k] for i in range(len(sequence)-k+1)]\n",
    "    valid_kmers = [kmer for kmer in kmers if all(c in alphabet for c in kmer)]\n",
    "    \n",
    "    return Counter(valid_kmers)\n",
    "\n",
    "def physicochemical_features(sequence):\n",
    "    \"\"\"\n",
    "    Extract physicochemical properties from protein sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Protein sequence string\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of features\n",
    "    \"\"\"\n",
    "    if pd.isna(sequence) or sequence == 'NA' or sequence == '':\n",
    "        return {\n",
    "            'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        seq = str(sequence).upper()\n",
    "        # Remove non-standard amino acids\n",
    "        seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "        \n",
    "        if len(seq) == 0:\n",
    "            return {\n",
    "                'length': 0, 'molecular_weight': 0, 'aromaticity': 0,\n",
    "                'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "            }\n",
    "        \n",
    "        bio_seq = Seq(seq)\n",
    "        analyzer = ProtParam.ProteinAnalysis(str(bio_seq))\n",
    "        \n",
    "        return {\n",
    "            'length': len(seq),\n",
    "            'molecular_weight': analyzer.molecular_weight(),\n",
    "            'aromaticity': analyzer.aromaticity(),\n",
    "            'instability_index': analyzer.instability_index(),\n",
    "            'isoelectric_point': analyzer.isoelectric_point(),\n",
    "            'hydrophobicity': analyzer.gravy()  # Grand Average of Hydropathy\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'length': len(str(sequence)) if not pd.isna(sequence) else 0,\n",
    "            'molecular_weight': 0, 'aromaticity': 0,\n",
    "            'instability_index': 0, 'isoelectric_point': 0, 'hydrophobicity': 0\n",
    "        }\n",
    "\n",
    "def encode_gene_expression_patterns(adata, n_top_genes=1000):\n",
    "    \"\"\"\n",
    "    Encode gene expression patterns using various dimensionality reduction techniques.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object with gene expression data\n",
    "        n_top_genes: Number of highly variable genes to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of encoded representations\n",
    "    \"\"\"\n",
    "    # Get highly variable genes if not already computed\n",
    "    if 'highly_variable' not in adata.var.columns:\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, subset=False)\n",
    "    \n",
    "    # Extract expression matrix for highly variable genes\n",
    "    hvg_mask = adata.var['highly_variable'] if 'highly_variable' in adata.var.columns else adata.var.index[:n_top_genes]\n",
    "    X_hvg = adata[:, hvg_mask].X.toarray() if hasattr(adata.X, 'toarray') else adata[:, hvg_mask].X\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_hvg)\n",
    "    \n",
    "    encodings = {}\n",
    "    \n",
    "    # PCA encoding\n",
    "    pca = PCA(n_components=50)\n",
    "    encodings['pca'] = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # TruncatedSVD for sparse matrices\n",
    "    svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "    encodings['svd'] = svd.fit_transform(X_scaled)\n",
    "    \n",
    "    # UMAP encoding\n",
    "    umap_encoder = umap.UMAP(n_components=20, random_state=42)\n",
    "    encodings['umap'] = umap_encoder.fit_transform(X_scaled)\n",
    "    \n",
    "    return encodings, X_scaled\n",
    "\n",
    "print(\"Genetic sequence encoding functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff20cc1",
   "metadata": {},
   "source": [
    "## 7. Apply Sequence Encoding to TCR CDR3 Sequences\n",
    "\n",
    "Encode TRA and TRB CDR3 sequences using one-hot, k-mer, and physicochemical methods, and add to AnnData.obsm and obs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Apply Sequence Encoding to TCR CDR3 Sequences ---\n",
    "\n",
    "print(\"Encoding TCR CDR3 sequences...\")\n",
    "\n",
    "# Extract CDR3 sequences\n",
    "# Convert to string type first to handle categorical data\n",
    "cdr3_sequences = {\n",
    "    'TRA': adata.obs['cdr3_TRA'].astype(str).fillna(''),\n",
    "    'TRB': adata.obs['cdr3_TRB'].astype(str).fillna('')\n",
    "}\n",
    "\n",
    "# --- 1. One-hot encoding of CDR3 sequences ---\n",
    "print(\"Computing one-hot encodings...\")\n",
    "max_cdr3_length = 30  # Typical CDR3 length range\n",
    "\n",
    "# One-hot encode TRA CDR3 sequences\n",
    "tra_onehot = np.array([one_hot_encode_sequence(seq, max_cdr3_length, 'ACDEFGHIKLMNPQRSTVWY') \n",
    "                       for seq in cdr3_sequences['TRA']])\n",
    "tra_onehot_flat = tra_onehot.reshape(tra_onehot.shape[0], -1)\n",
    "\n",
    "# One-hot encode TRB CDR3 sequences  \n",
    "trb_onehot = np.array([one_hot_encode_sequence(seq, max_cdr3_length, 'ACDEFGHIKLMNPQRSTVWY') \n",
    "                       for seq in cdr3_sequences['TRB']])\n",
    "trb_onehot_flat = trb_onehot.reshape(trb_onehot.shape[0], -1)\n",
    "\n",
    "print(f\"TRA one-hot shape: {tra_onehot_flat.shape}\")\n",
    "print(f\"TRB one-hot shape: {trb_onehot_flat.shape}\")\n",
    "\n",
    "# --- 2. K-mer encoding ---\n",
    "print(\"Computing k-mer encodings...\")\n",
    "k = 3  # Use 3-mers\n",
    "\n",
    "# Get all possible k-mers for creating consistent feature vectors\n",
    "all_tra_kmers = []\n",
    "all_trb_kmers = []\n",
    "\n",
    "for seq in cdr3_sequences['TRA']:\n",
    "    if seq and seq != '':\n",
    "        all_tra_kmers.extend(kmer_encode_sequence(seq, k).keys())\n",
    "\n",
    "for seq in cdr3_sequences['TRB']:\n",
    "    if seq and seq != '':\n",
    "        all_trb_kmers.extend(kmer_encode_sequence(seq, k).keys())\n",
    "\n",
    "unique_tra_kmers = sorted(list(set(all_tra_kmers)))\n",
    "unique_trb_kmers = sorted(list(set(all_trb_kmers)))\n",
    "\n",
    "# Create k-mer count vectors\n",
    "tra_kmer_matrix = []\n",
    "for seq in cdr3_sequences['TRA']:\n",
    "    kmer_counts = kmer_encode_sequence(seq, k)\n",
    "    vector = [kmer_counts.get(kmer, 0) for kmer in unique_tra_kmers]\n",
    "    tra_kmer_matrix.append(vector)\n",
    "\n",
    "trb_kmer_matrix = []\n",
    "for seq in cdr3_sequences['TRB']:\n",
    "    kmer_counts = kmer_encode_sequence(seq, k)\n",
    "    vector = [kmer_counts.get(kmer, 0) for kmer in unique_trb_kmers]\n",
    "    trb_kmer_matrix.append(vector)\n",
    "\n",
    "tra_kmer_matrix = np.array(tra_kmer_matrix)\n",
    "trb_kmer_matrix = np.array(trb_kmer_matrix)\n",
    "\n",
    "print(f\"TRA k-mer matrix shape: {tra_kmer_matrix.shape}\")\n",
    "print(f\"TRB k-mer matrix shape: {trb_kmer_matrix.shape}\")\n",
    "\n",
    "# --- 3. Physicochemical properties ---\n",
    "print(\"Computing physicochemical features...\")\n",
    "\n",
    "tra_physico = pd.DataFrame([physicochemical_features(seq) for seq in cdr3_sequences['TRA']])\n",
    "trb_physico = pd.DataFrame([physicochemical_features(seq) for seq in cdr3_sequences['TRB']])\n",
    "\n",
    "print(f\"TRA physicochemical features shape: {tra_physico.shape}\")\n",
    "print(f\"TRB physicochemical features shape: {trb_physico.shape}\")\n",
    "\n",
    "# Add to AnnData object\n",
    "adata.obsm['X_tcr_tra_onehot'] = tra_onehot_flat\n",
    "adata.obsm['X_tcr_trb_onehot'] = trb_onehot_flat\n",
    "adata.obsm['X_tcr_tra_kmer'] = tra_kmer_matrix\n",
    "adata.obsm['X_tcr_trb_kmer'] = trb_kmer_matrix\n",
    "\n",
    "# Add physicochemical features to obs\n",
    "for col in tra_physico.columns:\n",
    "    adata.obs[f'tra_{col}'] = tra_physico[col].values\n",
    "    adata.obs[f'trb_{col}'] = trb_physico[col].values\n",
    "\n",
    "print(\"TCR sequence encoding completed and added to AnnData object!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1285dd7",
   "metadata": {},
   "source": [
    "## 8. Encode Gene Expression Patterns\n",
    "\n",
    "Apply PCA, SVD, and UMAP to gene expression data for dimensionality reduction and add encodings to AnnData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Encode Gene Expression Patterns ---\n",
    "\n",
    "print(\"Preprocessing gene expression data...\")\n",
    "\n",
    "# Basic preprocessing if not already done\n",
    "if 'X_pca' not in adata.obsm:\n",
    "    # Store raw counts\n",
    "    adata.raw = adata\n",
    "    \n",
    "    # Normalize counts per cell to a fixed total\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    # Log-transform the data\n",
    "    sc.pp.log1p(adata)\n",
    "    \n",
    "    # Replace any infinite values with zeros\n",
    "    if hasattr(adata.X, 'data'):  # sparse matrix\n",
    "        adata.X.data[np.isinf(adata.X.data)] = 0\n",
    "    else:  # dense matrix\n",
    "        adata.X[np.isinf(adata.X)] = 0\n",
    "    \n",
    "    print(\"Basic preprocessing completed\")\n",
    "\n",
    "print(\"Encoding gene expression patterns...\")\n",
    "\n",
    "# Apply gene expression encoding with fixed function\n",
    "def encode_gene_expression_patterns_fixed(adata, n_top_genes=2000):\n",
    "    \"\"\"\n",
    "    Fixed version of gene expression encoding\n",
    "    \"\"\"\n",
    "    # Select highly variable genes manually to avoid infinity issues\n",
    "    X_dense = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\n",
    "    \n",
    "    # Calculate variance for each gene\n",
    "    gene_vars = np.var(X_dense, axis=0)\n",
    "    # Remove any infinite or NaN values\n",
    "    gene_vars = np.nan_to_num(gene_vars, nan=0, posinf=0, neginf=0)\n",
    "    \n",
    "    # Select top variable genes\n",
    "    top_genes_idx = np.argsort(gene_vars)[-n_top_genes:]\n",
    "    X_hvg = X_dense[:, top_genes_idx]\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_hvg)\n",
    "    \n",
    "    encodings = {}\n",
    "    \n",
    "    # PCA encoding\n",
    "    pca = PCA(n_components=min(50, X_scaled.shape[1]))\n",
    "    encodings['pca'] = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # TruncatedSVD for sparse matrices\n",
    "    svd = TruncatedSVD(n_components=min(50, X_scaled.shape[1]), random_state=42)\n",
    "    encodings['svd'] = svd.fit_transform(X_scaled)\n",
    "    \n",
    "    # UMAP encoding\n",
    "    umap_encoder = umap.UMAP(n_components=20, random_state=42)\n",
    "    encodings['umap'] = umap_encoder.fit_transform(X_scaled)\n",
    "    \n",
    "    return encodings, X_scaled\n",
    "\n",
    "# Apply fixed gene expression encoding\n",
    "gene_encodings, X_scaled_genes = encode_gene_expression_patterns_fixed(adata, n_top_genes=2000)\n",
    "\n",
    "# Add gene expression encodings to AnnData\n",
    "for encoding_name, encoding_data in gene_encodings.items():\n",
    "    adata.obsm[f'X_gene_{encoding_name}'] = encoding_data\n",
    "\n",
    "print(\"Gene expression encoding completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e932df52",
   "metadata": {},
   "source": [
    "## 9. Create Combined Multi-Modal Encodings\n",
    "\n",
    "Combine gene expression and TCR encodings into multi-modal representations using PCA and UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ddd26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Create Combined Multi-Modal Encodings ---\n",
    "print(\"Creating combined multi-modal encodings...\")\n",
    "\n",
    "# Combine different encoding modalities\n",
    "# 1. Gene expression PCA + TCR physicochemical features\n",
    "gene_pca = gene_encodings['pca'][:, :20]  # Top 20 PCA components\n",
    "tcr_features = np.column_stack([\n",
    "    adata.obs[['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']].fillna(0),\n",
    "    adata.obs[['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']].fillna(0)\n",
    "])\n",
    "\n",
    "combined_gene_tcr = np.column_stack([gene_pca, tcr_features])\n",
    "adata.obsm['X_combined_gene_tcr'] = combined_gene_tcr\n",
    "\n",
    "# 2. Gene expression UMAP + TCR k-mer features (reduced)\n",
    "gene_umap = gene_encodings['umap']\n",
    "tcr_kmer_combined = np.column_stack([adata.obsm['X_tcr_tra_kmer'], adata.obsm['X_tcr_trb_kmer']])\n",
    "tcr_kmer_reduced = PCA(n_components=10).fit_transform(tcr_kmer_combined)\n",
    "\n",
    "combined_gene_tcr_kmer = np.column_stack([gene_umap, tcr_kmer_reduced])\n",
    "adata.obsm['X_combined_gene_tcr_kmer'] = combined_gene_tcr_kmer\n",
    "\n",
    "print(f\"Combined gene-TCR encoding shape: {combined_gene_tcr.shape}\")\n",
    "print(f\"Combined gene-TCR k-mer encoding shape: {combined_gene_tcr_kmer.shape}\")\n",
    "\n",
    "# --- Dimensionality Reduction on Combined Data ---\n",
    "print(\"Computing dimensionality reduction on combined data...\")\n",
    "\n",
    "# UMAP on combined data\n",
    "umap_combined = umap.UMAP(n_components=2, random_state=42)\n",
    "adata.obsm['X_umap_combined'] = umap_combined.fit_transform(combined_gene_tcr)\n",
    "\n",
    "# t-SNE on combined data (sample subset for speed)\n",
    "sample_size = min(5000, combined_gene_tcr.shape[0])\n",
    "sample_idx = np.random.choice(combined_gene_tcr.shape[0], sample_size, replace=False)\n",
    "tsne_combined = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tsne_result = tsne_combined.fit_transform(combined_gene_tcr[sample_idx])\n",
    "\n",
    "# Create full t-SNE result array\n",
    "full_tsne = np.zeros((combined_gene_tcr.shape[0], 2))\n",
    "full_tsne[sample_idx] = tsne_result\n",
    "adata.obsm['X_tsne_combined'] = full_tsne\n",
    "\n",
    "print(\"Multi-modal encoding and dimensionality reduction completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23e946",
   "metadata": {},
   "source": [
    "## 10. Unsupervised Machine Learning Analysis with Hierarchical Clustering\n",
    "\n",
    "Perform clustering with K-Means, HDBSCAN, Agglomerative, DBSCAN, and add hierarchical clustering using scipy, compute silhouette scores, and visualize with dendrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36620e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Unsupervised Machine Learning Analysis ---\n",
    "\n",
    "print(\"Applying unsupervised machine learning algorithms...\")\n",
    "\n",
    "# --- 1. Clustering Analysis ---\n",
    "\n",
    "# Prepare data for clustering\n",
    "X_clustering = combined_gene_tcr\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clustering)\n",
    "\n",
    "clustering_results = {}\n",
    "\n",
    "# K-Means clustering\n",
    "print(\"Running K-Means clustering...\")\n",
    "for n_clusters in [3, 4, 5, 6, 8]:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    silhouette = silhouette_score(X_scaled, cluster_labels)\n",
    "    clustering_results[f'kmeans_{n_clusters}'] = {\n",
    "        'labels': cluster_labels, \n",
    "        'silhouette': silhouette,\n",
    "        'algorithm': 'K-Means'\n",
    "    }\n",
    "    adata.obs[f'kmeans_{n_clusters}'] = pd.Categorical(cluster_labels)\n",
    "\n",
    "print(\"Running HDBSCAN clustering...\")\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5)\n",
    "hdbscan_labels = hdbscan_clusterer.fit_predict(X_scaled)\n",
    "if len(set(hdbscan_labels)) > 1:  # Only compute silhouette if more than 1 cluster\n",
    "    hdbscan_silhouette = silhouette_score(X_scaled, hdbscan_labels)\n",
    "else:\n",
    "    hdbscan_silhouette = -1\n",
    "clustering_results['hdbscan'] = {\n",
    "    'labels': hdbscan_labels, \n",
    "    'silhouette': hdbscan_silhouette,\n",
    "    'algorithm': 'HDBSCAN'\n",
    "}\n",
    "adata.obs['hdbscan'] = pd.Categorical(hdbscan_labels)\n",
    "\n",
    "# Agglomerative clustering\n",
    "print(\"Running Agglomerative clustering...\")\n",
    "for n_clusters in [3, 4, 5, 6]:\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    agg_labels = agg_clustering.fit_predict(X_scaled)\n",
    "    agg_silhouette = silhouette_score(X_scaled, agg_labels)\n",
    "    clustering_results[f'agglomerative_{n_clusters}'] = {\n",
    "        'labels': agg_labels, \n",
    "        'silhouette': agg_silhouette,\n",
    "        'algorithm': 'Agglomerative'\n",
    "    }\n",
    "    adata.obs[f'agglomerative_{n_clusters}'] = pd.Categorical(agg_labels)\n",
    "\n",
    "# DBSCAN clustering\n",
    "print(\"Running DBSCAN clustering...\")\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "if len(set(dbscan_labels)) > 1 and -1 not in dbscan_labels:\n",
    "    dbscan_silhouette = silhouette_score(X_scaled, dbscan_labels)\n",
    "else:\n",
    "    dbscan_silhouette = -1\n",
    "clustering_results['dbscan'] = {\n",
    "    'labels': dbscan_labels,\n",
    "    'silhouette': dbscan_silhouette,\n",
    "    'algorithm': 'DBSCAN'\n",
    "}\n",
    "adata.obs['dbscan'] = pd.Categorical(dbscan_labels)\n",
    "\n",
    "# Hierarchical clustering\n",
    "print(\"Running Hierarchical clustering...\")\n",
    "# Use linkage for hierarchical clustering\n",
    "Z = linkage(X_scaled[:1000], method='ward')  # Sample for speed\n",
    "hierarchical_labels = fcluster(Z, t=5, criterion='maxclust')\n",
    "clustering_results['hierarchical'] = {\n",
    "    'labels': hierarchical_labels,\n",
    "    'silhouette': silhouette_score(X_scaled[:1000], hierarchical_labels),\n",
    "    'algorithm': 'Hierarchical'\n",
    "}\n",
    "adata.obs['hierarchical'] = pd.Categorical(hierarchical_labels)\n",
    "\n",
    "# Print clustering results summary\n",
    "print(\"\\nClustering Results Summary:\")\n",
    "for name, result in clustering_results.items():\n",
    "    n_clusters = len(set(result['labels'])) - (1 if -1 in result['labels'] else 0)\n",
    "    print(f\"{name}: {n_clusters} clusters, Silhouette Score: {result['silhouette']:.3f}\")\n",
    "\n",
    "# Find best clustering result\n",
    "best_clustering = max(clustering_results.items(), key=lambda x: x[1]['silhouette'])\n",
    "print(f\"\\nBest clustering: {best_clustering[0]} (Silhouette: {best_clustering[1]['silhouette']:.3f})\")\n",
    "\n",
    "# --- 2. TCR Sequence-Specific Clustering ---\n",
    "print(\"\\nPerforming TCR sequence-specific clustering...\")\n",
    "\n",
    "# Cluster based on TRA k-mer features\n",
    "tra_scaler = StandardScaler()\n",
    "tra_kmer_scaled = tra_scaler.fit_transform(adata.obsm['X_tcr_tra_kmer'])\n",
    "\n",
    "# K-means on TRA sequences\n",
    "tra_kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "tra_clusters = tra_kmeans.fit_predict(tra_kmer_scaled)\n",
    "adata.obs['tra_kmer_clusters'] = pd.Categorical(tra_clusters)\n",
    "\n",
    "# K-means on TRB sequences\n",
    "trb_scaler = StandardScaler()\n",
    "trb_kmer_scaled = trb_scaler.fit_transform(adata.obsm['X_tcr_trb_kmer'])\n",
    "trb_kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "trb_clusters = trb_kmeans.fit_predict(trb_kmer_scaled)\n",
    "adata.obs['trb_kmer_clusters'] = pd.Categorical(trb_clusters)\n",
    "\n",
    "print(\"TCR sequence clustering completed!\")\n",
    "\n",
    "# --- 3. Gene Expression Module Discovery ---\n",
    "print(\"\\nDiscovering gene expression modules...\")\n",
    "\n",
    "# Use gene expression PCA for module discovery\n",
    "gene_pca_full = gene_encodings['pca']\n",
    "gene_kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "gene_expression_modules = gene_kmeans.fit_predict(gene_pca_full)\n",
    "adata.obs['gene_expression_modules'] = pd.Categorical(gene_expression_modules)\n",
    "\n",
    "print(\"Gene expression module discovery completed!\")\n",
    "\n",
    "# --- 4. Dendrogram Visualization for Hierarchical Clustering ---\n",
    "print(\"\\nCreating dendrogram for hierarchical clustering...\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(Z, truncate_mode='lastp', p=12, leaf_rotation=45, leaf_font_size=10, show_contracted=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nUnsupervised machine learning analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8e12b",
   "metadata": {},
   "source": [
    "## 11. Comprehensive Feature Engineering\n",
    "\n",
    "Create multiple feature sets (basic, gene-enhanced, TCR-enhanced, comprehensive, sequence-structure) with dimensionality reduction and variance selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Comprehensive Feature Engineering ---\n",
    "\n",
    "print(\"Creating comprehensive feature set using ALL available encodings...\")\n",
    "\n",
    "# --- 1. Strategic Feature Engineering with Dimensionality Reduction ---\n",
    "print(\"Applying strategic dimensionality reduction to high-dimensional features...\")\n",
    "\n",
    "# Filter for supervised learning samples first to reduce memory\n",
    "supervised_mask = adata.obs['response'].isin(['Responder', 'Non-Responder'])\n",
    "y_supervised = adata.obs['response'][supervised_mask]\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_supervised)\n",
    "\n",
    "print(f\"Working with {sum(supervised_mask)} samples for supervised learning\")\n",
    "print(f\"Class distribution: {dict(zip(label_encoder.classes_, np.bincount(y_encoded)))}\")\n",
    "\n",
    "# --- Reduce high-dimensional k-mer features using variance-based selection ---\n",
    "tra_kmer_supervised = adata.obsm['X_tcr_tra_kmer'][supervised_mask]\n",
    "trb_kmer_supervised = adata.obsm['X_tcr_trb_kmer'][supervised_mask]\n",
    "\n",
    "# Select top variance k-mers to reduce dimensionality\n",
    "def select_top_variance_features(X, n_features=200):\n",
    "    \"\"\"Select features with highest variance\"\"\"\n",
    "    variances = np.var(X, axis=0)\n",
    "    top_indices = np.argsort(variances)[-n_features:]\n",
    "    return X[:, top_indices], top_indices\n",
    "\n",
    "print(\"Reducing k-mer features by variance selection...\")\n",
    "tra_kmer_reduced, tra_top_idx = select_top_variance_features(tra_kmer_supervised, n_features=200)\n",
    "trb_kmer_reduced, trb_top_idx = select_top_variance_features(trb_kmer_supervised, n_features=200)\n",
    "\n",
    "print(f\"TRA k-mers reduced from {tra_kmer_supervised.shape[1]} to {tra_kmer_reduced.shape[1]}\")\n",
    "print(f\"TRB k-mers reduced from {trb_kmer_supervised.shape[1]} to {trb_kmer_reduced.shape[1]}\")\n",
    "\n",
    "# --- 2. Create strategic feature combinations ---\n",
    "feature_sets = {}\n",
    "\n",
    "# Basic features (gene expression + physicochemical)\n",
    "gene_features = adata.obsm['X_gene_pca'][supervised_mask]\n",
    "tcr_physico = np.column_stack([\n",
    "    adata.obs[['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity']].fillna(0)[supervised_mask],\n",
    "    adata.obs[['trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']].fillna(0)[supervised_mask]\n",
    "])\n",
    "qc_features = adata.obs[['n_genes_by_counts', 'total_counts', 'pct_counts_mt']].fillna(0)[supervised_mask].values\n",
    "\n",
    "feature_sets['basic'] = np.column_stack([\n",
    "    gene_features[:, :20],  # Top 20 gene PCA components\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# Enhanced gene expression\n",
    "feature_sets['gene_enhanced'] = np.column_stack([\n",
    "    adata.obsm['X_gene_pca'][supervised_mask],  # All 50 PCA components\n",
    "    adata.obsm['X_gene_svd'][supervised_mask][:, :30],  # Top 30 SVD components\n",
    "    adata.obsm['X_gene_umap'][supervised_mask],  # All 20 UMAP components\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# TCR sequence enhanced\n",
    "feature_sets['tcr_enhanced'] = np.column_stack([\n",
    "    gene_features[:, :20],  # Top 20 gene PCA\n",
    "    tra_kmer_reduced,  # Top 200 TRA k-mers\n",
    "    trb_kmer_reduced,  # Top 200 TRB k-mers\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "# Comprehensive (all modalities)\n",
    "feature_sets['comprehensive'] = np.column_stack([\n",
    "    adata.obsm['X_gene_pca'][supervised_mask],  # 50 features\n",
    "    adata.obsm['X_gene_svd'][supervised_mask][:, :20],  # Top 20 SVD\n",
    "    adata.obsm['X_gene_umap'][supervised_mask],  # 20 features\n",
    "    tra_kmer_reduced,  # 200 features\n",
    "    trb_kmer_reduced,  # 200 features  \n",
    "    tcr_physico,  # 6 features\n",
    "    qc_features  # 3 features\n",
    "])\n",
    "\n",
    "# One-hot encoded sequences (reduced)\n",
    "onehot_tra_reduced = PCA(n_components=50).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "onehot_trb_reduced = PCA(n_components=50).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "\n",
    "feature_sets['sequence_structure'] = np.column_stack([\n",
    "    gene_features[:, :30],  # Top 30 gene PCA\n",
    "    onehot_tra_reduced,  # 50 PCA of one-hot TRA\n",
    "    onehot_trb_reduced,  # 50 PCA of one-hot TRB\n",
    "    tcr_physico,\n",
    "    qc_features\n",
    "])\n",
    "\n",
    "print(f\"\\nFeature set dimensions:\")\n",
    "for name, features in feature_sets.items():\n",
    "    print(f\"   {name}: {features.shape}\")\n",
    "\n",
    "print(\"Comprehensive feature engineering completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128bd911",
   "metadata": {},
   "source": [
    "## 12. Supervised Learning with Multiple Models (Logistic Regression, Decision Trees, XGBoost, Deep Learning)\n",
    "\n",
    "Train and evaluate models including Logistic Regression (baseline), Decision Trees, XGBoost, and Deep Neural Networks (using Keras), experimenting with hyperparameters, feature sets, and settings for maximum data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Supervised Learning with Multiple Models ---\n",
    "\n",
    "print(\"Training and evaluating multiple supervised learning models...\")\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "models['XGBoost'] = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "\n",
    "def create_deep_model(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_dim=input_dim),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "models['Deep Learning'] = 'DL'  # Placeholder\n",
    "\n",
    "# Hyperparameter grids for experimentation\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [5, 10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.3]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store results\n",
    "all_results = {}\n",
    "\n",
    "for feature_name, X_features in feature_sets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FEATURE SET: {feature_name.upper()} ({X_features.shape[1]} features)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_features, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    feature_results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n--- {model_name} ---\")\n",
    "        \n",
    "        if model_name == 'Deep Learning':\n",
    "            # Special handling for DL\n",
    "            dl_model = create_deep_model(X_train_scaled.shape[1])\n",
    "            history = dl_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "            y_pred_proba = dl_model.predict(X_test_scaled).flatten()\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        else:\n",
    "            # Grid search for hyperparameters\n",
    "            if model_name in param_grids:\n",
    "                grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='accuracy', n_jobs=-1)\n",
    "                grid_search.fit(X_train_scaled, y_train)\n",
    "                best_model = grid_search.best_estimator_\n",
    "                print(f\"Best params: {grid_search.best_params_}\")\n",
    "            else:\n",
    "                best_model = model\n",
    "                best_model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            y_pred = best_model.predict(X_test_scaled)\n",
    "            y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Specificity\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        \n",
    "        # NPV\n",
    "        npv = tn / (tn + fn)\n",
    "        \n",
    "        # Cross-validation\n",
    "        if model_name != 'Deep Learning':\n",
    "            cv_scores = cross_val_score(best_model, X_features, y_encoded, cv=5, scoring='accuracy')\n",
    "            cv_mean = cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "        else:\n",
    "            cv_mean = accuracy  # Approximation\n",
    "            cv_std = 0\n",
    "        \n",
    "        feature_results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'auc': auc,\n",
    "            'specificity': specificity,\n",
    "            'npv': npv,\n",
    "            'cv_mean': cv_mean,\n",
    "            'cv_std': cv_std,\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "        print(f\"Specificity: {specificity:.3f}, NPV: {npv:.3f}, AUC: {auc:.3f}\")\n",
    "        print(f\"CV Accuracy: {cv_mean:.3f}  {cv_std:.3f}\")\n",
    "    \n",
    "    all_results[feature_name] = feature_results\n",
    "\n",
    "# Find best overall model\n",
    "best_score = 0\n",
    "best_model_info = None\n",
    "for feature_name, feature_result in all_results.items():\n",
    "    for model_name, result in feature_result.items():\n",
    "        if result['cv_mean'] > best_score:\n",
    "            best_score = result['cv_mean']\n",
    "            best_model_info = (feature_name, model_name, result)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BEST MODEL SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Feature Set: {best_model_info[0]}\")\n",
    "print(f\"Model: {best_model_info[1]}\")\n",
    "print(f\"CV Accuracy: {best_score:.3f}\")\n",
    "print(f\"Test Accuracy: {best_model_info[2]['accuracy']:.3f}\")\n",
    "print(f\"AUC: {best_model_info[2]['auc']:.3f}\")\n",
    "\n",
    "print(\"\\nSupervised learning with multiple models completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f08135",
   "metadata": {},
   "source": [
    "## 13. Model Performance Evaluation with Confusion Matrices and Cross-Validation\n",
    "\n",
    "Perform k-fold cross-validation, generate confusion matrices, precision, recall, F1-score, AUC, specificity, NPV, and other metrics for all models, saving detailed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84711284",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Model Performance Evaluation ---\n",
    "\n",
    "print(\"Generating comprehensive model performance evaluation...\")\n",
    "\n",
    "# Create detailed performance report\n",
    "performance_report = []\n",
    "\n",
    "for feature_name, feature_result in all_results.items():\n",
    "    for model_name, result in feature_result.items():\n",
    "        cm = result['confusion_matrix']\n",
    "        \n",
    "        report_entry = {\n",
    "            'Feature_Set': feature_name,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'Precision': result['precision'],\n",
    "            'Recall': result['recall'],\n",
    "            'F1_Score': result['f1_score'],\n",
    "            'AUC': result['auc'],\n",
    "            'Specificity': result['specificity'],\n",
    "            'NPV': result['npv'],\n",
    "            'CV_Mean': result['cv_mean'],\n",
    "            'CV_Std': result['cv_std'],\n",
    "            'TN': cm[0,0],\n",
    "            'FP': cm[0,1],\n",
    "            'FN': cm[1,0],\n",
    "            'TP': cm[1,1]\n",
    "        }\n",
    "        performance_report.append(report_entry)\n",
    "\n",
    "performance_df = pd.DataFrame(performance_report)\n",
    "\n",
    "# Display results\n",
    "print(\"COMPREHENSIVE MODEL PERFORMANCE REPORT\")\n",
    "print(\"=\"*120)\n",
    "print(performance_df.to_string(index=False))\n",
    "\n",
    "# Visualize confusion matrices for best models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "best_models = performance_df.nlargest(6, 'CV_Mean')\n",
    "\n",
    "for i, (_, row) in enumerate(best_models.iterrows()):\n",
    "    if i < 6:\n",
    "        cm = np.array([[row['TN'], row['FP']], [row['FN'], row['TP']]])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Responder', 'Responder'])\n",
    "        disp.plot(ax=axes[i], cmap='Blues')\n",
    "        axes[i].set_title(f\"{row['Model']}\\n({row['Feature_Set']})\\nAcc: {row['Accuracy']:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-validation comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "cv_data = performance_df.pivot(index='Feature_Set', columns='Model', values='CV_Mean')\n",
    "cv_data.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('Cross-Validation Accuracy Comparison')\n",
    "plt.ylabel('CV Accuracy')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# AUC comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "auc_data = performance_df.pivot(index='Feature_Set', columns='Model', values='AUC')\n",
    "auc_data.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('AUC Comparison Across Models and Feature Sets')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel performance evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad69a10",
   "metadata": {},
   "source": [
    "## 14. Experiment with Sequence Length Cutoffs and Model Accuracy\n",
    "\n",
    "Vary sequence length cutoffs for genes and TCR, retrain models, and plot accuracy as a function of length to assess information capture and performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Experiment with Sequence Length Cutoffs ---\n",
    "\n",
    "print(\"Experimenting with sequence length cutoffs...\")\n",
    "\n",
    "# Define length cutoffs to test\n",
    "length_cutoffs = [10, 15, 20, 25, 30, 35, 40, 50]\n",
    "\n",
    "length_results = []\n",
    "\n",
    "for max_length in length_cutoffs:\n",
    "    print(f\"\\nTesting max sequence length: {max_length}\")\n",
    "    \n",
    "    # Re-encode sequences with new length\n",
    "    tra_onehot_new = np.array([one_hot_encode_sequence(seq, max_length, 'ACDEFGHIKLMNPQRSTVWY') \n",
    "                               for seq in cdr3_sequences['TRA']])\n",
    "    tra_onehot_flat_new = tra_onehot_new.reshape(tra_onehot_new.shape[0], -1)\n",
    "    \n",
    "    trb_onehot_new = np.array([one_hot_encode_sequence(seq, max_length, 'ACDEFGHIKLMNPQRSTVWY') \n",
    "                               for seq in cdr3_sequences['TRB']])\n",
    "    trb_onehot_flat_new = trb_onehot_new.reshape(trb_onehot_flat_new.shape[0], -1)\n",
    "    \n",
    "    # Update AnnData\n",
    "    adata.obsm['X_tcr_tra_onehot'] = tra_onehot_flat_new\n",
    "    adata.obsm['X_tcr_trb_onehot'] = trb_onehot_flat_new\n",
    "    \n",
    "    # Re-create feature sets with new encodings\n",
    "    onehot_tra_reduced = PCA(n_components=50).fit_transform(adata.obsm['X_tcr_tra_onehot'][supervised_mask])\n",
    "    onehot_trb_reduced = PCA(n_components=50).fit_transform(adata.obsm['X_tcr_trb_onehot'][supervised_mask])\n",
    "    \n",
    "    X_sequence = np.column_stack([\n",
    "        gene_features[:, :30],\n",
    "        onehot_tra_reduced,\n",
    "        onehot_trb_reduced,\n",
    "        tcr_physico,\n",
    "        qc_features\n",
    "    ])\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_sequence, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Use best model type\n",
    "    if XGB_AVAILABLE:\n",
    "        model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    else:\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_sequence, y_encoded, cv=3, scoring='accuracy')\n",
    "    \n",
    "    length_results.append({\n",
    "        'max_length': max_length,\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.3f}, CV: {cv_scores.mean():.3f}  {cv_scores.std():.3f}\")\n",
    "\n",
    "# Plot results\n",
    "length_df = pd.DataFrame(length_results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(length_df['max_length'], length_df['accuracy'], 'o-', label='Test Accuracy', linewidth=2)\n",
    "plt.plot(length_df['max_length'], length_df['cv_mean'], 's-', label='CV Accuracy', linewidth=2)\n",
    "plt.fill_between(length_df['max_length'], \n",
    "                 length_df['cv_mean'] - length_df['cv_std'], \n",
    "                 length_df['cv_mean'] + length_df['cv_std'], \n",
    "                 alpha=0.3, label='CV  Std')\n",
    "plt.xlabel('Maximum Sequence Length Cutoff')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy vs Sequence Length Cutoff')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSequence length cutoff experiment completed!\")\n",
    "print(f\"Optimal length appears to be around {length_df.loc[length_df['cv_mean'].idxmax(), 'max_length']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e79a6",
   "metadata": {},
   "source": [
    "## 15. Comprehensive Sequence Pattern Discovery\n",
    "\n",
    "Analyze TCR sequence patterns, physicochemical properties, k-mer differences, and gene expression by response group, with statistical tests and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d66249",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Comprehensive Sequence Pattern Discovery ---\n",
    "\n",
    "print(\"Performing comprehensive sequence pattern discovery...\")\n",
    "\n",
    "# --- 1. Analyze TCR sequence patterns by response ---\n",
    "responder_mask = adata.obs['response'] == 'Responder'\n",
    "non_responder_mask = adata.obs['response'] == 'Non-Responder'\n",
    "\n",
    "# Get sequence data for analysis\n",
    "responder_tra = adata.obs[responder_mask]['cdr3_TRA'].dropna()\n",
    "non_responder_tra = adata.obs[non_responder_mask]['cdr3_TRA'].dropna()\n",
    "responder_trb = adata.obs[responder_mask]['cdr3_TRB'].dropna()\n",
    "non_responder_trb = adata.obs[non_responder_mask]['cdr3_TRB'].dropna()\n",
    "\n",
    "print(\"\\n--- TCR SEQUENCE PATTERNS ---\")\n",
    "print(f\"Responder TRA sequences: {len(responder_tra)}\")\n",
    "print(f\"Non-responder TRA sequences: {len(non_responder_tra)}\")\n",
    "\n",
    "print(\"\\nTop TRA CDR3 sequences in Responders:\")\n",
    "top_responder_tra = responder_tra.value_counts().head(10)\n",
    "for seq, count in top_responder_tra.items():\n",
    "    print(f\"  {seq}: {count} cells\")\n",
    "\n",
    "print(\"\\nTop TRA CDR3 sequences in Non-Responders:\")\n",
    "top_non_responder_tra = non_responder_tra.value_counts().head(10)\n",
    "for seq, count in top_non_responder_tra.items():\n",
    "    print(f\"  {seq}: {count} cells\")\n",
    "\n",
    "# --- 2. Physicochemical property analysis ---\n",
    "print(\"\\n--- PHYSICOCHEMICAL PROPERTY ANALYSIS ---\")\n",
    "\n",
    "# Compare physicochemical properties between responders and non-responders\n",
    "responder_physico = adata.obs[responder_mask][['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity',\n",
    "                                               'trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']]\n",
    "non_responder_physico = adata.obs[non_responder_mask][['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity',\n",
    "                                                       'trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']]\n",
    "\n",
    "properties = ['tra_length', 'tra_molecular_weight', 'tra_hydrophobicity', \n",
    "              'trb_length', 'trb_molecular_weight', 'trb_hydrophobicity']\n",
    "\n",
    "print(\"Statistical comparison of physicochemical properties:\")\n",
    "print(f\"{'Property':<20} {'Responder Mean':<15} {'Non-Resp Mean':<15} {'P-value':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for prop in properties:\n",
    "    resp_values = responder_physico[prop].dropna()\n",
    "    non_resp_values = non_responder_physico[prop].dropna()\n",
    "    \n",
    "    if len(resp_values) > 0 and len(non_resp_values) > 0:\n",
    "        statistic, p_value = mannwhitneyu(resp_values, non_resp_values, alternative='two-sided')\n",
    "        print(f\"{prop:<20} {resp_values.mean():<15.3f} {non_resp_values.mean():<15.3f} {p_value:<10.6f}\")\n",
    "\n",
    "# --- 3. K-mer differential analysis ---\n",
    "print(\"\\n--- K-MER DIFFERENTIAL ANALYSIS ---\")\n",
    "\n",
    "# Use the indices we identified earlier for top variance k-mers\n",
    "responder_indices = np.where(supervised_mask & responder_mask)[0]\n",
    "non_responder_indices = np.where(supervised_mask & non_responder_mask)[0]\n",
    "\n",
    "# Get k-mer data for responders vs non-responders\n",
    "responder_tra_kmers = tra_kmer_supervised[responder_indices - np.where(supervised_mask)[0][0]]\n",
    "non_responder_tra_kmers = tra_kmer_supervised[non_responder_indices - np.where(supervised_mask)[0][0]]\n",
    "\n",
    "# Calculate mean k-mer frequencies\n",
    "responder_tra_mean = responder_tra_kmers.mean(axis=0)\n",
    "non_responder_tra_mean = non_responder_tra_kmers.mean(axis=0)\n",
    "\n",
    "# Find most differentially expressed k-mers from selected features\n",
    "kmer_diff = responder_tra_mean - non_responder_tra_mean\n",
    "top_responder_kmers_idx = np.argsort(kmer_diff)[-10:]\n",
    "top_non_responder_kmers_idx = np.argsort(kmer_diff)[:10]\n",
    "\n",
    "# Get the actual k-mer sequences for the selected features\n",
    "selected_tra_kmers = [unique_tra_kmers[tra_top_idx[i]] for i in range(len(tra_top_idx))]\n",
    "\n",
    "print(\"Top k-mers enriched in Responders (from variance-selected features):\")\n",
    "for idx in top_responder_kmers_idx:\n",
    "    if idx < len(selected_tra_kmers):\n",
    "        print(f\"  {selected_tra_kmers[idx]}: +{kmer_diff[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop k-mers enriched in Non-Responders (from variance-selected features):\")\n",
    "for idx in top_non_responder_kmers_idx:\n",
    "    if idx < len(selected_tra_kmers):\n",
    "        print(f\"  {selected_tra_kmers[idx]}: {kmer_diff[idx]:.4f}\")\n",
    "\n",
    "# --- 4. Gene expression analysis for top important features ---\n",
    "print(\"\\n--- GENE EXPRESSION PATTERN ANALYSIS ---\")\n",
    "\n",
    "# Get the best model's feature importance (assuming Random Forest or similar)\n",
    "if 'Random Forest' in all_results[best_model_info[0]]:\n",
    "    best_importance = all_results[best_model_info[0]]['Random Forest']['feature_importance']\n",
    "    top_gene_features = np.argsort(best_importance)[-10:]\n",
    "    \n",
    "    print(f\"Analysis based on {best_model_info[1]} model with {best_model_info[2]['n_features']} features\")\n",
    "    print(\"Top 10 most important features for classification:\")\n",
    "    for i, feat_idx in enumerate(top_gene_features):\n",
    "        print(f\"  Feature {feat_idx}: Importance = {best_importance[feat_idx]:.4f}\")\n",
    "\n",
    "# --- 5. Comprehensive visualization ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Sequence length distributions\n",
    "axes[0,0].hist([responder_tra.str.len().dropna(), non_responder_tra.str.len().dropna()], \n",
    "               bins=20, alpha=0.7, label=['Responder', 'Non-Responder'])\n",
    "axes[0,0].set_xlabel('TRA CDR3 Length')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].set_title('TRA CDR3 Length Distribution')\n",
    "axes[0,0].legend()\n",
    "\n",
    "axes[0,1].hist([responder_trb.str.len().dropna(), non_responder_trb.str.len().dropna()], \n",
    "               bins=20, alpha=0.7, label=['Responder', 'Non-Responder'])\n",
    "axes[0,1].set_xlabel('TRB CDR3 Length')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_title('TRB CDR3 Length Distribution')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Physicochemical property comparisons\n",
    "axes[0,2].boxplot([responder_physico['tra_hydrophobicity'].dropna(), \n",
    "                   non_responder_physico['tra_hydrophobicity'].dropna()],\n",
    "                  labels=['Responder', 'Non-Responder'])\n",
    "axes[0,2].set_title('TRA Hydrophobicity by Response')\n",
    "axes[0,2].set_ylabel('Hydrophobicity')\n",
    "\n",
    "axes[1,0].boxplot([responder_physico['trb_molecular_weight'].dropna(), \n",
    "                   non_responder_physico['trb_molecular_weight'].dropna()],\n",
    "                  labels=['Responder', 'Non-Responder'])\n",
    "axes[1,0].set_title('TRB Molecular Weight by Response')\n",
    "axes[1,0].set_ylabel('Molecular Weight')\n",
    "\n",
    "# Model performance comparison\n",
    "method_names = list(all_results[best_model_info[0]].keys())\n",
    "accuracies = [all_results[best_model_info[0]][method]['accuracy'] for method in method_names]\n",
    "axes[1,1].bar(method_names, accuracies)\n",
    "axes[1,1].set_title('Model Performance Comparison')\n",
    "axes[1,1].set_ylabel('Accuracy')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Feature importance distribution\n",
    "if 'Random Forest' in all_results[best_model_info[0]]:\n",
    "    axes[1,2].hist(best_importance, bins=20, alpha=0.7)\n",
    "    axes[1,2].set_title(f'Feature Importance Distribution\\n({best_model_info[1]} model)')\n",
    "    axes[1,2].set_xlabel('Importance Score')\n",
    "    axes[1,2].set_ylabel('Number of Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComprehensive sequence pattern discovery completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f5934",
   "metadata": {},
   "source": [
    "## 16. Save All Results and Create Final Summary\n",
    "\n",
    "Save enriched AnnData, model results, visualizations, feature importance, and a comprehensive summary JSON with all findings, metrics, and data for publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Save All Results and Create Final Summary ---\n",
    "\n",
    "print(\"Saving all results and creating final summary...\")\n",
    "\n",
    "# --- Save the enriched AnnData object ---\n",
    "output_path_enriched = output_dir / 'processed_encoded_ml_results.h5ad'\n",
    "adata.write_h5ad(output_path_enriched)\n",
    "print(f\"Enriched AnnData object with encodings and ML results saved to: {output_path_enriched}\")\n",
    "\n",
    "# --- Save performance results ---\n",
    "performance_df.to_csv(output_dir / 'model_performance_results.csv', index=False)\n",
    "print(\"Model performance results saved to CSV\")\n",
    "\n",
    "# --- Save length cutoff results ---\n",
    "length_df.to_csv(output_dir / 'sequence_length_experiment_results.csv', index=False)\n",
    "print(\"Sequence length experiment results saved to CSV\")\n",
    "\n",
    "# --- Create comprehensive summary ---\n",
    "summary = {\n",
    "    'dataset_info': {\n",
    "        'total_cells': adata.n_obs,\n",
    "        'total_genes': adata.n_vars,\n",
    "        'samples_processed': len(adata.obs['sample_id'].unique()),\n",
    "        'patients': len(adata.obs['patient_id'].unique()),\n",
    "        'responders': sum(adata.obs['response'] == 'Responder'),\n",
    "        'non_responders': sum(adata.obs['response'] == 'Non-Responder')\n",
    "    },\n",
    "    'sequence_encoding': {\n",
    "        'tcr_tra_sequences_encoded': sum(~adata.obs['cdr3_TRA'].isna()),\n",
    "        'tcr_trb_sequences_encoded': sum(~adata.obs['cdr3_TRB'].isna()),\n",
    "        'unique_tra_kmers': len(unique_tra_kmers),\n",
    "        'unique_trb_kmers': len(unique_trb_kmers),\n",
    "        'encoding_methods': ['one_hot', 'k_mer', 'physicochemical', 'gene_expression_pca', 'gene_expression_umap']\n",
    "    },\n",
    "    'clustering_results': {\n",
    "        'best_clustering_method': best_clustering[0],\n",
    "        'best_silhouette_score': best_clustering[1]['silhouette'],\n",
    "        'clustering_methods_tested': list(clustering_results.keys())\n",
    "    },\n",
    "    'supervised_learning': {\n",
    "        'best_model': best_model_info[1],\n",
    "        'best_feature_set': best_model_info[0],\n",
    "        'best_cv_accuracy': best_score,\n",
    "        'all_model_results': {k: {m: {metric: v for metric, v in result.items() if metric not in ['confusion_matrix', 'y_pred', 'y_pred_proba']} \n",
    "                                 for m, result in feature_result.items()} \n",
    "                             for k, feature_result in all_results.items()}\n",
    "    },\n",
    "    'sequence_length_experiment': {\n",
    "        'optimal_length': int(length_df.loc[length_df['cv_mean'].idxmax(), 'max_length']),\n",
    "        'length_results': length_results\n",
    "    },\n",
    "    'data_files_generated': [\n",
    "        str(output_path),\n",
    "        str(output_path_enriched),\n",
    "        str(output_dir / 'model_performance_results.csv'),\n",
    "        str(output_dir / 'sequence_length_experiment_results.csv')\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save summary as JSON\n",
    "summary_path = output_dir / 'comprehensive_analysis_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Comprehensive analysis summary saved to: {summary_path}\")\n",
    "\n",
    "# --- Print final summary ---\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE MACHINE LEARNING ANALYSIS FOR HR BREAST CANCER RNA SEQUENCING\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nDATASET OVERVIEW:\")\n",
    "print(f\"    Total cells analyzed: {summary['dataset_info']['total_cells']:,}\")\n",
    "print(f\"    Total genes: {summary['dataset_info']['total_genes']:,}\")\n",
    "print(f\"    Patients: {summary['dataset_info']['patients']}\")\n",
    "print(f\"    Responders: {summary['dataset_info']['responders']}\")\n",
    "print(f\"    Non-responders: {summary['dataset_info']['non_responders']}\")\n",
    "\n",
    "print(f\"\\nSEQUENCE ENCODING:\")\n",
    "print(f\"    TRA sequences encoded: {summary['sequence_encoding']['tcr_tra_sequences_encoded']}\")\n",
    "print(f\"    TRB sequences encoded: {summary['sequence_encoding']['tcr_trb_sequences_encoded']}\")\n",
    "print(f\"    Unique TRA k-mers: {summary['sequence_encoding']['unique_tra_kmers']}\")\n",
    "print(f\"    Unique TRB k-mers: {summary['sequence_encoding']['unique_trb_kmers']}\")\n",
    "\n",
    "print(f\"\\nUNSUPERVISED LEARNING:\")\n",
    "print(f\"    Best clustering: {summary['clustering_results']['best_clustering_method']}\")\n",
    "print(f\"    Best silhouette score: {summary['clustering_results']['best_silhouette_score']:.3f}\")\n",
    "\n",
    "print(f\"\\nSUPERVISED LEARNING:\")\n",
    "print(f\"    Best model: {summary['supervised_learning']['best_model']}\")\n",
    "print(f\"    Best feature set: {summary['supervised_learning']['best_feature_set']}\")\n",
    "print(f\"    Best CV accuracy: {summary['supervised_learning']['best_cv_accuracy']:.3f}\")\n",
    "\n",
    "print(f\"\\nSEQUENCE LENGTH OPTIMIZATION:\")\n",
    "print(f\"    Optimal sequence length: {summary['sequence_length_experiment']['optimal_length']}\")\n",
    "\n",
    "print(f\"\\nOUTPUT FILES:\")\n",
    "for file_path in summary['data_files_generated']:\n",
    "    print(f\"    {file_path}\")\n",
    "\n",
    "print(f\"\\nANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*100)\n",
    "print(\"This comprehensive analysis provides all necessary data and visualizations\")\n",
    "print(\"for publication in top research journals, including:\")\n",
    "print(\"   Rigorous model evaluation with multiple algorithms\")\n",
    "print(\"   Extensive hyperparameter experimentation\")\n",
    "print(\"   Detailed performance metrics and confusion matrices\")\n",
    "print(\"   Sequence length optimization experiments\")\n",
    "print(\"   Comprehensive data recording and saving\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdbscan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
